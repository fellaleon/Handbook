\chapter{Statistical Tests}

\section{Basic Tests}

\subsection{$T$-test}

\subsection{$F$-test}

\section{Analysis of Variance}

\subsection{Kruskal-Wallis test}

\section{Distribution Tests}

\subsection{Kolmogorov-Smirnov test}

Kolmogorov-Smirnov test, based on Kolmogorov-Smirnov limit theorem, is applied to check the difference between the empirical distribution of a large sample and the corresponding theoretical distribution or the difference
between the distributions of two large samples.\footnote{details needed.}


Given $D_N = \underset{x}{\sup}\abs{F_N(x) - F(x)}$ with
\be
\pro\brb{D_N \leq D_{N,\alpha}} = 1 -\alpha
\ee

We can have as $N\to \infty$,
\be
\alpha = 2\sum^\infty_{k=1} (-1)^{k-1}\exp\brb{-2k^2 D_{N,\alpha}^2},
\ee

\begin{center}
\begin{longtable}{c|ccccc}
\caption{values of $D_{N,\alpha}$} \\ %\label{tab:long} \\
\hline%[width=5em,trim=l]
\diagbox{$N$}{$D_{N,\alpha}$}{$\alpha$} & 0.01 & 0.05 & 0.1 & 0.15 &  0.2 \\
\hline
1 & 0.995 & 0.975 & 0.950 & 0.925 & 0.900 \\
2 & 0.929 & 0.842 & 0.776 & 0.726 & 0.684 \\
3 & 0.828 & 0.708 & 0.642 & 0.597 & 0.565 \\
4 & 0.733 & 0.624 & 0.564 & 0.525 & 0.494 \\
5 & 0.669 & 0.565 & 0.510 & 0.474 & 0.446 \\
6 & 0.618 & 0.521 & 0.470 & 0.436 & 0.410 \\
7 & 0.577 & 0.486 & 0.438 & 0.405 & 0.381 \\
8 & 0.543 & 0.457 & 0.411 & 0.381 & 0.358 \\
9 & 0.514 & 0.432 & 0.388 & 0.360 & 0.339 \\
10 & 0.490 & 0.410 & 0.368 & 0.342 & 0.322 \\
11 & 0.468 & 0.391 & 0.352 & 0.326 & 0.307 \\
12 & 0.450 & 0.375 & 0.338 & 0.313 & 0.295 \\
13 & 0.433 & 0.361 & 0.325 & 0.302 & 0.284 \\
14 & 0.418 & 0.349 & 0.314 & 0.292 & 0.274 \\
15 & 0.404 & 0.349 & 0.314 & 0.283 & 0.266 \\
16 & 0.392 & 0.328 & 0.295 & 0.274 & 0.258 \\
17 & 0.381 & 0.318 & 0.286 & 0.266 & 0.250 \\
18 & 0.371 & 0.309 & 0.278 & 0.259 & 0.244 \\
19 & 0.363 & 0.301 & 0.272 & 0.252 & 0.237 \\
20 & 0.356 & 0.294 & 0.264 & 0.246 & 0.231 \\
25 & 0.320 & 0.270 & 0.240 & 0.220 & 0.210 \\
30 & 0.290 & 0.240 & 0.220 & 0.200 & 0.190 \\
35 & 0.270 & 0.230 & 0.210 & 0.190 & 0.180 \\
40 & 0.250 & 0.210 & 0.190 & 0.180 & 0.170 \\
45 & 0.240 & 0.200 & 0.180 & 0.170 & 0.160 \\
50 & 0.230 & 0.190 & 0.170 & 0.160 & 0.150 \\
over 50 & $1.628/\sqrt{N}$  & $1.358/\sqrt{N}$ & $1.224/\sqrt{N}$ & $1.138/\sqrt{N}$ &  $1.073/\sqrt{N}$ \\
\hline
\end{longtable}
\end{center}

\section{Unit Root test}

In statistics, a unit root test tests whether a time series variable is non-stationary using an autoregressive model.

\subsection{Augmented Dickey-Fuller test}

\footnote{citation needed. Dickey, D. A.; Fuller, W. A. (1979). "Distribution of the Estimators for Autoregressive Time Series with a Unit Root". Journal of the American Statistical Association 74 (366a): 427–431.}

\subsection{Optimal finite sample test}

This unit root test in autoregressive models were developed by Denis Sargan and Alok Bhargava\footnote{Sargan, J. D.; Bhargava, Alok (1983). "Testing Residuals from Least Squares Regression for Being Generated by the Gaussian Random Walk". Econometrica 51 (1): 153–174.

Bhargava, A. (1986). "On the Theory of Testing for Unit Roots in Observed Time Series". The Review of Economic Studies 53 (3): 369–384. doi:10.2307/2297634. JSTOR 2297634. edit}.

\subsection{Phillips-Perron test}



\section{Cointegration Tests}

\subsection{Engle-Granger cointegration test}

\subsection{Johansen cointegration test}

trace test / maximum eigenvalue test

\section{Test for Detecting Non-randomness}

%\section{Randomness Tests}

\subsection{Wald-Wolfowitz runs test}

The runs test (also called Wald-Wolfowitz test after Abraham Wald and Jacob Wolfowitz\cite{Wald_Wolfowitz_1940}) is a non-parametric statistical test that checks a randomness hypothesis for a two-valued data sequence (or decide if a data set is from a random process). More precisely, it can be used to test the hypothesis that the elements of the sequence are mutually independent.

Let $X$ and $Y$ be two independent stochastic variables about whose cumulative distribution functions nothing is known except that they are continuous. Let $x_1,x_2,\dots,x_m$ be a set of $m$ independent observations on $X$ and let $y_1,y_2,\dots,y_n$ be a set of $n$ independent observations on $Y$. It is desired to test the hypothesis (the null hypothesis) that the distribution functions $X$ and $Y$ (say, $f(x)$ and $g(y)$) are identical.

Let the set of $m+n$ elements $x_1,\dots,x_m$ and $y_1,\dots,y_n$ be arranged in ascending order of magnitude, and let the sequence be designated by $Z$, thus $Z= z_1,\dots,z_{m+n}$, where $z_1<z_2<\dots<z_{m+n}$.  Since $f(x)$ and $g(y)$ are assumed to be continuous, the probability is 0 that $z_i = z_{i+1}$.

\begin{definition}[run of random sequence]\label{def:run_random_sequence}
We define a sequence $V$ such that $v_i = 0$ if $z_i$ is a member of the set $x_1,\dots,x_m$ and $v_i = 1$ if $z_i$ is a member of the set $y_1,\dots,y_n$. A subsequence $v_{s+1},v_{s+2},\dots,v_{s+r}$ of $V$ (where $r$ may also be 1) will be callded a run\index{run!random sequence} if $v_{s+1} = v_{s+2}= \dots = v_{s+r}$ and if $v_s\neq v_{s+1}$ when $s>0$ and $v_{s+r}\neq v_{s+r+1}$ when $s+r < m+n$.

We use $U$ to define the number of runs in $V$.
\end{definition}

\begin{example}
For the sequence $1,0,0,1,1,0$, we have $m=3,n=3$ and $u=4$.
\end{example}

\begin{remark}
Note that a difference between $f(x)$ and $g(y)$ tends to decrease $U$. This is because that the discrimination of $X$ and $Y$ values will form the clusterings of 0 and 1 and therefore decrease the number of runs.
\end{remark}

\begin{theorem}\label{thm:run_probability}
Assume that $X$ and $Y$ have the same distriubtion and give the sequence $V$ and random variable $U$ by Definition \ref{def:run_random_sequence}, then we have that
\be
\pro\brb{U=2k} = \frac{2C^{k-1}_{m-1}C^{k-1}_{n-1}}{C^m_{m+n}},\qquad \pro\brb{U=2k+1} = \frac{\brb{C^{k}_{m-1}C^{k-1}_{n-1}+ C^{k-1}_{m-1}C^{k}_{n-1}}}{C^m_{m+n}}
\ee
for $k = 1,2,\dots,\min\bra{m,n}$ with $C^a_b =0$ if $a>b$.
\end{theorem}

\begin{proof}[\bf Proof]
Without loss of generality, we may assume that $m\leq n$.

If $U = 2k$, we can have that there are $k$ 0's run and $k$ 1's run. Thus, there are two possible cases that the $v_1$ is either 0 or 1.  Then we need to split $m$ 0s and $n$ 1s into $k$ sessions with $k-1$ cuts. Therefore, the total number of this case is $2C^{k-1}_{m-1}C^{k-1}_{n-1}$.

If $U = 2k+1$, we can have that there are either $k+1$ and $k$ runs of 0 and 1 or  $k$ and $k+1$ runs of 0 and 1 as the runs of 0,1 take place one after the other. Using the same arguement, we have that if $v_1=0$ the number of possibilities is $C^k_{m-1}C^{k-1}_{n-1}$. According, the number is $C^{k-1}_{m-1}C^{k}_{n-1}$ if $v_1 =1$.
\end{proof}

\begin{theorem}
Assume that $X$ and $Y$ have the same distriubtion and give the sequence $V$ and random variable $U$ by Definition \ref{def:run_random_sequence}, then we have that
\be
\E U = \frac{2mn}{m+n}+1,\qquad \var U = \frac{2mn(2mn - m - n)}{(m+n)^2(m+n-1)}.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
By Theorem \ref{thm:run_probability}, we have that
\be
\E U = \sum^m_{k=1} 2k \frac{2C^{k-1}_{m-1}C^{k-1}_{n-1}}{C^m_{m+n}} + \sum^m_{k=1} (2k+1)\frac{\brb{C^{k}_{m-1}C^{k-1}_{n-1}+ C^{k-1}_{m-1}C^{k}_{n-1}}}{C^m_{m+n}}.
\ee

Consider
\be
(1+a)^{m-2}a\brb{1+\frac 1a}^{n-1}\qquad (*)
\ee
and apply Definition \ref{def:binomial_series} to expand it we have
\be
\sum^{m-2}_{k=0} \sum^{n-1}_{l=0}C^k_{m-2}C^l_{n-1}a^k a a^{-l}
\ee
with constant term
\be
\sum^{m-2}_{k=0}C^k_{m-2}C^{k+1}_{n-1} = \sum^{m-1}_{k=1}C^{k-1}_{m-2}C^{k}_{n-1} = \frac 1{m-1}\sum^{m-1}_{k=1}k C^{k}_{m-1}C^{k}_{n-1}= \frac1{m-1}\sum^{m}_{k=1}k C^{k}_{m-1}C^{k}_{n-1}.
\ee

For ($*$), we have it is actually $(1+a)^{m+n-3}a^{2-n}$ with constant term $C^{n-2}_{m+n-3}$. Therefore, we have
\be
\sum^{m-1}_{k=0}k C^{k}_{m-1}C^{k}_{n-1}  = \sum^{m}_{k=1}k C^{k}_{m-1}C^{k}_{n-1} = (m-1)C^{n-2}_{m+n-3}.
\ee

Similarly, we consider
\be
(1+a)^{m-1}\brb{1+\frac 1a}^{n-1}\qquad (\dag)
\ee
and apply Definition \ref{def:binomial_series} to expand it we have
\be
\sum^{m-1}_{k=0} \sum^{n-1}_{l=0}C^k_{m-1}C^l_{n-1}a^k a^{-l}
\ee
with constant term
\be
\sum^{m-1}_{k=0}C^k_{m-1}C^{k}_{n-1}.
\ee

For ($\dag$), we have it is actually $(1+a)^{m+n-2}a^{1-n}$ with constant term $C^{n-1}_{m+n-2}$. Therefore,
\be
\sum^{m-1}_{k=0}C^k_{m-1}C^{k}_{n-1} = C^{n-1}_{m+n-2}.
\ee

Hence,
\beast
\sum^m_{k=1} k C^{k-1}_{m-1}C^{k-1}_{n-1} & = & \sum^m_{k=1} (k-1) C^{k-1}_{m-1}C^{k-1}_{n-1} + \sum^m_{k=1} C^{k-1}_{m-1}C^{k-1}_{n-1}  \\
& = & \sum^{m-1}_{k=0} k C^{k}_{m-1}C^{k}_{n-1} + \sum^{m-1}_{k=0} k C^{k}_{m-1}C^{k}_{n-1} \\
& = & (m-1)C^{n-2}_{m+n-3} + C^{n-1}_{m+n-2}.
\eeast

Using the same argument as above we have that
\be
\sum^m_{k=1}(2k+1)C^k_{m-1}C^{k-1}_{n-1} = 2(m-1)C^{n-1}_{m+n-3} + C^n_{m+n-2},
\ee

\be
\sum^m_{k=1}(2k+1)C^{k-1}_{m-1}C^{k}_{n-1} = 2(n-1)C^{n-2}_{m+n-3} + C^{n-2}_{m+n-2},
\ee

Therefore,
\beast
\E U & = & \frac 1{C^m_{m+n}}\brb{4(m-1)C^{n-2}_{m+n-3} + 4C^{n-1}_{m+n-2} + 2(m-1)C^{n-1}_{m+n-3} + C^n_{m+n-2} + 2(n-1)C^{n-2}_{m+n-3} + C^{n-2}_{m+n-2}} \\
& = & \frac 1{C^m_{m+n}}\frac{(m+n-3)!}{(m-2)!(n-2)!} \brb{4 + \frac{2(m-1)}{n-1} + \frac{2(n-1)}{m-1}}  \\
& & \qquad +  \frac 1{C^m_{m+n}}\frac{(m+n-2)!}{(m-2)!(n-2)!}\brb{  \frac{4}{(m-1)(n-1)} + \frac 1{n(n-1)} + \frac 1{m(m-1)}} \\
& = & \frac 1{C^m_{m+n}}\frac{(m+n-2)!}{(m-2)!(n-2)!} \brb{ \frac {2(m+n-2)}{(m-1)(n-1)} + \frac{4}{(m-1)(n-1)} + \frac 1{n(n-1)} + \frac 1{m(m-1)}} \\
& = & \frac 1{(m+n)(m+n-1)}\brb{2mn(m+n) + m(m-1) + n(n-1)} = \frac {2mn}{m+n} + 1.
\eeast

Similarly, we have the required result\footnote{This can be checked by Mathematica programme 20150403\_wald\_wolfowitz\_runs\_test.nb} for $\var U$.
\end{proof}

\begin{remark}
We can also get the higher order moment\footnote{See maths tool profile, Mathematica programme 20150403\_wald\_wolfowitz\_runs\_test.nb}. The skewness of $U$ is
\be
-\frac{(m-n)^2(4mn - 3m-3n)\sqrt{m+n-1}}{(m+n-2)\sqrt{2mn(2mn-m-n)}}
\ee
which means that usually the distribution is negative skewed. Also, if $m=n$, the distribution is symmetric as skewness is 0.

If we check the excess kurtosis of $U$, we can find that it converges to normal distribution quickly. For example, $m=30$ and $n=20$, we can have the value -0.0541 which is very closed to 0. Thus, we can use the normal distribution parameter to do the test.
\end{remark}

We know discrimination of $f(x)$ and $g(y)$ will cause the decrease of $U$, thus we can have the following test.% for negative (single) side.

\begin{proposition}[Wald-Wolfowitz runs test\index{Wald-Wolfowitz runs test}]\label{pro:wald_wolfowitz_runs_test}
Assume that $X$ and $Y$ have the same distriubtion and give the sequence $V$ and random variable $U$ by Definition \ref{def:run_random_sequence}. With the observation $m,n,u$, we can check the value
\be
z = \frac{u - \E U}{\sqrt{\var U}}.
\ee

Then runs test rejects the null hypothesis if
\be
\abs{z} > Z_{1-\alpha/2}
\ee
where $Z$ is $(0,1)$ standard normal random variable and $Z_{1-\alpha/2}$ is the value of $Z$ corresponding to confidence level of $1-\alpha/2$.

%which should be similar to a standard normal distributed random variable when $n_1,n_2\to \infty$ if 0 and 1 have the same distribution .

For a large-sample runs test (where $m > 10$ and $n > 10$), the test statistic is compared to a standard normal table (as it converges to normal distribution as $m,n\to \infty$\footnote{See \cite{Wald_Wolfowitz_1940}}). That is, at the $\alpha = 5\%$ significance level, a test statistic with an absolute value greater than 1.96 indicates non-randomness (see Table \ref{tab:guassian_distribution_confidence_level}).

For a small-sample runs test, there are tables to determine critical values that depend on values of $m$ and $n$\footnote{details needed} (Mendenhall, 1982, \cite{Mendenhall_Reinmuth_1982}).\footnote{We can use $z = \frac{\abs{u - \E U}-0.5}{\sqrt{\var U}}$ in Aldridge's high-frequency trading, \cite{Aldridge_2013}.}%If $z$ value is smaller than -1.64 , we can reject the null hypothesis and conclude that $X$ and $Y$ have different distribution.
\end{proposition}

\begin{example}
Assume $V$ is $0,0,1,1,0,1,1,1,0,0,0,1,1,1,1,0,0,0,0,1,1,0$ with $m=n=11$ and number of runs is $u = 9$. We can have that
\be
\E U = \frac{2\times 121}{22} + 1 = 12,\quad \var U = \frac{242 (242 - 22)}{22^2\times 21} = \frac{110}{21}.
\ee

Then
\be
Z = \frac{-3}{\sqrt{\frac {110}{21}}} = -1.3108
\ee
which implies that we can not reject the null hypothesis and we have to say 0,1 sequence have enough randomness.
\end{example}

%\begin{remark}
%In some paper (when ), $z$ is calculate by
%\end{remark}






