\chapter{Matrix}

%\section{Matrix}

\section{Basic Definitions and Properties}

\subsection{Definitions}

\begin{definition}[matrix\index{matrix}]\label{def:matrix}
An $m\times n$ matrix over $\F$ is an array $A = (a_{ij})$ with $m$ rows and $n$ columns and $a_{ij} \in \F$ for $1 \leq  i \leq  m$, $1 \leq  j \leq  n$. Write $M_{m,n}(\F)$ for the set of all $m \times n$ matrices over $\F$. If $m=n$, we can simply write $M_n(\F)$.
\end{definition}

\begin{example}
\be
A = \bepm 1 & 0 \\ 0 & 1 \eepm,\quad B = \bepm 0 & 1\\ 1 & 0 \eepm.
\ee
\end{example}

\begin{proposition}\label{pro:matrix_dimension}
$M_{m,n}(\F)$ is a vector space under operations 
\beast
(a_{ij}) + (b_{ij}) & = & (a_{ij} + b_{ij})\\
\lm(a_{ij}) & = & (\lm a_{ij})
\eeast
with $\dim_\F M_{m,n}(\F) = mn$.
\end{proposition}

\begin{proof}[\bf Proof]
Obviously, $M_{m,n}(\F)$ is a vector space by Definition \ref{def:vector_space}. 

We now prove the dimension claim. For $1 \leq  i \leq  m$, $1 \leq  j \leq  n$ define
\be
E_{ij} = \left\{\ba{ll}
e_{i'j'} = 1 & (i', j') = (i, j) \\
e_{i'j'} = 0 \quad\quad & (i', j') \neq (i, j).
\ea\right.
\ee

This is a natural basis.
\end{proof}

\begin{definition}
We say two matrics $A = (a_{ij})$ ,$B = (b_{ij})$ are same, $A =B$ if 
\be
a_{ij} = b_{ij},\ \forall i,j.
\ee
\end{definition}

\subsection{Multiplication of matrices}

\begin{proposition}[associativity of multiplication of matrices]\label{pro:associativity_multiplication_matrix}
Let $A\in M_{m,n}(\F)$, $B \in M_{n,p}(\F)$ and $C \in M_{p,q}(\F)$. Then
\be
A(BC) = (AB)C.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
We have
\beast
\bb{A(BC)}_{ij} & = & \sum_k a_{ik}(BC)_{kj} = \sum_k \sum_l a_{ik} b_{kl}c_{lj}\\
\bb{(AB)C}_{ij} & = & \sum_k (AB)_{ik}C_{kj} = \sum_k \sum_l a_{il} b_{lk}c_{kj}
\eeast

Then we have the required result by switching $k$ and $l$.
\end{proof}

\begin{remark}
Note that multiplication of matrices are not commutative, i.e. $AB \neq BA$.
\end{remark}

\begin{example}
\be
\bepm 1 & 0 \\ 0 & 2 \eepm \bepm 1 & 2 \\ 3 & 4 \eepm = \bepm 1 & 2 \\ 6 & 8\eepm \neq \bepm 1 & 4 \\ 3 & 8 \eepm =  \bepm 1 & 2 \\ 3 & 4 \eepm\bepm 1 & 0 \\ 0 & 2 \eepm.
\ee
\end{example}

\subsection{Transpose}

\begin{definition}[transpose\index{transpose!matrix}]\label{def:transpose_matrix}
If $A \in M_{m,n}(\F)$ with $A = (a_{ij})$, then its transpose $A^T$ is definite to be a $n\times m$ matrix with
\be
A^T = (a_{ji}).
\ee
\end{definition}

%\begin{remark}\end{remark}

\begin{example}
\ben
\item [(i)]
\be
\bepm 1 & 2\\ 3 & 4 \\ 5 & 6 \eepm^T = \bepm 1 & 3 & 5\\ 2 & 4 & 6 \eepm.
\ee
\item [(ii)] If $x = \bepm x_1\\ x_2 \\ \vdots \\ x_n \eepm$ is column vector, $x^T = \bepm x_1 & x_2 & \dots & x_n \eepm$ is a row vector.
\een
\end{example}

\begin{proposition}\label{pro:matrix_multiple_transpose}
Let $A \in M_{m,n}(\F)$ and $B \in M_{n,l}(\F)$. Then 
\be
\bb{A^T}^T = A,\qquad (AB)^T = B^T A^T.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
$\bb{A^T}^T = A$ is obvious. Let $A = (a_{ij})$ and $B = (b_{ij})$. Then $A^T = (a'_{ij}) = (a_{ji}) $ and $B^T = (b'_{ij}) = (b_{ji})$. Then
\be
\bb{(AB)^T}_{ij} = (AB)_{ji} = \sum_k a_{jk}b_{ki} = \sum_k b_{ki}a_{jk} =\sum_k b'_{ik}a'_{kj} = (B^TA^T)_{ij}. 
\ee

Thus, we have the required result.
\end{proof}


\begin{proposition}\label{pro:matrix_multiple_hermitian}
Let $A \in M_{m,n}(\F)$ and $B \in M_{n,l}(\F)$ ($A$ and $B$ are completx matrices).
\be
\bb{A^\dag}^\dag = A,\qquad (AB)^\dag = B^\dag A^\dag.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Similar arguement with Proposition \ref{pro:matrix_multiple_transpose}.
\end{proof}


\subsection{Identity matrix}

\begin{definition}[identity matrix\index{identity matrix}]\label{def:identity_matrix}
The identity $n\times n$ matrix is defined to be 
\be
I = \bepm 1 & 0 & \dots & 0\\ 0 & 1 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & 1 \eepm,
\ee
i.e., all the elements are 0 except for the diagonal elements that are 1.
\end{definition}

\begin{example}
The $3\times 3$ identity matrix is given by
\be
I = \bepm 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\eepm = \bb{\delta_{ij}}
\ee
where $\delta_{ij}$ is the Kronecker delta.
\end{example}

\begin{proposition}\label{pro:identity_matrix_property}
Let $A,I \in M_n(\F)$ and $I$ is an identity matrix. Then $AI = IA = A$.
\end{proposition}

\begin{proof}[\bf Proof]
We have 
\be
(IA)_{ij} = \sum_k \delta_{ik} a_{kj} = a_{ij} = \sum_k a_{ik}\delta_{kj} = (AI)_{ij},
\ee
i.e., $IA = AI = A$.
\end{proof}

\subsection{Inverible matrices}

\begin{definition}[left inverse\index{left inverse!matrix}, right inverse\index{right inverse!matrix}]\label{def:inverse_left}
Let $A\in M_n(\F)$. Then $B$ is a left inverse of $A$ if $BA =I$ and $C$ is a right inverse of $A$ if $AC = I$.
\end{definition}

\begin{proposition}[inverse\index{inverse!matrix}, invertible matrix\index{invertible!matrix}]\label{pro:inverse_matrix}
Let $A\in M_n(\F)$. If $B$ is a left inverse of $A$ and $C$ is a right inverse of $A$ then $B=C$ and we write $B=C = A^{-1}$. Then we say that $A$ is invertible (or non-singular\index{non-singular!matrix}). Note that $A^{-1}$ is unique.
\end{proposition}

\begin{proof}[\bf Proof]
By definition of left and right inverse, we have $BA =I$ and $AC = I$. Then
\be
B = BI = B(AC) = (BA)C = IC = C.
\ee
by Proposition \ref{pro:identity_matrix_property}, \ref{pro:associativity_multiplication_matrix}.

If $B$ and $C$ are both inverses of $A$, then $B = BI = BAC = IC = C$. Hence, $A^{-1}$ is unique.
\end{proof}

\begin{remark}
This property is based on the premise that both a left inverse and right inverse exist. In general the existence of a left inverse does not necessarily guarantee the existence of a right inverse or vice versa. However, in the case of a square matrix, the existence of a left inverse does imply the existence of a right inverse, and vice versa. 

The above property then implies that they are the same matrix.
\end{remark}

\begin{proposition}\label{pro:inverse_matrix_property}
If $A,B\in M_n(\F)$ are invertible, then
\be
\text{(i) }\bb{A^{-1}}^{-1} = A,\qquad \text{(ii) }(AB)^{-1} = B^{-1}A^{-1}. 
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Let $C = A^{-1}$. Then
\be
\ba{l}
C A = I = CC^{-1} \\
AC = I = C^{-1}C
\ea \quad \ra \quad A = \bb{A^{-1}}^{-1}.
\ee

Also, we have
\beast
B^{-1}A^{-1}(AB) & = & B^{-1}\bb{A^{-1}A}B = B^{-1} I B = B^{-1} B = I,\\
(AB)B^{-1}A^{-1} & = & A\bb{BB^{-1}}A^{-1} = AIA^{-1} = AA^{-1} = I
\eeast
by Proposition \ref{pro:identity_matrix_property}, \ref{pro:inverse_matrix}, \ref{pro:associativity_multiplication_matrix}.
\end{proof}




%\section{Ranks, Determinants and Adjugate Matrices}

\subsection{Equivalent matrices}

\begin{definition}[equivalent matrices\index{equivalent!matrix}]\label{def:equivalence_matrix}
The $m\times n$ matrices $A,A' \in M_{m,n}(\F)$ are equivalent if there exist invertible matrices $Q \in M_{m,m}(\F)$, $P \in M_{n,n}(\F)$ such that $A' = QAP$. This defines an equivalence relation on $M_{m,n}(\F)$.
\end{definition}

\begin{remark}
Equivalent matrices arise as representing the same linear map from a space $U$ to a space $V$ of dimension $m$ and $n$ with respect to different bases.
\end{remark}


\begin{lemma}\label{lem:matrix_equivalent_to_identity}
Any $m \times n$ matrix is equivalent to $\bepm I_r & 0\\ 0 & 0\eepm$ for some $r$.

\end{lemma}

\begin{proof}[\bf Proof]
Let $A \in M_{m,n}(\F)$. Define $\alpha : \F^n \to \F^m$, $x \mapsto Ax$. With respect to the standard bases of $\F^n$, $\F^m$, $\alpha$ has matrix $A$. By Lemma \ref{lem:linear_map_equivalent_to_identity} and Lemma \ref{lem:change_of_matrix_basis}, $A$ is equivalent to $\bepm I_r & 0\\ 0 & 0\eepm$ for some $r$.
\end{proof}


\subsection{Ranks}

\begin{definition}[column space\index{column space!matrix}]\label{def:column_space_matrix}
The column space (sometimes called the range) of a matrix is the set of all possible linear combinations of its column vectors. 
Let $A \in M_{m,n}(\F)$. The column space of $A$ is the subspace of $\F^m$ generated by the column vectors of $A$, denoted by $\colsp(A)$.
\end{definition}

\begin{definition}[column rank\index{rank!column, matrix}]\label{def:column_rank_matrix}
The (column) rank of $A$, denoted $\rank(A)$ is the dimension of the column space of $A$. %C(A) of a matrix (sometimes called the range of a matrix) is the set of all possible linear combinations of its column vectors. The column space of an m Ã— n matrix is a subspace of m-dimensional Euclidean space. The dimension of the column space is called the rank of the matrix.[1]
\end{definition}

\begin{proposition}\label{pro:equivalent_rank}
The matrices $A,A'\in M_{m,n}(\F)$ are equivalent if and only if $\rank(A) = \rank(A')$.
\end{proposition}

\begin{proof}[\bf Proof]
($\ra$). Assume $A,A'$ are equivalent such that $A' = Q^{-1}AP$ with $Q, P$ invertible. Let $\alpha : \F^n \to \F^m$, $x \mapsto Ax$. Then $\alpha$ with respect to standard bases $B,C$ has matrix $A$. Let $B'$ be the set of columns of $P$, let $C'$ be the set of columns of $Q'$. Then $[\alpha]_{B',C'} = Q^{-1}AP$ by Lemma \ref{lem:change_of_matrix_basis} since $P$ and $C$ are the change of basis matrices from $B$ to $B'$ and $C$ to $C'$, respectively. 

So $[\alpha]_{B',C'} = Q^{-1}AP =A'$ and $[\alpha]_{B,C} = A$. Then by Lemma \ref{lem:rank_linear_map_matrix}, $\rank(A') = \rank(\alpha) = \rank(A)$.

($\la$). We have that $A$ and $A'$ are equivalent to 
\be
\bepm I_r & 0\\ 0 & 0\eepm\quad \text{ and }\quad \bepm I_{r'} & 0\\ 0 & 0\eepm
\ee
for some $r$ and $r'$, respectively by Lemma \ref{lem:matrix_equivalent_to_identity}. By the first part, $\rank(A) = r$, $\rank(A') = r'$. Since $\rank(A) = \rank(A')$ we have $r = r'$ so $A$ and $A'$ are equivalent by transitivity of the equivalence relation.
\end{proof}


\begin{definition}[row space\index{row space!matrix}]\label{def:row_space_matrix}
The row space of a matrix is the set of all possible linear combinations of its row vectors. Let $A \in M_{m,n}(\F)$. The row space of $A$ is the subspace of $\F^n$ generated by the row vectors of $A$, denoted by $\rowsp(A)$.
\end{definition}


\begin{definition}[row rank\index{rank!row, matrix}]\label{def:row_rank_matrix}
The row rank of $A$, denoted $\rowrk(A)$ is the dimension of the column space of $A$. That is, $\rowrk(A) = \dim \rowsp(A) = \rank(A^T)$.
\end{definition}

\begin{theorem}\label{thm:rank_matrix_transpose}
For $A \in M_{m,n}(\F)$, $\rowrk(A) = \rank(A)$. That is,  $\rank(A^T) = \rank(A)$.
\end{theorem}

\begin{proof}[\bf Proof]
Let $A \in M_{m,n}(\F)$, let $r = \rank(A)$. Then $A$ is equivalent to $\bepm I_r & 0\\ 0 & 0 \eepm_{m\times n}$ by Lemma \ref{lem:matrix_equivalent_to_identity}, so $\bepm I_r & 0\\ 0 & 0\eepm_{m\times n} = QAP$ for some invertible matrices $Q, P$ by Definition \ref{def:equivalence_matrix}. Consider the transpose, by Proposition \ref{pro:matrix_multiple_transpose},
\be
P^TA^TQ^T = \bepm I_r & 0\\ 0 & 0\eepm_{n\times m} .
\ee

So $\spa(A^T)$ is equivalent to $\bepm I_r & 0\\ 0 & 0\eepm_{n\times m}$. Visibly, $\rank\bepm I_r & 0\\ 0 & 0\eepm_{n\times m} = r$, so 
\be
\rowrk(A) = \rank(A^T ) = \rank\bepm I_r & 0\\ 0 & 0\eepm_{n\times m} = r = \rank(A).
\ee
\end{proof}


%\begin{proposition}\label{pro:invertible_equivalent}
%Let $A\in M_n(\F)$. The following statements are equivalent:
%\ben
%\item [(i)] $A$ is invertible.
%    A is row-equivalent to the n-by-n identity matrix In.
%    A is column-equivalent to the n-by-n identity matrix In.
%    A has n pivot positions.
%    det A â‰  0. In general, a square matrix over a commutative ring is invertible if and only if its determinant is a unit in that ring.
%    rank A = n.
%    The equation Ax = 0 has only the trivial solution x = 0 (i.e., Null A = {0})
%   The equation Ax = b has exactly one solution for each b in Kn.
%\item [(ii)] The columns of $A$ are linearly independent.
%    The columns of A span Kn (i.e. Col A = Kn).
%\een
%\end{proposition}

%\begin{proof}[\bf Proof]
%\footnote{need proof}
%\end{proof}

\begin{theorem}\label{thm:invertible_full_rank}
If $A\in M_n(\F)$, then $A$ is invertible iff $\rank(A) = n$.%, i.e., the columns of $A$ are linearly independent.
\end{theorem}

\begin{proof}[\bf Proof]
($\ra$). If $A$ is invertible, then we have invertible matrix $P$ such that $AP = I_n$. This means $A$ is equivalent to $I_n$. Then by Proposition \ref{pro:equivalent_rank}, we have $\rank(A) =\rank(I_n) = n$.

($\la$). Approach 1. If $\rank(A)$, we have $A$ is equivalent to $I_n$ by Lemma \ref{lem:matrix_equivalent_to_identity}. Thus, there exist invertible matrices $P,Q$ such that $Q^{-1}AP = I_n$ by Definition \ref{def:equivalence_matrix}. Therefore, we have $A = QP^{-1}$ and $A^{-1} = PQ^{-1}$ by Proposition \ref{pro:inverse_matrix_property}. Thus, $A$ is invertible as 
\be
AA^{-1} = QP^{-1}PQ^{-1} = QQ^{-1} = I = PP^{-1} = PQ^{-1}QP^{-1} = A^{-1}A.
\ee

Approach 2. Since $\rank(A) =n$, we have that the corresponding linear map $\alpha$ is surjective. Then by Corollary \ref{cor:same_dimension_linear_map_equivalent}, we have that the linear map is bijective (since $\dim V = n=m = \dim W$). Thus, the corresponding inverse matrix $A^{-1}$ exists and therefore $A$ is invertible.
\end{proof}

\begin{lemma}\label{lem:full_rank_linearly_independent}
Let $A\in M_n(\F)$. Then $\rank(A) = n$ iff the columns of $A$ are linearly independent.
\end{lemma}

\begin{proof}[\bf Proof]
($\ra$). If $\rank(A) = n$, we have that the columns of $A$ are linearly independent by Lemma \ref{lem:finite_basis_linearly_independent_spanning_equivalent} as the columns span the column space.

($\la$). If the columns of $A$ are linearly independent, then by Lemma \ref{lem:independent_spanning_basis}, the columns of $A$ is a basis. Furthermore, by definition of dimension (Theorem \ref{thm:dimension_vector_space}), $\rank(A) = n$.
\end{proof}

\begin{corollary}\label{cor:invertible_column_linearly_independent}
If $A\in M_n(\F)$, then $A$ is invertible iff the columns of $A$ are linearly independent.
\end{corollary}

\begin{proof}[\bf Proof]
Direct result from Lemma \ref{thm:invertible_full_rank} and \ref{lem:full_rank_linearly_independent}.
\end{proof}

\begin{proposition}\label{pro:matrix_rank_properties}
Let $A\in M_{m,n}(\F)$. Then
\ben
\item [(i)] If $B\in M_{n,k}(\F)$, $\rank(AB) \leq \min\bra{\rank(A), \rank(B)}$.
\item [(ii)]
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] We know that $\colsp(AB)$ is generated by the column vectors of $AB$. Also, it is generated by the column vectors of $B$. Thus, $\colsp(AB)\subseteq \colsp(B)$. Then by the definition of rank (Definition \ref{def:column_rank_matrix}), we have $\rank(AB) \leq \rank(B)$.   %\footnote{prove $\rank(AB) \leq \rank(A)$ and then use rank of matrix transpose}.

Also, we have $\rank(B^TA^T) \leq \rank(A^T)$. Then by Theorem \ref{thm:rank_matrix_transpose} and Proposition \ref{pro:matrix_multiple_transpose},
\be
\rank(AB) = \rank\bb{(AB)^T} = \rank(B^TA^T) \leq \rank(A^T) = \rank(A) \ \ra \ \rank(AB) \leq \min\bra{\rank(A), \rank(B)}.
\ee

\item [(ii)] \footnote{need proof}
\een
\end{proof}

\subsection{Elementary operations}%Calculations}

\begin{definition}[elementary matrix\index{elementary matrix}]\label{def:elementary_matrix}
The following are the elementary column operations on an $m \times n$ matrix over $\F$.
\ben
\item [(i)] Swap columns $i$ and $j$ (switching).
\item [(ii)] Replace column $i$ by $\lm$ column $i$, where $\lm \in \F \bs \bra{0}$ (multiplication).
\item [(iii)] Add $\lm$ column $i$ to column $j$, where $i \neq j$ and $\lm \in \F$ (addition).
\een

The corresponding elementary column operation matrices are obtained by applying these operations to $I_n$. Call these $T_{i,j}$, $M_{i,\lm}$ and $C_{i,j,\lm}$. An elementary column operation on $A$ can be performed by postmultiplying $A$ with the corresponding elementary matrix. All these operations are reversible.

Similarly for elementary row operations. We write $E_i$ for elementary (column and row) matrices.

For square matrices, the elementary (column or row) matrices of switching and multiplication are same, given by
\be
T_{i,j} =\bepm 1 & & & & & & \\ & \ddots & & & & & \\ & & 0 & & 1 & & \\ & & & \ddots & & & \\ & & 1 & & 0 & & \\ & & & & & \ddots & \\ & & & & & & 1\eepm ,\quad M_{i,\lm} = \bepm 1 & & & & & & \\ & \ddots & & & & & \\ & & 1 & & & & \\ & & & \lm & & & \\ & & & & 1 & & \\ & & & & & \ddots & \\ & & & & & & 1\eepm.
\ee

The matrices of addition is %are
\be
C_{i,j,\lm} = \bepm 1 & & & & & & & \\ & \ddots & & & & & \\ & & 1 & & \lm & & \\ & & & \ddots & & & \\ & & & & 1 & & \\ & & & & & \ddots & \\ & & & & & & 1\eepm.%, \quad C_{i,j,\lm}^T = \bepm 1 & & & & & & & \\ & \ddots & & & & & \\ & & 1 & & & & \\ & & & \ddots & & & \\ & & \lm & & 1 & & \\ & & & & & \ddots & \\ & & & & & & 1\eepm.
\ee

$AC_{i,j,\lm}$ is adding $\lm$ times column $i$ to column $j$ of $A$ and $C_{i,j,\lm}A$ is adding $\lm$ times row $j$ to row $i$ of $A$.
\end{definition}


\begin{definition}[column echolon form]\label{def:column_echolon_form}
$A$ is an $m\times n$ matrix with the following properties is said to be in column echelon form.
\ben
\item [(i)] The highest placed non-zero entry in column $j$ is 1 in row $i_j$ , with $i_1 \leq  i_2 \leq  \dots$.
\item [(ii)] The entry in row $i_j$ and column $k$ with $k < j$ is 0.
\een
\end{definition}

\begin{example}
\be
%\bepm
%1 & 6 & 2 & 4\\
%0 & 1 & 3 & 5\\
%0 & 0 & 1 & 2
%\eepm.
\bepm
1 & 0 & 0 \\
6 & 1 & 0 \\
2 & 3 & 1 \\
4 & 5 & 2 
\eepm,\qquad \bepm
0 & 0 & 0 \\
1 & 0 & 0 \\
6 & 1 & 0 \\
2 & 3 & 1 
\eepm,\qquad \bepm
0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 6 & 1 & 0 \\
0 & 2 & 3 & 1 
\eepm
\ee
\end{example}


\begin{lemma}\label{lem:matrix_column_echelon_elementary_column}
Any matrix $A$ can be reduced to a matrix in column echelon form by a sequence of elementary column matrices.
\end{lemma}

\begin{remark}
If $A$ is a square $n\times n$ matrix and is invertible, the equivalent column echelon form is $I_n$. This can be used to find $A^{-1}$.
\be
A \mapsto AE_1E_2 \dots E_k = I_n,\quad I_n \mapsto I_nE_1E_2 \dots E_k = A^{-1}.
\ee
\end{remark}

\begin{proof}[\bf Proof]
\footnote{need proof.}
\end{proof}

\begin{proposition}\label{pro:square_elementary_matrix_invertible}
Let $E \in M_n(\F)$ be elementary matrix. Then it is invertible.
\end{proposition}

\begin{remark}
The square elementary matrices generate the general linear group of invertible matrices\footnote{need details}.
\end{remark}

\begin{proof}[\bf Proof]
It is easy to check that the inverse matrices are
\be
\bb{T_{i,j}}^{-1} = T_{i,j},\quad \bb{M_{i,\lm}}^{-1} = M_{i,1/\lm},\quad \bb{C_{i,j,\lm}}^{-1} = C_{i,j,-\lm}.
\ee
\end{proof}

\begin{lemma}\label{lem:invertible_product_elementary_matrices}
If $A$ is an invertible $n \times n$ matrix then $A$ is a product of elementary matrices.
\end{lemma}

\begin{proof}[\bf Proof]
By the remark of Lemma \ref{lem:matrix_column_echelon_elementary_column}, $A^{-1} = E_1 \dots E_k$ is a product of elementary matrices, hence so is $A = E^{-1}_k \dots E^{-1}_1$ by Proposition \ref{pro:inverse_matrix_property}.
\end{proof}


\section{Square Matrices}

\subsection{Basic square matrices} 

%, Hermitian matrices and symmetric and anti-symmetic matrices 

\begin{definition}[symmetric matrix\index{symmetric!matrix}]\label{def:symmetric_matrix}
A square $n\times n$ matrix $A = (a_{ij})$ is symmetric if
\be
A = A^T,\quad \text{i.e., } a_{ij} = a_{ji}. 
\ee
\end{definition}

\begin{definition}[anti-symmetric matrix\index{anti-symmetric!matrix}]\label{def:antisymmetric_matrix}
A square $n\times n$ matrix $A = (a_{ij})$ is anti-symmetric if
\be
A = -A^T,\quad \text{i.e., } a_{ij} = -a_{ji}. 
\ee
\end{definition}

\begin{remark}
For an anti-symmetric matrix, $a_{11} = -a_{11}$, i.e., $a_{11} = 0$. Similarly we deduce that all the diagonal elements of anti-symmetric matrices are zero, i.e., $a_{11} = a_{22} = \dots = a_{nn} = 0$.
\end{remark}

\begin{definition}[adjoint matrix\index{adjoint matrix}]\label{def:adjoint_matrix}
For $A = \bb{a_{ij}} \in M_{m,n}(\F)$, the adjoint matrix of $A$ is
\be
A^\dag = \bb{a'_{ij}} := \bb{\ol{A}}^T,\qquad \text{i.e.,}\  a'_{ij} = \ol{a_{ji}}.
\ee
\end{definition}

\begin{definition}[Hermitian matrix\index{Hermitian matrix}]\label{def:hermitian_matrix}
A square $n\times n$ complex matrix $A = (a_{ij})$ is Hermitian if
\be
A = A^\dag,\quad \text{i.e., } a_{ij} = \ol{a_{ji}}. 
\ee
\end{definition}

\begin{remark}
Hermitian matrix is also called self-adjoint matrix.
\end{remark}

\begin{definition}[skew-Hermitian matrix\index{skew-Hermitian matrix}]\label{def:skew_hermitian_matrix}
A square $n\times n$ complex matrix $A = (a_{ij})$ is skew-Hermitian if
\be
A = -A^\dag,\quad \text{i.e., } a_{ij} = -\ol{a_{ji}}. 
\ee
\end{definition}

\begin{remark}
All the diagonal elements of Hermitian and skew-Hermitian matrices are real and pure imaginary respectively.
\end{remark}

\begin{example}
\ben
\item [(i)] A symmetric $3\times 3$ matrix $S$ has the form
\be
S = \bepm a & b & c \\ b & d & e \\ c & e & f \eepm,
\ee
i.e., it has six independent elements.

\item [(ii)] An anti-symmetric $3\times 3$ matrix $A$ has the form
\be
A = \bepm 0 & a & -b \\ -a & 0 & c \\ b & -c & 0 \eepm,
\ee
i.e., it has three independent elements.
\een
\end{example}

%\subsection{Orthogonal and unitary matrices}

\begin{definition}[orthogonal matrix\index{orthogonal matrix}]\label{def:orthogonal_matrix}
$A\in M_n(\F)$ is orthogonal matrix if 
\be
AA^T = I = A^TA,
\ee
i.e., if $A$ is invertible and $A^{-1} = A^T$.
\end{definition}

\begin{remark}
Note that $\det A = \pm 1$ for orthogonal matrix $A$.
\end{remark}

\begin{definition}[unitary matrix\index{unitary matrix}]\label{def:unitary_matrix}
$A \in M_n(\C)$ is said to be unitary if its Hermitian conjugate is equal to its inverse, i.e., if
\be
U^\dag = U^{-1}.
\ee
\end{definition}

\begin{remark}
Unitary matrices are to complex matrices what orthogonal matrices are to real matrices. Similar properties to those above for orthogonal matrices also hold for unitary matrices when references to real scalar products are replaced by references to complex scalar products\footnote{need definition}.
\end{remark}

%\subsection{Trace}

\begin{definition}[trace\index{trace!matrix}]\label{def:trace_matrix}
The trace of a square $n\times n$ matrix $A = (a_{ij})$ ($A\in M_n(\F)$) is equal to the sum of the diagonal elements, i.e.,
\be
\tr A = \sum^n_{i=1}a_{ii}.
\ee
\end{definition}

\begin{remark}
Note that $\tr:M_n(\F)\to \F$ is linear.
\end{remark}

\begin{proposition}\label{pro:trace_change_order}
Suppose that $A\in M_{m,n}(\F)$ and $B \in M_{n,m}(\F)$. Then $\tr(AB) = \tr(BA)$.
\end{proposition}

\begin{proof}[\bf Proof]
We have
\be
\tr(AB) = \sum_i \bb{\sum_j a_{ij}b_{ji}} = \sum_j \bb{\sum_i b_{ji}a_{ij}} = \tr(BA). 
\ee
\end{proof}

\begin{definition}[commute matrices\index{commute!matrices}]\label{def:commute_matrices}
For matrices $A,B\in M_n(\F)$, we say that $A,B$ are commute if $AB = BA$. We can write this by 
\be
\bsb{A,B} = 0, \quad \text{ where}\quad \bsb{A,B} := AB - BA.
\ee
\end{definition}


\subsection{Determinant}

\begin{definition}[determinant\index{determinant!matrix}]\label{def:determinant_matrix}
For $A \in M_n(\F)$ define
\be
\det A =\sum_{\sigma \in S_n} \ve(\sigma)a_{\sigma(1)1} \dots a_{\sigma(n)n}.
\ee
where $\ve(\sigma)$ is sign of permutation (see Definition \ref{def:sign_permutation}) and it is homomorphism by Theorem \ref{thm:sgn_homomorphism}.

Writing $A_i$ for the $i$th column of a matrix $A$ we have $A = (A_1, \dots,A_n)$, so we can think of $A$ is an $n$-tuple of columns in $\F^n$. Write $\bra{e_1, \dots, e_n}$ for the standard basis of $\F^n$.
\end{definition}

%Recall that $S_n$ is the group of all permutations of $\bra{1, \dots, n}$. The elements are permutations, the group operation is composition of permutations $(\sigma \circ \tau )(j) = \sigma (\tau (j))$. Any $\sigma$ can be written as a product of transpositions.
%\be
%\ve(\sigma) =\left\{ \ba{ll}
%+1\quad\quad & \text{if \# permutations is even,}\\
%-1 & \text{if \# permutations is odd.}
%\ea\right.
%\ee
%$\ve : S_n \to \bra{+1,-1}$ is a homomorphism.

\begin{example}
For $2\times 2$ matrix $A$, we have $\det A = a_{11} a_{22} - a_{12}a_{21}$.
\end{example}

\begin{definition}[volume form\index{volume form}]\label{def:volumn_form}
The function $d : \F^n \times  \dots \times \F^n \to\F$ is a volumne form on $\F^n$ if
\ben
\item [(i)] it is multilinear, i.e.
\beast
d(v_1, \dots, \lm_iv_i, \dots, v_n) & = & \lm d(v_1, \dots, v_i, \dots, v_n)\\
d(v_1, \dots, v_i + v'_i , \dots, v_n) & = & d(v_1, \dots, v_i, \dots, v_n) + d(v_1, \dots, v'_i, \dots, v_n)
\eeast
\item [(ii)] it is alternating, i.e. whenever $i\neq j$ and $v_i = v_j$ then $d(v_1, \dots, v_n) = 0$.
\een
\end{definition}

\begin{definition}[determinant function\index{determinant function}]\label{def:determinant_function}
$d$ is a determinant function if it is normalised volume form, i.e. $d(e_1, \dots, e_n) = 1$.
\end{definition}

\begin{lemma}\label{lem:swap_column_sign}
Swapping columns in a volume form changes the sign. For $i \neq j$,
\be
d(v_1, \dots, v_j , \dots, v_i, \dots, v_n) = -d(v_1, \dots, v_i, \dots, v_j , \dots, v_n).
\ee
\end{lemma}

\begin{proof}[\bf Proof]
By Definition \ref{def:volumn_form}.(ii),
\be
0 = d(v_1, \dots, v_i + v_j , \dots, v_i + v_j , \dots, v_n) = 0 + d(v_1, \dots, v_i, \dots, v_j , \dots, v_n) + d(v_1, \dots, v_j , \dots, v_i, \dots, v_n) + 0 
\ee
by Definition \ref{def:volumn_form}.(i).
\end{proof}

\begin{corollary}\label{cor:volume_form_sign_permutation}
If $\sigma \in S_n$ and $d$ is a volume form on $\F^n$ then
\be
d(v_{\sigma(1)}, \dots, v_{\sigma(n)}) = \ve(\sigma)d(v_1, \dots, v_n)
\ee
for $v_1, \dots, v_n \in \F^n$. In particular,
\be
d(e_{\sigma(1)}, \dots, e_{\sigma(n)})= \ve(\sigma)d(e_1, \dots, e_n) = \ve(\sigma)
\ee
if $d$ is a determinant function.
\end{corollary}

\begin{proof}[\bf Proof]
Think about number of transpositions.
\end{proof}

\begin{theorem}\label{thm:volume_form_determinant}
If $d$ is a volume form on $\F^n$ and $A = (a_{ij}) = \bb{A_1, \dots,A_n} \in M_n(\F)$ then $d\bb{A_1, \dots,A_n} = (\det A) d(e_1, \dots, e_n)$.
\end{theorem}

\begin{proof}[\bf Proof]
\beast
d(A_1, \dots,A_n) & = & d\bb{\sum_{j_1} a_{j_1,1}e_{j_1}, A_2, \dots,A_n} = \sum_{j_1} a_{j_1,1}d\bb{e_{j_1} ,A_2, \dots,A_n}\\
& = & \sum_{j_1,j_2} a_{j_1,1} a_{j_2,2} d(e_{j_1} , e_{j_2} ,A_3, \dots,A_n) =  \sum_{j_1\neq j_2} a_{j_1,1} a_{j_2,2} d(e_{j_1} , e_{j_2} ,A_3, \dots,A_n)
\eeast
as $d$ is zero when $j_1 = j_2$ by Definition \ref{def:volumn_form}.(ii). Then for $j_1,\dots,j_n$ forming a permutation
\beast
d(A^{(1)}, \dots,A^{(n)}) = \sum_{j_1,\dots,j_n} a_{j_1,1} \dots a_{j_n,n} d(e_{j_1}, \dots, e_{j_n}) = \sum_{\sigma \in S_n} a_{\sigma(1)1} \dots a_{\sigma(n)n}\ve(\sigma)d(e_1, \dots, e_n) = (\det A)d(e_1, \dots, e_n).
\eeast
by Definition \ref{def:determinant_matrix}.
\end{proof}

\begin{theorem}\label{thm:determinant_determinant_function}
If we define $d : \F^n \times  \dots \times  \F^n \to \F,\ d(A_1, \dots,A_n) = \det A$ for $A = (A_1, \dots,A_n)$ then $d$ is a determinant function.
\end{theorem}

\begin{proof}[\bf Proof]
\ben
\item [(i)] $\prod^n_{j=1} a_{\sigma(j)j}$ is multilinear, hence so is the linear combination $\det A$.
\item [(ii)] We want to show that if $A_k = A_l$ with $k \neq l$ then $\det A = 0$. Write $\tau = (k\ l)$, a transposition in $S_n$.
\be
\det A = \sum_{\sigma \in S_n} \ve(\sigma) \prod_j a_{\sigma (j)j}
\ee

Reorder the sum, take each even permutation $\sigma$ followed by the odd permutation $\sigma \tau$ and note $\ve(\sigma ) = 1$, $\ve(\sigma \tau) = -1$. Thus
\be
\det A = \sum_{\sigma\text{ even}} \bb{\prod_j a_{\sigma (j)j} - \prod_j a_{\sigma \tau (j)j}} = 0
\ee
as each of the summands is zero since $a_{\sigma (k)k} = a_{\sigma (k)l}$, $a_{\sigma (l)k} = a_{\sigma (l)l}$ and 
\be
a_{\sigma (1)1} \dots a_{\sigma (k)k} \dots a_{\sigma (l)l} \dots a_{\sigma (n)n} - a_{\sigma (1)1} \dots a_{\sigma (l)k} \dots a_{\sigma (k)l} \dots a_{\sigma (n)n} = 0.
\ee

\item [(iii)] For normal basis $\bra{e_1,\dots,e_n}$, $d(e_1,\dots,e_n) = \sum_{\sigma \in S_n} \ve(\sigma ) \prod^n_{j=1} \delta_{\sigma (j)j} = \ve(\iota) \cdot 1 = 1$.
\een

Thus, $d$ is a determinant function.
\end{proof}

\begin{proposition}\label{pro:determinant_matrix_property}
Let $A\in M_n(\F)$ and $\lm \in \F$. Then
\beast
\text{(i)} & &  \det \bb{A_1,\dots,\lm A_i, \dots, A_n} = \lm \det \bb{A_1,\dots,A_i,\dots,A_n} = \det A,\\
\text{(ii)} & &  \det \bb{A_1,\dots, A_i + A_{i}' , \dots, A_n} = \det\bb{A_1,\dots, A_i, \dots, A_n}  + \det\bb{A_1,\dots,A_{i}', \dots, A_n},\\
\text{(iii)} & &  \det \bb{A_1,\dots, A_i ,\dots, A_j , \dots, A_n} = -\det \bb{A_1,\dots, A_j ,\dots, A_i , \dots, A_n},\\
\text{(iv)} & & \text{If }A_i = A_j,\ \det \bb{A_1,\dots,A_i,\dots,A_j,\dots, A_n} = 0.
\eeast   
\end{proposition}

\begin{proof}[\bf Proof]
This is direct result from Theorem \ref{thm:determinant_determinant_function}, Lemma \ref{lem:swap_column_sign}.
\end{proof}

\begin{proposition}\label{pro:determinant_transpose}
$\det A^T = \det A$.
\end{proposition}

\begin{proof}[\bf Proof]
If $\sigma \in S_n$ then $\prod^n_{j=1} a_{\sigma (j)j} = \prod^n_{j=1} a_{j\sigma (j)}$ as they contain the same factors but in a different order. Also, as $\sigma$ runs through $S_n$, so does $\sigma^{-1}$ and $\ve(\sigma^{-1}) = \ve(\sigma)$ (as $\sigma \sigma^{-1} = \iota$). Hence
\be
\det A = \sum_{\sigma \in S_n} \ve(\sigma )\prod^n_{j=1} a_{\sigma (j)j} = \sum_{\sigma \in S_n} \ve(\sigma^{-1} ) \prod^n_{j=1} a_{j\sigma^{-1}(j)} = \sum_{\sigma' \in S_n} \ve(\sigma' ) \prod^n_{j=1} a_{j\sigma' (j)} = \det A^T.
\ee
\end{proof}



%\begin{lemma} 
%$\det$ is the unique multilinear alternating form in rows normalised at $I$.
%\end{lemma}

\begin{proposition}\label{pro:determinant_element_matrix}
For elementary matrices, the determinants are
\be
\det T_{ij} =  -1,\quad \det M_{i,\lm} = \lm,\quad \det C_{i,j,\lm} = 1.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Obvious from Definition \ref{def:elementary_matrix}.
\end{proof}


\begin{lemma}\label{lem:determinant_element_matrix_product}
If $E$ is an elementary matrix, then for any $n \times n$ matrix $A$,
\be
\det(AE) = \det A \det E = \det(EA).
\ee

Performing an elementary column or row operation on $A$ multiplies $\det A$ by the determinant of the corresponding elementary matrix.
\end{lemma}

\begin{proof}[\bf Proof]
By Proposition \ref{pro:determinant_element_matrix}, the determinants of the elementary matrices are %(see Definition \ref{def:elementary_matrix}) we have 
\be
\det T_{ij} =  -1,\quad \det M_{i,\lm} = \lm,\quad \det C_{i,j,\lm} = 1
\ee
as we can see that the column and row operation have the same determinant by Proposition \ref{pro:determinant_transpose}. 

Performing the corresponding elementary column or row operation multiplies $\det A$ by $-1, \lm, 1$, respectively. Then
\be
\det (AE) = \det A \det E = \det \bb{A^T} \det \bb{E^T} = \det \bb{A^TE^T} = \det\bb{(EA)^T} = \det(EA)
\ee
by Proposition \ref{pro:determinant_transpose}.
\end{proof}

\begin{theorem}\label{thm:matrix_invertible_determinant_non_zero}
Let $A\in M_n(\F)$. Then $A$ is invertible if and only if $\det A \neq 0$.
\end{theorem}

\begin{proof}[\bf Proof]
If $A$ is invertible then $A$ can be written as a product of elementary matrices by Lemma \ref{lem:invertible_product_elementary_matrices}, so $\det A$ is the product of the corresponding determinants, so $\det A \neq 0$. 

If $A$ is singular (not invertible), we have that the columns of $A$ are linearly dependent (Corollary \ref{cor:invertible_column_linearly_independent}). Then we can obtain a 0 column as a non-trivial combination of columns of $A$, so using elementary column operations on $A$ we can obtain a matrix with a 0 column. Hence $\det A = 0$ by Lemma \ref{lem:determinant_element_matrix_product} as determinants of elementary matrices are no-zero.
\end{proof}

\begin{theorem}\label{thm:determinant_product}
If $A,B \in M_n(\F)$, then $\det(AB) = \det A \det B = \det(BA)$.
\end{theorem}

\begin{proof}[\bf Approach 1]
First fix $A$. Consider $d_A : \bb{B_1, \dots,B_n} \mapsto \det(AB)$ for $B = \bb{B_1, \dots,B_n}$. Note that $\det(AB) = d_A\bb{B_1, \dots,B_n} = d\bb{(AB)_1, \dots,(AB)_n}$ where $d$ is volume form by Theorem \ref{thm:determinant_determinant_function}. So $d_A$ is multilinear and alternating and $d_A$ is a volume form on $\F^n$. Hence
\be
\det(AB) = d_A\bb{B_1, \dots,B_n }= (\det B) d_A(e_1, \dots, e_n) = \det B \det A,
\ee
using Theorem \ref{thm:volume_form_determinant}.
\end{proof}

\begin{proof}[\bf Approach 2]
Expand by columns and by Corollary \ref{cor:volume_form_sign_permutation},
%as before.
\beast
\det(AB) & = & \det \bb{\sum^n_{j_1=1} b_{j_1,1} A_{j_1}, \dots, \sum^n_{j_n=1} b_{j_n,n}A_{j_n} } = \sum_{\sigma \in S_n} \bb{\prod^n_{j=1} b_{\sigma (j)j}} \det(A_{\sigma (1)}, \dots,A_{\sigma (n)})\\
& = & \sum_{\sigma \in S_n}\bb{\prod^n_{j=1} b_{\sigma (j)j}} \ve(\sigma ) \det(A_1, \dots,A_n) = \sum_{\sigma \in S_n}\bb{\prod^n_{j=1} b_{\sigma (j)j}} \ve(\sigma ) \det A = \det A \det B.
\eeast
\end{proof}

\begin{proof}[\bf Approach 3]
If $B$ is singular, so $\rank(B) <n$ by Theorem \ref{thm:invertible_full_rank} and let $\rank( B) = r$. Then by Lemma \ref{lem:matrix_equivalent_to_identity}, we have for some invertible matrices $P,Q$,
\be 
B \sim \bepm I_r & 0 \\ 0 & 0 \eepm \ B = Q\bepm I_r & 0 \\ 0 & 0 \eepm P^{-1} = \bepm C_r & 0 \\ 0 & 0 \eepm\ \ra \ AB = A\bepm C_r & 0 \\ 0 & 0 \eepm = \bepm D_r & 0 \\ 0 & 0 \eepm
\ee
where $C_r$ and $D_r$ are $r\times r$ submatrices. Therefore, we see that $\rank(AB) \leq r \neq n$. So we have that $AB$ is singular as well by Theorem \ref{thm:invertible_full_rank}. %is $AB$ by definition of inverse. %That is if $AB$ is singular, then there exists an invertible matrix $P$ such that \be P(AB) = (AB)P = I \ \ra \ (PA)B =  \ee
Hence $\det B = 0 = \det(AB)$. 

So assume $B$ is non-singular (invertible) and write it as a product of elementary matrices, $B = E_1 \dots E_k$ by Lemma \ref{lem:invertible_product_elementary_matrices}. Using Lemma \ref{lem:determinant_element_matrix_product},
\be
\det(AB) = \det(AE_1 \dots E_k) = \det A\det E_1 \dots \det E_k = \det A\det B
\ee
as required.
\end{proof}

\begin{corollary}\label{cor:determinant_inverse}
If $A$ is invertible then $\det A^{-1} = (\det A)^{-1}$.
\end{corollary}

\begin{proof}[\bf Proof]
As $A$ is invertible, $AA^{-1} = I$ so $(\det A)(\det A^{-1}) = \det I = 1$ and hence $\det A^{-1} = (\det A)^{-1}$.
\end{proof}

Consider the system of linear equations $Ax = b$ with $m$ equations and $n$ unknowns. Here $A$ is an $m\times n$ matrix, $b$ is a column vector in $\F^m$. This has a solution if $\rank (A) = \rank(A| b)$. The solution is unique if and only if $n = \rank A = \rank(A| b)$, then the solution is $x = A^{-1}b$. To solve this equation, use Gaussian elimination\footnote{need details}. In the case $m = n$, there is another method.

\begin{lemma}[Cramer's rule]\label{lem:cramer_rule}
If $A \in M_n(\F)$ is non-singular then $Ax = b$ has $x = (x_1, \dots, x_n)^T$ with $x_i = \frac 1{\det A} \det A_{ib}$ for $i = 1, \dots, n$ as its unique solution, where $A_{ib}$ is the matrix obtained from $A$ by deleting column $i$ and inserting $b$. 
\end{lemma}

\begin{proof}[\bf Proof]
Assume $x$ is the solution of $Ax = b$. Then
\beast
\det A_{ib} & = &  \det\bb{A_1, \dots,A_{i-1}, b,A_{i+1}, \dots,A_n} = \det\bb{A_1, \dots,A_{i-1}, Ax ,A_{i+1}, \dots,A_n}  \\
& = & \sum_{j=1} x_j \det(A_1, \dots,A_{i-1},A_j,A_{i+1}, \dots,A_n) = x_i \det(A_1, \dots,A_{i-1},A_i,A_{i+1}, \dots,A_n) = x_i\det A,
\eeast
so $x_i = \frac 1{\det A} \det A_{ib}$ for $i = 1, \dots, n$.
\end{proof}

\begin{corollary}
If $A \in M_n(\Z)$ with $\det A = \pm 1$ and if $b \in \Z^n$ then we can solve $Ax = b$ over $\Z$.
\end{corollary}

\begin{remark}
The solution over $\Z$ is guaranteed since $\det A = \pm 1$.
\end{remark}



\subsection{Special square matrices}

\begin{definition}[diagonal matrix\index{diagonal matrix}]\label{def:diagonal_matrix}
The matrix $D = (d_{ij}) \in M_n(\F)$ is called diagonal matrix if $d_{ij} = 0$ whenever $i\neq j$, i.e.,
\be
D = \bepm d_{11} & & & \\ & d_{22} & & & \\ & & \ddots & \\ & & & d_{nn} \eepm.
\ee 

We denote such a matrix as 
\be
D = \diag\bb{d_{11},\dots,d_{nn}}\quad \text{or}\quad D = \diag d,
\ee
where $d$ is the vector of diagonal entries of $D$.

If all the diagonal entries of a diagonal matrix are positive (non-negative) real numbers, we refer to it as a positive (non-negative) diagonal matrix\index{diagonal matrix!positive, non-negative}.
\end{definition}

\begin{remark}
The identity matrix $I \in M_n(\F)$ is an example of a positive diagonal matrix.
\end{remark}

\begin{definition}[scalar matrix\index{scalar matrix}]\label{def:scalar_matrix}
A diagonal matrix $D\in M_n(\F)$ is called scalar matrix if the diagonal entries of $D$ are equal; that is, $D= \alpha I$ for some $\alpha \in \C$.
\end{definition}

\begin{proposition}[diagonal matrices properties]\label{pro:diagonal_matrix_property}
\ben
\item [(i)] The determinant of a diagonal matrix is just the product of its diagonal entries: $\det D = \prod^n_{i=1} d_{ii}$.
\item [(ii)] A diagonal matrix is invertible if and only if all its diagonal entries are non-zero (by Theorem \ref{thm:matrix_invertible_determinant_non_zero}). 
\item [(iii)] All diagonal matrices commute with each other under multiplication, i.e., $D_1 D_2 = D_2 D_1$, for diagonal matrices $D_1,D_2$.
\item [(iv)] A diagonal matrix $D$ commutes with a given matrix $A = (a_{ij})\in M_n(\F)$ if and only if $a_{ij} = 0$ whenever the $i$th and $j$th diagonal entries of $D$ differ.
\item [(v)] The product of two diagonal matrices is just the diagonal matrix of pairwise products of their respective diagonal entries and similarly for postive integer powers of a single diagonal matrix.
\een
\end{proposition}


\begin{definition}[block diagonal matrix\index{block diagonal matrix}]\label{def:block_diagonal_matrix}
A matrix $A = M_{n}(\F)$ of the form
\be
A = \bepm A_{11} & & & 0\\ & A_{22} & & \\ & & \ddots & \\ 0 & & & A_{kk} \eepm
\ee
in which $A_{ii} \in M_{n_i}(\F)$, $i = 1,\dots,k$ and $n = \sum^k_{j=1} n_j$, is called block diagonal matrix. Notationally, such a matrix is often indicated as 
\be
A = A_{11} \oplus A_{22} \oplus \dots \oplus A_{kk}, \quad \text{ or }\quad \bigoplus^k_{i=1}A_{ii},
\ee
which is called the direct sum\index{direct sum!matrices} of the matrices $A_{11},\dots,A_{kk}$.
\end{definition}

\begin{proposition}[block diagonal matrices properties]\label{pro:block_diagonal_matrix_property}
For matrix $A = \bigoplus^k_{i=1}A_{ii}$ and $B = \bigoplus^k_{i=1}B_{ii}$, in which each pair $A_{ii}$, $B_{ii}$ has the same size.
\ben
\item [(i)] $\det A = \prod^k_{i=1} \det \bb{A_{ii}}$.
\item [(ii)] $A$ is invertible if and only if each $A_{ii}$, $i=1,\dots,k$ is invertible.
\item [(iii)] $\rank(A) = \sum^k_{i =1} \rank(A_{ii})$.
\item [(iv)] $A,B$ commute if and only if $A_{ii}$ and $B_{ii}$ commute, $i=1,\dots,k$.
\een
\end{proposition}


\begin{definition}[triangular matrix\index{triangular matrix}]\label{def:triangular_matrix}
$A\in M_n(\F)$ is an upper triangular matrix if $a_{ij} = 0$ whenever $i > j$. If $a_{ij} = 0$ whenever $i\geq j$, then $A$ is said to be strictly upper triangular.

$A$ is said to be lower triangular (or strictly upper triangular) if its transpose is upper triangular (or strictly upper triangular).
\end{definition}

\begin{example}
\be
A = \bepm 1 & 2 & 3\\ 0 & 4 & 5 \\ 0 & 0 & 6 \eepm, \qquad B = \bepm 1 & 2 & 3\\ 0 & 4 & 5 \\ 0 & 0 & 6 \eepm, 
\ee

$A$ is upper triangular.
\end{example}

\begin{lemma}\label{lem:upper_triangular_matrix_determinant}
If $A$ is an upper triangular matrix, then $\det A = a_{11} \dots a_{nn}$.
\end{lemma}

\begin{proof}[\bf Proof]
From the definition the determinant (Definition \ref{def:determinant_matrix}),
\be
\det A = \sum_{\sigma \in S_n} \ve(\sigma )a_{\sigma (1)1} \dots a_{\sigma (n)n}.
\ee

For a product to contribute, we must have $\sigma (i) \leq  i$ for all $i = 1, \dots, n$. Hence $\sigma (1) = 1$, $\sigma (2) = 2$, $\dots$, $\sigma (n) = n$, so $\sigma  = \iota$ and hence $\det A = a_{11} \dots a_{nn}$.
\end{proof}



\begin{proposition}\label{pro:block_matrix_determinant}
If $A \in M_m(\F)$, $B \in M_n(\F)$ and $C \in M_{m,n}(\F)$. Then $\det \bepm A & C\\ 0 & B \eepm = \det A \det B$.
\end{proposition}

\begin{proof}[\bf Approach 1]
Fix $B,C$. Then $d_{B,C} : A \mapsto \det\bepm A & C\\ 0 & B\eepm$ is a volume form on the column space $\F^m$ by Proposition \ref{pro:determinant_matrix_property}. Hence by Theorem \ref{thm:volume_form_determinant}, $d_{B,C}(A) = \det A \det \bepm I & C\\ 0 & B\eepm$. 

Now keep $C$ fixed. The map $B \mapsto \det\bepm I & C\\ 0 & B\eepm$ is a volume form on the row space $\F^n$ by Proposition \ref{pro:determinant_matrix_property}. Hence $\det\bepm I & C\\ 0 & B \eepm= \det B\bepm I & C\\ 0 & I \eepm$ by Theorem \ref{thm:volume_form_determinant}. 

Now $\det\bepm I & C\\ 0 & I\eepm = 1$ as $\bepm I & C\\ 0 & I\eepm$ is upper triangular matrix. So $\det\bepm A & C\\ 0 & B \eepm = \det A\det B \cdot 1 = \det A\det B$ by Lemma \ref{lem:upper_triangular_matrix_determinant}.
\end{proof}

\begin{proof}[\bf Approach 2]
Write $X =\bepm A & C\\ 0 & B\eepm$ and expand the expression for the determinant.
\be
\det \bepm A & C\\ 0 & B \eepm = \sum_{\sigma \in S_{m+n}} \ve(\sigma ) \prod^{m+n}_{j=1} x_{\sigma (j)j}
\ee

Note $x_{\sigma (j)j} = 0$ if $j \leq  m$, $\sigma (j) > m$, so we only sum over $\sigma$ with the following properties.
\ben
\item [(i)] For $j\in [1,m]$, $\sigma (j) \in [1,m]$. Here $x_{\sigma (j)j} = a_{\sigma_1(j)j}$ where $\sigma_1\in S_m$ is the restriction of $\sigma$ to $[1,m]$.
\item [(ii)] For $j \in [m + 1,m + n]$, $\sigma (j) \in [m + 1,m + n]$. Here, writing $k = j - m$, we have $x_{\sigma (j)j} = b_{\sigma_2(k)k}$ where $\sigma_2(k) = \sigma (m + k) - m$.
\een

Noting also that $\ve(\sigma ) = \ve(\sigma_1)\ve(\sigma_2)$ for such $\sigma$, we obtain
\be
\det\bepm A & C\\ 0 & B\eepm = \bb{\sum_{\sigma_1\in S_m} \ve(\sigma_1)\prod^m_{j=1} a_{\sigma_1(j)j}} \bb{\sum_{\sigma_2\in S_k} \ve(\sigma_2) \prod^k_{l=1} a_{\sigma_2(j)j}} = \det A\det B.
\ee
\end{proof}








\subsection{Adjugate matrices}

\begin{definition}[minor\index{minor!matrix}]\label{def:minor_matrix}
Let $A = (a_{ij})$ be an $n \times n$ matrix. Write $\wh{A}_{ij}$ for the $(n - 1) \times (n - 1)$ matrix obtained from $A$ by deleting row $i$ and column $j$. Let $M_{ij} = \det \wh{A}_{ij}$. We call $M_{ij}$ the $(i,j)$ minor of $A$.
\end{definition}

\begin{example}
As a specific example, we have
\be
A= \bepm \!-3 & \, 2 & \!-5 \\ \!-1 & \, 0 & \!-2 \\ \, 3 & \!-4 & \, 1 \eepm \ \ra \ \adj A = \bepm \!-8 & \,18 & \!-4 \\ \!-5 & \!12 & \,-1 \\ \, 4 & \!-6 & \, 2 \eepm.
\ee

The $-6$ in the third row, second column of the adjugate was computed as follows:
\be
(-1)^{2+3}\det \bepm \!-3&\,2\\ \,3&\!-4\eepm =-((-3)(-4)-(3)(2))=-6.
\ee

Again, the (3,2) entry of the adjugate is the (2,3) cofactor of $A$. Thus, the submatrix $\bepm \!-3&\,\!2\\ \,\!3&\!-4\eepm$ was obtained by deleting the second row and third column of the original matrix $A$.
\end{example}


\begin{lemma}\label{lem:determinant_minor_matrix}
Let $A\in M_n(\F)$ and $M$ is the minor matrix of $A$.
\ben
\item [(i)] For fixed $j$, $\det A = \sum^n_{i=1} (-1)^{i+j} a_{ij} M_{ij}$.
\item [(ii)] For fixed $i$, $\det A = \sum^n_{j=1} (-1)^{i+j} a_{ij} M_{ij} $.
\een
\end{lemma}

\begin{remark}
This can be used as a definition of determinant.
\end{remark}

\begin{proof}[\bf Proof]
By Proposition \ref{pro:determinant_matrix_property},
\beast
\det A & = & \det(A_1, \dots,A_n) = \sum^n_{i=1} \det \bb{A_1,\dots,A_{j-1},A'_j,A_{j+1},\dots,A_n} \\
& = & \sum^n_{i=1} a_{ij} \det \bb{A_1,\dots,A_{j-1}, e_i,A_{j+1},\dots,A_n} 
\eeast
where $A_j' = \bepm 0,\dots,0,a_{ij},0,\dots,0\eepm^T$ (the only non-zero element is in row $i$). 

Now we can swap row $i$ and row $i-1$, and then swap row $i-1$ and $i-2$,... Finally, we have shift original row $i$ to row 1 and this need $i-1$ transpositions (similarly for column operations which need $j-1$ transpositions, so totally $i+j-2$ transpositions). Thus, by Proposition \ref{pro:determinant_matrix_property}.(iii), % column $j$ and column 1.
\be
\det A = \sum^n_{i=1} a_{ij}(-1)^{i+j-2} \det\bepm 1 & * \\ 0 & \wh{A}_{ij}\eepm = \sum^n_{i=1} (-1)^{i+j} a_{ij} M_{ij},
\ee
using Proposition \ref{pro:block_matrix_determinant}. (ii) is similar by using determinant of transpose matrix (Proposition \ref{pro:determinant_transpose}).
\end{proof}

\begin{definition}[cofactor matrix\index{cofactor matrix}]\label{def:cofactor_matrix}
Let $A \in M_n(\F)$ and $M_{ij}$ is the $(i,j)$ minor of $A$. Then cofactor matrix of $A$ is $C = (c_{ij})$ where $c_{ij} = (-1)^{i+j} M_{ij}$.
\end{definition}

\begin{definition}[adjugate matrix\index{adjugate matrix}]\label{def:adjugate_matrix}
Let $A \in M_n(\F)$ and $C$ is cofactor matrix of $A$. The adjugate matrix is $\adj A = C^T$, i.e. the $n \times n$ matrix with $(i, j)$ entry equal to $(-1)^{i+j} M_{ji} = (-1)^{i+j} \det \bb{\wh{A}_{ji}}$.
\end{definition}

\begin{theorem}\label{thm:adjugate_inverse_matrix}
\ben
\item [(i)] $(\adj A)A = (\adj A)A = (\det A)I$.
\item [(ii)] If $A$ is invertible then $A^{-1} = \frac 1{\det A} \adj A$.
\een
\end{theorem}

\begin{proof}[\bf Proof]
\ben
\item [(i)] By Lemma \ref{lem:determinant_minor_matrix}.(i), fix $j$,
\be
\det A = \sum^n_{i=1} (\adj A)_{ji} a_{ij} = ((\adj A) A)_{jj}.
\ee

Also, for fix $j < k$, by Proposition \ref{pro:determinant_matrix_property} and Lemma \ref{lem:determinant_minor_matrix},
\be
0 = \det(A_1, \dots,A_k, \dots,A_k, \dots,A_n) = \sum^n_{i=1} (\adj A)_{ji} a_{ik} = ((\adj A) A)_{jk}.
\ee

Similarly, from Lemma \ref{lem:determinant_minor_matrix}.(ii), we have $(\adj A)A = (\det A)I$.

\item [(ii)] If $A$ is invertible, then $\det A \neq 0$ (by Theorem \ref{thm:matrix_invertible_determinant_non_zero}), so $\frac 1{\det A} (\adj A) A = I$ and hence we deduce $A^{-1} = \frac 1{\det A} \adj A$.
\een
\end{proof}





\begin{proposition}\label{pro:adjugate_matrix_basic_properties}
\ben
\item [(i)] $\adj I = I$ where $I \in M_n(\F)$ is identity matrix.
\item [(ii)] For $A\in M_n(\F)$, $\adj\bb{\lm A} = \lm^{n-1}\adj A$.
\item [(iii)] $\adj \bb{A^T} = \bb{\adj A}^T$.
\item [(iv)] $\adj\bb{\ol{A}} = \ol{\adj A}$.
\item [(v)] Adjugate matrices of diagonal matrices are still diagonal matrices, i.e., if $A = \diag\bb{a_1,a_2,\dots,a_n}$, then
\be
\adj A = \diag\bb{\prod^n_{i=2}a_i,\prod^n_{i=1,i\neq 2}a_i, \prod^n_{i=1,i\neq 3}a_i,\dots, \prod^{n-1}_{i=1}a_i}.
\ee
\item [(vi)] Adjugate matrices of symmetric matrices are still symmetric matrices, i.e., if $A^T = A$, then $\bb{\adj A}^T = \adj A$.
\item [(vii)] If $A\in M_n(\F)$ is antisymmetric, i.e., $A^T = -A$, then
\be
\bb{\adj A}^T = \left\{\ba{ll}
-\adj A \quad\quad & n \text{ is even}\\
\adj A & n \text{ is odd} 
\ea\right.
\ee
\item [(viii)] For elementary operations,
\be
\adj T_{i,j} = -T_{i,j},\quad i\neq j, \qquad \adj M_{i,\lm} = \lm M_{i,1/\lm},\quad c\neq 0,\qquad \adj C_{i,j,\lm} = C_{i,j,-\lm}.
\ee
\item [(ix)] Let $Q$ be an othogonal matrix. Then
\be
\adj Q = \left\{\ba{ll}
Q^T & \det Q = 1\\
-Q^T \quad\quad & \det Q = -1
\ea\right.
\ee
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [](i)-(iv) are obvious from definition of adjugate matrix (Definition \ref{def:adjugate_matrix}).
\item [(v)] \footnote{need proof}
\een
\end{proof}

\begin{lemma}\label{lem:adjugate_invertible_product}
For any $A\in M_n(\F)$ and invertible matices $P,Q\in M_n(\F)$, $\adj\bb{PAQ} = \adj Q\adj A\adj P$.
\end{lemma}

\begin{proof}[\bf Proof]
Let $E$ be an elementary matrix.% and $B_{ij} = (-1)^{i+j}\det\bb{\wh{A}_{ij}}$. %$P,Q$ are elementary matrices (We only need to consider elementary column matrices as we can take the transpose of them to get elementary row matrices). Thus, we have three cases. %\footnote{need proof}

If $E$ is switching matrix $T_{i,j}$ ($i<j$), then by Proposition \ref{pro:determinant_matrix_property}.(iii),
\beast
\adj\bb{AT_{i,j}} & = & \adj \bepm A_1,\dots, A_j,\dots, A_i,\dots, A_n\eepm  \\
& = & \bepm -(-1)^{1+1}\det\bb{\wh{A}_{11}} & -(-1)^{2+1}\det\bb{\wh{A}_{21}} & \dots & -(-1)^{n+1}\det\bb{\wh{A}_{n1}} \\ \vdots\\ (-1)^{1+i}(-1)^{j-i-1}\det\bb{\wh{A}_{1j}} & (-1)^{2+i}(-1)^{j-i-1}\det\bb{\wh{A}_{2j}} & \dots & (-1)^{n+i}(-1)^{j-i-1}\det\bb{\wh{A}_{nj}} \\ \vdots \\  (-1)^{1+j}(-1)^{j-i-1}\det\bb{\wh{A}_{1i}} & (-1)^{2+j}(-1)^{j-i-1}\det\bb{\wh{A}_{2i}} & \dots & (-1)^{n+j}(-1)^{j-i-1}\det\bb{\wh{A}_{ni}} \\ \vdots \\  -(-1)^{1+n}\det\bb{\wh{A}_{1n}} & -(-1)^{2+n}\det\bb{\wh{A}_{2n}} & \dots & -(-1)^{n+n}\det\bb{\wh{A}_{nn}}  \eepm
\eeast
where $(-1)^{j-i}\det\bb{\wh{A}_{kj}}$ (as we shift $A_i$ to its original position with $j-i-1$ times) is $(k,j)$ minor of $AT_{i,j}$. Thus,
\beast
\adj\bb{AT_{i,j}} & = & \bepm -(-1)^{1+1}\det\bb{\wh{A}_{11}} & -(-1)^{2+1}\det\bb{\wh{A}_{21}} & \dots & -(-1)^{n+1}\det\bb{\wh{A}_{n1}} \\ \vdots\\ -(-1)^{1+j}\det\bb{\wh{A}_{1j}} & -(-1)^{2+j}\det\bb{\wh{A}_{2j}} & \dots & -(-1)^{n+j}\det\bb{\wh{A}_{nj}} \\ \vdots \\ -(-1)^{1+i}\det\bb{\wh{A}_{1i}} & -(-1)^{2+i}\det\bb{\wh{A}_{2i}} & \dots & -(-1)^{n+i}\det\bb{\wh{A}_{ni}} \\ \vdots \\  -(-1)^{1+n}\det\bb{\wh{A}_{1n}} & -(-1)^{2+n}\det\bb{\wh{A}_{2n}} & \dots & -(-1)^{n+n}\det\bb{\wh{A}_{nn}}  \eepm = \adj T_{i,j}\adj A.
\eeast

Also, since $T_{i,j}^T = T_{i,j}$, by Proposition \ref{pro:adjugate_matrix_basic_properties}.(iii) and Proposition \ref{pro:matrix_multiple_transpose}
\be
\adj\bb{T_{i,j}A} = \adj\bb{T_{i,j}^TA} = \adj\bb{\bb{A^TT_{i,j}}^T} = \bb{\adj\bb{A^TT_{i,j}}}^T = \bb{\adj T_{i,j}\adj \bb{A^T}}^T = \adj A \adj T_{i,j}.
\ee

If $E$ is multiplication matrix $M_{i,\lm}$ ($\lm \neq 0$), we have by Theorem \ref{thm:adjugate_inverse_matrix}.(ii) and Proposition \ref{pro:square_elementary_matrix_invertible},
\beast
\adj\bb{AM_{i,\lm}} & = & \adj \bepm A_1,\dots, \lm A_i,\dots, A_n\eepm  \\
& = & \bepm \lm (-1)^{1+1}\det\bb{\wh{A}_{11}} & \lm (-1)^{2+1}\det\bb{\wh{A}_{21}} & \dots & \lm (-1)^{n+1}\det\bb{\wh{A}_{n1}} \\ \vdots\\ (-1)^{1+i}\det\bb{\wh{A}_{1i}} & (-1)^{2+i}\det\bb{\wh{A}_{2i}} & \dots & (-1)^{n+i}\det\bb{\wh{A}_{ni}} \\ \vdots \\  \lm (-1)^{1+n}\det\bb{\wh{A}_{1n}} & \lm(-1)^{2+n}\det\bb{\wh{A}_{2n}} & \dots & \lm (-1)^{n+n}\det\bb{\wh{A}_{nn}}  \eepm \\
& = & \lm M_{i,1/\lm} \adj A = \det\bb{M_{i,\lm}} \bb{M_{i,\lm}}^{-1} \adj A= \adj M_{i,\lm} \adj A.
\eeast

Similarly, since $M_{i,\lm}^T = M_{i,\lm}$, by Proposition \ref{pro:adjugate_matrix_basic_properties}.(iii) and Proposition \ref{pro:matrix_multiple_transpose}
\be
\adj\bb{M_{i,\lm}A} = \adj\bb{M_{i,\lm}^TA} = \adj\bb{\bb{A^TM_{i,\lm}}^T} = \bb{\adj\bb{A^TM_{i,\lm}}}^T = \bb{\adj M_{i,\lm}\adj \bb{A^T}}^T = \adj A \adj M_{i,\lm}.
\ee

If $E$ is addition matrix $C_{i,j,\lm}$ ($i<j$), we have by Proposition \ref{pro:determinant_matrix_property}.(ii,iv),
\beast
\adj\bb{AC_{i,j,\lm}} & = & \adj \bepm A_1,\dots, A_i,\dots, A_j + \lm A_i,\dots, A_n\eepm  \\
& = & \bepm (-1)^{1+1}\det\bb{\wh{A}_{11}} & \dots & \lm (-1)^{n+1}\det\bb{\wh{A}_{n1}} \\ \vdots\\ (-1)^{1+i}\bb{\lm (-1)^{j-i-1}\det\bb{\wh{A}_{1j}}  + \det\bb{\wh{A}_{1i}}} & \dots & (-1)^{n+i}\bb{\lm (-1)^{j-i-1}\det\bb{\wh{A}_{nj}} + \det\bb{\wh{A}_{ni}}}  \\ \vdots \\ (-1)^{1+j}\det\bb{\wh{A}_{1j}} & \dots &  (-1)^{n+j}\det\bb{\wh{A}_{nj}}  \\ \vdots \\   (-1)^{1+n}\det\bb{\wh{A}_{1n}} & \dots &  (-1)^{n+n}\det\bb{\wh{A}_{nn}}  \eepm 
\eeast

Thus, by Theorem \ref{thm:adjugate_inverse_matrix}.(ii), Proposition \ref{pro:square_elementary_matrix_invertible} and Proposition \ref{pro:determinant_element_matrix} ($\det\bb{C_{i,j,\lm}} = 1$),
\beast
\adj\bb{AC_{i,j,\lm}} & = & \bepm (-1)^{1+1}\det\bb{\wh{A}_{11}} & \dots & \lm (-1)^{n+1}\det\bb{\wh{A}_{n1}} \\ \vdots\\ -\lm (-1)^{1+j} \det\bb{\wh{A}_{1j}}  + (-1)^{1+i}\det\bb{\wh{A}_{1i}} & \dots & -\lm (-1)^{n+j} \det\bb{\wh{A}_{nj}} + (-1)^{n+i}\det\bb{\wh{A}_{ni}}  \\ \vdots \\ (-1)^{1+j}\det\bb{\wh{A}_{1j}} & \dots &  (-1)^{n+j}\det\bb{\wh{A}_{nj}}  \\ \vdots \\   (-1)^{1+n}\det\bb{\wh{A}_{1n}} & \dots &  (-1)^{n+n}\det\bb{\wh{A}_{nn}}  \eepm \\
& = & C_{i,j,-\lm} \adj A = \det\bb{C_{i,j,\lm}} \bb{C_{i,j,-\lm}}^{-1} \adj A= \adj C_{i,j,\lm} \adj A.
\eeast

With the same argument as above, we have $\adj\bb{C_{i,j,\lm}A} =  \adj A \adj C_{i,j,\lm}$. Therefore, for any elementary matrix $E$, we have
\be
\adj\bb{AE} =  \adj E \adj A,\quad \adj\bb{EA} =  \adj A \adj E.
\ee

Recall Lemma \ref{lem:invertible_product_elementary_matrices}, any invertible matrices $P,Q$ can be written as the products of elementary matrices $P = E_1\dots E_m$, $Q = F_1\dots F_k$. Then 
\beast
\adj\bb{PAQ} & = & \adj\bb{E_1\dots E_m A F_1\dots F_k} = \bb{\adj F_k \dots \adj F_1}\adj A \bb{\adj E_m \dots \adj E_1} \\
& = & \adj \bb{F_1\dots F_k} \adj A \adj \bb{E_1\dots E_m} = \adj Q \adj A \adj P.
\eeast
\end{proof}


\begin{theorem}\label{thm:adjugate_matrix_rank}
Let $A\in M_n(\F)$ ($n > 1$). Then
\be
\rank\bb{\adj A} = \left\{\ba{ll}
n \quad\quad & \rank(A) = n\\
1 & \rank(A) = n-1\\
0 & \text{otherwise}
\ea\right.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
If $\rank(A) = n$, $\det A \neq 0$. Then by Theorem \ref{thm:adjugate_inverse_matrix}, $\adj A$ is invertible and hence $\rank\bb{\adj A} = n$ by Theorem \ref{thm:invertible_full_rank}.

If $\rank(A) = n-1$, it is equivalent to matrix $I' = \bepm I_{n-1} & 0 \\ 0 & 0 \eepm$. That is, there exists invertible matrices $P,Q$ such that $A = PI'Q$. By Lemma \ref{lem:adjugate_invertible_product}, we have $\adj A = \adj Q \adj I' \adj P$. Since $P,Q$ are full-rank, by Proposition \ref{pro:matrix_rank_properties}.(i), we have
\be
\rank\bb{\adj A} \leq \rank\bb{\adj I'} = \rank\bepm 0 & 0 \\ 0 & 1\eepm = 1.
\ee

Also, $I' = P^{-1}A Q^{-1}$, Proposition \ref{pro:matrix_rank_properties}.(i) gives that $1 = \rank\bb{\adj I'} \leq \rank\bb{\adj A} \ \ra \ \rank\bb{\adj A} = 1$.

With similar argument, we have that $\rank\bb{\adj A} = 0$ if $\rank(A) < n-1$.
\end{proof}

\begin{proposition}
Let $A\in M_n(\F)$. Then the $k$-fold adjugate matrix of $A$, $\adj^{[k]}(A) := \adj\bb{\adj\bb{\dots \adj A \dots}}$ ($k$ times) is
\be
\adj^{[k]}(A) = \left\{\ba{ll}
\bb{\det A}^{\frac{(n-1)^k-(n-1)}{n}}\adj A \quad\quad & \rank(A) = n,\ k \text{ is odd}\\
\bb{\det A}^{\frac{(n-1)^k-1}{n}}A \quad\quad & \rank(A) = n,\ k \text{ is even}\\
0 & \rank(A) = n-1,\ k\geq 2\\
0 & \rank(A) < n-1,\ k\geq 1
\ea\right.
\ee

In particular, $\adj\bb{\adj A} = \bb{\det A}^{n-2}A$.
\end{proposition}

\begin{proof}[\bf Proof]
For $k=1$, by Theorem \ref{thm:adjugate_matrix_rank}, the conclusion holds.

For $k\geq 2$, $\adj^{[k]}(A) = 0$ if $\rank(A)\leq n-1$ by Theorem \ref{thm:adjugate_matrix_rank}. Thus, we only need to check the full-rank situation.

Now assume $\rank(A)=n$. If $k=2$, we have
\be
\adj\bb{\adj A} = \det\bb{\adj A} \bb{\adj A}^{-1} = \det\bb{\det A A^{-1}} \bb{\det A A^{-1}}^{-1} = \bb{\det A}^{n-1} \det\bb{ A^{-1}} A = \bb{\det A}^{n-2}A.
\ee

Now assume the conclusion holds for $k$. If $k$ is odd, $\adj^{[k]}(A) = \bb{\det A}^{\frac{(n-1)^k-(n-1)}{n}}\adj A$. Then by Proposition \ref{pro:adjugate_matrix_basic_properties}.(ii) and the case $k=2$,
\beast
\adj^{[k+1]}(A) & = & \adj\bb{\bb{\det A}^{\frac{(n-1)^k-(n-1)}{n}}\adj A} = \bb{\bb{\det A}^{\frac{(n-1)^k-(n-1)}{n}}}^{n-1} \adj^{[2]}(A)\\
& = & \bb{\det A}^{\frac{(n-1)^{k+1}-(n-1)^2}{n}} \bb{\det A}^{n-2}A = \bb{\det A}^{\frac{(n-1)^{k+1}-1}{n}}A.
\eeast

If $k$ is even, $\adj^{[k]}(A) = \bb{\det A}^{\frac{(n-1)^k-1}{n}} A$. Then by Proposition \ref{pro:adjugate_matrix_basic_properties}.(ii),
\beast
\adj^{[k+1]}(A) & = & \adj\bb{\bb{\det A}^{\frac{(n-1)^k-1}{n}} A} = \bb{\bb{\det A}^{\frac{(n-1)^k-1}{n}}}^{n-1} \adj A = \bb{\det A}^{\frac{(n-1)^{k+1}-(n-1)}{n}} \adj A.
\eeast
\end{proof}


\begin{theorem}\label{thm:adjugate_matrix_product}
For any $A,B\in M_n(\F)$, $\adj\bb{AB} = \adj B \adj A$.
\end{theorem}

\begin{proof}[\bf Proof]
If $A,B$ are invertible, we have $AB$ is also invertible. Then by Theorem \ref{thm:determinant_product}, Proposition \ref{pro:inverse_matrix_property} and Theorem \ref{thm:adjugate_inverse_matrix},
\be
\adj\bb{AB} = \det\bb{AB}\bb{AB}^{-1} = \det A \det B B^{-1}A^{-1} = \adj B \adj A.
\ee

If $\min\bra{\rank(A),\rank(B)} < n-1$ with $\adj A = 0$ or $\adj B = 0$, then by Proposition \ref{pro:matrix_rank_properties}.(i),%\footnote{need proof}
\be
\rank(AB) \leq \min\bra{\rank(A),\rank(B)} < n-1 \ \ra \ \adj\bb{AB} = 0 = \adj B\adj A
\ee
by Theorem \ref{thm:adjugate_matrix_rank}. 

If $\min\bra{\rank(A),\rank(B)} = n-1$ and $\max\bra{\rank(A),\rank(B)} = n$, we can assume $\rank(A) = n$ and $\rank(B) = n-1$ wlog as we can take transpose of $AB$. Then by Lemma \ref{lem:adjugate_invertible_product}, we have $\adj\bb{AB} = \adj B\adj A$.

Therefore, there leaves the last case, $\rank(A) = \rank(B) = n-1$. Assume that (by Lemma \ref{lem:matrix_equivalent_to_identity})
\be
A = P\bepm I_{n-1} & 0 \\ 0 & 0 \eepm Q,\quad B = U\bepm I_{n-1} & 0 \\ 0 & 0 \eepm V
\ee
where $P,Q,U,V$ are invertible matrices and 
\be
QU = \bepm C_{(n-1)\times (n-1)} & D_{(n-1)\times 1} \\ E_{1\times (n-1)} & F_{1\times 1} \eepm.
\ee

Therefore, by Lemma \ref{lem:adjugate_invertible_product},
\beast
\adj\bb{AB} & = & \adj\bb{P \bepm I_{n-1} & 0 \\ 0 & 0 \eepm \bepm C_{(n-1)\times (n-1)} & D_{(n-1)\times 1} \\ E_{1\times (n-1)} & F_{1\times 1} \eepm \bepm I_{n-1} & 0 \\ 0 & 0 \eepm V} = \adj\bb{P \bepm C_{(n-1)\times (n-1)} & 0 \\0 & 0 \eepm V} \\
& = & \adj V \adj\bepm C_{(n-1)\times (n-1)} & 0 \\0 & 0 \eepm \adj P = \adj V \bepm 0 & 0 \\0 & \det C_{(n-1)\times (n-1)} \eepm \adj P.
\eeast

But $\adj \bepm C_{(n-1)\times (n-1)} & 0 \\ E_{1\times (n-1)} & 0 \eepm \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm = \bepm 0 & 0 \\ \alpha & \det C_{(n-1)\times (n-1)} \eepm \bepm 0 & 0 \\0 & 1 \eepm = \bepm 0 & 0 \\0 & \det C_{(n-1)\times (n-1)} \eepm$. Then we have (by Lemma \ref{lem:adjugate_invertible_product})
\beast
\adj\bb{AB} & = & \adj V \bb{\adj \bepm C_{(n-1)\times (n-1)} & 0 \\ E_{1\times (n-1)} & 0 \eepm \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm} \adj P\\
& = & \adj V \bb{\adj \bb{\bepm C_{(n-1)\times (n-1)} & D_{(n-1)\times 1} \\ E_{1\times (n-1)} & F_{1\times 1} \eepm \bepm I_{n-1} & 0 \\ 0 & 0 \eepm} \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm} \adj P\\
& = & \adj V \bb{\adj \bb{QU \bepm I_{n-1} & 0 \\ 0 & 0 \eepm} \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm} \adj P \\
& = & \adj V \adj \bb{\bepm I_{n-1} & 0 \\ 0 & 0 \eepm}\adj\bb{QU} \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm \adj P\\
& = & \adj V \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm\adj U \adj Q \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm \adj P \\
& = & \adj\bb{U  \bepm I_{n-1} & 0 \\ 0 & 0 \eepm V}\adj \bb{P \bepm I_{n-1} & 0 \\ 0 & 0 \eepm Q} = \adj B \adj A.
\eeast

Therefore, the equation holds for any $A,B\in M_n(\F)$.
\end{proof}


\begin{corollary}\label{cor:adjugate_matrix_product}
For given $k$ and any $A\in M_n(\F)$,
\ben
\item [(i)] $\adj\bb{A_1A_2\dots A_k} = \adj A_k \adj A_{k-1} \dots \adj A_1$.
\item [(ii)] $\adj\bb{A^k} = \bb{\adj A}^k$.
\een
\end{corollary}

\begin{proposition}\label{pro:adjugate_matrix_property}
For given $k \geq 1$,
\ben
\item [(i)] $\adj\bb{A^{-1}} = \bb{\adj A}^{-1}$.
\item [(ii)] $\det\bb{\adj A} = \bb{\det A}^{n-1}$ for $n\geq 2$.
\item [(iii)] $\det\bb{\adj^{[k]}(A)} = \bb{\det A}^{(n-1)^k}$.
\item [(iv)] $\det\bb{\adj\bb{A_1A_2\dots A_k}} = \prod^k_{i=1} \det\bb{\adj A_i} = \bb{\prod^k_{i=1} \det A_i}^{n-1}$.
\item [(v)] $\det\bb{\adj (A^k)} = \bb{\det \bb{\adj A}}^k = \bb{\det A}^{k(n-1)}$.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] $\adj\bb{A^{-1}} \adj A = \adj \bb{A A^{-1}} = \adj I = I$. $\adj A \adj\bb{A^{-1}} = \adj \bb{A^{-1}A} = \adj I = I$. Thus, $\adj\bb{A^{-1}}$ is the inverse of $\adj A$ by Proposition \ref{pro:inverse_matrix}.
\item [(ii)] If $\rank(A) < n$, we have $\det A = 0$ and thus $\rank\bb{\adj A} < n$ by Theorem \ref{thm:adjugate_matrix_rank}. Thus, $\det\bb{\adj A} = 0$, which satisfies the statement. 

If $A$ is full-rank, then we have $\det\bb{\det A A^{-1}} = \bb{\det A}^{n} \det\bb{A^{-1}} = \bb{\det A}^{n-1}$.

\item [(iii)] If $k = 1$, it is (ii). If $\rank(A) < n$, we have $\det A = 0 = \det\bb{\adj^{[k]}(A)}$. Now we assume the conclusion holds for $k$ and $A$ is full-rank (thus $\adj^{[k]}(A)$ is full-rank). Then
\beast
\det\bb{\adj^{[k+1]}(A)} & = & \det \bb{\adj\bb{\adj^{[k]}(A)}} = \det\bb{\det\bb{\adj^{[k]}(A)} \bb{\adj^{[k]}(A)}^{-1}} \\
& = & \bb{\det\bb{\adj^{[k]}(A)}}^{n}\det\bb{\bb{\adj^{[k]}(A)}^{-1}} = \bb{\det\bb{\adj^{[k]}(A)}}^{n-1}\\
& = & \bb{\bb{\det A}^{(n-1)^k}}^{n-1} = \bb{\det A}^{(n-1)^{k+1}}.
\eeast
\een

(iv,v) are direct results from (iii) and Corollary \ref{cor:adjugate_matrix_product}.
\end{proof}


%\section{Matrix Anaylsis}

%\section{}

%Another use of Schur's result is to make it clear that every matrix is `almost' diagonalizable in two possible interpretations of the phrase. The first says that arbitarily close to a given matrix there is a diagonalizable matrix, and the second says that any given matrix is similar to an upper triangular matrix whose off-diagonal entries are arbitarily small\cite{}.

\subsection{Similar Matrices}

\begin{definition}[similar matrices\index{similar!matrix}]\label{def:similar_matrix}
Two $n \times n$ matrices $A$, $A'$ are similar if $A' = P^{-1}AP$ for some invertible matrix $P$.
\end{definition}

\begin{proposition}
Similar matrices have the same determinant.
\end{proposition}

\begin{proof}[\bf Proof]
If $A$ and $A'$ are similar, then there exists invertible matrix $P$ such that $A' = P^{-1}AP$. Then by Theorem \ref{thm:determinant_product} and Corollary \ref{cor:determinant_inverse},
\be
\det A' = \det\bb{P^{-1}AP} = \det \bb{P^{-1}} \det A\det P = (\det A)(\det P)(\det P)^{-1} = \det A.
\ee
\end{proof}


\begin{proposition}
Similar matrices have the same trace, i.e., for $A' = P^{-1}AP$ for some invertible matrix $P$, $\tr A' = \tr A$.
\end{proposition}

\begin{proof}[\bf Proof]
By Proposition \ref{pro:trace_change_order}, 
\be
\tr A' = \tr\bb{P^{-1}AP} = \tr\bb{APP^{-1}} = \tr\bb{AI} = \tr A.
\ee
\end{proof}

\section{Groups of Matrices}


\section{Matrix Calculus}

\section{Problems}

\section{Summary}

\begin{center}
\begin{tabular}{cccccccc}
\hline
 & matrix  & rank & determinant & trace & & & \\\hline 
equivalence & $m \times n$ & $\surd$ & - & - &  & & \\
similarity & $n\times n$ & $\surd$ & $\surd$ & $\surd$  &  & & \\ 
\hline 
\end{tabular}
\end{center}

