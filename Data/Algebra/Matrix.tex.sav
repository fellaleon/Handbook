\chapter{Matrix}

%\section{Matrix}

\section{Basic Definitions and Properties}

\subsection{Definitions}

\begin{definition}[matrix\index{matrix}]\label{def:matrix}
An $m\times n$ matrix over $\F$ is an array $A = (a_{ij})$ having $m$ rows and $n$ columns with entry\index{entry!matrix} $a_{ij} \in \F$ for $1 \leq  i \leq  m$, $1 \leq  j \leq  n$. Write
$M_{m,n}(\F)$ for the set of all $m \times n$ matrices over $\F$. If $m=n$, we can simply write $M_n(\F)$.
\end{definition}

\begin{remark}
As the notation in section \ref{sec:vector_space}, we have that $\F$ will almost always be the real numbers $\R$ or the complex numbers $\C$ under the usual addition and multiplication, but it could be the rational numbers, the integers modulo a specified prime number, or some other field.
\end{remark}

\begin{example}
\be
A = \bepm 1 & 0 \\ 0 & 1 \eepm,\quad B = \bepm 0 & 1\\ 1 & 0 \eepm, \quad C = \bepm 1 & 2 & 3 \\ 4 & 5 & 6\eepm.
\ee
\end{example}


\begin{definition}
We say two matrics $A = (a_{ij})$, $B = (b_{ij})$ are same, $A =B$ if
\be
a_{ij} = b_{ij},\ \forall i,j.
\ee
\end{definition}


\begin{definition}[partitioned matrix\index{partitioned matrix}]\label{def:partitioned_matrix}
For matrix $P \in M_{m+n,p+q}(\F)$ with $m,n,p,q\in \Z^+$, we can denote $P$ as \be P = \bepm A & B \\ C & D \eepm \ee where $A\in M_{m,p}(\F)$, $B\in M_{m,q}(\F)$, $C\in M_{n,p}(\F)$ and $D\in
M_{n,q}(\F)$.
\end{definition}

\begin{example}\label{exa:partitioned_matrix}
For matrix
\be
P = \bepm 1 & 2 & 3 \\  4 & 5 & 6 \\ 7 & 8 & 9 \eepm := \bepm P_{11} & P_{12} \\ P_{21} & P_{22} \eepm,
\ee

we can have that
\be
P_{11} = \bepm 1 & 2 \\ 4 & 5 \eepm,\quad P_{12} = \bepm 3 \\ 6 \eepm,\quad P_{21} = \bepm 7 & 8 \eepm ,\quad P_{22} = \bb{9}.
\ee
\end{example}


\begin{definition}[submatrix\index{submatrix}]\label{def:submatrix}
Let $A\in M_{m,n}(\F)$. For index sets $\alpha \subseteq \bra{1,\dots,m}$ and $\beta \subseteq \bra{1,\dots,n}$, we denoted the (sub)matrix that lies in the rows of $A$ indexed by $\alpha$ and the
columns indexed by $\beta$ as $A_{\alpha,\beta}$.

Also, the submatrix of deleting the rows indicated by $\alpha$ and the columns indicated by $\beta$ is denoted as $\wh{A}_{\alpha,\beta}$.
\end{definition}

\begin{example}
For $\alpha = \bra{1,3}$ and $\beta = \bra{1,2}$
\be
A = \bepm 1 & 2 & 3\\ 4 & 5 & 6 \\ 7 & 8 & 9 \eepm,\qquad \ra \quad A_{\alpha,\beta} = \bepm  1 & 2 \\ 7 & 8 \eepm.
\ee
\end{example}





\subsection{Matrix operations}


\begin{definition}[addition of matrices]
Let $A,B\in M_{m,n}(\F)$. Matrix addition is defined entry-wise for arrays for the same dimensions and is denoted by $A+B$, i.e., $A+B = (a_{ij} + b_{ij})$.
\end{definition}

\begin{remark}
Addition of matrices corresponds to addition of linear transformations (relative to the same basis), and it inherits commutativity and associativity from the scalar field. The zero matrix (all
entries zero) is the identity under addition.
\end{remark}

\begin{example}
\be
A = \bepm 1 & 2 \\ 3 & 4 \eepm,\quad B = \bepm 3 & 4 \\ 5 & 6 \eepm \quad \ra \quad A+B  = \bepm 4 & 6 \\ 8 & 10 \eepm.
\ee
\end{example}

\begin{definition}[scale of matrices]
Let $A\in M_{m,n}(\F)$. The scale of matrix is defined entry-wise for arrays multiplied by the scalar $\lm\in \F$, i.e., $\lm A = \bb{\lm a_{ij}}$.
\end{definition}

\begin{example}
\be
A = \bepm 1 & 2 \\ 3 & 4 \eepm,\quad \lm = 2 \quad \ra \quad \lm A =  \bepm 2 & 4 \\ 6 & 8 \eepm.
\ee
\end{example}



\begin{proposition}\label{pro:matrix_dimension}
$M_{m,n}(\F)$ is a vector space under operations \beast
(a_{ij}) + (b_{ij}) & = & (a_{ij} + b_{ij})\\
\lm(a_{ij}) & = & (\lm a_{ij}),\qquad \lm\in \F \eeast with $\dim_\F M_{m,n}(\F) = mn$.
\end{proposition}

\begin{proof}[\bf Proof]
Obviously, $M_{m,n}(\F)$ is a vector space by Definition \ref{def:vector_space}.

We now prove the dimension claim. For $1 \leq  i \leq  m$, $1 \leq  j \leq  n$ define \be E_{ij} = \left\{\ba{ll}
e_{i'j'} = 1 & (i', j') = (i, j) \\
e_{i'j'} = 0 \quad\quad & (i', j') \neq (i, j). \ea\right. \ee

This is a natural basis and thus the dimension is $mn$.%\footnote{details needed.}.
\end{proof}


\begin{definition}[multiplication of matrices\index{multiplication of matrices}]\label{def:multiplication_matrices}
Let $A\in M_{m,n}(\F)$ and $B\in M_{n,l}(\F)$. Then $AB = C = \bb{c_{ij}} \in M_{m,l}(\F)$ with elements
\be
c_{ij} = \sum^{n}_{k=1} a_{ik}b_{kj},\qquad 1\leq i\leq m, 1\leq j\leq l.
\ee
\end{definition}

\begin{remark}
Note that the column number of $A$ and row number of $B$ must be consistent.

If $B_i$ denotes the $i$th column of the matrix $B$, then the $j$th column of the product $AB$ is just $AB_i$.

If $A_j$ denotes the $j$th row of the matrix $A$, then the $j$th row of the product $AB$ is just $A_jB$.

If $A\in M_{m,n}(\F)$ and $x\in \F^n$, then $Ax$ is a linear combination of the columns of $A$.

If $A\in M_{m,n}(\F)$ and $y\in \F^m$, then $y^TA$ is a linear combination of the rows of $A$.
\end{remark}

\begin{proposition}[associativity of multiplication of matrices]\label{pro:associativity_multiplication_matrix}
Let $A\in M_{m,n}(\F)$, $B \in M_{n,p}(\F)$ and $C \in M_{p,q}(\F)$. Then
\be
A(BC) = (AB)C.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
We have
\beast
\bb{A(BC)}_{ij} & = & \sum_k a_{ik}(BC)_{kj} = \sum_k \sum_l a_{ik} b_{kl}c_{lj}\\
\bb{(AB)C}_{ij} & = & \sum_k (AB)_{ik}C_{kj} = \sum_k \sum_l a_{il} b_{lk}c_{kj}
\eeast

Then we have the required result by switching $k$ and $l$.
\end{proof}

\begin{remark}
Note that multiplication of matrices are not, in general, commutative, i.e. $AB \neq BA$, but it can be commutative when restricted to certain subsets of $M_n(\F)$.
\end{remark}

\begin{example}
\be
\bepm 1 & 0 \\ 0 & 2 \eepm \bepm 1 & 2 \\ 3 & 4 \eepm = \bepm 1 & 2 \\ 6 & 8\eepm \neq \bepm 1 & 4 \\ 3 & 8 \eepm =  \bepm 1 & 2 \\ 3 & 4 \eepm\bepm 1 & 0 \\ 0 & 2 \eepm.
\ee
\end{example}

\begin{proposition}[matrix multiplication is distributive over matrix addition]
For $A\in M_{m,n}(\F)$ and $B,C\in M_{n,l}(\F)$, $A(B+C) = AB + BC$.
\end{proposition}

\begin{proof}[\bf Proof]
By the distributive property of field $\F$, we have
\be
\bb{A(B+C)}_{ij} = \sum_k a_{ik}(B+C)_{kj} = \sum_k \bb{a_{ik}b_{kj} + a_{ik}c_{kj}} = \sum_k \bb{a_{ik}b_{kj} + \sum_k a_{ik}c_{kj}} = (AB)_{ij} + (AC)_{ij},
\ee
as required.
\end{proof}

\begin{proposition}\label{pro:component_wise_conjugate_matrix}
For $A\in M_{m,n}(\F)$ and $B\in M_{n,l}(\F)$, $\ol{AB} = \ol{A}\ol{B}$ where $\ol{A}$ is the component-wise conjugate of $A$.
\end{proposition}

\begin{proof}[\bf Proof]
By the conjugate property on $\C$, we have
\be
\bb{\ol{AB}}_{ij} = \ol{\sum_k a_{ik} b_{kj}} = \sum_k \ol{a_{ik}}\cdot \ol{b_{kj}} = \ol{A}\ol{B},
\ee
as required.
\end{proof}

\begin{definition}[singular\index{singular!matrix}, nonsingular\index{nonsingular!matrix}]\label{def:singularity_matrix}
A linear transformation or matrix $A\in M_{m,n}(\F)$ is said to be nonsingular if it produces the output 0 only for the input 0.

Otherwise, it is singular, i.e. \be A \text{ is singular }\ \ra \ Ax = 0 = 0x \quad\text{for some }0 \neq x \in \C^n. \ee
\end{definition}

\begin{remark}
If $A\in M_{m,n}(\F)$ and $m<n$, then $A$ is necessarily singular\footnote{details needed.}%\footnote{This statement is implies by the similar methods in proof of Proposition \ref{pro:invertible_non_singular_equivalent}}. %We can find $(x_1^T,x_2^T)^T\in \F^n$ with $0 = x_1 \in \F^m$ and $ 0\neq x_2 \in \F^{n-m}$ such that be A x = A \bepm x_1 \\ x_2 \eepm\ee
\end{remark}

\begin{definition}[power matrix\index{power matrix}]\label{def:power_matrix}
For matrix $A\in M_n(\F)$ and given integer $k\geq 1$. Then $B = A^k$ is called the $k$th power matrix of $A$.
\end{definition}

\begin{definition}[polynomial of matrix]
Let $p(x)$ be a polynomial of $x$ with \be p(x) = a_n x^n + \dots + a_1 x + a_0,\qquad a_0,a_1,\dots,a_n\in \sF. \ee

Then for $A\in M_n(\F)$, we have polynomial of matrix \be p(A) = a_n A^n + \dots + a_1 A + a_0 I. \ee

Note that $p(A)$ is also a square matrix.
\end{definition}




\begin{definition}[root matrix\index{root matrix}]\label{def:root_matrix}
For matrix $A\in M_n(\F)$ and given integer $k\geq 1$. If there exists $B\in M_n(\F)$ such that $B^k = A$, then $B$ is called the $k$th root matrix of $A$.
\end{definition}

By matrix multiplication (Definition \ref{def:multiplication_matrices}), we have the following definition of partitioned matrix multiplication.

\begin{definition}[partitioned matrix multiplication\index{partitioned matrices multiplication}]\label{def:partitioned_matrices_multiplication}
For matrix $A\in M_{m+n,k+l}(\F)$ and $B\in M_{k+l,p+q}(\F)$ with
\be
A = \bepm A_{11} & A_{12} \\  A_{21} & A_{22} \eepm,\qquad B = \bepm B_{11} & B_{12} \\  B_{21} & B_{22} \eepm
\ee
where
\beast
& & A_{11}\in M_{m,k}(\F),\quad A_{12}\in M_{m,l}(\F),\quad A_{21}\in M_{n,k}(\F),\quad A_{22}\in M_{n,l}(\F),\\
& & B_{11}\in M_{k,p}(\F),\quad B_{12}\in M_{k,q}(\F),\quad B_{21}\in M_{l,p}(\F),\quad B_{22}\in M_{l,q}(\F).
\eeast

Thus, for $C\in M_{m+n,p+q}(\F)$,
\be
C = AB = \bepm A_{11}B_{11} + A_{12}B_{21} & A_{11}B_{12} + A_{12}B_{22} \\ A_{21}B_{11} + A_{22}B_{21} & A_{21}B_{12} + A_{22}B_{22} \eepm = \bepm C_{11} & C_{12} \\  C_{21} & C_{22} \eepm.
\ee
\end{definition}

\begin{example}
For matrices $A,B\in M_4(\R)$ with $2\times 2$ partitioned matrices,
\be
A = \bepm 1 & 2 & 3 & 4 \\  2 & 3 & 4 & 5 \\ 3 & 4 & 5 & 6 \\ 4 & 5 & 6 & 7\eepm := \bepm A_{11} & A_{12} \\ A_{21} & A_{22} \eepm,  \qquad
B = \bepm 1 & 1 & 2 & 2 \\  1 & 1 & 2 & 2 \\ 3 & 3 & 4 & 4 \\ 3 & 3 & 4 & 4\eepm := \bepm B_{11} & B_{12} \\ B_{21} & B_{22} \eepm,
\ee

we have
\beast
C_{11} & = & A_{11}B_{11} + A_{12}B_{21} = \bepm 1 & 2 \\ 2 & 3 \eepm \bepm 1 & 1 \\ 1 & 1 \eepm +  \bepm 3 & 4 \\ 4 & 5 \eepm \bepm 3 & 3 \\ 3 & 3 \eepm = \bepm 3 & 3 \\ 5 & 5 \eepm +  \bepm 21 & 21 \\ 27 & 27 \eepm = \bepm 24 & 24 \\ 32 & 32 \eepm \\
C_{12} & = & A_{11}B_{12} + A_{12}B_{22} = \bepm 1 & 2 \\ 2 & 3 \eepm \bepm 2 & 2 \\ 2 & 2 \eepm +  \bepm 3 & 4 \\ 4 & 5 \eepm \bepm 4 & 4 \\ 4 & 4 \eepm = \bepm 6 & 6 \\ 10 & 10 \eepm +  \bepm 28 & 28 \\ 36 & 36 \eepm = \bepm 34 & 34 \\ 46 & 46 \eepm \\
C_{21} & = & A_{21}B_{11} + A_{22}B_{21} = \bepm 3 & 4 \\ 4 & 5 \eepm \bepm 1 & 1 \\ 1 & 1 \eepm +  \bepm 5 & 6 \\ 6 & 7 \eepm \bepm 3 & 3 \\ 3 & 3 \eepm = \bepm 7 & 7 \\ 9 & 9 \eepm +  \bepm 33 & 33 \\ 39 & 39 \eepm = \bepm 40 & 40 \\ 48 & 48 \eepm \\
C_{12} & = & A_{21}B_{12} + A_{22}B_{22} = \bepm 3 & 4 \\ 4 & 5 \eepm \bepm 2 & 2 \\ 2 & 2 \eepm +  \bepm 5 & 6 \\ 6 & 7 \eepm \bepm 4 & 4 \\ 4 & 4 \eepm = \bepm 14 & 14 \\ 18 & 18 \eepm +  \bepm 44 &
44 \\ 52 & 52 \eepm = \bepm 58 & 58 \\ 70 & 70 \eepm
\eeast
which is consistent with $C = AB$.
\end{example}


\subsection{Transpose}

\begin{definition}[transpose\index{transpose!matrix}]\label{def:transpose_matrix}
If $A \in M_{m,n}(\F)$ with $A = (a_{ij})$, then its transpose $A^T$ is definite to be a $n\times m$ matrix with
\be
A^T = (a_{ji}) \in M_{n,m}(\F).
\ee
\end{definition}

%\begin{remark}\end{remark}

\begin{example}
\ben
\item [(i)]
\be
\bepm 1 & 2\\ 3 & 4 \\ 5 & 6 \eepm^T = \bepm 1 & 3 & 5\\ 2 & 4 & 6 \eepm.
\ee
\item [(ii)] If $x = \bepm x_1\\ x_2 \\ \vdots \\ x_n \eepm$ is column vector, $x^T = \bb{x_1, x_2, \dots , x_n }$ is a row vector. \een
\end{example}

\begin{proposition}\label{pro:matrix_multiple_transpose}
Let $A \in M_{m,n}(\F)$ and $B \in M_{n,l}(\F)$. Then
\be
\bb{A^T}^T = A,\qquad (AB)^T = B^T A^T.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
$\bb{A^T}^T = A$ is obvious. Let $A = (a_{ij})$ and $B = (b_{ij})$. Then $A^T = (a'_{ij}) = (a_{ji}) $ and $B^T = (b'_{ij}) = (b_{ji})$. Then
\be
\bb{(AB)^T}_{ij} = (AB)_{ji} = \sum_k a_{jk}b_{ki} = \sum_k b_{ki}a_{jk} =\sum_k b'_{ik}a'_{kj} = (B^TA^T)_{ij}.
\ee

Thus, we have the required result.
\end{proof}

\begin{definition}[adjoint matrix\index{adjoint matrix}]\label{def:adjoint_matrix}
For $A = \bb{a_{ij}} \in M_{m,n}(\F)$, the (Hermitian) adjoint matrix (or conjugate transpose matrix\index{conjugate transpose matrix}) of $A$ is
\be
A^\adjoint := \bb{\ol{a_{ji}}} = \bb{\ol{A}}^T%,\qquad \text{i.e.,}\  a'_{ij} = .
\ee where $\ol{A}$ is the component-wise conjugate.
\end{definition}

\begin{proposition}\label{pro:matrix_multiple_hermitian}
Let $A \in M_{m,n}(\F)$ and $B \in M_{n,l}(\F)$ ($A$ and $B$ are completx matrices).
\be
\bb{A^\adjoint}^\adjoint = A,\qquad (AB)^* = B^* A^*.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Similar arguement with Proposition \ref{pro:matrix_multiple_transpose}.
\end{proof}


\begin{remark}
For vectors $x,y\in \C^n$, we have
\be
\bb{y^*x}^* = \ol{y^* x} = x^* y = y^T \ol{x}.
\ee
\end{remark}





\subsection{Identity matrix and zero matrix}

\begin{definition}[identity matrix\index{identity matrix}]\label{def:identity_matrix}
The identity $n\times n$ matrix is defined to be
\be
I = \bepm 1 & 0 & \dots & 0\\ 0 & 1 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & 1 \eepm,
\ee
i.e., all the elements are 0 except for the diagonal elements that are 1. In particular, we can write $I_n$ for $n\times n$ identity matrix.
\end{definition}

\begin{example}
The $3\times 3$ identity matrix is given by
\be
I = \bepm 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\eepm = \bb{\delta_{ij}}
\ee
where $\delta_{ij}$ is the Kronecker delta.
\end{example}

\begin{proposition}\label{pro:identity_matrix_property}
Let $A,I \in M_n(\F)$ and $I$ is an identity matrix. Then $AI = IA = A$, i.e., identity matrix commutes with all other matrices in $M_n(\F)$.
\end{proposition}

\begin{remark}
Identity matrix is the identity under multiplication.
\end{remark}

\begin{proof}[\bf Proof]
We have
\be
(IA)_{ij} = \sum_k \delta_{ik} a_{kj} = a_{ij} = \sum_k a_{ik}\delta_{kj} = (AI)_{ij},
\ee
i.e., $IA = AI = A$.
\end{proof}

\begin{definition}[zero matrix]
Let $A\in M_{m,n}(\F)$. If each entry of $A$ is zero, then $A$ is called zero matrix.% is the matrix with all entries zero
\end{definition}

\begin{remark}
We note that the symbol 0 is used throughout the denote each of the following: the zero scalar, the zero vector (all components equal to the zero scalar), and the zero matrix (all entries of array
equal to the zero scalar).
\end{remark}

\subsection{Inverible matrices}

\begin{definition}[left inverse\index{left inverse!matrix}, right inverse\index{right inverse!matrix}]\label{def:inverse_left}
Let $A\in M_n(\F)$. Then $B$ is a left inverse of $A$ if $BA =I$ and $C$ is a right inverse of $A$ if $AC = I$.
\end{definition}

\begin{proposition}[inverse\index{inverse!matrix}, invertible matrix\index{invertible!matrix}]\label{pro:inverse_matrix}
Let $A\in M_n(\F)$. If $B$ is a left inverse of $A$ and $C$ is a right inverse of $A$ then $B=C$ and we write $B=C = A^{-1}$. Then we say that $A$ is invertible (or non-singular\index{non-singular!matrix}). Note that $A^{-1}$ is unique.
\end{proposition}

\begin{proof}[\bf Proof]
By definition of left and right inverse, we have $BA =I$ and $AC = I$. Then
\be
B = BI = B(AC) = (BA)C = IC = C.
\ee
by Proposition \ref{pro:identity_matrix_property}, \ref{pro:associativity_multiplication_matrix}.

If $B$ and $C$ are both inverses of $A$, then $B = BI = BAC = IC = C$. Hence, $A^{-1}$ is unique.
\end{proof}

\begin{remark}
This property is based on the premise that both a left inverse and right inverse exist. In general the existence of a left inverse does not necessarily guarantee the existence of a right inverse or
vice versa. However, in the case of a square matrix, the existence of a left inverse does imply the existence of a right inverse, and vice versa. The above property then implies that they are the
same matrix.

Equivalently, $A$ is invertible if the linear transformation $A$ is one-to-one, and its inverse transformation (also linear) exists. This is actually bijective.
\end{remark}

\begin{proposition}\label{pro:inverse_matrix_property}
If $A,B\in M_n(\F)$ are invertible, then
\be
\text{(i) }\bb{A^{-1}}^{-1} = A,\qquad \text{(ii) }(AB)^{-1} = B^{-1}A^{-1}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Let $C = A^{-1}$. Then
\be
\ba{l}
C A = I = CC^{-1} \\
AC = I = C^{-1}C
\ea \quad \ra \quad A = \bb{A^{-1}}^{-1}.
\ee

Also, we have
\beast
B^{-1}A^{-1}(AB) & = & B^{-1}\bb{A^{-1}A}B = B^{-1} I B = B^{-1} B = I,\\
(AB)B^{-1}A^{-1} & = & A\bb{BB^{-1}}A^{-1} = AIA^{-1} = AA^{-1} = I
\eeast
by Proposition \ref{pro:identity_matrix_property}, \ref{pro:inverse_matrix}, \ref{pro:associativity_multiplication_matrix}.
\end{proof}

\subsection{Elementary operations}%Calculations}

\begin{definition}[elementary matrix\index{elementary matrix}]\label{def:elementary_matrix}
The following are the elementary column operations on an $m \times n$ matrix over $\F$.
\ben
\item [(i)] Swap columns $i$ and $j$ (switching).
\item [(ii)] Replace column $i$ by $\lm$ column $i$, where $\lm \in \F \bs \bra{0}$ (multiplication).
\item [(iii)] Add $\lm$ column $i$ to column $j$, where $i \neq j$ and $\lm \in \F$ (addition).
\een

The corresponding elementary column operation matrices are obtained by applying these operations to $I_n$. Call these $T_{i,j}$, $M_{i,\lm}$ and $C_{i,j,\lm}$. An elementary column operation on $A$ can be performed by postmultiplying $A$ with the corresponding elementary matrix. All these operations are reversible.

Similarly for elementary row operations. We write $E_i$ for elementary (column and row) matrices.

For square matrices, the elementary (column or row) matrices of switching and multiplication are same, given by
\be
T_{i,j} =\bepm 1 & & & & & & \\ & \ddots & & & & & \\ & & 0 & & 1 & & \\ & & & \ddots & & & \\ & & 1 & & 0 & & \\ & & & & & \ddots & \\ & & & & & & 1\eepm ,\quad M_{i,\lm} = \bepm 1 & & & & & & \\ & \ddots & & & & & \\ & & 1 & & & & \\ & & & \lm & & & \\ & & & & 1 & & \\ & & & & & \ddots & \\ & & & & & & 1\eepm.
\ee

The matrices of addition is %are
\be
C_{i,j,\lm} = \bepm 1 & & & & & & & \\ & \ddots & & & & & \\ & & 1 & & \lm & & \\ & & & \ddots & & & \\ & & & & 1 & & \\ & & & & & \ddots & \\ & & & & & & 1\eepm.%, \quad C_{i,j,\lm}^T = \bepm 1 & & & & & & & \\ & \ddots & & & & & \\ & & 1 & & & & \\ & & & \ddots & & & \\ & & \lm & & 1 & & \\ & & & & & \ddots & \\ & & & & & & 1\eepm.
\ee

$AC_{i,j,\lm}$ is adding $\lm$ times column $i$ to column $j$ of $A$ and $C_{i,j,\lm}A$ is adding $\lm$ times row $j$ to row $i$ of $A$ (as $C_{i,j,\lm}^T A$ is adding $\lm$ times row $i$ to row $j$ of $A$).
\end{definition}


\begin{definition}[column echolon form]\label{def:column_echolon_form}
$A$ is an $m\times n$ matrix with the following properties is said to be in column echelon form.
\ben
\item [(i)] The highest placed non-zero entry in column $j$ is 1 in row $i_j$ , with $i_1 \leq  i_2 \leq  \dots$.
\item [(ii)] The entry in row $i_j$ and column $k$ with $k < j$ is 0.
\een
\end{definition}

\begin{example}
\be
%\bepm
%1 & 6 & 2 & 4\\
%0 & 1 & 3 & 5\\
%0 & 0 & 1 & 2
%\eepm.
\bepm
1 & 0 & 0 \\
6 & 1 & 0 \\
2 & 3 & 1 \\
4 & 5 & 2
\eepm,\qquad \bepm
0 & 0 & 0 \\
1 & 0 & 0 \\
6 & 1 & 0 \\
2 & 3 & 1
\eepm,\qquad \bepm
0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 6 & 1 & 0 \\
0 & 2 & 3 & 1
\eepm
\ee
\end{example}


\begin{lemma}\label{lem:matrix_column_echelon_elementary_column}
Any matrix $A$ can be reduced to a matrix in column echelon form by a sequence of elementary column matrices.
\end{lemma}

\begin{remark}
If $A$ is a square $n\times n$ matrix and is invertible, the equivalent column echelon form is $I_n$. This can be used to find $A^{-1}$.
\be
A \mapsto AE_1E_2 \dots E_k = I_n,\quad I_n \mapsto I_nE_1E_2 \dots E_k = A^{-1}.
\ee
\end{remark}

\begin{proof}[\bf Proof]
\footnote{need proof.}
\end{proof}

\begin{proposition}\label{pro:square_elementary_matrix_invertible}
Let $E \in M_n(\F)$ be elementary matrix. Then it is invertible.
\end{proposition}

\begin{remark}
The square elementary matrices generate the general linear group of invertible matrices\footnote{need details}.
\end{remark}

\begin{proof}[\bf Proof]
It is easy to check that the inverse matrices are
\be
\bb{T_{i,j}}^{-1} = T_{i,j},\quad \bb{M_{i,\lm}}^{-1} = M_{i,1/\lm},\quad \bb{C_{i,j,\lm}}^{-1} = C_{i,j,-\lm}.
\ee
\end{proof}

\begin{lemma}\label{lem:invertible_product_elementary_matrices}
If $A$ is an invertible $n \times n$ matrix then $A$ is a product of elementary matrices.
\end{lemma}

\begin{proof}[\bf Proof]
By the remark of Lemma \ref{lem:matrix_column_echelon_elementary_column}, $A^{-1} = E_1 \dots E_k$ is a product of elementary matrices, hence so is $A = E^{-1}_k \dots E^{-1}_1$ by Proposition \ref{pro:inverse_matrix_property}.
\end{proof}



%\section{Ranks, Determinants and Adjugate Matrices}

\subsection{Equivalent matrices}

\begin{definition}[equivalent matrices\index{equivalent!matrix}]\label{def:equivalent_matrix}
The $m\times n$ matrices $A,B \in M_{m,n}(\F)$ are equivalent if there exist invertible matrices $Q \in M_{m}(\F)$, $P \in M_{n}(\F)$ such that $B = QAP$. This defines an equivalence relation on $M_{m,n}(\F)$.
\end{definition}

\begin{remark}
Equivalent matrices arise as representing the same linear map from a space $U$ to a space $V$ of dimension $m$ and $n$ with respect to different bases.
\end{remark}


\begin{lemma}\label{lem:matrix_equivalent_to_identity}
Any $m \times n$ matrix is equivalent to $\bepm I_r & 0\\ 0 & 0\eepm$ for some $r$.

\end{lemma}

\begin{proof}[\bf Proof]
Let $A \in M_{m,n}(\F)$. Define $\alpha : \F^n \to \F^m$, $x \mapsto Ax$. With respect to the standard bases of $\F^n$, $\F^m$, $\alpha$ has matrix $A$. By Lemma \ref{lem:linear_map_equivalent_to_identity} and Lemma \ref{lem:change_of_matrix_basis}, $A$ is equivalent to $\bepm I_r & 0\\ 0 & 0\eepm$ for some $r$.
\end{proof}

\begin{proposition}[equivalence is equivalent relation]\label{pro:equivalence_matrix_is_equivalent_relation}
Equivalence is an equivalent relation on $M_{m,n}(\F)$, i.e., equivalence is for $A,B,C\in M_{m,n}(\F)$,
\ben
\item [(i)] reflexive: $A$ is equivalent to $A$
\item [(ii)] symmetric: $A$ is equivalent to $B$ $\ \ra\ $ $B$ is equivalent to $A$.
\item [(iii)] transitive: $A$ is equivalent to $B$, $B$ is equivalent to $C$ $\ \ra\ $ $A$ is equivalent to $C$.
\een
\end{proposition}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
\ben
\item [(i)] It is obvious that invertible matrice $P = I_n$ and $Q = I_m$ such that $A = I_mAI_n = QAP$.
\item [(ii)] If $A$ is equivalent to $B$, there exist invertible matrices $P$ and $Q$ such that $B = QAP$. Thus, for $P' = P^{-1}$ and $Q' = Q^{-1}$, $A = Q'QAPP'^{-1} = Q'BP$. Hence, $B$ is equivalent to $A$.
\item [(iii)] If $A$ is equivalent to $B$ and $B$ is equivalent to $C$, there exist invertible matrices $P,Q,U,V$ such that $B = QAP$ and $C = VBU$. Thus, for invertible matrices $R = PU$ and $S = VQ$,
\be
C = VBU = VQ A PU = S A R \ \ra \ A \text{ is equivalent to } C.
\ee
\een
\end{proof}





\subsection{Ranks}

\begin{definition}[column space\index{column space!matrix}]\label{def:column_space_matrix}
The column space (sometimes called the range) of a matrix is the set of all possible linear combinations of its column vectors.
Let $A \in M_{m,n}(\F)$. The column space of $A$ is the subspace of $\F^m$ generated by the column vectors of $A$, denoted by $\colsp(A)$.
\end{definition}

\begin{definition}[column rank\index{rank!column, matrix}]\label{def:column_rank_matrix}
The (column) rank of $A$, denoted $\rank(A)$ is the dimension of the column space of $A$. %C(A) of a matrix (sometimes called the range of a matrix) is the set of all possible linear combinations of its column vectors. The column space of an m × n matrix is a subspace of m-dimensional Euclidean space. The dimension of the column space is called the rank of the matrix.[1]
\end{definition}

\begin{remark}
For $A\in M_{m,n}(\F)$, $\rank(A) = n$ iff $A$ is nonsingular\footnote{detail needed.}.
\end{remark}

\begin{proposition}\label{pro:equivalent_rank}
The matrices $A,A'\in M_{m,n}(\F)$ are equivalent if and only if $\rank(A) = \rank(A')$.
\end{proposition}

\begin{proof}[\bf Proof]
($\ra$). Assume $A,A'$ are equivalent such that $A' = Q^{-1}AP$ with $Q, P$ invertible. Let $\alpha : \F^n \to \F^m$, $x \mapsto Ax$. Then $\alpha$ with respect to standard bases $B,C$ has matrix $A$. Let $B'$ be the set of columns of $P$, let $C'$ be the set of columns of $Q'$. Then $[\alpha]_{B',C'} = Q^{-1}AP$ by Lemma \ref{lem:change_of_matrix_basis} since $P$ and $C$ are the change of basis matrices from $B$ to $B'$ and $C$ to $C'$, respectively.

So $[\alpha]_{B',C'} = Q^{-1}AP =A'$ and $[\alpha]_{B,C} = A$. Then by Lemma \ref{lem:rank_linear_map_matrix}, $\rank(A') = \rank(\alpha) = \rank(A)$.

($\la$). We have that $A$ and $A'$ are equivalent to
\be
\bepm I_r & 0\\ 0 & 0\eepm\quad \text{ and }\quad \bepm I_{r'} & 0\\ 0 & 0\eepm
\ee
for some $r$ and $r'$, respectively by Lemma \ref{lem:matrix_equivalent_to_identity}. By the first part, $\rank(A) = r$, $\rank(A') = r'$. Since $\rank(A) = \rank(A')$ we have $r = r'$ so $A$ and $A'$ are equivalent by transitivity of the equivalence relation.
\end{proof}


\begin{definition}[row space\index{row space!matrix}]\label{def:row_space_matrix}
The row space of a matrix is the set of all possible linear combinations of its row vectors. Let $A \in M_{m,n}(\F)$. The row space of $A$ is the subspace of $\F^n$ generated by the row vectors of $A$, denoted by $\rowsp(A)$.
\end{definition}


\begin{definition}[row rank\index{rank!row, matrix}]\label{def:row_rank_matrix}
The row rank of $A$, denoted $\rowrk(A)$ is the dimension of the column space of $A$. That is, $\rowrk(A) = \dim \rowsp(A) = \rank(A^T)$.
\end{definition}

\begin{theorem}\label{thm:rank_matrix_transpose}
For $A \in M_{m,n}(\F)$, $\rowrk(A) = \rank(A)$. That is,  $\rank(A^T) = \rank(A)$.
\end{theorem}

\begin{proof}[\bf Proof]
Let $A \in M_{m,n}(\F)$, let $r = \rank(A)$. Then $A$ is equivalent to $\bepm I_r & 0\\ 0 & 0 \eepm_{m\times n}$ by Lemma \ref{lem:matrix_equivalent_to_identity}, so $\bepm I_r & 0\\ 0 & 0\eepm_{m\times n} = QAP$ for some invertible matrices $Q, P$ by Definition \ref{def:equivalent_matrix}. Consider the transpose, by Proposition \ref{pro:matrix_multiple_transpose},
\be
P^TA^TQ^T = \bepm I_r & 0\\ 0 & 0\eepm_{n\times m} .
\ee

So $\spa(A^T)$ is equivalent to $\bepm I_r & 0\\ 0 & 0\eepm_{n\times m}$. Visibly, $\rank\bepm I_r & 0\\ 0 & 0\eepm_{n\times m} = r$, so
\be
\rowrk(A) = \rank(A^T ) = \rank\bepm I_r & 0\\ 0 & 0\eepm_{n\times m} = r = \rank(A).
\ee
\end{proof}


%\begin{proposition}\label{pro:invertible_equivalent}
%Let $A\in M_n(\F)$. The following statements are equivalent:
%\ben
%\item [(i)] $A$ is invertible.
%    A is row-equivalent to the n-by-n identity matrix In.
%    A is column-equivalent to the n-by-n identity matrix In.
%    A has n pivot positions.
%    det A ≠ 0. In general, a square matrix over a commutative ring is invertible if and only if its determinant is a unit in that ring.
%    rank A = n.
%    The equation Ax = 0 has only the trivial solution x = 0 (i.e., Null A = {0})
%   The equation Ax = b has exactly one solution for each b in Kn.
%\item [(ii)] The columns of $A$ are linearly independent.
%    The columns of A span Kn (i.e. Col A = Kn).
%\een
%\end{proposition}

%\begin{proof}[\bf Proof]
%\footnote{need proof}
%\end{proof}

\begin{theorem}\label{thm:invertible_full_rank}
If $A\in M_n(\F)$, then $A$ is invertible iff $\rank(A) = n$.%, i.e., the columns of $A$ are linearly independent.
\end{theorem}

\begin{proof}[\bf Proof]
($\ra$). If $A$ is invertible, then we have invertible matrix $P$ such that $AP = I_n$. This means $A$ is equivalent to $I_n$. Then by Proposition \ref{pro:equivalent_rank}, we have $\rank(A) =\rank(I_n) = n$.

($\la$). Approach 1. If $\rank(A)$, we have $A$ is equivalent to $I_n$ by Lemma \ref{lem:matrix_equivalent_to_identity}. Thus, there exist invertible matrices $P,Q$ such that $Q^{-1}AP = I_n$ by Definition \ref{def:equivalent_matrix}. Therefore, we have $A = QP^{-1}$ and $A^{-1} = PQ^{-1}$ by Proposition \ref{pro:inverse_matrix_property}. Thus, $A$ is invertible as
\be
AA^{-1} = QP^{-1}PQ^{-1} = QQ^{-1} = I = PP^{-1} = PQ^{-1}QP^{-1} = A^{-1}A.
\ee

Approach 2. Since $\rank(A) =n$, we have that the corresponding linear map $\alpha$ is surjective. Then by Corollary \ref{cor:same_dimension_linear_map_equivalent}, we have that the linear map is bijective (since $\dim V = n=m = \dim W$). Thus, the corresponding inverse matrix $A^{-1}$ exists and therefore $A$ is invertible.
\end{proof}

\begin{lemma}\label{lem:full_rank_linearly_independent}
Let $A\in M_n(\F)$. Then $\rank(A) = n$ iff the columns of $A$ are linearly independent.
\end{lemma}

\begin{proof}[\bf Proof]
($\ra$). If $\rank(A) = n$, we have that the columns of $A$ are linearly independent by Lemma \ref{lem:finite_basis_linearly_independent_spanning_equivalent} as the columns span the column space.

($\la$). If the columns of $A$ are linearly independent, then by Lemma \ref{lem:independent_spanning_basis}, the columns of $A$ is a basis. Furthermore, by definition of dimension (Theorem \ref{thm:dimension_vector_space}), $\rank(A) = n$.
\end{proof}

\begin{corollary}\label{cor:invertible_column_linearly_independent}
If $A\in M_n(\F)$, then $A$ is invertible iff the columns of $A$ are linearly independent.
\end{corollary}

\begin{proof}[\bf Proof]
Direct result from Lemma \ref{thm:invertible_full_rank} and \ref{lem:full_rank_linearly_independent}.
\end{proof}


\begin{proposition}\label{pro:invertible_non_singular_equivalent}
A square matrix $A\in M_n(\F)$ is invertible if and only if $A$ is nonsingular.% (that is, $0\in \sigma(A)$ by Definition \ref{def:singularity_matrix}).
\end{proposition}

\begin{remark}
So singular matrix and invertible matrix are the same term from now on.
\end{remark}
%
\begin{proof}[\bf Proof]
Let $A_1,\dots,A_n$ be the column of $A$, then for $0\neq x = (x_1,\dots,x_n)^T\in \F^n$ we have \beast
\text{$A$ is singular} & \lra & A x = 0 \ \lra\ A_1 x_1 + A_2 x_2 + \dots + A_n x_n = 0 \\
& \lra & \text{the columns of $A$ are linearly dependent}\qquad \text{(by Definition \ref{def:linearly_independent_vector})}\\
& \lra & A\text{ is not invertible}\qquad\qquad  \text{(by Corollary \ref{cor:invertible_column_linearly_independent})},
\eeast%\footnote{proof needed. needed definition of singular and the fact singularity equals to invertibility.}
as required.
\end{proof}




\begin{proposition}[rank inequalities]\label{pro:matrix_rank_properties}
Let $A\in M_{m,n}(\F)$. Then
\ben
\item [(i)] If $B\in M_{n,k}(\F)$, $\rank(AB) \leq \min\bra{\rank(A), \rank(B)}$.
\item [(ii)]
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] We know that $\colsp(AB)$ is generated by the column vectors of $AB$ and it is also generated by the column vectors of $A$. That is, $\forall X\in \colsp(AB)$, we can find $C = \bepm c_1,\dots,c_k\eepm^T \in \F^k$ such that $X = c_1 (AB)_1 + \dots + c_k (AB)_k = (AB) C = A(BC) \in \colsp(A)$. Thus, $\colsp(AB)\subseteq \colsp(A)$. Then by the definition of rank (Definition \ref{def:column_rank_matrix}), we have $\rank(AB) \leq \rank(A)$.   %\footnote{prove $\rank(AB) \leq \rank(A)$ and then use rank of matrix transpose}.

Also, we have $\rank(B^TA^T) \leq \rank(B^T)$. Then by Theorem \ref{thm:rank_matrix_transpose} and Proposition \ref{pro:matrix_multiple_transpose},
\be
\rank(AB) = \rank\bb{(AB)^T} = \rank(B^TA^T) \leq \rank(B^T) = \rank(B) \ \ra \ \rank(AB) \leq \min\bra{\rank(A), \rank(B)}.
\ee

\item [(ii)] \footnote{need proof}
\een
\end{proof}

%\subsection{Singularity}


\begin{proposition}[rank equalities]\label{pro:rank_equalities}
\ben
\item [(i)] If $A\in M_{m,n}(\F)$, $\rank\bb{A^*} = \rank\bb{A^T} = \rank\bb{\ol{A}} = \rank(A)$.
\item [(ii)] If $A\in M_m(\F)$ and $C\in M_n(\F)$ are nonsingular and $B\in M_{m,n}(\F)$, then
\be
\rank\bb{AB} = \rank\bb{B} = \rank\bb{BC} = \rank\bb{ABC}.
\ee

That is, rank is unchanged upon left or right multiplication by a nonsingular matrix.

\item [(iii)] If $A,B\in M_{m,n}(\F)$, then $\rank\bb{A} = \rank\bb{B}$ if and only if there exist nonsingular $X\in M_m(\F)$ and $Y\in M_n(\F)$ such that $B = XAY$.

\item [(iv)] If $A\in M_{m,n}(\C)$, $\rank\bb{A^*A} = \rank\bb{A}$.

\item [(v)] If $A\in M_{m,n}(\F)$ has rank $k$, then we can write $A = XBY$ where $X\in M_{m,k}(\F)$, $Y \in M_{k,n}(\F)$ and $B\in M_k(\F)$ is nonsingular.

In particular, a matrix $A$ that has rank 1 may always be written in the form $A = xy^T$ for some $x\in \F^m$, $y\in \F^n$.
\een
\end{proposition}

\begin{remark}
Note that $\rank\bb{A^2} \neq \rank\bb{A}$ in general (comparing to (iv)) as we can see that
\be
A = \bepm 0 & 1 \\ 0 & 0 \eepm \ \ra \ A^2 = \bepm 0 & 0 \\ 0 & 0 \eepm \ \ra \ \rank\bb{A^2} = 0 \neq 1 = \rank\bb{A}.
\ee
\end{remark}

\begin{proof}[\bf Proof]
\ben
\item [(i)] If $\rank(A)= m$, then (by Lemma \ref{lem:finite_basis_linearly_independent_spanning_equivalent}) there are at most $m$ linearly independent columns $A_1,\dots,A_m$ of $A$ such that
\be
\ol{\alpha_1} A_1 + \dots + \ol{\alpha}_m A_m = 0 \ \ra \ \alpha_1 = \alpha_2 = \dots = \alpha_m = 0.
\ee

This gives that for the corresponding columns $\ol{A}_1,\dots,\ol{A}_m$ of $\ol{A}$,
\be
\alpha_1 \ol{A}_1 + \dots + \alpha_m \ol{A}_m = 0 \ \ra \ \alpha_1 = \alpha_2 = \dots = \alpha_m = 0.
\ee

Thus, there are at most $m$ linearly independent columns in $\ol{A}$. Therefore, $m = \rank\bb{\ol{A}}$.

\item [(ii)] (see \cite{Horn_Johnson_1990}.$P_{13}$)\footnote{proof needed.}
\een
\end{proof}


\section{Square Matrices}

\subsection{Basic square matrices}

%, Hermitian matrices and symmetric and anti-symmetic matrices

\begin{definition}[symmetric matrix\index{symmetric!matrix}]\label{def:symmetric_matrix}
A square $n\times n$ matrix $A = (a_{ij})$ is symmetric if
\be
A = A^T,\quad \text{i.e., } a_{ij} = a_{ji}.
\ee
\end{definition}

\begin{definition}[anti-symmetric matrix\index{anti-symmetric!matrix}]\label{def:antisymmetric_matrix}
A square $n\times n$ matrix $A = (a_{ij})$ is anti-symmetric if
\be
A = -A^T,\quad \text{i.e., } a_{ij} = -a_{ji}.
\ee
\end{definition}

\begin{remark}
For an anti-symmetric matrix, $a_{11} = -a_{11}$, i.e., $a_{11} = 0$. Similarly we deduce that all the diagonal elements of anti-symmetric matrices are zero, i.e., $a_{11} = a_{22} = \dots = a_{nn} = 0$.
\end{remark}



\begin{definition}[Hermitian matrix\index{Hermitian matrix}]\label{def:hermitian_matrix}
A square $n\times n$ complex matrix $A = (a_{ij})$ is Hermitian if
\be
A = A^\adjoint,\quad \text{i.e., } a_{ij} = \ol{a_{ji}}.
\ee
\end{definition}

\begin{remark}
Hermitian matrix is also called self-adjoint matrix.
\end{remark}

\begin{definition}[skew-Hermitian matrix\index{skew-Hermitian matrix}]\label{def:skew_hermitian_matrix}
A square $n\times n$ complex matrix $A = (a_{ij})$ is skew-Hermitian if
\be
A = -A^\adjoint,\quad \text{i.e., } a_{ij} = -\ol{a_{ji}}.
\ee
\end{definition}

\begin{remark}
All the diagonal elements of Hermitian and skew-Hermitian matrices are real and pure imaginary respectively.
\end{remark}

\begin{example}
\ben
\item [(i)] A symmetric $3\times 3$ matrix $S$ has the form
\be
S = \bepm a & b & c \\ b & d & e \\ c & e & f \eepm,
\ee
i.e., it has six independent elements.

\item [(ii)] An anti-symmetric $3\times 3$ matrix $A$ has the form
\be
A = \bepm 0 & a & -b \\ -a & 0 & c \\ b & -c & 0 \eepm,
\ee
i.e., it has three independent elements.
\een
\end{example}

%\subsection{Orthogonal and unitary matrices}

\begin{definition}[orthogonal matrix\index{orthogonal matrix}]\label{def:orthogonal_matrix}
$A\in M_n(\F)$ is orthogonal matrix if
\be
AA^T = I = A^TA,
\ee
i.e., if $A$ is invertible and $A^{-1} = A^T$.
\end{definition}

\begin{remark}
Note that $\det A = \pm 1$ for orthogonal matrix $A$.
\end{remark}

\begin{definition}[unitary matrix\index{unitary matrix}]\label{def:unitary_matrix}
$A \in M_n(\C)$ is said to be unitary if its adjoint matrix (or Hermitian conjugate matrix) is equal to its inverse, i.e., if
\be
U^* = U^{-1}.
\ee

In other words,
\be
U^* U = I = UU^*.
\ee
\end{definition}

\begin{remark}
Unitary matrices are to complex matrices what orthogonal matrices are to real matrices. Similar properties to those above for orthogonal matrices also hold for unitary matrices when references to real scalar products are replaced by references to complex scalar products\footnote{need definition}.
\end{remark}

%\subsection{Trace}

\begin{definition}[trace\index{trace!matrix}]\label{def:trace_matrix}
The trace of a square $n\times n$ matrix $A = (a_{ij})$ ($A\in M_n(\F)$) is equal to the sum of the diagonal elements, i.e.,
\be
\tr A = \sum^n_{i=1}a_{ii}.
\ee
\end{definition}

\begin{remark}
Note that $\tr:M_n(\F)\to \F$ is linear.
\end{remark}

\begin{proposition}\label{pro:trace_change_order}
Suppose that $A\in M_{m,n}(\F)$ and $B \in M_{n,m}(\F)$. Then $\tr(AB) = \tr(BA)$.
\end{proposition}

\begin{proof}[\bf Proof]
We have
\be
\tr(AB) = \sum_i \bb{\sum_j a_{ij}b_{ji}} = \sum_j \bb{\sum_i b_{ji}a_{ij}} = \tr(BA).
\ee
\end{proof}

\begin{definition}[commute matrices\index{commute!matrices}]\label{def:commute_matrices}
For matrices $A,B\in M_n(\F)$, we say that $A,B$ are commute if $AB = BA$. We can write this by
\be
\bsb{A,B} = 0, \quad \text{ where}\quad \bsb{A,B} := AB - BA.
\ee
\end{definition}

\begin{definition}[principal submatrix\index{principal submatrix}]\label{def:principal_submatrix}
Recalling the definition of submatrix, the submatrix $A_{\alpha,\alpha}$ of $A\in M_n(\F)$ with $\alpha \subseteq \bra{1,\dots,n}$ is called a principal submatrix of $A$ and is abbreviated $A_\alpha$.

If $\alpha = {1,2,\dots,k}$, we call $A_k := A_\alpha$ the leading principal submatrix.
\end{definition}




\subsection{Determinant}

\begin{definition}[determinant\index{determinant!matrix}]\label{def:determinant_matrix}
For $A \in M_n(\F)$ define
\be
\det A =\sum_{\sigma \in S_n} \ve(\sigma)a_{1,\sigma(1)} \dots a_{n,\sigma(n)} = \sum_{\sigma \in S_n} \ve(\sigma) \prod^n_{i=1} a_{i,\sigma(i)}
\ee
where $\ve(\sigma)$ is sign of permutation (see Definition \ref{def:sign_permutation}) and it is homomorphism by Theorem \ref{thm:sgn_homomorphism}.

Writing $A_i$ for the $i$th column of a matrix $A$ we have $A = (A_1, \dots,A_n)$, so we can think of $A$ is an $n$-tuple of columns in $\F^n$. Write $\bra{e_1, \dots, e_n}$ for the standard basis of $\F^n$.
\end{definition}

\begin{remark}
It is easy to see that $\det I = 1$.
\end{remark}

%Recall that $S_n$ is the group of all permutations of $\bra{1, \dots, n}$. The elements are permutations, the group operation is composition of permutations $(\sigma \circ \tau )(j) = \sigma (\tau (j))$. Any $\sigma$ can be written as a product of transpositions.
%\be
%\ve(\sigma) =\left\{ \ba{ll}
%+1\quad\quad & \text{if \# permutations is even,}\\
%-1 & \text{if \# permutations is odd.}
%\ea\right.
%\ee
%$\ve : S_n \to \bra{+1,-1}$ is a homomorphism.

\begin{example}
For $2\times 2$ matrix $A = \bepm a_{11} & a_{12} \\ a_{21} & a_{22}\eepm$, we have $\det A = a_{11} a_{22} - a_{12}a_{21}$.

For $3\times 3$ matrix $A = \bepm a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}\eepm$, we have
\be
\det A = a_{11} a_{22}a_{33} + a_{12}a_{12}a_{31} + a_{13}a_{32}a_{21} - _{11}a_{23}a_{32} - a_{12}a_{21}a_{33} - a_{13}a_22a_{31}.
\ee
\end{example}

\begin{definition}[volume form\index{volume form}]\label{def:volumn_form}
The function $d : \F^n \times  \dots \times \F^n \to\F$ is a volumne form on $\F^n$ if
\ben
\item [(i)] it is multilinear, i.e.
\beast
d(v_1, \dots, \lm_iv_i, \dots, v_n) & = & \lm d(v_1, \dots, v_i, \dots, v_n)\\
d(v_1, \dots, v_i + v'_i , \dots, v_n) & = & d(v_1, \dots, v_i, \dots, v_n) + d(v_1, \dots, v'_i, \dots, v_n)
\eeast
\item [(ii)] it is alternating, i.e. whenever $i\neq j$ and $v_i = v_j$ then $d(v_1, \dots, v_n) = 0$.
\een
\end{definition}

\begin{definition}[determinant function\index{determinant function}]\label{def:determinant_function}
$d$ is a determinant function if it is normalised volume form, i.e. $d(e_1, \dots, e_n) = 1$.
\end{definition}

\begin{lemma}\label{lem:swap_column_sign}
Swapping columns in a volume form changes the sign. For $i \neq j$,
\be
d(v_1, \dots, v_j , \dots, v_i, \dots, v_n) = -d(v_1, \dots, v_i, \dots, v_j , \dots, v_n).
\ee
\end{lemma}

\begin{proof}[\bf Proof]
By Definition \ref{def:volumn_form}.(ii),
\be
0 = d(v_1, \dots, v_i + v_j , \dots, v_i + v_j , \dots, v_n) = 0 + d(v_1, \dots, v_i, \dots, v_j , \dots, v_n) + d(v_1, \dots, v_j , \dots, v_i, \dots, v_n) + 0
\ee
by Definition \ref{def:volumn_form}.(i).
\end{proof}

\begin{corollary}\label{cor:volume_form_sign_permutation}
If $\sigma \in S_n$ and $d$ is a volume form on $\F^n$ then
\be
d(v_{\sigma(1)}, \dots, v_{\sigma(n)}) = \ve(\sigma)d(v_1, \dots, v_n)
\ee
for $v_1, \dots, v_n \in \F^n$. In particular,
\be
d(e_{\sigma(1)}, \dots, e_{\sigma(n)})= \ve(\sigma)d(e_1, \dots, e_n) = \ve(\sigma)
\ee
if $d$ is a determinant function.
\end{corollary}

\begin{proof}[\bf Proof]
Think about number of transpositions.
\end{proof}

\begin{theorem}\label{thm:volume_form_determinant}
If $d$ is a volume form on $\F^n$ and $A = (a_{ij}) = \bb{A_1, \dots,A_n} \in M_n(\F)$ then $d\bb{A_1, \dots,A_n} = (\det A) d(e_1, \dots, e_n)$.
\end{theorem}

\begin{proof}[\bf Proof]
\beast
d(A_1, \dots,A_n) & = & d\bb{\sum_{j_1} a_{j_1,1}e_{j_1}, A_2, \dots,A_n} = \sum_{j_1} a_{j_1,1}d\bb{e_{j_1} ,A_2, \dots,A_n}\\
& = & \sum_{j_1,j_2} a_{j_1,1} a_{j_2,2} d(e_{j_1} , e_{j_2} ,A_3, \dots,A_n) =  \sum_{j_1\neq j_2} a_{j_1,1} a_{j_2,2} d(e_{j_1} , e_{j_2} ,A_3, \dots,A_n)
\eeast
as $d$ is zero when $j_1 = j_2$ by Definition \ref{def:volumn_form}.(ii). Then for $j_1,\dots,j_n$ forming a permutation
\beast
d(A^{(1)}, \dots,A^{(n)}) = \sum_{j_1,\dots,j_n} a_{j_1,1} \dots a_{j_n,n} d(e_{j_1}, \dots, e_{j_n}) = \sum_{\sigma \in S_n} a_{\sigma(1)1} \dots a_{\sigma(n)n}\ve(\sigma)d(e_1, \dots, e_n) = (\det A)d(e_1, \dots, e_n).
\eeast
by Definition \ref{def:determinant_matrix}.
\end{proof}

\begin{theorem}\label{thm:determinant_determinant_function}
If we define $d : \F^n \times  \dots \times  \F^n \to \F,\ d(A_1, \dots,A_n) = \det A$ for $A = (A_1, \dots,A_n)$ then $d$ is a determinant function.
\end{theorem}

\begin{proof}[\bf Proof]
\ben
\item [(i)] $\prod^n_{j=1} a_{\sigma(j)j}$ is multilinear, hence so is the linear combination $\det A$.
\item [(ii)] We want to show that if $A_k = A_l$ with $k \neq l$ then $\det A = 0$. Write $\tau = (k\ l)$, a transposition in $S_n$.
\be
\det A = \sum_{\sigma \in S_n} \ve(\sigma) \prod_j a_{\sigma (j)j}
\ee

Reorder the sum, take each even permutation $\sigma$ followed by the odd permutation $\sigma \tau$ and note $\ve(\sigma ) = 1$, $\ve(\sigma \tau) = -1$. Thus
\be
\det A = \sum_{\sigma\text{ even}} \bb{\prod_j a_{\sigma (j)j} - \prod_j a_{\sigma \tau (j)j}} = 0
\ee
as each of the summands is zero since $a_{\sigma (k)k} = a_{\sigma (k)l}$, $a_{\sigma (l)k} = a_{\sigma (l)l}$ and
\be
a_{\sigma (1)1} \dots a_{\sigma (k)k} \dots a_{\sigma (l)l} \dots a_{\sigma (n)n} - a_{\sigma (1)1} \dots a_{\sigma (l)k} \dots a_{\sigma (k)l} \dots a_{\sigma (n)n} = 0.
\ee

\item [(iii)] For normal basis $\bra{e_1,\dots,e_n}$, $d(e_1,\dots,e_n) = \sum_{\sigma \in S_n} \ve(\sigma ) \prod^n_{j=1} \delta_{\sigma (j)j} = \ve(\iota) \cdot 1 = 1$.
\een

Thus, $d$ is a determinant function.
\end{proof}

\begin{proposition}\label{pro:determinant_matrix_property}
Let $A\in M_n(\F)$ and $\lm \in \F$. Then
\beast
\text{(i)} & &  \det \bb{A_1,\dots,\lm A_i, \dots, A_n} = \lm \det \bb{A_1,\dots,A_i,\dots,A_n} = \det A,\\
\text{(ii)} & &  \det \bb{A_1,\dots, A_i + A_{i}' , \dots, A_n} = \det\bb{A_1,\dots, A_i, \dots, A_n}  + \det\bb{A_1,\dots,A_{i}', \dots, A_n},\\
\text{(iii)} & &  \det \bb{A_1,\dots, A_i ,\dots, A_j , \dots, A_n} = -\det \bb{A_1,\dots, A_j ,\dots, A_i , \dots, A_n},\\
\text{(iv)} & & \text{If }A_i = A_j,\ \det \bb{A_1,\dots,A_i,\dots,A_j,\dots, A_n} = 0.
\eeast
\end{proposition}

\begin{proof}[\bf Proof]
This is direct result from Theorem \ref{thm:determinant_determinant_function}, Lemma \ref{lem:swap_column_sign}.
\end{proof}

\begin{proposition}\label{pro:determinant_transpose}
$\det A^T = \det A$.
\end{proposition}

\begin{proof}[\bf Proof]
If $\sigma \in S_n$ then $\prod^n_{j=1} a_{\sigma (j)j} = \prod^n_{j=1} a_{j\sigma (j)}$ as they contain the same factors but in a different order. Also, as $\sigma$ runs through $S_n$, so does $\sigma^{-1}$ and $\ve(\sigma^{-1}) = \ve(\sigma)$ (as $\sigma \sigma^{-1} = \iota$). Hence
\be
\det A = \sum_{\sigma \in S_n} \ve(\sigma )\prod^n_{j=1} a_{\sigma (j)j} = \sum_{\sigma \in S_n} \ve(\sigma^{-1} ) \prod^n_{j=1} a_{j\sigma^{-1}(j)} = \sum_{\sigma' \in S_n} \ve(\sigma' ) \prod^n_{j=1} a_{j\sigma' (j)} = \det A^T.
\ee
\end{proof}



%\begin{lemma}
%$\det$ is the unique multilinear alternating form in rows normalised at $I$.
%\end{lemma}

\begin{proposition}\label{pro:determinant_element_matrix}
For elementary matrices, the determinants are
\be
\det T_{ij} =  -1,\quad \det M_{i,\lm} = \lm,\quad \det C_{i,j,\lm} = 1.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Obvious from Definition \ref{def:elementary_matrix}.
\end{proof}


\begin{lemma}\label{lem:determinant_element_matrix_product}
If $E$ is an elementary matrix, then for any $n \times n$ matrix $A$,
\be
\det(AE) = \det A \det E = \det(EA).
\ee

Performing an elementary column or row operation on $A$ multiplies $\det A$ by the determinant of the corresponding elementary matrix.
\end{lemma}

\begin{proof}[\bf Proof]
By Proposition \ref{pro:determinant_element_matrix}, the determinants of the elementary matrices are %(see Definition \ref{def:elementary_matrix}) we have
\be
\det T_{ij} =  -1,\quad \det M_{i,\lm} = \lm,\quad \det C_{i,j,\lm} = 1
\ee
as we can see that the column and row operation have the same determinant by Proposition \ref{pro:determinant_transpose}.

Performing the corresponding elementary column or row operation multiplies $\det A$ by $-1, \lm, 1$, respectively. Then
\be
\det (AE) = \det A \det E = \det \bb{A^T} \det \bb{E^T} = \det \bb{A^TE^T} = \det\bb{(EA)^T} = \det(EA)
\ee
by Proposition \ref{pro:determinant_transpose}.
\end{proof}

\begin{theorem}\label{thm:matrix_invertible_determinant_non_zero}
Let $A\in M_n(\F)$. Then $A$ is invertible if and only if $\det A \neq 0$.
\end{theorem}

\begin{proof}[\bf Proof]
If $A$ is invertible then $A$ can be written as a product of elementary matrices by Lemma \ref{lem:invertible_product_elementary_matrices}, so $\det A$ is the product of the corresponding determinants, so $\det A \neq 0$.

If $A$ is singular (not invertible), we have that the columns of $A$ are linearly dependent (Corollary \ref{cor:invertible_column_linearly_independent}). Then we can obtain a 0 column as a non-trivial combination of columns of $A$, so using elementary column operations on $A$ we can obtain a matrix with a 0 column. Hence $\det A = 0$ by Lemma \ref{lem:determinant_element_matrix_product} as determinants of elementary matrices are no-zero.
\end{proof}

\begin{theorem}\label{thm:determinant_product}
If $A,B \in M_n(\F)$, then $\det(AB) = \det A \det B = \det(BA)$.
\end{theorem}

\begin{proof}[\bf Approach 1]
First fix $A$. Consider $d_A : \bb{B_1, \dots,B_n} \mapsto \det(AB)$ for $B = \bb{B_1, \dots,B_n}$. Note that $\det(AB) = d_A\bb{B_1, \dots,B_n} = d\bb{(AB)_1, \dots,(AB)_n}$ where $d$ is volume form by Theorem \ref{thm:determinant_determinant_function}. So $d_A$ is multilinear and alternating and $d_A$ is a volume form on $\F^n$. Hence
\be
\det(AB) = d_A\bb{B_1, \dots,B_n }= (\det B) d_A(e_1, \dots, e_n) = \det B \det A,
\ee
using Theorem \ref{thm:volume_form_determinant}.
\end{proof}

\begin{proof}[\bf Approach 2]
Expand by columns and by Corollary \ref{cor:volume_form_sign_permutation},
%as before.
\beast
\det(AB) & = & \det \bb{\sum^n_{j_1=1} b_{j_1,1} A_{j_1}, \dots, \sum^n_{j_n=1} b_{j_n,n}A_{j_n} } = \sum_{\sigma \in S_n} \bb{\prod^n_{j=1} b_{\sigma (j)j}} \det(A_{\sigma (1)}, \dots,A_{\sigma (n)})\\
& = & \sum_{\sigma \in S_n}\bb{\prod^n_{j=1} b_{\sigma (j)j}} \ve(\sigma ) \det(A_1, \dots,A_n) = \sum_{\sigma \in S_n}\bb{\prod^n_{j=1} b_{\sigma (j)j}} \ve(\sigma ) \det A = \det A \det B.
\eeast
\end{proof}

\begin{proof}[\bf Approach 3]
If $B$ is singular, so $\rank(B) <n$ by Theorem \ref{thm:invertible_full_rank} and let $\rank( B) = r$. Then by Lemma \ref{lem:matrix_equivalent_to_identity}, we have for some invertible matrices $P,Q$,
\be
B \sim \bepm I_r & 0 \\ 0 & 0 \eepm \ B = Q\bepm I_r & 0 \\ 0 & 0 \eepm P^{-1} = \bepm C_r & 0 \\ 0 & 0 \eepm\ \ra \ AB = A\bepm C_r & 0 \\ 0 & 0 \eepm = \bepm D_r & 0 \\ 0 & 0 \eepm
\ee
where $C_r$ and $D_r$ are $r\times r$ submatrices. Therefore, we see that $\rank(AB) \leq r \neq n$. So we have that $AB$ is singular as well by Theorem \ref{thm:invertible_full_rank}. %is $AB$ by definition of inverse. %That is if $AB$ is singular, then there exists an invertible matrix $P$ such that \be P(AB) = (AB)P = I \ \ra \ (PA)B =  \ee
Hence $\det B = 0 = \det(AB)$.

So assume $B$ is non-singular (invertible) and write it as a product of elementary matrices, $B = E_1 \dots E_k$ by Lemma \ref{lem:invertible_product_elementary_matrices}. Using Lemma \ref{lem:determinant_element_matrix_product},
\be
\det(AB) = \det(AE_1 \dots E_k) = \det A\det E_1 \dots \det E_k = \det A\det B
\ee
as required.
\end{proof}

\begin{corollary}\label{cor:determinant_inverse}
If $A$ is invertible then $\det A^{-1} = (\det A)^{-1}$.
\end{corollary}

\begin{proof}[\bf Proof]
As $A$ is invertible, $AA^{-1} = I$ so $(\det A)(\det A^{-1}) = \det I = 1$ and hence $\det A^{-1} = (\det A)^{-1}$.
\end{proof}

\begin{proposition}[derivative of a determinant]\label{pro:derivative_of_determinant}
If the entries in $A\in M_n(\F)$ are differentiable functions of $t$, then
\be
\frac{d\bb{\det A}}{dt} = \det D_1 + \det D_2 + \dots + \det D_n
\ee
where $D_i$ is identical to $A$ except that the entries in the $i$th row are replaced by their derivatives.%, i.e., [Di]k∗ =  Ak∗ if i = k, dAk∗/dt if i = k.
\end{proposition}

\begin{proof}[\bf Proof]
This follows directly from the definition of a determinant by writing (from definition of determinant)
\beast
\frac{d\bb{\det A}}{dt} & = & \sum_{\sigma \in S_n} \ve(\sigma) \frac{d\bb{\prod^n_{i=1} a_{i,\sigma(i)}}}{dt} = \sum_{\sigma \in S_n} \ve(\sigma) \sum^n_{j} \frac{d a_{j,\sigma(j)}}{dt} \prod_{i\neq j} a_{i,\sigma(i)}
\\
& = & \det D_1 + \det D_2 + \dots + \det D_n
\eeast
where $D_i$ is as assumed.
\end{proof}


Consider the system of linear equations $Ax = b$ with $m$ equations and $n$ unknowns. Here $A$ is an $m\times n$ matrix, $b$ is a column vector in $\F^m$. This has a solution if $\rank (A) = \rank(A| b)$. The solution is unique if and only if $n = \rank A = \rank(A| b)$, then the solution is $x = A^{-1}b$. To solve this equation, use Gaussian elimination\footnote{need details}. In the case $m = n$, there is another method.

\begin{lemma}[Cramer's rule\index{Cramer's rule!matrix}]\label{lem:cramer_rule}
If $A \in M_n(\F)$ is non-singular then $Ax = b$ has $x = (x_1, \dots, x_n)^T$ with $x_i = \frac 1{\det A} \det A_{ib}$ for $i = 1, \dots, n$ as its unique solution, where $A_{ib}$ is the matrix obtained from $A$ by deleting column $i$ and inserting $b$.
\end{lemma}

\begin{proof}[\bf Proof]
Assume $x$ is the solution of $Ax = b$. Then
\beast
\det A_{ib} & = &  \det\bb{A_1, \dots,A_{i-1}, b,A_{i+1}, \dots,A_n} = \det\bb{A_1, \dots,A_{i-1}, Ax ,A_{i+1}, \dots,A_n}  \\
& = & \sum_{j=1} x_j \det(A_1, \dots,A_{i-1},A_j,A_{i+1}, \dots,A_n) = x_i \det(A_1, \dots,A_{i-1},A_i,A_{i+1}, \dots,A_n) = x_i\det A,
\eeast
so $x_i = \frac 1{\det A} \det A_{ib}$ for $i = 1, \dots, n$.
\end{proof}

\begin{corollary}
If $A \in M_n(\Z)$ with $\det A = \pm 1$ and if $b \in \Z^n$ then we can solve $Ax = b$ over $\Z$.
\end{corollary}

\begin{remark}
The solution over $\Z$ is guaranteed since $\det A = \pm 1$.
\end{remark}





\subsection{Adjugate matrices}

\begin{definition}[minor\index{minor!matrix}]\label{def:minor_matrix}
Let $A \in M_n(\F)$. Write $\wh{A}_{ij}$ for the $(n - 1) \times (n - 1)$ matrix obtained from $A$ by deleting row $i$ and column $j$. Let $M_{ij} = \det \wh{A}_{ij}$. We call $M_{ij}$ the
$(i,j)$-minor of $A$.

More generally, if $\alpha, \beta \subseteq \bra{1,\dots,n}$ have the same size, then the $(\alpha,\beta)$-minor of $A$ is the determinant of submatrix $\wh{A}_{\alpha,\beta}$ (see Definition
\ref{def:submatrix}). If $\alpha = \beta$, we call $\det\bb{\wh{A}_\alpha}$ the principal minor\index{principal minor}. By convention, the $n\times n$ principal minor is 1, i.e., $\det
\wh{A}_{1,\dots,n} = \det A_\emptyset = 1$.
\end{definition}




\begin{lemma}[Laplace expansion\index{Laplace expansion}]\label{lem:determinant_minor_matrix}
Let $A\in M_n(\F)$ and $M$ is the minor matrix of $A$.
\ben
\item [(i)] For fixed $j$, $\det A = \sum^n_{i=1} (-1)^{i+j} a_{ij} M_{ij}$.
\item [(ii)] For fixed $i$, $\det A = \sum^n_{j=1} (-1)^{i+j} a_{ij} M_{ij} $.
\een

This is called Laplace expansion, which can be used as a definition of determinant.
\end{lemma}



\begin{proof}[\bf Proof]
By Proposition \ref{pro:determinant_matrix_property},
\beast
\det A & = & \det(A_1, \dots,A_n) = \sum^n_{i=1} \det \bb{A_1,\dots,A_{j-1},A'_j,A_{j+1},\dots,A_n} \\
& = & \sum^n_{i=1} a_{ij} \det \bb{A_1,\dots,A_{j-1}, e_i,A_{j+1},\dots,A_n}
\eeast
where $A_j' = \bepm 0,\dots,0,a_{ij},0,\dots,0\eepm^T$ (the only non-zero element is in row $i$).

Now we can swap row $i$ and row $i-1$, and then swap row $i-1$ and $i-2$,... Finally, we have shift original row $i$ to row 1 and this need $i-1$ transpositions (similarly for column operations which need $j-1$ transpositions, so totally $i+j-2$ transpositions). Thus, by Proposition \ref{pro:determinant_matrix_property}.(iii), % column $j$ and column 1.
\be
\det A = \sum^n_{i=1} a_{ij}(-1)^{i+j-2} \det\bepm 1 & * \\ 0 & \wh{A}_{ij}\eepm = \sum^n_{i=1} (-1)^{i+j} a_{ij} M_{ij},
\ee
using Proposition \ref{pro:block_matrix_determinant}. (ii) is similar by using determinant of transpose matrix (Proposition \ref{pro:determinant_transpose}).
\end{proof}

\begin{definition}[cofactor matrix\index{cofactor matrix}]\label{def:cofactor_matrix}
Let $A \in M_n(\F)$ and $M_{ij}$ is the $(i,j)$-minor of $A$. Then cofactor matrix of $A$ is $C = (c_{ij})$ where $c_{ij} = (-1)^{i+j} M_{ij}$.
\end{definition}

\begin{definition}[adjugate matrix\index{adjugate matrix}]\label{def:adjugate_matrix}
Let $A \in M_n(\F)$ and $C$ is cofactor matrix of $A$. The adjugate matrix is $\adj A = C^T$, i.e. the $n \times n$ matrix with $(i, j)$ entry equal to $(-1)^{i+j} M_{ji} = (-1)^{i+j} \det \bb{\wh{A}_{ji}}$.
\end{definition}


\begin{example}
As a specific example, we have \be A= \bepm \!-3 & \, 2 & \!-5 \\ \!-1 & \, 0 & \!-2 \\ \, 3 & \!-4 & \, 1 \eepm \ \ra \ \adj A = \bepm \!-8 & \,18 & \!-4 \\ \!-5 & \!12 & \,-1 \\ \, 4 & \!-6 & \, 2
\eepm. \ee

The $-6$ in the third row, second column of the adjugate was computed as follows: \be (-1)^{2+3}\det \bepm \!-3&\,2\\ \,3&\!-4\eepm =-((-3)(-4)-(3)(2))=-6. \ee

Again, the (3,2) entry of the adjugate is the (2,3) cofactor of $A$. Thus, the submatrix $\bepm \!-3&\,\!2\\ \,\!3&\!-4\eepm$ was obtained by deleting the second row and third column of the original
matrix $A$.
\end{example}


\begin{theorem}\label{thm:adjugate_inverse_matrix}
\ben
\item [(i)] $(\adj A)A = (\adj A)A = (\det A)I$.
\item [(ii)] If $A$ is invertible then $A^{-1} = \frac 1{\det A} \adj A$.
\een
\end{theorem}

\begin{proof}[\bf Proof]
\ben
\item [(i)] By Lemma \ref{lem:determinant_minor_matrix}.(i), fix $j$,
\be
\det A = \sum^n_{i=1} (\adj A)_{ji} a_{ij} = ((\adj A) A)_{jj}.
\ee

Also, for fix $j < k$, by Proposition \ref{pro:determinant_matrix_property} and Lemma \ref{lem:determinant_minor_matrix},
\be
0 = \det(A_1, \dots,A_k, \dots,A_k, \dots,A_n) = \sum^n_{i=1} (\adj A)_{ji} a_{ik} = ((\adj A) A)_{jk}.
\ee

Similarly, from Lemma \ref{lem:determinant_minor_matrix}.(ii), we have $(\adj A)A = (\det A)I$.

\item [(ii)] If $A$ is invertible, then $\det A \neq 0$ (by Theorem \ref{thm:matrix_invertible_determinant_non_zero}), so $\frac 1{\det A} (\adj A) A = I$ and hence we deduce $A^{-1} = \frac 1{\det A} \adj A$.
\een
\end{proof}





\begin{proposition}\label{pro:adjugate_matrix_basic_properties}
\ben
\item [(i)] $\adj I = I$ where $I \in M_n(\F)$ is identity matrix.
\item [(ii)] For $A\in M_n(\F)$, $\adj\bb{\lm A} = \lm^{n-1}\adj A$.
\item [(iii)] $\adj \bb{A^T} = \bb{\adj A}^T$.
\item [(iv)] $\adj\bb{\ol{A}} = \ol{\adj A}$.
\item [(v)] Adjugate matrices of diagonal matrices are still diagonal matrices, i.e., if $A = \diag\bb{a_1,a_2,\dots,a_n}$, then
\be
\adj A = \diag\bb{\prod^n_{i=2}a_i,\prod^n_{i=1,i\neq 2}a_i, \prod^n_{i=1,i\neq 3}a_i,\dots, \prod^{n-1}_{i=1}a_i}.
\ee
\item [(vi)] Adjugate matrices of symmetric matrices are still symmetric matrices, i.e., if $A^T = A$, then $\bb{\adj A}^T = \adj A$.
\item [(vii)] If $A\in M_n(\F)$ is antisymmetric, i.e., $A^T = -A$, then
\be
\bb{\adj A}^T = \left\{\ba{ll}
-\adj A \quad\quad & n \text{ is even}\\
\adj A & n \text{ is odd}
\ea\right.
\ee
\item [(viii)] For elementary operations,
\be
\adj T_{i,j} = -T_{i,j},\quad i\neq j, \qquad \adj M_{i,\lm} = \lm M_{i,1/\lm},\quad c\neq 0,\qquad \adj C_{i,j,\lm} = C_{i,j,-\lm}.
\ee
\item [(ix)] Let $Q$ be an othogonal matrix. Then
\be
\adj Q = \left\{\ba{ll}
Q^T & \det Q = 1\\
-Q^T \quad\quad & \det Q = -1
\ea\right.
\ee
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [](i)-(iv) are obvious from definition of adjugate matrix (Definition \ref{def:adjugate_matrix}).
\item [(v)] \footnote{need proof}
\een
\end{proof}

\begin{lemma}\label{lem:adjugate_invertible_product}
For any $A\in M_n(\F)$ and invertible matices $P,Q\in M_n(\F)$, $\adj\bb{PAQ} = \adj Q\adj A\adj P$.
\end{lemma}

\begin{proof}[\bf Proof]
Let $E$ be an elementary matrix.% and $B_{ij} = (-1)^{i+j}\det\bb{\wh{A}_{ij}}$. %$P,Q$ are elementary matrices (We only need to consider elementary column matrices as we can take the transpose of them to get elementary row matrices). Thus, we have three cases. %\footnote{need proof}

If $E$ is switching matrix $T_{i,j}$ ($i<j$), then by Proposition \ref{pro:determinant_matrix_property}.(iii),
\beast
\adj\bb{AT_{i,j}} & = & \adj \bepm A_1,\dots, A_j,\dots, A_i,\dots, A_n\eepm  \\
& = & \bepm -(-1)^{1+1}\det\bb{\wh{A}_{11}} & -(-1)^{2+1}\det\bb{\wh{A}_{21}} & \dots & -(-1)^{n+1}\det\bb{\wh{A}_{n1}} \\ \vdots\\ (-1)^{1+i}(-1)^{j-i-1}\det\bb{\wh{A}_{1j}} & (-1)^{2+i}(-1)^{j-i-1}\det\bb{\wh{A}_{2j}} & \dots & (-1)^{n+i}(-1)^{j-i-1}\det\bb{\wh{A}_{nj}} \\ \vdots \\  (-1)^{1+j}(-1)^{j-i-1}\det\bb{\wh{A}_{1i}} & (-1)^{2+j}(-1)^{j-i-1}\det\bb{\wh{A}_{2i}} & \dots & (-1)^{n+j}(-1)^{j-i-1}\det\bb{\wh{A}_{ni}} \\ \vdots \\  -(-1)^{1+n}\det\bb{\wh{A}_{1n}} & -(-1)^{2+n}\det\bb{\wh{A}_{2n}} & \dots & -(-1)^{n+n}\det\bb{\wh{A}_{nn}}  \eepm
\eeast
where $(-1)^{j-i}\det\bb{\wh{A}_{kj}}$ (as we shift $A_i$ to its original position with $j-i-1$ times) is $(k,j)$ minor of $AT_{i,j}$. Thus,
\beast
\adj\bb{AT_{i,j}} & = & \bepm -(-1)^{1+1}\det\bb{\wh{A}_{11}} & -(-1)^{2+1}\det\bb{\wh{A}_{21}} & \dots & -(-1)^{n+1}\det\bb{\wh{A}_{n1}} \\ \vdots\\ -(-1)^{1+j}\det\bb{\wh{A}_{1j}} & -(-1)^{2+j}\det\bb{\wh{A}_{2j}} & \dots & -(-1)^{n+j}\det\bb{\wh{A}_{nj}} \\ \vdots \\ -(-1)^{1+i}\det\bb{\wh{A}_{1i}} & -(-1)^{2+i}\det\bb{\wh{A}_{2i}} & \dots & -(-1)^{n+i}\det\bb{\wh{A}_{ni}} \\ \vdots \\  -(-1)^{1+n}\det\bb{\wh{A}_{1n}} & -(-1)^{2+n}\det\bb{\wh{A}_{2n}} & \dots & -(-1)^{n+n}\det\bb{\wh{A}_{nn}}  \eepm = \adj T_{i,j}\adj A.
\eeast

Also, since $T_{i,j}^T = T_{i,j}$, by Proposition \ref{pro:adjugate_matrix_basic_properties}.(iii) and Proposition \ref{pro:matrix_multiple_transpose}
\be
\adj\bb{T_{i,j}A} = \adj\bb{T_{i,j}^TA} = \adj\bb{\bb{A^TT_{i,j}}^T} = \bb{\adj\bb{A^TT_{i,j}}}^T = \bb{\adj T_{i,j}\adj \bb{A^T}}^T = \adj A \adj T_{i,j}.
\ee

If $E$ is multiplication matrix $M_{i,\lm}$ ($\lm \neq 0$), we have by Theorem \ref{thm:adjugate_inverse_matrix}.(ii) and Proposition \ref{pro:square_elementary_matrix_invertible},
\beast
\adj\bb{AM_{i,\lm}} & = & \adj \bepm A_1,\dots, \lm A_i,\dots, A_n\eepm  \\
& = & \bepm \lm (-1)^{1+1}\det\bb{\wh{A}_{11}} & \lm (-1)^{2+1}\det\bb{\wh{A}_{21}} & \dots & \lm (-1)^{n+1}\det\bb{\wh{A}_{n1}} \\ \vdots\\ (-1)^{1+i}\det\bb{\wh{A}_{1i}} & (-1)^{2+i}\det\bb{\wh{A}_{2i}} & \dots & (-1)^{n+i}\det\bb{\wh{A}_{ni}} \\ \vdots \\  \lm (-1)^{1+n}\det\bb{\wh{A}_{1n}} & \lm(-1)^{2+n}\det\bb{\wh{A}_{2n}} & \dots & \lm (-1)^{n+n}\det\bb{\wh{A}_{nn}}  \eepm \\
& = & \lm M_{i,1/\lm} \adj A = \det\bb{M_{i,\lm}} \bb{M_{i,\lm}}^{-1} \adj A= \adj M_{i,\lm} \adj A.
\eeast

Similarly, since $M_{i,\lm}^T = M_{i,\lm}$, by Proposition \ref{pro:adjugate_matrix_basic_properties}.(iii) and Proposition \ref{pro:matrix_multiple_transpose}
\be
\adj\bb{M_{i,\lm}A} = \adj\bb{M_{i,\lm}^TA} = \adj\bb{\bb{A^TM_{i,\lm}}^T} = \bb{\adj\bb{A^TM_{i,\lm}}}^T = \bb{\adj M_{i,\lm}\adj \bb{A^T}}^T = \adj A \adj M_{i,\lm}.
\ee

If $E$ is addition matrix $C_{i,j,\lm}$ ($i<j$), we have by Proposition \ref{pro:determinant_matrix_property}.(ii,iv),
\beast
\adj\bb{AC_{i,j,\lm}} & = & \adj \bepm A_1,\dots, A_i,\dots, A_j + \lm A_i,\dots, A_n\eepm  \\
& = & \bepm (-1)^{1+1}\det\bb{\wh{A}_{11}} & \dots & \lm (-1)^{n+1}\det\bb{\wh{A}_{n1}} \\ \vdots\\ (-1)^{1+i}\bb{\lm (-1)^{j-i-1}\det\bb{\wh{A}_{1j}}  + \det\bb{\wh{A}_{1i}}} & \dots & (-1)^{n+i}\bb{\lm (-1)^{j-i-1}\det\bb{\wh{A}_{nj}} + \det\bb{\wh{A}_{ni}}}  \\ \vdots \\ (-1)^{1+j}\det\bb{\wh{A}_{1j}} & \dots &  (-1)^{n+j}\det\bb{\wh{A}_{nj}}  \\ \vdots \\   (-1)^{1+n}\det\bb{\wh{A}_{1n}} & \dots &  (-1)^{n+n}\det\bb{\wh{A}_{nn}}  \eepm
\eeast

Thus, by Theorem \ref{thm:adjugate_inverse_matrix}.(ii), Proposition \ref{pro:square_elementary_matrix_invertible} and Proposition \ref{pro:determinant_element_matrix} ($\det\bb{C_{i,j,\lm}} = 1$),
\beast
\adj\bb{AC_{i,j,\lm}} & = & \bepm (-1)^{1+1}\det\bb{\wh{A}_{11}} & \dots & \lm (-1)^{n+1}\det\bb{\wh{A}_{n1}} \\ \vdots\\ -\lm (-1)^{1+j} \det\bb{\wh{A}_{1j}}  + (-1)^{1+i}\det\bb{\wh{A}_{1i}} & \dots & -\lm (-1)^{n+j} \det\bb{\wh{A}_{nj}} + (-1)^{n+i}\det\bb{\wh{A}_{ni}}  \\ \vdots \\ (-1)^{1+j}\det\bb{\wh{A}_{1j}} & \dots &  (-1)^{n+j}\det\bb{\wh{A}_{nj}}  \\ \vdots \\   (-1)^{1+n}\det\bb{\wh{A}_{1n}} & \dots &  (-1)^{n+n}\det\bb{\wh{A}_{nn}}  \eepm \\
& = & C_{i,j,-\lm} \adj A = \det\bb{C_{i,j,\lm}} \bb{C_{i,j,-\lm}}^{-1} \adj A= \adj C_{i,j,\lm} \adj A.
\eeast

With the same argument as above, we have $\adj\bb{C_{i,j,\lm}A} =  \adj A \adj C_{i,j,\lm}$. Therefore, for any elementary matrix $E$, we have
\be
\adj\bb{AE} =  \adj E \adj A,\quad \adj\bb{EA} =  \adj A \adj E.
\ee

Recall Lemma \ref{lem:invertible_product_elementary_matrices}, any invertible matrices $P,Q$ can be written as the products of elementary matrices $P = E_1\dots E_m$, $Q = F_1\dots F_k$. Then
\beast
\adj\bb{PAQ} & = & \adj\bb{E_1\dots E_m A F_1\dots F_k} = \bb{\adj F_k \dots \adj F_1}\adj A \bb{\adj E_m \dots \adj E_1} \\
& = & \adj \bb{F_1\dots F_k} \adj A \adj \bb{E_1\dots E_m} = \adj Q \adj A \adj P.
\eeast
\end{proof}


\begin{theorem}\label{thm:adjugate_matrix_rank}
Let $A\in M_n(\F)$ ($n > 1$). Then
\be
\rank\bb{\adj A} = \left\{\ba{ll}
n \quad\quad & \rank(A) = n\\
1 & \rank(A) = n-1\\
0 & \text{otherwise}
\ea\right.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
If $\rank(A) = n$, $\det A \neq 0$. Then by Theorem \ref{thm:adjugate_inverse_matrix}, $\adj A$ is invertible and hence $\rank\bb{\adj A} = n$ by Theorem \ref{thm:invertible_full_rank}.

If $\rank(A) = n-1$, it is equivalent to matrix $I' = \bepm I_{n-1} & 0 \\ 0 & 0 \eepm$. That is, there exists invertible matrices $P,Q$ such that $A = PI'Q$. By Lemma \ref{lem:adjugate_invertible_product}, we have $\adj A = \adj Q \adj I' \adj P$. Since $P,Q$ are full-rank, by Proposition \ref{pro:matrix_rank_properties}.(i), we have
\be
\rank\bb{\adj A} \leq \rank\bb{\adj I'} = \rank\bepm 0 & 0 \\ 0 & 1\eepm = 1.
\ee

Also, $I' = P^{-1}A Q^{-1}$, Proposition \ref{pro:matrix_rank_properties}.(i) gives that $1 = \rank\bb{\adj I'} \leq \rank\bb{\adj A} \ \ra \ \rank\bb{\adj A} = 1$.

With similar argument, we have that $\rank\bb{\adj A} = 0$ if $\rank(A) < n-1$.
\end{proof}

\begin{proposition}
Let $A\in M_n(\F)$. Then the $k$-fold adjugate matrix of $A$, $\adj^{[k]}(A) := \adj\bb{\adj\bb{\dots \adj A \dots}}$ ($k$ times) is
\be
\adj^{[k]}(A) = \left\{\ba{ll}
\bb{\det A}^{\frac{(n-1)^k-(n-1)}{n}}\adj A \quad\quad & \rank(A) = n,\ k \text{ is odd}\\
\bb{\det A}^{\frac{(n-1)^k-1}{n}}A \quad\quad & \rank(A) = n,\ k \text{ is even}\\
0 & \rank(A) = n-1,\ k\geq 2\\
0 & \rank(A) < n-1,\ k\geq 1
\ea\right.
\ee

In particular, $\adj\bb{\adj A} = \bb{\det A}^{n-2}A$.
\end{proposition}

\begin{proof}[\bf Proof]
For $k=1$, by Theorem \ref{thm:adjugate_matrix_rank}, the conclusion holds.

For $k\geq 2$, $\adj^{[k]}(A) = 0$ if $\rank(A)\leq n-1$ by Theorem \ref{thm:adjugate_matrix_rank}. Thus, we only need to check the full-rank situation.

Now assume $\rank(A)=n$. If $k=2$, we have
\be
\adj\bb{\adj A} = \det\bb{\adj A} \bb{\adj A}^{-1} = \det\bb{\det A A^{-1}} \bb{\det A A^{-1}}^{-1} = \bb{\det A}^{n-1} \det\bb{ A^{-1}} A = \bb{\det A}^{n-2}A.
\ee

Now assume the conclusion holds for $k$. If $k$ is odd, $\adj^{[k]}(A) = \bb{\det A}^{\frac{(n-1)^k-(n-1)}{n}}\adj A$. Then by Proposition \ref{pro:adjugate_matrix_basic_properties}.(ii) and the case $k=2$,
\beast
\adj^{[k+1]}(A) & = & \adj\bb{\bb{\det A}^{\frac{(n-1)^k-(n-1)}{n}}\adj A} = \bb{\bb{\det A}^{\frac{(n-1)^k-(n-1)}{n}}}^{n-1} \adj^{[2]}(A)\\
& = & \bb{\det A}^{\frac{(n-1)^{k+1}-(n-1)^2}{n}} \bb{\det A}^{n-2}A = \bb{\det A}^{\frac{(n-1)^{k+1}-1}{n}}A.
\eeast

If $k$ is even, $\adj^{[k]}(A) = \bb{\det A}^{\frac{(n-1)^k-1}{n}} A$. Then by Proposition \ref{pro:adjugate_matrix_basic_properties}.(ii),
\beast
\adj^{[k+1]}(A) & = & \adj\bb{\bb{\det A}^{\frac{(n-1)^k-1}{n}} A} = \bb{\bb{\det A}^{\frac{(n-1)^k-1}{n}}}^{n-1} \adj A = \bb{\det A}^{\frac{(n-1)^{k+1}-(n-1)}{n}} \adj A.
\eeast
\end{proof}


\begin{theorem}\label{thm:adjugate_matrix_product}
For any $A,B\in M_n(\F)$, $\adj\bb{AB} = \adj B \adj A$.
\end{theorem}

\begin{proof}[\bf Proof]
If $A,B$ are invertible, we have $AB$ is also invertible. Then by Theorem \ref{thm:determinant_product}, Proposition \ref{pro:inverse_matrix_property} and Theorem \ref{thm:adjugate_inverse_matrix},
\be
\adj\bb{AB} = \det\bb{AB}\bb{AB}^{-1} = \det A \det B B^{-1}A^{-1} = \adj B \adj A.
\ee

If $\min\bra{\rank(A),\rank(B)} < n-1$ with $\adj A = 0$ or $\adj B = 0$, then by Proposition \ref{pro:matrix_rank_properties}.(i),%\footnote{need proof}
\be
\rank(AB) \leq \min\bra{\rank(A),\rank(B)} < n-1 \ \ra \ \adj\bb{AB} = 0 = \adj B\adj A
\ee
by Theorem \ref{thm:adjugate_matrix_rank}.

If $\min\bra{\rank(A),\rank(B)} = n-1$ and $\max\bra{\rank(A),\rank(B)} = n$, we can assume $\rank(A) = n$ and $\rank(B) = n-1$ wlog as we can take transpose of $AB$. Then by Lemma \ref{lem:adjugate_invertible_product}, we have $\adj\bb{AB} = \adj B\adj A$.

Therefore, there leaves the last case, $\rank(A) = \rank(B) = n-1$. Assume that (by Lemma \ref{lem:matrix_equivalent_to_identity})
\be
A = P\bepm I_{n-1} & 0 \\ 0 & 0 \eepm Q,\quad B = U\bepm I_{n-1} & 0 \\ 0 & 0 \eepm V
\ee
where $P,Q,U,V$ are invertible matrices and
\be
QU = \bepm C_{(n-1)\times (n-1)} & D_{(n-1)\times 1} \\ E_{1\times (n-1)} & F_{1\times 1} \eepm.
\ee

Therefore, by Lemma \ref{lem:adjugate_invertible_product},
\beast
\adj\bb{AB} & = & \adj\bb{P \bepm I_{n-1} & 0 \\ 0 & 0 \eepm \bepm C_{(n-1)\times (n-1)} & D_{(n-1)\times 1} \\ E_{1\times (n-1)} & F_{1\times 1} \eepm \bepm I_{n-1} & 0 \\ 0 & 0 \eepm V} = \adj\bb{P \bepm C_{(n-1)\times (n-1)} & 0 \\0 & 0 \eepm V} \\
& = & \adj V \adj\bepm C_{(n-1)\times (n-1)} & 0 \\0 & 0 \eepm \adj P = \adj V \bepm 0 & 0 \\0 & \det C_{(n-1)\times (n-1)} \eepm \adj P.
\eeast

But $\adj \bepm C_{(n-1)\times (n-1)} & 0 \\ E_{1\times (n-1)} & 0 \eepm \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm = \bepm 0 & 0 \\ \alpha & \det C_{(n-1)\times (n-1)} \eepm \bepm 0 & 0 \\0 & 1 \eepm = \bepm 0 & 0 \\0 & \det C_{(n-1)\times (n-1)} \eepm$. Then we have (by Lemma \ref{lem:adjugate_invertible_product})
\beast
\adj\bb{AB} & = & \adj V \bb{\adj \bepm C_{(n-1)\times (n-1)} & 0 \\ E_{1\times (n-1)} & 0 \eepm \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm} \adj P\\
& = & \adj V \bb{\adj \bb{\bepm C_{(n-1)\times (n-1)} & D_{(n-1)\times 1} \\ E_{1\times (n-1)} & F_{1\times 1} \eepm \bepm I_{n-1} & 0 \\ 0 & 0 \eepm} \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm} \adj P\\
& = & \adj V \bb{\adj \bb{QU \bepm I_{n-1} & 0 \\ 0 & 0 \eepm} \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm} \adj P \\
& = & \adj V \adj \bb{\bepm I_{n-1} & 0 \\ 0 & 0 \eepm}\adj\bb{QU} \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm \adj P\\
& = & \adj V \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm\adj U \adj Q \adj \bepm I_{n-1} & 0 \\ 0 & 0 \eepm \adj P \\
& = & \adj\bb{U  \bepm I_{n-1} & 0 \\ 0 & 0 \eepm V}\adj \bb{P \bepm I_{n-1} & 0 \\ 0 & 0 \eepm Q} = \adj B \adj A.
\eeast

Therefore, the equation holds for any $A,B\in M_n(\F)$.
\end{proof}


\begin{corollary}\label{cor:adjugate_matrix_product}
For given $k$ and any $A\in M_n(\F)$,
\ben
\item [(i)] $\adj\bb{A_1A_2\dots A_k} = \adj A_k \adj A_{k-1} \dots \adj A_1$.
\item [(ii)] $\adj\bb{A^k} = \bb{\adj A}^k$.
\een
\end{corollary}

\begin{proposition}\label{pro:adjugate_matrix_property}
For given $k \geq 1$,
\ben
\item [(i)] $\adj\bb{A^{-1}} = \bb{\adj A}^{-1}$.
\item [(ii)] $\det\bb{\adj A} = \bb{\det A}^{n-1}$ for $n\geq 2$.
\item [(iii)] $\det\bb{\adj^{[k]}(A)} = \bb{\det A}^{(n-1)^k}$.
\item [(iv)] $\det\bb{\adj\bb{A_1A_2\dots A_k}} = \prod^k_{i=1} \det\bb{\adj A_i} = \bb{\prod^k_{i=1} \det A_i}^{n-1}$.
\item [(v)] $\det\bb{\adj (A^k)} = \bb{\det \bb{\adj A}}^k = \bb{\det A}^{k(n-1)}$.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] $\adj\bb{A^{-1}} \adj A = \adj \bb{A A^{-1}} = \adj I = I$. $\adj A \adj\bb{A^{-1}} = \adj \bb{A^{-1}A} = \adj I = I$. Thus, $\adj\bb{A^{-1}}$ is the inverse of $\adj A$ by Proposition \ref{pro:inverse_matrix}.
\item [(ii)] If $\rank(A) < n$, we have $\det A = 0$ and thus $\rank\bb{\adj A} < n$ by Theorem \ref{thm:adjugate_matrix_rank}. Thus, $\det\bb{\adj A} = 0$, which satisfies the statement.

If $A$ is full-rank, then we have $\det\bb{\det A A^{-1}} = \bb{\det A}^{n} \det\bb{A^{-1}} = \bb{\det A}^{n-1}$.

\item [(iii)] If $k = 1$, it is (ii). If $\rank(A) < n$, we have $\det A = 0 = \det\bb{\adj^{[k]}(A)}$. Now we assume the conclusion holds for $k$ and $A$ is full-rank (thus $\adj^{[k]}(A)$ is full-rank). Then
\beast
\det\bb{\adj^{[k+1]}(A)} & = & \det \bb{\adj\bb{\adj^{[k]}(A)}} = \det\bb{\det\bb{\adj^{[k]}(A)} \bb{\adj^{[k]}(A)}^{-1}} \\
& = & \bb{\det\bb{\adj^{[k]}(A)}}^{n}\det\bb{\bb{\adj^{[k]}(A)}^{-1}} = \bb{\det\bb{\adj^{[k]}(A)}}^{n-1}\\
& = & \bb{\bb{\det A}^{(n-1)^k}}^{n-1} = \bb{\det A}^{(n-1)^{k+1}}.
\eeast
\een

(iv,v) are direct results from (iii) and Corollary \ref{cor:adjugate_matrix_product}.
\end{proof}




%\subsection{Special square matrices}

\subsection{Diagonal matrices and block diagonal matrices}

\begin{definition}[diagonal matrix\index{diagonal matrix}]\label{def:diagonal_matrix}
The matrix $D = (d_{ij}) \in M_n(\F)$ is called diagonal matrix if $d_{ij} = 0$ whenever $i\neq j$, i.e.,
\be
D = \bepm d_{11} & & & \\ & d_{22} & & & \\ & & \ddots & \\ & & & d_{nn} \eepm.
\ee

We denote such a matrix as
\be
D = \diag\bb{d_{11},\dots,d_{nn}}\quad \text{or}\quad D = \diag d,
\ee
where $d$ is the vector of diagonal entries of $D$.

If all the diagonal entries of a diagonal matrix are positive (non-negative) real numbers, we refer to it as a positive (non-negative) diagonal matrix\index{diagonal matrix!positive, non-negative}.
\end{definition}

\begin{remark}
The identity matrix $I \in M_n(\F)$ is an example of a positive diagonal matrix.
\end{remark}

\begin{definition}[scalar matrix\index{scalar matrix}]\label{def:scalar_matrix}
A diagonal matrix $D\in M_n(\F)$ is called scalar matrix if the diagonal entries of $D$ are equal; that is, $D= \alpha I$ for some $\alpha \in \C$.
\end{definition}

\begin{remark}
Left or right multiplication of a matrix by a scalar matrix has the same effect as multiplying it by the corresponding scalar.
\end{remark}


\begin{proposition}[diagonal matrices properties]\label{pro:diagonal_matrix_property}
\ben
\item [(i)] The determinant of a diagonal matrix is just the product of its diagonal entries: $\det D = \prod^n_{i=1} d_{ii}$.
\item [(ii)] A diagonal matrix is invertible if and only if all its diagonal entries are non-zero.
\item [(iii)] All diagonal matrices commute with each other under multiplication, i.e., $AB = BA$, for diagonal matrices $A,B$.
\item [(iv)] A diagonal matrix $D$ commutes with a given matrix $A = (a_{ij})\in M_n(\F)$ if and only if $a_{ij} = 0$ whenever the $i$th and $j$th diagonal entries of $D$ differ.
\item [(v)] The product of two diagonal matrices is just the diagonal matrix of pairwise products of their respective diagonal entries and similarly for postive integer powers of a single diagonal matrix.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] This is direct result from Laplace expansion (Lemma \ref{lem:determinant_minor_matrix}).
\item [(ii)] This is the result from (i) and Theorem \ref{thm:matrix_invertible_determinant_non_zero}.
\item [(iii)] We have %$a_{ij} = a_{ji}$ and $b_{ij} = b_{ji}$ for all $i,j$,
\be
(AB)_{ij} = \sum_k a_{ik}b_{kj} = \left\{
\ba{ll}
a_{ii}b_{ii} \quad\quad & i = j \\
0 & i \neq j
\ea\right.  = (BA)_{ij} .
\ee
\item [(iv)] If $a_{ij} = 0$ whenever the $i$th and $j$th diagonal entries of $D$ differ, we have
\be
(DA)_{ij} = \sum d_{ik}a_{kj} = d_{ii}a_{ij} = \left\{
\ba{ll}
d_{jj}a_{ij}\quad\quad & d_{ii} = d_{jj} \\
0 & d_{ii} \neq d_{jj}
\ea\right.
\ee

\be (AD)_{ij} = \sum a_{ik}d_{kj} = a_{ij}d_{jj} = \left\{ \ba{ll}
a_{ij}d_{jj}\quad\quad & d_{ii} = d_{jj} \\
0 & d_{ii} \neq d_{jj} \ea\right. \ee

Therefore $AD = DA$.

If $A$ and $D$ are commute, we have that $d_{ii}a_{ij} = d_{jj}a_{ij}$ by definition. Thus, $(d_{ii} - d_{jj})a_{ij} = 0$ implies that $a_{ij} = 0$ whenever the $i$th and $j$th diagonal entries of
$D$ differ.

\item [(v)] The same argument with (iii).
\een
\end{proof}




\begin{definition}[block diagonal matrix\index{block diagonal matrix}, direct sum\index{direct sum!matrices}]\label{def:block_diagonal_matrix}
A matrix $A = M_{n}(\F)$ of the form
\be
A = \bepm A_{11} & & & 0\\ & A_{22} & & \\ & & \ddots & \\ 0 & & & A_{kk} \eepm
\ee
in which $A_{ii} \in M_{n_i}(\F)$, $i = 1,\dots,k$ and $n = \sum^k_{j=1} n_j$, is called block diagonal matrix. Notationally, such a matrix is often indicated as
\be
A = A_{11} \oplus A_{22} \oplus \dots \oplus A_{kk}, \quad \text{ or }\quad \bigoplus^k_{i=1}A_{ii},
\ee
which is called the direct sum of the matrices $A_{11},\dots,A_{kk}$.
\end{definition}

\begin{proposition}[block diagonal matrices properties]\label{pro:block_diagonal_matrix_property}
For matrix $A = \bigoplus^k_{i=1}A_{ii}$ and $B = \bigoplus^k_{i=1}B_{ii}$, in which each pair $A_{ii}$, $B_{ii}$ has the same size.
\ben
\item [(i)] $\det A = \prod^k_{i=1} \det \bb{A_{ii}}$.
\item [(ii)] $A$ is invertible if and only if each $A_{ii}$, $i=1,\dots,k$ is invertible.
\item [(iii)] $\rank(A) = \sum^k_{i =1} \rank(A_{ii})$.
\item [(iv)] $A,B$ commute if and only if $A_{ii}$ and $B_{ii}$ commute, $i=1,\dots,k$.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Direct result from Proposition \ref{pro:block_matrix_determinant}.
\item [(ii)] Direct result from (i) and Theorem \ref{thm:matrix_invertible_determinant_non_zero}.
\item [(iii)] \footnote{proof needed.}
\item [(iv)] $AB = BA$ if and only if $A_{ii} B_{ii} = B_{ii} A_{ii} $ for any $i$th block where $i=1,\dots,k$. Therefore, the conclusion holds.
\een
\end{proof}


\subsection{Triangular matrices and block triangular matrices}

\begin{definition}[triangular matrix\index{triangular matrix}]\label{def:triangular_matrix}
$A\in M_n(\F)$ is an upper triangular matrix if $a_{ij} = 0$ whenever $i > j$. If $a_{ij} = 0$ whenever $i\geq j$, then $A$ is said to be strictly upper triangular.

$A$ is said to be lower triangular (or strictly upper triangular) if its transpose is upper triangular (or strictly upper triangular).
\end{definition}

\begin{example}
\be
A = \bepm 1 & 2 & 3\\ 0 & 4 & 5 \\ 0 & 0 & 6 \eepm \qquad \text{is upper triangular.}
\ee
\end{example}

\begin{proposition}
The product of two upper triangular matrices is upper triangular.
\end{proposition}

\begin{proof}[\bf Proof]
Let $A,B\in M_n(\F)$ be two upper triangular matrices. Then
\be
a_{ij} = 0, \ b_{ij} = 0,\qquad \forall i>j.
\ee

Thus, $AB = C = \bb{c_{ij}}$ and for any $i>j$
\be
c_{ij} = \sum^n_{k=1}a_{ik}b_{kj} = \sum^n_{k=1} a_{ik} \ind_{i<k} b_{kj}\ind_{k < j} = \sum^n_{k=1} a_{ik} b_{kj}\ind_{i<k < j} = 0.
\ee

Hence $C$ is upper triangular.
\end{proof}

Similarly, we have the following proposition.

\begin{proposition}
The product of two similarly partitioned block upper triangular matrices is block upper triangular.
\end{proposition}



\begin{lemma}\label{lem:upper_triangular_matrix_determinant}
If $A$ is an upper triangular matrix, then $\det A = a_{11} \dots a_{nn}$.
\end{lemma}

\begin{proof}[\bf Proof]
From the definition the determinant (Definition \ref{def:determinant_matrix}),
\be
\det A = \sum_{\sigma \in S_n} \ve(\sigma )a_{1,\sigma (1)} \dots a_{n,\sigma (n)}.
\ee

For a product to contribute, we must have $\sigma (i) \leq  i$ for all $i = 1, \dots, n$. Hence $\sigma (1) = 1$, $\sigma (2) = 2$, $\dots$, $\sigma (n) = n$, so $\sigma  = \iota$ and hence $\det A = a_{11} \dots a_{nn}$.
\end{proof}


\begin{definition}[block triangular matrices]
A matrix $A\in M_n(\F)$ of the form \be A = \bepm A_{11} & & & * \\ & A_{22}  & & \\ & & \ddots & \\ 0 & & & A_{kk} \eepm \ee in which $A_{ii}\in M_{n_i}(\F)$, $i = 1,\dots,k$, $\sum^k_{i=1}n_i = n$ and
``$*$'' denotes any entry, is called block upper triangular. Block lower triangular, strictly block lower triangular and strictly block upper triangular ($A_{ii} = 0$ for $i=1,\dots,k$) may be defined similarly.
\end{definition}


\begin{proposition}\label{pro:block_matrix_determinant}
If $A \in M_m(\F)$, $B \in M_n(\F)$ and $C \in M_{m,n}(\F)$. Then $\det \bepm A & C\\ 0 & B \eepm = \det A \det B$.
\end{proposition}

\begin{proof}[\bf Approach 1]
Fix $B,C$. Then $d_{B,C} : A \mapsto \det\bepm A & C\\ 0 & B\eepm$ is a volume form on the column space $\F^m$ by Proposition \ref{pro:determinant_matrix_property}. Hence by Theorem \ref{thm:volume_form_determinant}, $d_{B,C}(A) = \det A \det \bepm I & C\\ 0 & B\eepm$.

Now keep $C$ fixed. The map $B \mapsto \det\bepm I & C\\ 0 & B\eepm$ is a volume form on the row space $\F^n$ by Proposition \ref{pro:determinant_matrix_property}. Hence $\det\bepm I & C\\ 0 & B \eepm= \det B\bepm I & C\\ 0 & I \eepm$ by Theorem \ref{thm:volume_form_determinant}.

Now $\det\bepm I & C\\ 0 & I\eepm = 1$ as $\bepm I & C\\ 0 & I\eepm$ is upper triangular matrix. So $\det\bepm A & C\\ 0 & B \eepm = \det A\det B \cdot 1 = \det A\det B$ by Lemma \ref{lem:upper_triangular_matrix_determinant}.
\end{proof}

\begin{proof}[\bf Approach 2]
Write $X =\bepm A & C\\ 0 & B\eepm$ and expand the expression for the determinant.
\be
\det \bepm A & C\\ 0 & B \eepm = \sum_{\sigma \in S_{m+n}} \ve(\sigma ) \prod^{m+n}_{j=1} x_{\sigma (j)j}
\ee

Note $x_{\sigma (j)j} = 0$ if $j \leq  m$, $\sigma (j) > m$, so we only sum over $\sigma$ with the following properties.
\ben
\item [(i)] For $j\in [1,m]$, $\sigma (j) \in [1,m]$. Here $x_{\sigma (j)j} = a_{\sigma_1(j)j}$ where $\sigma_1\in S_m$ is the restriction of $\sigma$ to $[1,m]$.
\item [(ii)] For $j \in [m + 1,m + n]$, $\sigma (j) \in [m + 1,m + n]$. Here, writing $k = j - m$, we have $x_{\sigma (j)j} = b_{\sigma_2(k)k}$ where $\sigma_2(k) = \sigma (m + k) - m$.
\een

Noting also that $\ve(\sigma ) = \ve(\sigma_1)\ve(\sigma_2)$ for such $\sigma$, we obtain
\be
\det\bepm A & C\\ 0 & B\eepm = \bb{\sum_{\sigma_1\in S_m} \ve(\sigma_1)\prod^m_{j=1} a_{j,\sigma_1(j)}} \bb{\sum_{\sigma_2\in S_k} \ve(\sigma_2) \prod^k_{l=1} a_{j,\sigma_2(j)}} = \det A\det B.
\ee
\end{proof}

\begin{proposition}
Let $A$ be block upper triangular matrix with diagonal blocks $A_{11},\dots,A_{kk}$. Thus,
\ben
\item [(i)] $\det A = \prod^k_{i=1} \det A_{ii}$.
\item [(ii)] $\rank (A) \geq \sum^k_{i=1} \rank\bb{A_{ii}}$.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Direct result from Proposition \ref{pro:block_matrix_determinant}.

\item [(ii)] \footnote{proof needed.}
\een
\end{proof}

\subsection{Permutation matrices and block permutation matrices}


\subsection{Circulant matrices}

\subsection{Toeplitz matrices}

\subsection{Hankel matrices}


\subsection{Hessenberg matrices and tridiagonal matrices}

\begin{definition}[Hessenberg matrix]\label{def:hessenberg_matrix}
The matrix $A = \bb{a_{ij}} \in M_n(\F)$ is said to be in upper Hessenberg form or to be an upper Hessenbery matrix if $a_{ij} = 0$ for $i>j+1$,
\be
A = \bepm
a_{11} & a_{12} & & \dots & & a_{1n} \\
a_{21} & a_{22} & &\dots  & & a_{2n} \\
0 & a_{32} & & & & a_{3n} \\
0 & 0 & a_{43} & \ddots & & \vdots \\
\vdots & \vdots & & &\ddots　&  \\
0 & 0 & \dots & 0　& a_{n,n-1} & a_{nn}
\eepm
\ee

The matrix $A\in M_n(\F)$ is called lower Hessenberg if $A^T$ is upper Hessenberg.
\end{definition}

\begin{definition}
A matrix $A = \bb{a_{ij}} \in M_n(\F)$ that is both upper and lower Hessenberg is called tridiagonal, that is, $A$ is tridiagonal if $a_{ij} = 0$, whenever $\abs{i-j} >1$,
\be
A =
\bepm
a_{11} & a_{12} & 0 & 0 & \dots & 0 \\
a_{21} & a_{22} & a_{32} & 0 & \dots & 0 \\
0 & a_{32} & a_{33} & a_{34} & \dots & 0 \\
0 & 0 & \ddots & \ddots & \ddots & \vdots \\
\vdots & \vdots & & \ddots & \ddots　& a_{n-1,n}\\
0 & 0 & \dots & 0　& a_{n,n-1} & a_{nn}
\eepm
\ee
\end{definition}

\begin{proposition}
For a tridiagonal matrix $A\in M_n(\F)$ and the set $\alpha_k = \bra{1,\dots,k}$,
\be
\det\bb{A_{\alpha_{k+1}}} = a_{k+1,k+1} \det\bb{A_{\alpha_k}} - a_{k+1,k}a_{k,k+1}\det\bb{A_{\alpha_{k-1}}},\qquad k=2,\dots,n-1.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Direct result from Laplace expansion (Lemma \ref{lem:determinant_minor_matrix}).
\end{proof}



\subsection{Vandermonde matrices and Lagrange interpolating polynomials}


\begin{definition}[Vandermonde matrix\index{Vandermonde matrix}]\label{def:vandermonde_matrix}
A Vandermonde matrix $V\in M_{m,n}(\F)$ is a matrix of the form
\be
V = \bepm
1 & x_1 & x_1^2 & x_1^3 & \dots & x_1^{n-1} \\
1 & x_2 & x_2^2 & x_2^3 & \dots & x_2^{n-1} \\
\vdots & \vdots & \vdots & \vdots &  & \vdots \\
1 & x_m & x_m^2 & x_m^3 & \dots & x_m^{n-1} \\
\eepm
\ee
where $x_1,\dots,x_m\in \F$. That is, $v_{ij} = x_i^{j-1}$.
\end{definition}

\begin{proposition}\label{pro:vandermonde_determinant}
The determinant of a square Vandermonde matrix (where $m = n$) can be expressed as:
\be
\det V = \prod_{1\le i<j\le n} (x_j-x_i).
\ee
\end{proposition}

\begin{remark}
This means that square Vandermonde matrix is nonsingular if and only if the $n$ parameters $x_1,\dots,x_n$ are distinct.
\end{remark}

\begin{proof}[\bf Proof]%If $x_i = x_j$ for some $i\neq j$, then $\det V = 0$ by Proposition \ref{pro:determinant_matrix_property}.(iv).assume that $x_i$ are distinct and
First replace the $i$th row of $V$ by
\be
\bepm 1 & t & t^2 & \dots & t^{n-1} \eepm.
\ee

Then, take the determinant. The determinant is a function of $t$ is called $V_i(t)$. It is a polynomial in $t$ of degree $n-1$ at most. Hence, it has $n-1$ roots\footnote{theorem needed here.}.
Furthermore, $V_i(x_j)$ for any $j$ between 1 and $n$ except $i$ makes the determinant 0 by Proposition \ref{pro:determinant_matrix_property}.(iv). Hence, $x_j$ with $i\neq j$ is a root of the
polynomial $V_i(t)$. Thus, $(t - x_j)$ is a factor in the expansion of $V_i(t)$. Since $t$ is of degree $n-1$ at most, we have that %Repeating this with each of the i rows tells us that that
\be
\det V = V_i(x_i) = C_n \prod_{1\leq i<j\leq n} (x_j - x_i),
\ee
where $C_n$ is a constant. The fact that $C_n=1$ follows from induction.

It is easy to check that $\det V_{2\times 2} = x_2 -x_1$ satisfies the condition. Now assume the conclusion holds for $k$ such that $C_k = 1$ and
\be
\det V_{k\times k} = C_k \prod_{1\leq i<j\leq k} (x_j - x_i).
\ee

Thus, For $V_{(k+1)\times(k+1)}$, we can use Laplace expansion (Lemma \ref{lem:determinant_minor_matrix}) to have that the coefficient of $x_{k+1}^{k}$, the bottom right entry of
$V_{(k+1)\times(k+1)}$, in its determinant is
\be
(-1)^{k+1+k+1} \det V_{k\times k} = C_k \prod_{1\leq i<j\leq k} (x_j - x_i) = \prod_{1\leq i<j\leq k} (x_j - x_i)
\ee
which implies that $C_{k+1} = 1$. Therefore, we get the required result.
\end{proof}


\begin{proposition}
Let $V$ be a Vandermonde matrix with $x_i$ and $x_i\neq x_j$ for all $i\neq j$. Then the columns of $V$ form a linearly independent set whenever $n\leq m$ (proof in \cite{Meyer_2001}.$P_{185}$).
\end{proposition}

\begin{proof}[\bf Proof]
If
\be
\bepm
1 & x_1 & x_1^2 & x_1^3 & \dots & x_1^{n-1} \\
1 & x_2 & x_2^2 & x_2^3 & \dots & x_2^{n-1} \\
\vdots & \vdots & \vdots & \vdots &  & \vdots \\
1 & x_m & x_m^2 & x_m^3 & \dots & x_m^{n-1} \\
\eepm \bepm a_0\\ a_1 \\ \vdots \\ a_{n-1} \eepm = \bepm 0 \\ 0\\ \vdots \\ 0 \eepm,
\ee
then for each $i = 1,2,\dots,m$,
\be
a_0 + a_1 x_i + a_2 x_i^2 + \dots + a_{n-1}x_i^{n-1} = 0.
\ee
has $m$ distinct roots, namely, the $x_i$'s.

However, the degree of $p(x)$ is smaller than $n-1$ and the fundamental theorem of algebra\footnote{theorem needed.} guarantees that if $p(x)$ is not the zero polynomial, then
$p(x)$ can have at most $n-1$ distinct roots. Therefore, $a_0=a_1=\dots = a_{n-1}$, which means that the column of $V$ form a linearly independent set.
\end{proof}



\begin{definition}[Lagrange interpolation polynomial\index{Lagrange interpolation polynomial}]\label{def:lagrange_interpolation_polynomial}
Let $x_1,\dots,x_n,y_1,\dots,y_n\in \F$ with distinct $x_1,\dots,x_n$ and $X = \bb{x_1,\dots,x_n}^T$, $Y = \bb{y_1,\dots,y_n}^T$. We can define Lagrange interpolation polynomial of degree $n-1$ by
\be
L(t,X,Y) = \sum^n_{i=1} \bb{y_i\ \frac{\prod^n_{j\neq i}\bb{t - x_j}}{\prod^n_{j\neq i}\bb{x_i - x_j}}}.
\ee
\end{definition}

\begin{proposition}[interpolation problem (see \cite{Meyer_2001}.$P_{186}$)]
Given a set of $n$ points $S = \bra{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)}$ in which the $x_i$'s are distinct, then Lagrange interpolation polynomial, $L(t,X,Y)$ with $X = \bb{x_1,\dots,x_n}^T$ and $Y
= \bb{y_1,\dots,y_n}^T$, is the unique polynomial of degree less than $n-1$ that passes through each point of $S$.
\end{proposition}

\begin{proof}[\bf Proof]
The polynomial $p(x) = a_0 + a_1 x + \dots + a_{n-1}x^{n-1}$ must satisfy the equations
\beast
y_1 & = & p(x_1) = a_0 + a_1 x_1 + \dots + a_{n-1}x_1^{n-1}\\
& \vdots & \\
y_n & = & p(x_n) = a_0 + a_1 x_n + \dots + a_{n-1}x_n^{n-1}
\eeast

Writing this $n\times n$ system as
\be
\bepm
1 & x_1 & x_1^2 & x_1^3 & \dots & x_1^{n-1} \\
1 & x_2 & x_2^2 & x_2^3 & \dots & x_2^{n-1} \\
\vdots & \vdots & \vdots & \vdots &  & \vdots \\
1 & x_n & x_n^2 & x_n^3 & \dots & x_n^{n-1} \\
\eepm \bepm a_0\\ a_1 \\ \vdots \\ a_{n-1} \eepm = \bepm y_1 \\ y_2\\ \vdots \\ y_n\eepm
\ee

reveals that the coefficient matrix is a square Vandermonde matrix $V$, so Proposition \ref{pro:vandermonde_determinant} guarantees that it is nonsingular. Consequently, the system has a unique
solution, and thus there is one and only one possible set of coefficients for polynomial $p(x)$. Thus, by the fact $a = V^{-1} Y$ and Theorem \ref{thm:adjugate_inverse_matrix}.(ii), we have the
Lagrange interpolation polynomial $L(t,X,Y)$ is this only polynomial of degree less than $n-1$.\footnote{Note that coefficient $a_i$ could be zero.}
\end{proof}

%\section{Matrix Anaylsis}

%\section{}

% Another use of Schur's result is to make it clear that every matrix is `almost' diagonalizable in two possible interpretations of the phrase. The first says that arbitarily close to a given matrix
% there is a diagonalizable matrix, and the second says that any given matrix is similar to an upper triangular matrix whose off-diagonal entries are arbitarily small\cite{}.




\section{Eigenvalues and Eigenvectors}

\subsection{The eigenvalue-eigenvector equation}

\begin{definition}[eigenvalue\index{eigenvalue}, eigenvector\index{eigenvector}]\label{def:eigenvalue_eigenvector}
If $A\in M_n(\F)$ ($\F = \R$ or $\C$) and $x\in \C^n$, we consider the equation
\be
A x = \lm x,\qquad x\neq 0
\ee
where $\lm\in \C$ is a scalar. If a scalar $\lm$ and a non-zero vector $x$ happen to satisfy this equation, then $\lm$ is called eigenvalue of $A$ and $x$ is called eigenvector of $A$ associated with $\lm$.
Also, the equation is called eigenvalue-eigenvector equation\index{eigenvalue-eigenvector equation}.
\end{definition}

\begin{remark}
Note that the eigenvalue and eigenvector occur inextricably as a pair, and that an eigenvector cannot be the zero vector.

Also, if $x$ is an eigenvector associated with the eigenvalue $\lm$ of $A$, any non-zero scalar multiple of $x$ is an eigenvector as well.
\end{remark}



\begin{definition}[spectrum of matrix\index{spectrum!matrix}]
The set of all $\lm\in \C$ that are eigenvalues of $A\in M_n(\F)$ is called the spectrum of $A$ and is denoted by $\sigma(A)$.

The spectral radius\index{spectral radius!matrix} of $A$ is the non-negative real number
\be
\rho(A) = \max\bra{\abs{\lm}:\lm \in \sigma(A)}.
\ee

It is just the radius of the smallest disc centered at the origin in the complex plane that includes all the eigenvalues of $A$.
\end{definition}

\begin{example}
Consider the matrix
\be
A = \bepm 7 & -2 \\ 4 & 1 \eepm \in M_2(\R).
\ee

Then we have $3\in \sigma(A)$ with $(1,2)^T$ as an associated eigenvector since
\be
A \bepm 1 \\ 2 \eepm = \bepm 3\\ 6 \eepm = 3\bepm 1\\ 2\eepm.
\ee

Also, $5\in \sigma(A)$ with associated eigenvector $(1,1)^T$.
\end{example}

\begin{proposition}\label{pro:zero_eigenvalue_singular_equivalent}
$0\in \sigma(A)$ iff $A$ is singular.
\end{proposition}

\begin{proof}[\bf Proof]
Direct result from Definition \ref{def:singularity_matrix}.
\end{proof}







\begin{proposition}\label{pro:polynomial_matrix_eigenvalue_eigenvector}
Let $p(A)$ be a polynomial of matrix $A\in M_n(\F)$. If $\lm$ is an eigenvalue of $A$, while $x$ is an associated eigenvector, then $p(\lm)$ is an eigenvalue of the matrix $p(A)$ and $x$ is an
eigenvector of $p(A)$ associated with $p(\lm)$.
\end{proposition}

\begin{proof}[\bf Proof]
Consider $p(A)x$. First,
\be
p(A)x = a_n A^n x + \dots + a_1 A x + a_0 x.
\ee

Second, $A^n x = A^{n-1}A x = A^{n-1}\lm x = \lm A^{n-1} x = \dots =\lm^n x$ by repeated application of the eigenvalue-eigenvector equation. Thus,
\be
p(A)x = a_n \lm^n x + \dots + a_1 \lm x + a_0 x = \bb{a_n\lm^n + \dots + a_1 \lm + a_0}x = p(\lm) x
\ee
which is the required result by definition of eigenvalue and eigenvector.
\end{proof}

\begin{example}
If $\sigma(A) = \bra{-1,2}$ for $A\in M_2(\F)$, then $\sigma(A^2) = \bra{1,4}$ by Proposition \ref{pro:polynomial_matrix_eigenvalue_eigenvector}.
\end{example}




\subsection{Characteristic polynomial}

%The may be rewritten equivalently as
%\be
%(A - \lm I ) x = 0,\qquad x\neq 0.
%\ee
%
%Thus, $\lm \in \sigma(A)$ if and only if $A - \lm I$ is a singular matrix. By Proposition \ref{pro:invertible_non_singular_equivalent} and Theorem \ref{thm:matrix_invertible_determinant_non_zero}, we have
%\be
%\det(A - \lm I) = 0.
%\ee

\begin{definition}[characteristic polynomial of matrix\index{characteristic polynomial!matrix}]\label{def:characteristic_polynomial_matrix}
The characteristic polynomial of $A\in M_n(\F)$ is defined by
\be
p_A(t) = \det\bb{tI - A}.
\ee
\end{definition}

\begin{proposition}\label{pro:characteristic_polynomial_root_coincide_spectrum}
If $A\in M_n(\F)$, the characteristic polynomial $p_A(t)$ has degree $n$ and the set of roots of $p_A(t) = 0$ coincides with $\sigma(A)$.
\end{proposition}

\begin{remark}
It means that the matrix has $n$ distinct eigenvalues at most\footnote{How can we say polynomial with degree $n$ has $n$ roots at most}.
\end{remark}

\begin{proof}[\bf Proof]
That $p_A(t)$ has degree $n$ follows inductively from Laplace expansion of $\det(tI -A)$: each row of $tI -A$ contributes one and only one power of $t$ as the determinant is expanded.

By Proposition \ref{pro:invertible_non_singular_equivalent} and Theorem \ref{thm:matrix_invertible_determinant_non_zero}, we have
\be
0 = p_A(\lm) = \det\bb{\lm I - A} \ \lra \ A - \lm I \text{ is singular matrix}\ \lra \ \lm \in \sigma(A) \ \lra \  (A - \lm I ) x = 0,\quad x\neq 0,
\ee
which is the equivalent form of eigenvalue-eigenvector equation (see Definition \ref{def:eigenvalue_eigenvector}) .
\end{proof}

\begin{definition}[elementary symmetric function\index{elementary symmetric function}]\label{def:elementary_symmetric_function}
Let $A\in M_n(\F)$ with eigenvalues $\lm_1,\dots,\lm_n$ by repeating eigenvalues according to multiplicity. The $k$th ($k\leq n$) elementary symmetric function of the eigenvalues is
\be
S_k(\lm_1,\dots,\lm_n) = \sum_{1\leq i_1\leq \dots \leq i_k\leq n}\bb{ \prod^k_{j=1} \lm_{i_j}},
\ee
the sum of all $\binom{n}{k}$ $k$-fold products of distinct items from $\lm_1,\dots,\lm_n$. Note that $S_0(\lm_1,\dots,\lm_n) = 1$.
\end{definition}

\begin{example}
If $n= 4$, then
\beast
S_1(\lm_1,\lm_2,\lm_3,\lm_4) & = & \lm_1 + \lm_2 + \lm_3 + \lm_4,\\
S_2(\lm_1,\lm_2,\lm_3,\lm_4) & = & \lm_1 \lm_2 + \lm_1 \lm_3 + \lm_1 \lm_4 + \lm_2 \lm_3 + \lm_2 \lm_4 + \lm_3 \lm_4,\\
S_3(\lm_1,\lm_2,\lm_3,\lm_4) & = & \lm_1 \lm_2 \lm_3 + \lm_1 \lm_2 \lm_4 + \lm_1 \lm_3 \lm_4 + \lm_2 \lm_3 \lm_4,\\
S_4(\lm_1,\lm_2,\lm_3,\lm_4) & = & \lm_1 \lm_2 \lm_3 \lm_4.
\eeast
\end{example}

\begin{definition}\label{def:sum_of_determinants_of_principal_submatrices}
Recalling Definition \ref{def:principal_submatrix}, there are $\binom{n}{k}$ different $k\times k$ principal submatrices of $A\in M_n(\F)$ and the sum of their determinants is denoted by $E_k(A)$.

In particular, $E_1(A) = \sum^n_{i = 1}a_{ii}$ is the trace of $A$, $\tr A$. Also, $E_n(A) = \det A$.

By convention, we have $\det(A_\emptyset) = 1$ and thus, $E_0(A) = 1$.
\end{definition}


\begin{theorem}\label{thm:elementary_symmetric_function_sum_of_determinant_principal_submatrix_equivalent}
If $\lm_1,\dots,\lm_n$ are the eigenvalues of $A\in M_n(\F)$, then
\be
S_k(\lm_1,\dots,\lm_n) = E_k(A).
\ee

That is, the $k$th elementary symmetric function of eigenvalues of $A$ is the sum of the $k\times k$ determinants of principal submatrix of $A$ (see \cite{Meyer_2001}.$P_{495}$). In particular, \be
\tr A = \sum^n_{i=1} \lm_i,\qquad \det A = \prod^n_{i=1} \lm_i. \ee
\end{theorem}

\begin{remark}
Note that Theorem \ref{thm:elementary_symmetric_function_sum_of_determinant_principal_submatrix_equivalent} implies that
\be
0\in \sigma(A) \ \ra \ \det A = 0 \ \ra \ A\text{ is singular (not invertible)}.
\ee
\end{remark}

\begin{proof}[\bf Proof]
By Proposition \ref{pro:characteristic_polynomial_root_coincide_spectrum}, we have
\be
p_A(t) = \prod^n_{i=1} (t-\lm_i) = t^n - S_1(\lm_1,\dots,\lm_n)t^{n-1} + \dots \pm S_n(\lm_1,\dots,\lm_n) = \sum^n_{k=0} (-1)^{k} S_k(\lm_1,\dots,\lm_n)t^{n-k}
\ee
which can be verified directly by picking out the coefficient of $t^k$ in the product.

Taking $k$ differentiation of $p_A(t)$ with respect to $t$, we have
\be
(n-k)!(-1)^{k} S_k(\lm_1,\dots,\lm_n) = p^{(n-k)}_A(0)\qquad (*).
\ee

By Proposition \ref{pro:derivative_of_determinant} (as $p_A(t)$ is a determinant), we have for $0\leq r\leq n$,
\be
p^{(r)}_A(t) = \sum_{i_1,\dots,i_r\text{ are distinct}} D_{i_1,\dots,i_r}(t),
\ee
where $D_{i_1,\dots,i_r}(t)$ is the determinant of the matrix identical to $tI -A$ expect that rows $i_1,\dots,i_r$ have been replaced by $e^T_{i_1},\dots, e^T_{i_r}$, respectively\footnote{If $i_j
= i_k$ for some $j\neq k$, we have that the determinant should be zero by Proposition \ref{pro:derivative_of_determinant}}. It follows that
\be
D_{i_1,\dots,i_r}(0) = (-1)^{n-r}\det\bb{A_{i_1,\dots,i_r}},
\ee
where $A_{i_1,\dots,i_r}$ is idential to $A$ except that $i_1,\dots,i_r$ have been replaced by $e^T_{i_1},\dots,e^T_{i_r}$, respectively.

Furthermore, $\det\bb{A_{i_1,\dots,i_r}}$ is the determinant of $(n-r)\times (n-r)$ principal submatrix obtained by deleting rows and columns $i_1,\dots,i_r$ from $A$. Thus,
\be
p^{(r)}_A(0) = \sum_{i_1,\dots,i_r\text{ are distinct}} D_{i_1,\dots,i_r}(0) = (-1)^{n-r} r! E_{n-r}(A)\qquad (\dag).
\ee

Note that $r!$ appears because each of the $r!$ permutations of the subscripts on $A_{i_1,\dots,i_r}$ describes the same matrix. Hence, let $k = n-r$ and combine $(*)$ and $(\dag)$, we have
\be
(n-k)!(-1)^{k} S_k(\lm_1,\dots,\lm_n) = p^{(n-k)}_A(0) = (-1)^{k} (n-k)! E_{n-r}(A) \ \ra\ S_k(\lm_1,\dots,\lm_n) = E_{n-r}(A),
\ee
as required.
\end{proof}

\begin{example}
If $A\in M_2(\F)$, then
\be
p_A(t) = t^2 - t \cdot \tr A  + \det A
\ee
and that
\be
\sum_{\lm_i \in \sigma(A)} \lm_i = \tr A,\qquad \prod_{\lm_i \in \sigma(A)} \lm_i = \det A.
\ee
\end{example}

\section{Similar Matrices}

\subsection{Similarity}

\begin{definition}[similar matrices\index{similar!matrix}]\label{def:similar_matrix}
Two matrices $A,B\in M_n(\F)$ are similar if $B = P^{-1}AP$ for some invertible matrix $P\in M_n(\F)$.

The transformation $A \to P^{-1}AP$ is called a similarity transformation\index{similarity transformation} by the similarity matrix\index{similarity matrix} $P$. The relation `$A$ is similar to $B$' is sometimes abbreviated $A \sim B$.
\end{definition}

\begin{proposition}\label{pro:similarity_implies_equivalent_matrices}
Let $A,B\in M_n(\F)$ with $A\sim B$. Then $A$ is equivalent to $B$.
\end{proposition}

\begin{proof}[\bf Proof]
Direct result from definitions of similar matrices (Definition \ref{def:similar_matrix}) and equivalent matrices (Definition \ref{def:equivalent_matrix}) as we can let $P = Q$.
\end{proof}

\begin{proposition}[similarity is equivalent relation]
Similarity is an equivalent relation on $M_n(\F)$, i.e., similarity is for $A,B,C\in M_n(\F)$,
\ben
\item [(i)] reflexive: $A\sim A$
\item [(ii)] symmetric: $A\sim B \ \ra\ B\sim A$.
\item [(iii)] transitive: $A\sim B, B\sim C \ \ra\ A\sim C$.
\een
\end{proposition}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
\ben
\item [(i)] It is obvious that invertible matrix $P = I$ such that $A = IAI = P^{-1}AP$.
\item [(ii)] If $A\sim B$, there exists invertible matrix $P$ such that $B = P^{-1}AP$. Thus, for $Q = P^{-1}$, $B = QAQ^{-1} \ \lra \ Q^{-1}BQ = A$. Hence, $B\sim A$.
\item [(iii)] If $A\sim B$ and $B\sim C$, there exist invertible matrices $P$ and $Q$ such that $B = P^{-1}AP$ and $C = Q^{-1}BQ$. Thus, for invertible matrix $R = PQ$,
\be
C = Q^{-1}BQ = Q^{-1}P^{-1}A PQ = R^{-1}AR \ \ra \ A \sim C,
\ee
as $(PQ)^{-1} = Q^{-1}P^{-1}$ by Proposition \ref{pro:inverse_matrix_property}.(ii).
\een
\end{proof}


\begin{proposition}
Similar matrices have the same determinant.
\end{proposition}

\begin{proof}[\bf Proof]
If $A$ and $B$ are similar, then there exists invertible matrix $P$ such that $B= P^{-1}AP$. Then by Theorem \ref{thm:determinant_product} and Corollary \ref{cor:determinant_inverse},
\be
\det B = \det\bb{P^{-1}AP} = \det \bb{P^{-1}} \det A\det P = (\det A)(\det P)(\det P)^{-1} = \det A.
\ee
\end{proof}


\begin{proposition}
Similar matrices have the same trace, i.e., for $A,B,P\in M_n(\F)$, if $B = P^{-1}AP$ for some invertible matrix $P$, then $\tr A = \tr B$.
\end{proposition}

\begin{proof}[\bf Proof]
By Proposition \ref{pro:trace_change_order},
\be
\tr B = \tr\bb{P^{-1}AP} = \tr\bb{APP^{-1}} = \tr\bb{AI} = \tr A.
\ee
\end{proof}

\begin{theorem}\label{thm:similar_matrices_have_same_characteristic_polynomial}
Let $A,B\in M_n(\F)$. If $B$ is similar to $A$, then the characteristic polynomial of $B$ is the same as that of $A$. That is, $p_A(t) = p_B(t)$.
\end{theorem}

\begin{proof}[\bf Proof]
For any $t$ we have
\beast
p_B(t) & = & \det\bb{tI - B} = \det\bb{tP^{-1}P - P^{-1}AP} = \det\bb{P^{-1}(tI -A)P} \\
& = &  \det P^{-1} \det(tI -A) \det P \qquad (\text{Theorem \ref{thm:determinant_product}}) \\
& = &  \det(tI -A) \det P \det P^{-1} = \det(tI -A) = p_A(t)
\eeast
as required.
\end{proof}

\begin{corollary}\label{cor:similarity_same_eigenvalues}
If $A,B\in M_n(\F)$ and if $A$ and $B$ are similar, then they have the same eigenvalues, counting multiplicity.
\end{corollary}

\begin{proof}[\bf Proof]
Direct result from Theorem \ref{thm:similar_matrices_have_same_characteristic_polynomial} and Proposition \ref{pro:characteristic_polynomial_root_coincide_spectrum}.
\end{proof}

\begin{example}\label{exa:same_eigenvalues_do_not_imply_similarity}
Having the same eigenvalues is a necessary but not sufficient condition for similarity. Consider the matrices
\be
A = \bepm 0 & 1 \\ 0 & 0 \eepm, \qquad B = \bepm 0 & 0 \\ 0 & 0 \eepm.
\ee

Each has the eigenvalue 0 with multiplicity 2, but they are not similar.
\end{example}


\subsection{Diagonalizable matrices}

\begin{definition}[diagonalizable matrix\index{diagonalizable matrix}]\label{def:diagonalizable_matrix}
If the matrix $A\in M_n(\F)$ is similar to a diagonal matrix, then $A$ is said to be diagonalizable.
\end{definition}

\begin{theorem}\label{thm:diagonalizable_linearly_independent_eigenvector_equivalent}
Let $A\in M_n(\F)$. Then $A$ is diagonalizable if and only if there is a set of $n$ linearly independent vectors, each of which is an eigenvector of $A$.
\end{theorem}

\begin{proof}[\bf Proof]
If $A$ has $n$ linearly independent eigenvectors $x^1,\dots, x^n$, form a nonsingular matrix $P$ with them as columns (by Corollary \ref{cor:invertible_column_linearly_independent}) and calculate
\beast
P^{-1} A P & = & P^{-1}A \bb{x^1, \dots , x^n } = P^{-1} \bb{ Ax^1 , \dots , Ax^n} = P^{-1} \bb{ \lm_1 x^1 , \dots , \lm_n x^n}   \\
& = & P^{-1} \bb{x^1,\dots,x^n} \Lambda = P^{-1}P \Lambda = \Lambda
\eeast
where
\be
\Lambda := \diag\bb{\lm_1,\dots,\lm_n} = \bepm \lm_1 & & \\ & \ddots & \\ & & \lm_n \eepm
\ee
and $\lm_1,\dots,\lm_n$ are the corresponding eigenvalues of $A$.

Conversely, suppose that there is a similarity matrix $P$ such that $P^{-1}AP = \Lambda$, then $AP = P\Lambda$. This means that $A$ times the $i$th column of $P$ (i.e., the $i$th column of $AP$) is
the $i$th diagonal entry of $\Lambda$ times the $i$th column of $P$ (i.e., the $i$th column of $P\Lambda$), or that the $i$th column of $P$ is an eigenvector of $A$ associated with the $i$th
diagonal entry of $\Lambda$. Since $P$ is invertible, there are $n$ linearly independent eigenvectors by Corollary \ref{cor:invertible_column_linearly_independent}.
\end{proof}


\begin{lemma}\label{lem:distinct_eigenvalues_implies_linearly_independent_eigenvectors}
Suppose that $\lm_1,\dots,\lm_k$ are distinct eigenvalues of $A\in M_n(\F)$, and suppose that $x^{(i)}$ is an eigenvector associated with $\lm_i$, $i=1,\dots,k$. Then $\bra{x^{(1)},\dots,x^{(k)}}$ is a
linearly independent set.
\end{lemma}

\begin{proof}[\bf Proof]
The proof is essentially by contradiction (see \cite{Horn_Johnson_1990}.$P_{47}$).

Suppose that $x^{(1)},\dots,x^{(k)}$ is actually a linearly independent set. Then there is a nontrivial linear transformation which produces the zero vector, and in fact there is such a linear combination
with the fewest non-zero coefficients. Suppose that such a minimal linear dependence relation is
\be
a_1 x^{(1)} + \dots + a_r x^{(r)} = 0,\qquad r\leq k.\qquad (*)
\ee

We have $r>1$ because all $x^i \neq 0$ for $i = 1,\dots,r$. We may assume for convenience (renumber if necessary) that it involves the first $r$ vectors. We also have
\be
A \bb{a_1 x^{(1)} + \dots + a_r x^{(r)}} = a_1 A x^{(1)} + \dots + a_r A x^{(r)} = a_1 \lm_1 x^{(1)} + \dots + a_r \lm_r x^{(r)} = 0\qquad (\dag)
\ee
by $(*)$, which is another dependence relation. Now we have $(\dag) - (*)\times \lm_r$ is
\be
a_1 (\lm_1 - \lm_r) x^{(1)} + \dots + a_{r-1}(\lm_{r-1}-\lm_r) x^{(r-1)} = 0, \qquad (**)
\ee
which has fewer non-zero coefficients than ($*$). Note that $(**)$ is nontrivial since $\lm_i \neq \lm_r$ for $i = 1,\dots, r-1$. This contradicts the minimality assumption for ($*$).
\end{proof}


\begin{theorem}\label{thm:distinct_eigenvalues_implies_diagonalizable}
If $A\in M_n(\F)$ has $n$ distinct eigenvalues, then $A$ is diagonalizable.
\end{theorem}

\begin{proof}[\bf Proof]
Direct result from Theorem \ref{thm:diagonalizable_linearly_independent_eigenvector_equivalent} and Lemma \ref{lem:distinct_eigenvalues_implies_linearly_independent_eigenvectors}
\end{proof}

\begin{example}
If $A$ is diagonalizable, it is not necessary to have distinct eigenvalues. For instance, $\bepm 0 & 0 \\ 0 & 0 \eepm$.
\end{example}

\begin{proposition}
Let $A\in M_n(\F)$ and $B\in M_m(\F)$ be given matrices and let
\be
C = \bepm A & 0 \\ 0 & B \eepm \in M_{m+n}(\F)
\ee
be the direct sum (see Definition \ref{def:block_diagonal_matrix}) of $A$ and $B$. Then $C$ is diagonalizable if and only if both $A$ and $B$ are diagonalizable.
\end{proposition}

\begin{proof}[\bf Proof]
If $A$ and $B$ are diagonalizable, there are invertible matrices $P\in M_n(\F),Q \in M_m(\F)$ such that
\be
P^{-1}AP,\ Q^{-1}BQ\ \text{ are diagonal matrices.}
\ee

Then it is easy to check that $R^{-1}C R$ is diagonal if $R = \bepm P & 0 \\ 0 & Q \eepm$.

Conversely, if $C$ is diagonalizable, there is an invertible matrix $R\in M_{m+n}(\F)$ such that
\be
R^{-1}C R = \Lambda = \diag\bra{\lm_1,\dots,\lm_{m+n}} \text{ is diagonal.}
\ee

We can write that $R = \bb{R_1,\dots,R_{m+n}}$ with
\be
R_i = \bepm P_i \\ Q_i \eepm \in \C^{m+n},\quad P_i \in \C^n,\quad Q_i \in \C^m,\qquad i = 1,2,\dots,m+n.
\ee

Then $C R_i = \lm_i R_i$ implies that
\be
A P_i = \lm_i P_i,\ B Q_i = \lm_i Q_i \quad \text{for }i = 1,2,\dots,m+n.
\ee

If there were fewer than $n$ independent vector in the set $\bra{P_1,\dots,P_{m+n}}$, then the column rank (and hence the row rank) of the matrix
\be
\bepm P_1 & \dots & P_{m+n} \eepm \in M_{n,m+n}(\F)
\ee
would be less than $n$ (by Lemma \ref{lem:finite_basis_linearly_independent_spanning_equivalent} and definitions of rank (Definition \ref{def:column_rank_matrix}) and dimension
(Theorem \ref{thm:dimension_vector_space})). By the same reasoning, if there were fewer than $m$ independent vectors in the set $\bra{Q_1,\dots,Q_{m+n}}$, then the column rank (and hence the row rank) of matrix
\be
\bepm Q_1 & \dots & Q_{m+n} \eepm \in M_{m,m+n}(\F)
\ee
would be less than $m$. In either event (or both), the matrix would have row rank (and hence rank) less than $m+n$, which is impossible since $R$ is invertible. %\footnote{theorem needed here.}

Thus, there are exactly $n$ linearly independent vectors in the set $\bra{P_1,\dots,P_{m+n}}$. Let these vectors be $\bra{P'_1,\dots,P'_n}$ and $P' = \bb{P'_1,\dots,P'_n}$ is invertible by Corollary
\ref{cor:invertible_column_linearly_independent}. Since each of these $n$ vectors is an eigenvector of $A$, we have that
\be
AP'_i = \lm_i' P'_i \ \ra \ A P' = P' \Lambda'
\ee
where $\lm'_i$ is the corresponding eigenvalue of $P'_i$ and $\Lambda' = \diag\bra{\lm'_1,\dots,\lm'_n}$. Since $P'$ is invertible, we have $P^{-1}AP = \Lambda'$ and thus the matrix $A$ must be
diagonalizable. The same argument shows that the matrix $B$ is diagonalizable.
\end{proof}

\subsection{Simultaneously diagonalizable matrices}

\begin{definition}[simultaneously diagonalizable matrices\index{simultaneously diagonalizable!matrices}]\label{def:simultaneously_diagonalizable_matrices}
Two diagonalizable matrices $A,B\in M_n(\F)$ are said to be simultaneously diagonalizable if there is a single similarity matrix $P\in M_n(\F)$ such that $P^{-1}AP$ and  $P^{-1}BP$ are both
diagonal, i.e., if there is a single basis in which the representations of both linear transformations are diagonal.
\end{definition}

\begin{theorem}\label{thm:commute_iff_simultaneously diagonalizable}
Let $A,B\in M_n(\F)$ be diagonalizable. Then $A$ and $B$ commute if and only if they are simultaneously diagonalizable.
\end{theorem}

\begin{proof}[\bf Proof]
\footnote{proof needed.}
\end{proof}


\section{Unitary Equivalence}

We use $\F = \C$ in this section.

\subsection{Unitary matrices}



\begin{theorem}\label{thm:unitary_matrix_property}
If $U\in M_n(\C)$, the following are equivalent:
\ben
\item [(i)] $U$ is unitary.
\item [(ii)] $U$ is invertible (nonsingular) and $U^* = U^{-1}$.
\item [(iii)] $UU^* = I$.
\item [(iv)] $U^*$ is unitary.
\item [(v)] The columns of $U$ form an orthonormal set.
\item [(vi)] The rows of $U$ form an orthonormal set.
\item [(vii)] For all $x\in \C^n$, the Euclidean length of $y = Ux$ is the same as that of $x$, i.e., $y^*y = x^*x$.
\item [(viii)] $U$ is an isometry with respect to the usual norm.
\item [(ix]) $U$ is a normal matrix with eigenvalues lying on the unit circle.
\een
\end{theorem}

\begin{proof}[\bf Proof]
\footnote{proof needed.}
\end{proof}

\subsection{Unitary equivalence}

\begin{definition}[unitarily equivalent\index{unitarily equivalent!matrix}]\label{def:unitarily_equivalent_matrix}
Let $A,B\in M_n(\C)$ and $B$ is said to be unitarily equivalent (unitarily similar) to $A$ if there is a unitary matrix $U\in M_n(\C)$ such that
\be
B = U^* A U.
\ee

If $U$ can be taken to be real (and hence is real orthogonal), then $B$ is said to be (real) orthogonally equivalent (orthogonally similar) to $A$.
\end{definition}



\begin{proposition}[unitary equivalence is equivalent relation]\label{pro:unitary_equivalence_implies_equivalent_matrices}
Unitary equivalence is an equivalent relation on $M_n(\C)$, i.e., unitary equivalence is for $A,B,C\in M_n(\C)$
\ben
\item [(i)] reflexive: $A$ is unitarily equivalent to $A$
\item [(ii)] symmetric: $A$ unitarily equivalent to $B$ $\ \ra\ $ $B$ unitarily equivalent to $A$.
\item [(iii)] transitive: $A$ unitarily equivalent to $B$, $B$ unitarily equivalent to $C$ $\ \ra\ $ $A$ unitarily equivalent to $C$.
\een
\end{proposition}


\begin{proof}[\bf Proof]%\footnote{proof needed.}
\ben
\item [(i)] It is obvious that invertible matrix $U = I$ such that $A = IAI = U^*AU$.
\item [(ii)] If $A$ unitarily equivalent to $B$, there exists unitary matrix $U$ such that $B = U^*AU$. Thus, for unitary matrix $V = U^{-1}$, $A = U B U^{-1} = V^{-1}BV = V^*BV $. Hence, $B$
    unitarily equivalent to $A$.

\item [(iii)] If $A$ unitarily equivalent to $B$ and $B$ unitarily equivalent to $C$, there exist unitary matrices $U$ and $V$ such that $B = U^*AU$ and $C = V^*BV$. Thus, for unitary matrix $R = UV$,
\be
C = V^*BV = V^*U^* A UV = R^* A R \ \ra \ A \text{ unitarily equivalent to } C,
\ee
as $(UV)^* = V^*U^*$ by Proposition \ref{pro:matrix_multiple_hermitian}.
\een
\end{proof}

\begin{proposition}[unitary equivalence implies similarity]\label{pro:unitary_equivalence_implies_similarity}
Let $A,B\in M_n(\C)$ with $A$ is unitarily equivalent to $B$. Then $A\sim B$.
\end{proposition}

\begin{proof}[\bf Proof]
Direct result from the definition by letting $P = U$ as $U^* = U^{-1}$.
\end{proof}

\begin{example}
The converse is not true as
\be
\bepm 3 & 1 \\ -2 & 0 \eepm\quad \text{and}\quad \bepm 1 & 1 \\ 0 & 2 \eepm
\ee
are similar but are not unitarily equivalent.
\end{example}

This can be checked by the following theorem.

\begin{theorem}\label{thm:unitary_equivalence_elements_square_sum}
If $A,B\in M_n(\C)$ are unitarily equivalent, then
\be
\sum^n_{i,j =1}\abs{a_{ij}}^2 = \sum^n_{i,j =1}\abs{b_{ij}}^2.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Observe that
\be
\tr\bb{A^* A} = \sum^n_{i,j =1}\abs{a_{ij}}^2,\qquad \tr\bb{B^* B} = \sum^n_{i,j =1}\abs{b_{ij}}^2
\ee
by carrying out the matrix multiplication. Thus, it suffices to show that $\tr(A^*A) = \tr (B^*B)$. But if $B= U^*A U$ for unitary matrix $U$,
\be
\tr\bb{B^*B} = \tr\bb{\bb{U^*AU}^*U^*AU} = \tr\bb{U^*A^*UU^*AU} = \tr\bb{U^*A^*AU} = \tr\bb{A^*A UU^*} =  \tr\bb{A^*A}
\ee
by Proposition \ref{pro:matrix_multiple_hermitian} and Proposition \ref{pro:trace_change_order}.
\end{proof}

\subsection{Schur's unitary triangularization theorem}

Perhaps the most fundamentally useful fact of elementary matrix theory is that any matrix $A\in M_n(\C)$ is unitarily equivalent to an upper triangular matrix $T$ (and also to a lower triangular
matrix). The diagonal entries of $T$ are, of course, the eigenvalues of $A$. Although this form is far from unique, it represents the simplest form achievable under unitary equivalence.

\begin{theorem}[Schur's unitary triangularization theorem\index{Schur's unitary triangularization theorem}]\label{thm:schur_unitary_triangularization}
Given $A\in M_n(\C)$ with eigenvalues $\lm_1,\dots,\lm_n$ in any prescribed order, there a unitary matrix $U\in M_n(\C)$ such that
\be
U^* A U = T = \bb{t_{ij}}
\ee
is upper triangular, with diagonal entries $t_{ii} = \lm_i$, $i=1,\dots,n$. That is, every square matrix $A$ is unitarily equivalent to a triangular matrix whose diagonal entries are the eigenvalues
of $A$ in a prescribed order.

Furthermore, if $A\in M_n(\R)$ and if all the eigenvalues of $A$ are real, then $U$ may be chosen to real and orthogonal.
\end{theorem}

\begin{remark}
Note that ``upper triangular'' could be replaced by ``lower triangular'' in the statement of the theorem with, of course, a different unitary equivalence $U$.
\end{remark}

\begin{proof}[\bf Proof]
The proof is algorithmic and proceeds by a sequence of reductions of like type. Let $x^{(1)}$ be a normalized eigenvector (i.e., ${x^{(1)}}^* x^{(1)} = 1$) of $A$ associated with eigenvalue $\lm_1$.
$x^{(1)}$ may be extended to a basis (by Lemma \ref{lem:linearly_independent_extend_basis})
\be
x^{(1)},\ y^{(2)},\ y^{(3)},\ \dots\ ,\ y^{(n)}
\ee
of $\C^n$. Apply the Gram-Schmidt orthonormalization (Algorithm \ref{alg:gram_schmidt_orthonormalization}) to this basis to produce an orthonormal basis
\be
x^{(1)},\ z^{(2)},\ z^{(3)},\ \dots\ ,\ z^{(n)}
\ee
of $\C^n$. Array these orthonormal vectors left to right as the columns of a unitary matrix $U_1$ (by Theorem \ref{thm:unitary_matrix_property}). Since the first columns of $AU_1$ is $\lm_1 x^{(1)}$, a
calculation reveals that $U^*_1(AU_1)$ has the form
\be
U^*_1(AU_1) = \bepm \lm_1 & * \\ 0 & A_1 \eepm.
\ee

The matrix $A_1 \in M_{n-1}(\C)$ has eigenvalues $\lm_2,\dots,\lm_n$ as $U^*_1AU_1$ has the same eigenvalues with $A$ (This is implied by
\be
\det\bb{tI -A} = \det \bb{U_1 tIU_1^* - U_1U_1^*A} = \det \bb{\bb{tIU_1^* - U_1^*A}U_1 } =  \det \bb{tI - U_1^*AU_1}% = \det\bb{U_1^*(tI)U_1 - U_1AU_1}
\ee
by Theorem \ref{thm:determinant_product}) by Proposition \ref{pro:characteristic_polynomial_root_coincide_spectrum}. Let $x^{(2)}\in \C^{n-1}$ be a normalized eigenvector of $A_1$ corresponding
to $\lm_2$, and do it all over again. Determine a unitary $U_2 \in M_{n-1}(\C)$ such that
\be
U^*_2 A_1U_2  = \bepm \lm_2 & * \\ 0 & A_2 \eepm
\ee
and let
\be
V_2 = \bepm 1 & 0 \\ 0 & U_2 \eepm.
\ee

The matrices $V_2$ and $U_1V_2$ are then unitary (by definition of unitary matrix), and $V_2^*U_1^*AU_1V_2$ has the form
\be
V_2^*U_1^*AU_1V_2 = \bb{\ba{c;{2pt/2pt}c}\ba{cc} \lm_1 & * \\ 0 & \lm_2 \ea & \text{\Large $*$}  \\ \hdashline[2pt/2pt] \ba{c}\text{\Large $\stackrel{\qquad}{0}$} \ea & A_2 \ea}
\ee

%\be
%V_2^*U_1^*AU_1V_2 =
%\begin{array}{c@{\hspace{-5pt}}l}
%\left(\begin{array}{cc;{2pt/2pt}c}
%\lm_1 & * &  \multicolumn{1}{c}{\Large *} \\
%0 & \lm_2 & \\
%\hdashline[2pt/2pt]
%\multicolumn{2}{c;{2pt/2pt}}{\raisebox{1ex}{\large 0}} & A_2
%\end{array}\right)
%\end{array}
%\ee



Continue this reduction to produce unitary matrices $U_i\in M_{n-i+1}(\C)$, $i=1,\dots,n-1$ and unitary matrices $V_i\in M_n(\C)$, $i=2,\dots,n-1$. The matrix
\be
U = U_1 V_2 V_3 \dots V_{n-1}
\ee
is unitary and $U^*AU$ yields the desired form.

If all eigenvalues of $A\in M_n(\R)$ happen to be real, then the corresponding eigenvectors can be chosen to be real and all the above steps may be carried out in real arithmetic, verifying the final assertion.
\end{proof}

\begin{example}
Neither the unitary matrix $U$ nor the triangular matrix $T$ of Schur's unitary triangularization theorem is unique. Not only may the diagonal entries of $T$ (the eigenvalues of $A$) appear in any
order, but unitarily equivalent upper triangular matrices may appear very different above the diagonal. For example,
\be
T_1 = \bepm 1 & 1 & 4 \\ 0 & 2 & 2 \\ 0 & 0 & 3 \eepm \qquad \text{and}\qquad T_2 = \bepm 2 & -1 & 3\sqrt{2} \\ 0 & 1 & \sqrt{2} \\ 0 & 0 & 3 \eepm
\ee
are unitarily equivalent via
\be
U = \frac 1{\sqrt{2}}\bepm 1 & 1 & 0 \\ 1 & -1 & 0 \\ 0 & 0 & \sqrt{2} \eepm.
\ee

In general, many different upper triangular matrices can be in the same unitary equivalence class.
\end{example}

A strictly real version of Schur's unitary triangularization theorem (Theorem \ref{thm:schur_unitary_triangularization}) is contained in the following theorem.

\begin{theorem}
If $A\in M_n(\R)$, there is a real orthogonal matrix $Q\in M_n(\R)$ such that
\be
Q^TAQ = \bepm \ba{cc} A_1 & \\ & A_2 \ea & \text{\Large $*$} \\ \text{\Large 0} & \ba{cc} \ddots & \\ & A_k \ea \eepm\in M_n(\R),\quad 1\leq k\leq n
\ee
where each $A_i$ is a real $1\times 1$ matrix, or a real $2\times 2$ matrix with a non-real pair of complex conjugate eigenvalues. The diagonal blocks $A_i$ may be arranged in any prescribed order.
\end{theorem}

\begin{proof}[\bf Proof]
We can modify the argument for Theorem \ref{thm:schur_unitary_triangularization} to prove this theorem.

If $\lm$ is a real eigenvalue of the real matrix $A$, then there is a corresponding real eigenvector that can be used to deflate $A$ as in Theorem \ref{thm:schur_unitary_triangularization}.

Now let $\lm = \alpha + i\beta$ with $\beta \neq 0$ is a non-real eigenvalue of $A$ and $Ax = \lm x$, $x = u + iv\neq 0$, $u,v\in \R^n$. By Proposition \ref{pro:component_wise_conjugate_matrix}, we have $A\ol{x} =
\ol{\lm} \ol{x}$ (and thus $\ol{\lm}$ is also an eigenvalue associated with eigenvector $\ol{x}$). If $x$ and $\ol{x}$ are linearly dependent, then there exists $a\in \C$ such that $ax = \ol{x}$. Then we have
\be
\left\{\ba{l}
A ax = A\ol{x} \ \ra \ a\lm x = \ol{\lm}\ol{x} \\
a\lm x = \lm \ol{x} \ea\right.  \ \ra \ 0 = \bb{\lm - \ol{\lm}}\ol{x} \ \ra \ \lm = \ol{\lm} \quad \text{ since }\ \ol{x} \neq 0,
\ee
which is a contradiction to $\beta \neq 0$. Thus, we have that $x$ and $\ol{x}$ are linearly independent. Therefore, we can apply Gram-Schmidt orthonormalization (Algorithm
\ref{alg:gram_schmidt_orthonormalization}) with respect to inner product (dot product) to $\bra{u,v}$ ($\bra{x,\ol{x}}$) to obtain a real orthonormal set $\bra{w,z}$ with
\be
w = \frac{u}{\sqrt{\inner{u}{u}}},\quad y = v - \inner{v}{w}w,\quad z = \frac{y}{\sqrt{\inner{y}{y}}}.   %= \frac{v - \inner{v}{w}w}{\sqrt{\inner{v}{v} - \ol{\inner{v}{w}}\inner{v}{w} - \inner{v}{w}\inner{w}{v} - \inner{v}{w}\ol{\inner{v}{w}} \inner{w}{w}}}  .
\ee

Let $Q_1$ be the real orthogonal matrix whose first two columns are $w$ and $z$ by Theorem \ref{thm:unitary_matrix_property}. Thus, by the orthogonormality of $w,z$ and other columns of $Q_1$,
\beast
Q_1^T A Q_1 & = &  \bepm w^T \\ z^T \\ * \eepm A \bepm w & z & * \eepm = \bepm w^T \\ z^T \\ * \eepm \bepm Aw & Az & * \eepm  =  \bepm w^T \\ z^T \\ * \eepm \bepm \lm w & \frac 1{\sqrt{\inner{y}{y}}}\bb{\ol{\lm} v - \lm\inner{v}{w}w} & * \eepm
\\
& = & \bepm w^T \\ z^T \\ * \eepm \bepm \lm w & \frac 1{\sqrt{\inner{y}{y}}}\bb{\ol{\lm} y + \bb{\ol{\lm}-\lm}\inner{v}{w}w} & * \eepm
=  \bepm w^T \\ z^T \\ * \eepm \bepm \lm w & \ol{\lm} z + \frac 1{\sqrt{\inner{y}{y}}}\bb{\ol{\lm}-\lm}\inner{v}{w}w & * \eepm\\
& = &  \bepm w^T \lm w & w^T \bb{\ol{\lm} z + \frac 1{\sqrt{\inner{y}{y}}}\bb{\ol{\lm}-\lm}\inner{v}{w}w} & * \\ z^T \lm w & z^T \bb{\ol{\lm} z + \frac 1{\sqrt{\inner{y}{y}}}\bb{\ol{\lm}-\lm}\inner{v}{w}w} & * \\ 0 & 0 & * \eepm
=  \bepm \lm & * & * \\ 0 & \ol{\lm} & * \\ 0 & 0 & \wt{A} \eepm
\eeast
so that $A$ may be deflated two columns at a time in this case. Note that blocks $A_i$ corresponding to each real eigenvalue and each pair of complex conjugate eigenvalues can be arranged in any prescribed order.

Hence, we can repeat the similar steps in proof of Theorem \ref{thm:schur_unitary_triangularization}.
\end{proof}

\begin{lemma}\label{lem:product_of_triangular_matrices}
Suppose that $R = \bb{r_{ij}}$ and $T = \bb{t_{ij}} \in M_n(\F)$ are upper triangular and that $r_{ij} = 0$, $1\leq i,j\leq k \leq n$ and $t_{k+1,k+1} = 0$. Let $S = (s_{ij}) = RT$. Then
\be
s_{ij} = 0,\qquad 1\leq i,j\leq k+1.
\ee
\end{lemma}

\begin{proof}[\bf Proof]
Since $R\bb{\bra{1,2,\dots,k}} = 0$ and $t_{k+1,k+1} =0$, $R$ and $T$ have the form
\be
R = \bb{\ba{c;{2pt/2pt}c} \text{\large 0} & \text{\large $*$} \\ \hdashline[2pt/2pt] \text{\large 0} & \ba{ccc} * & & * \\ & \ddots & \\ 0 & & * \ea \ea} ,\qquad
T = \bb{\ba{c;{2pt/2pt}c} \ba{ccc} * & & * \\ & \ddots & \\ 0 & & * \ea & \text{\large $*$} \\ \hdashline[2pt/2pt] \text{\large 0} & \ba{cccc} 0 & & & * \\ & * & & \\ & & \ddots & \\ 0 & & & * \ea  \ea}
\ee
where both upper left blocks in the partitions are $k\times k$. The upper left $k\times k$ block of $S$ is clearly 0 by partitioned multiplication. Also, inspection reveals that the first $k+1$ rows
of $R$ have 0's in all non-zero positions of column $k+1$ of $T$, and that the first $k+1$ columns of $T$ have 0's in all non-zero positions of row $k+1$ of $R$. Matrix multiplication then shows that $S$ has the form
\be
S = \bb{\ba{c;{2pt/2pt}c;{2pt/2pt}c} \text{\Large 0}  & \ba{c} 0 \\ \vdots \\ 0 \ea & \text{\Large $*$} \\ \hdashline[2pt/2pt] 0 & 0 & * \\ \hdashline[2pt/2pt] \text{\Large 0} & \ba{c} 0 \\  \vdots \\ 0 \ea & \ba{cccc} * & & * \\ & \ddots & \\ 0 & & * \ea  \ea}
\ee
and $S\bb{\bra{1,2,\dots,k+1}} = 0$, as was to be shown.
\end{proof}

\begin{definition}[annihilation polynomial\index{annihilation polynomia}l]\label{def:annihilation_polynomial}
A polynomial $p(t)$ whose value is the zero matrix at $A$ is said to annihilate $A$. That is, $p(A) = 0$.
\end{definition}

\begin{theorem}[Cayley-Hamilton theorem\index{Cayley-Hamilton theorem}]\label{thm:cayley_hamilton}
Let $p_A(t)$ be the characteristic polynomial of $A\in M_n(\C)$. Then $p_A(t)$ annihilate $A$, i.e.,
\be
p_A(A) = 0.
\ee
\end{theorem}

\begin{remark}
\ben
\item [(i)] Note that we can not use the following proof
\be
p_A(t) = \det\bb{tI -A}\ \ra\ p_A(A) = \det\bb{AI - A} = \det 0 = 0
\ee
since $p_A(A)$ is a matrix and $\det\bb{AI - A}$ is a number.

\item [(ii)] Since we have the Cayley-Hamilton theorem for matrices with complex entries, and hence it must hold for matrices whose entries come from any subfield of the complex numbers (the reals
    or the rationals).

    In fact, the Cayley-Hamilton theorem is a completely formal result that holds for matrices whose entries come from any field or, more generally, any commutative ring\footnote{checking needed.}.
    \een
\end{remark}

\begin{proof}[\bf Proof]
Since $p_A(t)$ is of degree $n$ with leading coefficient 1 and the roots of $p_A(t) = 0$ are precisely the eigenvalues $\lm_1,\dots,\lm_n$ of $A$ (by Proposition
\ref{pro:characteristic_polynomial_root_coincide_spectrum}), counting multiplicity, we may factor $p_A(t)$ as
\be
p_A(t) = (t-\lm_1)(t-\lm_2)\dots (t-\lm_n).
\ee

By Schur's unitary triangularization theorem (Theorem \ref{thm:schur_unitary_triangularization}), we can write $A$ as $A = UTU^*$ where $T$ is upper triangular with $\lm_i$ in the $i$th diagonal
position, $i=1,\dots,n$. Now compute
\beast
p_A(A) & = & p_A\bb{UTU^*} = \bb{UTU^* - \lm_1 I}\bb{UTU^* - \lm_2 I} \dots \bb{UTU^* - \lm_n I} \\
& = &  \bb{U(T - \lm_1 I)U^*}\bb{U(T - \lm_2 I)U^*} \dots \bb{U(T - \lm_n I)U^*} \\
& = &  U(T - \lm_1 I)(T - \lm_2 I) \dots (T - \lm_n I)U^* = U p_A(T)U^*
\eeast
and notice that $p_A(A) = 0$ if and only if $p_A(T) = 0$. However, Lemma \ref{lem:product_of_triangular_matrices} allows us to conclude that $p_A(T) =0$. The upper left $1\times 1$ block of $T-\lm_1
I$ is 0 and $(2,2)$ entry of $T-\lm_2 I$ is 0; since both are upper triangular, the upper left $2\times 2$ block of $(T-\lm_1 I)(T-\lm_2 I)$ is 0. Inductively, since the upper left $k\times k$ block
of $(T-\lm_1 I)(T-\lm_2 I)\dots (T-\lm_k I)$ and the $(k+1,k+1)$ of $(T-\lm_{k+1} I)$ are 0 and both are upper triangular, the upper left $(k+1)\times (k+1)$ block of $(T-\lm_1 I)\dots(T-\lm_{k+1}
I)$ is 0. Continuation until $n$ is reached allows us to conclude the product $p_A(T) = (T-\lm_1 I)\dots(T-\lm_n I) = 0$, which completes the proof.
\end{proof}

One important use of the Cayley-Hamilton theorem is to write powers $A^k$ of $A\in M_n(\C)$, for $k\geq n$, as linear combinations of $I,A,A^2,\dots,A^{n-1}$. By a linear dependence argument, it is
easy to show (since the dimension of $M_n(\C)$, considered as a vector space over the complex numbers, is $n^2$) that powers $A^{n^2}$ and beyond can be expressed as linear combinations of lower
powers, but the Cayley-Hamilton theorem provides a notable improvement.

\begin{example}
Let $A = \bepm 3 & 1 \\ -2 & 0 \eepm$. Then $p_A(t) = t^2 - 3t + 2$, and $A^2 - 3A + 2I = 0$. Thus,
\beast
A^2 & = & 3A - 2I \\
A^3 & = & A(A^2) = 3A^2 - 2A = 3\bb{3A - 2I} = 7A - 6I\\
A^4 & = & 7A^2 - 6A = 15A - 14I,
\eeast
and so on. Also, since the constant term in $p_A(t)$, the determinant of $A$, is non-zero, $A$ is nonsingular, and we may write $A^{-1}$ as a polynomial in $A$. Again from $p_A(A) = A^2 - 3A + 2I =
0$, we get $2I = -A^2 + 3A = A\bb{-A + 3I}$, or $I = \frac 12 A\bb{-A + 3I}$. This means that
\be
A^{-1} = -\frac 12 A + \frac 32 I = \bepm 0 & -1/2 \\ 1 & 3/2 \eepm.
\ee
\end{example}

\begin{corollary}
If $A\in M_n(\C)$ is nonsingular, then there is a polynomial $q(t)$ (whose coefficients depend upon $A$), of degree at $n-1$, such that $A^{-1} =  q(A)$.
\end{corollary}

\begin{proof}[\bf Proof]
By Cayley-Hamilton theorem, we have that $p_A(A) =0$. If $p_A(t) = a_n t^n + \dots + a_1 t + a_0$, we can have
\be
0 = a_n A^n + \dots + a_1 A + a_0 I.
\ee

If $a_0 \neq 0$, we can have that
\be
-a_0 I = A \bb{a_n A^{n-1} + \dots + a_1I} \ \ra \ A^{-1} = - \frac 1{a_0} \bb{a_n A^{n-1} + \dots + a_1I}
\ee
where $- \frac 1{a_0} \bb{a_n t^{n-1} + \dots + a_2 t + a_1}$ is a polynomial $q(t)$. Otherwise, if $a_0 = 0$, we can use the nonsingularity of $A$ and get
\be
0 = a_n A^{n-1} + \dots + a_2 A + a_1 I.
\ee

Then we can repeat the above steps to get the required conclusion.
\end{proof}

Another use of Schur's unitary triangularization theorem is to make it clear that every matrix is `almost' diagonalizable in two possible interpretations of the phrase. The first says that
arbitrarily close to a given matrix there is a diagonalizable matrix, and the second says that any given matrix is similar to an upper triangular matrix whose off-diagonal entries are arbitrarily
small.

\begin{theorem}
Let $A = \bb{a_{ij}} \in M_n(\C)$. For every $\ve >0$, there exists a matrix $A\bb{\ve} = \bb{a_{ij}\bb{\ve}} \in M_n(\C)$ that has $n$ distinct eigenvalues (and is therefore diagonalizable by
Theorem \ref{thm:distinct_eigenvalues_implies_diagonalizable}) and is such that

\be
\sum^n_{i,j = 1}\abs{a_{ij} - a_{ij}(\ve)}^2 < \ve.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Let $U\in M_n(\C)$ be unitary and such that $U^*AU = T$ is upper triangular. Let $E = \diag\bb{e_1,e_2,\dots,e_n}$ in which $e_1,\dots,e_n$ are numbers chosen so that
\be
\abs{e_i} < \bb{\frac{\ve}{n}}^{1/2}
\ee
and so the numbers $t_{11} + e_1,\dots,t_{nn} + e_n$ are distinct\footnote{theorem needed.}. Then $T+E$ has $n$ distinct eigenvalues: $t_{11}+e_1,\dots,t_{nn}+e_n$, and
so does $A + UEU^*$, which is similar to $T+E$ by Corollary \ref{cor:similarity_same_eigenvalues}. Let $A(\ve) = A + UEU^*$, so that $A - A(\ve) = -UEU^*$ (therefore $A(\ve)- A$ and $E$ are
unitarily equivalent). By Theorem \ref{thm:unitary_equivalence_elements_square_sum}, we have
\be
\sum^n_{i,j = 1}\abs{a_{ij} - a_{ij}(\ve)}^2 = \sum^n_{i=1} \abs{e_i}^2 < n \bb{\frac {\ve}n} = \ve.
\ee

Therefore, $A(\ve)$ satisfies the assertion of the theorem.
\end{proof}


\begin{theorem}
Let $A \in M_n(\C)$. For every $\ve >0$ there is a nonsingular matrix $S_\ve\in M_n(\C)$ such that
\be
S^{-1}_\ve A S_\ve = T(\ve) = \bb{t_{ij}(\ve)}
\ee
is upper triangular and $\abs{t_{ij}(\ve)} < \ve$ for $1\leq i < j\leq n$.
\end{theorem}

\begin{proof}[\bf Proof]
First apply Schur's theorem (Theorem \ref{thm:schur_unitary_triangularization}) to produce a unitary matrix $U\in M_n(\C)$ and an upper triangular matrix $T\in M_n(\C)$ such that
\be
U^* AU = T.
\ee

Define $D_\alpha = \diag\bra{1,\alpha,\alpha^2,\dots,\alpha^{n-1}}$ for a non-zero scalar $\alpha\in \R$ and set $t = \max_{i<j}\abs{t_{ij}}$. Assume that $\ve < 1$, since it certainly suffices to
prove the statement in this case.

If $t\leq 1$, let $S_\ve = UD_\ve$, and, if $t>1$, let $S_\ve = UD_{1/t}D_\ve$. In either case, the appropriate $S_\ve$ substantiates the claim of the theorem.

If $t\leq 1$, a simple calculation reveals that
\be
t_{ij}(\ve) = \bb{S^{-1}_\ve A S_\ve}_{ij} = \bb{D_{1/\ve} T D_\ve}_{ij} =  t_{ij}\ve^{-i}\ve^j = t_{ij}\ve^{j-i},
\ee
whose absolute value is no more than $\ve^{j-i}$, which is, in turn, no more than $\ve$ if $i<j$.

If $t>1$, on the other hand, the similarity by $D_{1/t}$,
\be
T(\ve) = S^{-1}_\ve A S_\ve = D_{1/\ve} D_{t} TD_{1/t}D_\ve.
\ee

It's obvious that $D_{t} TD_{1/t}$ is upper triangular and for any $i<j$,
\be
\bb{D_{t} TD_{1/t}}_{ij} = t_{ij}t^it^{-j} \leq t_{ij} t^{-1} \leq 1.
\ee

Therefore, all off-diagonal entries are no more than 1 in absolute value and $\abs{t(\ve)_{ij}} \leq \abs{\bb{D_{1/\ve}D_{\ve}}_{ij}} = \ve^{j-i}$, which is, in turn, no more than $\ve$ if $i<j$.%\footnote{proof needed.}
\end{proof}


An extension of Schur's theorem, easily proved from it, is an important step toward the Jordan canonical form (Theorem \ref{thm:jordan_canonical_form}).

\begin{theorem}\label{thm:every_complex_matrix_similar_to_diagonal_block_upper_triangular}
Suppose that $A\in M_n(\C)$ has eigenvalues $\lm_i$ with multiplicity $n_i$, $i=1,\dots,k$, and that $\lm_1,\dots,\lm_k$ are distinct. Then $A$ is similar to a matrix of the form
\be
\bepm \ba{cc} T_1 & \\  & T_2 \ea &  \text{\Large 0} \\ \text{\Large 0} & \ba{cc} \ddots & \\  & T_k \ea \eepm
\ee
where $T_i\in M_{n_i}(\C)$ is upper triangular with all diagonal entries equal to $\lm_i$, $i=1,\dots,k$. If $A\in M_n(\R)$ and if all the eigenvalues of $A$ are real, then the same result holds,
and the similarity matrix may be taken to be real.
\end{theorem}

\begin{proof}[\bf Proof]%we can swap the rows and columns of $T$ by elementary matrices ($T_{i,j}$ in Definition \ref{def:elementary_matrix})
First apply Schur's theorem (Theorem \ref{thm:schur_unitary_triangularization}) to exhibit unitary equivalence to an upper triangular matrix $T = (t_{ij})$, and suppose that we have arranged that
all the $\lm_1$ terms come first (as the order can be prescribed in Schur's theorem again), the $\lm_2$ terms next, and so on, on the diagonal of $T$, i.e., $U^*AU = T$.

We next perform a sequence of simple similarities (non-unitary equivalence) on $T$ that produce the desired above-diagonal 0's, without changing the diagonal or the upper triangular structure of
$T$.

Let $E_{ij}$ be the matrix in $M_n(\C)$ with a 1 in the $(i,j)$ position and 0's elsewhere (which is a natural basis of $M_n(\C)$). Note that for $i\neq j$ and $\alpha$ any scalar, $I + \alpha
E_{ij}$ is nonsingular (which is actually an elementary matrix $C_{i,j,\alpha}$, see Definition \ref{def:elementary_matrix}) and
\be
\bb{I + \alpha E_{ij}}^{-1} = I - \alpha E_{ij},\qquad (\text{Proposition \ref{pro:square_elementary_matrix_invertible}}).
\ee

Furthermore, straightforward calculation reveals the similarity by $I + \alpha E_{ij}$ for $i<j$,
\be
\bb{I + \alpha E_{ij}}^{-1} T \bb{I + \alpha E_{ij}} = \bb{I - \alpha E_{ij}}  T \bb{I + \alpha E_{ij}}
\ee
changes entries of $T$ by adding $i$th row by $j$th row times $-\alpha$ and then adding $j$th column by $i$th column times $\alpha$. %only in the $i$th row, to the right of column $j$, and in the $j$th column above the $i$th row.
Thus, it only changes $t_{ij}$ replaces it by
\be
t_{ij} + \alpha (t_{ii} -t_{jj}).
\ee

Thus, if $t_{ii} \neq t_{jj}$, the $(i,j)$ entry may be made 0 by choosing
\be
\alpha_{ij} = -\frac{t_{ij}}{t_{ii} - t_{jj}}
\ee
without otherwise altering the relevant structure.

Now consider the sequnce of positions in $T$:
\be
(n-1,n),\quad (n-2,n-1),\quad (n-2,n),\quad (n-3,n-2) ,\quad (n-3,n-1),\quad (n-3,n),\quad (n-3,n-3)\dots.
\ee

Make each of these 0, in turn, via a similarity of the indicated sort, if $t_{ii} \neq t_{jj}$, and notice that no previously created 0 entry will be distributed. Let
\be
P = U \prod^n_{i<j}\bb{I + \alpha_{ij}E_{ij}}.
\ee

Then we have for the invertible matrix $P$,
\beast
P^{-1}AP &=& \bb{\prod^n_{i<j}\bb{I + \alpha_{ij}E_{ij}}}^{-1}U^{-1}AU \bb{\prod^n_{i<j}\bb{I + \alpha_{ij}E_{ij}}} = \bb{\prod^n_{i<j}\bb{I + \alpha_{ij}E_{ij}}}^{-1}U^*AU \bb{\prod^n_{i<j}\bb{I + \alpha_{ij}E_{ij}}} \\
& = & \bb{\prod^n_{i<j}\bb{I + \alpha_{ij}E_{ij}}}^{-1}T \bb{\prod^n_{i<j}\bb{I + \alpha_{ij}E_{ij}}} := S.
\eeast

The resulting matrix will be similar to $A$ and will have the desired form.%\footnote{proof needed.}
\end{proof}





\subsection{Normal Matrices}

\begin{definition}
Let $A\in M_n(\C)$. It is said to be normal if $A^*A = AA^*$, i.e., if $A$ commutes with its (Hermitian) adjoint.
\end{definition}

\begin{example}
\ben
\item [(i)] Since $U^*U = I = UU^*$ if $U$ is unitary, all unitary matrices are normal.
\item [(ii)] Since $A^*A = AA^*$ trivially if $A^* = A$, all Hermitian matrices are normal.
\item [(iii)] If $A \in M_n(\C)$ is such that $A^* = -A$ ($A$ is skew-Hermitian), then $A^*A = -A^2 = AA^*$. So all skew-Hermitian matrices are also normal.
\item [(iv)] $\bepm 1 & -1 \\ 1 & 1 \eepm$ is normal, but it does not fall into any of the above categories.
\een
\end{example}



\begin{proposition}[unitary equivalence preserves normality]\label{pro:unitary_equivalence_preserves_normality}
$A\in M_n(\C)$ is normal if and only if every matrix that is unitarily equivalent to $A$ is normal.
\end{proposition}

\begin{remark}
The class of normal matrices is closed under unitary equivalence.
\end{remark}

\begin{proof}[\bf Proof]%By Schur's theorem (Theorem \ref{thm:schur_unitary_triangularization}), for any $A\in M_n(\C)$ we have a unitary matrix $U$ such that $U^*A U = T$ where $T$ is upper triangular matrix.
If $A$ and $B$ are unitarily equivalent, then there exists a unitary matrix $U$ such that $B = U^*A U$. Then %Thus, we have $T = U^*V^* B VU$.
\be
B^*B = U^*A*U U^*AU = U^*A^*AU = U^*AA^*U = U^*AUU^*A^*U = BB^* \ \lra \ A\text{ is normal, i.e. }\ A^*A = AA^*,
\ee %\footnote{proof needed.}
as required.
\end{proof}

\begin{proposition}[class of Hermitian matrices is closed under unitary equivalence]\label{pro:hermitian_matrices_closed_under_unitary_equivalence}
$A\in M_n(\C)$ is Hermitian if and only if every matrix that is unitarily equivalent to $A$ is Hermitian.
\end{proposition}

\begin{proof}[\bf Proof]
If $A$ and $B$ are unitarily equivalent, then there exists a unitary matrix $U$ such that $B = U^*A U$. Then by Proposition \ref{pro:matrix_multiple_hermitian}%Thus, we have $T = U^*V^* B VU$.
\be B^* = U^*A*U = \bb{U^*AU} = B \ \lra \ A\text{ is Hermitian, i.e. }\ A = A^*,
\ee %\footnote{proof needed.}
as required.
\end{proof}



\begin{definition}[unitarily diagonalizable matrices\index{unitarily diagonalizable matrices}]
If $A\in M_n(\C)$ is unitarily equivalent to a diagonal matrix, $A$ is said to be unitarily diagonalizable, with a similar definition for orthogonally diagonalizable with respect to $M_n(\R)$.
\end{definition}

\begin{proposition}
Unitarily (or orthogonally diagonalizable) implies diagonalizable (but not conversely).
\end{proposition}

\begin{proof}[\bf Proof]
Direct result from definitions of Unitary diagonalizability and diagonalizability.
\end{proof}

\begin{lemma}\label{lem:normal_triangular_matrix_is_diagonal}
A normal triangular matrix $T\in M_n(\C)$ must be diagonal.
\end{lemma}

\begin{proof}[\bf Proof]
First, since $T$ is normal matrix, we have that $TT^* = T^*T$. Then (1,1) entry of $T^*T$, $\bb{T^*T}_{11}$ is equal to $\bb{TT^*}_{11}$, (1,1) entry of $TT^*$. This is,
\be
\ol{t_{11}}t_{11} = \sum^n_{i=1} t_{1i}\ol{t_{1i}} = \ol{t_{11}}t_{11} + \sum^n_{i=2}\abs{t_{1i}}^2 \ \ra \ \sum^n_{i=2}\abs{t_{1i}}^2 = 0 \ \ra \ t_{1i} = 0,\quad i = 2,\dots,n.
\ee

Similarly, the (2,2) entries of $T^*T$ and $TT^*$ are equivalent, i.e.,
\be
\sum^2_{i=1} \ol{t_{i2}}t_{i2} = \sum^n_{i=1} t_{2i}\ol{t_{2i}}.
\ee

Since $t_{1i}$ and $t_{i1}$ are all zero for $i=2,\dots,n$, we have
\be
\ol{t_{22}}t_{22} = \ol{t_{22}}t_{22} + \sum^n_{i=3} t_{2i}\ol{t_{2i}} = \ol{t_{22}}t_{22} + \sum^n_{i=3} \abs{t_{2i}}^2 \ \ra \ t_{2i} = 0,\quad i = 3,\dots, n.
\ee

In the same manner, assuming we have verified that $t_{ij} = 0$ and $t_{ji} = 0$ for $j>i$ and $i=1,\dots,k-1$, we may conclude that
\be
t_{ij} = 0,\quad j>i,\ i = 1,\dots,k.
\ee

Thus, we can have $t_{ij} = 0$ for all $j>i$ and therefore it implies that $T$ is diagonal.
\end{proof}


\begin{theorem}[spectral theorem for normal matrices]\label{thm:spectral_normal_matrices}
If $A = \bb{a_{ij}}\in M_n(\C)$ has eigenvalues $\lm_1,\dots,\lm_n$, the following statements are equivalent:
\ben
\item [(i)] $A$ is normal.
\item [(ii)] $A$ is unitarily diagonalizable.
\item [(iii)] $\sum^n_{i,j=1} \abs{a_{ij}}^2 = \sum^n_{i=1} \abs{\lm_i}^2$.
\item [(iv)] There is an orthonormal set of $n$ eigenvectors of $A$.
\een
\end{theorem}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
We suppose that throughout that $T = \bb{t_{ij}} \in M_n(\C)$ is an upper triangular matrix which is unitarily equivalent to $A$, as guaranteed by Schur's theorem (Theorem
\ref{thm:schur_unitary_triangularization}). That is, $T = U^*AU$ for some unitary matrix $U\in M_n(\C)$.

(i) is equivalent to the statement that $T$ is normal by Proposition \ref{pro:unitary_equivalence_preserves_normality}.

(i) $\ra$ (ii). If $A$ is normal, then $T$ is normal. Then by Lemma \ref{lem:normal_triangular_matrix_is_diagonal}, we know that $T$ is diagonal and thus $A$ is unitarily diagonalizable.

(i) $\ra$ (ii). Since diagonal matrices are clearly normal and unitary equivalence preserves normality (Proposition \ref{pro:unitary_equivalence_preserves_normality}), we can have the required.

(ii) $\ra$ (iii). Since the diagonal entries of any diagonalization of $A$ are eigenvalues $\lm_1,\dots,\lm_n$ (in some order) by Corollary \ref{cor:similarity_same_eigenvalues}, we can have (iii)
from (ii) by Theorem \ref{thm:unitary_equivalence_elements_square_sum}.

(iii) $\ra$ (ii). Since $\lm_i$, $i=1,\dots,n$, are diagonal entries of $T$ (in some order), by Theorem \ref{thm:unitary_equivalence_elements_square_sum}, we have
\be
\sum^n_{i,j=1} \abs{a_{ij}}^2 = \sum^n_{i=1} \abs{\lm_{i}}^2 + \sum^n_{i<j} \abs{t_{ij}}^2 \ \ra \  \sum^n_{i<j} \abs{t_{ij}}^2 = 0 \qquad \text{from (iii)} \ \ra\  t_{ij} = 0,\quad i<j
\ee
and thus $A$ is unitarily diagonalizable.

(ii) $\ra$ (iv). If $A$ is unitarily diagonalizable, there exists unitary matrix $U$ such that $U^*AU = \Lambda$ where $\Lambda$ is a diagonal matrix whose eigenvalues are consistent with $A$. Thus,
\be
AU = \bb{U^{*}}^{-1}\Lambda = U\Lambda \qquad (*).
\ee

Thus, the $i$th column of $U\Lambda$ is $\lm_i U_i$ where $U_i$ is the $ith$ column of $U$. Also, the $i$th column of $AU$, $(AU)_i$, is $AU_i$. Therefore,
\be
A U_i = \lm_i U_i \ \ra \ U_i \text{ is eigenvector associated with eigenvalue }\lm_i.
\ee

Thus, since U is unitary, we have that $U_i$, $i=1,\dots,n$ form an orthonormal set by Theorem \ref{thm:unitary_matrix_property}.

(ii) $\ra$ (iv). Assume we have an orthonormal set $U_1,\dots,U_n$ of $n$ eigenvectors of $A$. Then by using ($*$), it is easy to show that $U = \bb{U_1,\dots,U_n}$ is the unitary matrix such that
$U^*AU = \Lambda$ where $\Lambda$ is diagonal matrix formed by eigenvalues (in some order) of $A$ .
\end{proof}

\begin{theorem}[spectral theorem for Hermitian matrices]\label{thm:spectral_hermitian_matrices}
If $A \in M_n(\C)$ is Hermitian, then
\ben
\item [(i)] All eigenvalues of $A$ are real.
\item [(ii)] $A$ is unitarily diagonalizable.
\een

If $A\in M_n(\R)$ is real (and thus symmetric), then $A$ is real orthogonally diagonalizable.
\end{theorem}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
(ii) follows from Theorem \ref{thm:spectral_normal_matrices} as Hermitian is normal.

Since (ii) and the fact that the set of Hermitian matrices is closed under unitary equivalence (by Proposition \ref{pro:hermitian_matrices_closed_under_unitary_equivalence}), the corresponding
diagonal matrix must be Hermitian. As a diagonal Hermitian matrix must have real diagonal entries, we can have that all the eigenvalues of $A$ are real by Corollary
\ref{cor:similarity_same_eigenvalues}.

If $A\in M_n(\R)$ is symmetric, then it is Hermitian, and thus all the eigenvalues of $A$ are real. Then by Schur's theorem (Theorem \ref{thm:schur_unitary_triangularization}), there exists
orthogonal matrix $U$ such that $U^T AU = T$ where $T$ is an upper triangular matrix. Since $T$ is also symmetric ($T^T = U^TA^T U = U^TA U = T$), we have that $T$ is diagonal (or we can get this by
Lemma \ref{lem:normal_triangular_matrix_is_diagonal}). Thus, $A$ is real orthogonally diagonalziable.
\end{proof}

\begin{theorem}
Let $A\in M_n(\R)$. Then $A$ is normal if and only if there is a real orthogonal matrix $Q\in M_n(\R)$ such that
\be Q^TAQ =
\bepm
\ba{cc} A_1 & \\ & A_2 \ea & \text{\Large $*$} \\ \text{\Large 0} & \ba{cc} \ddots & \\
& A_k \ea \eepm\in M_n(\R),\quad 1\leq k\leq n
\ee
where each $A_i$ is either a real $1\times 1$ matrix or a real $2\times 2$ matrix of the form
\be
A_i = \bepm \alpha_i & \beta_i \\ -\beta_i & \alpha_i \eepm.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
\footnote{proof needed.}
\end{proof}


\subsection{QR factorization and algorithm}

A particular means for calculating a specific unitary Schur's upper triangularization of a given matrix $A\in M_n(\C)$, and a popular numerical method for calculating eigenvalues (under some
assumptions) is called the $QR$ algorithm. It is based on the so-called $QR$ factorization of a general matrix $A\in M_{m,n}(\C)$.

\begin{theorem}[$QR$ factorization\index{$QR$ factorization}]\label{thm:qr_factorization}
If $A \in M_{m,n}(\C)$ and $m\geq n$, there is a matrix $Q \in M_{m,n}(\C)$ with orthonormal columns and an upper triangular matrix $R\in M_{n}(\C)$ such that $A = QR$.

If $m=n$, $Q$ is unitary; if in addition $A$ is nonsingular, then $R$ may be chosen so that all its diagonal entries are positive, and in this case, the factors $Q$ and $R$ are both unique.

If $A\in M_{m,n}(\R)$, then $Q$ and $R$ may be taken to be real.
\end{theorem}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
If $A\in M_{m,n}(\C)$ and $\rank (A) = n$, the QR factorization of $A$ is just a description, in matrix notation, of the result of applying the Gram-Schmidt orthonormalization (Algorithm
\ref{alg:gram_schmidt_orthonormalization}) to the columns of $A$, which comprise an independent set in $\C^m$.

A natural extension of the Gram-Schmidt algorithm permits the same description to apply to the general case in which the columns of $A$ may be dependent (see the remark of Algorithm
\ref{alg:gram_schmidt_orthonormalization}).

Let $A = \bb{a_1,\dots,a_n}$ be written in partitioned form in terms of its columns $a_i\in \C^m$. If $a_1 = 0$, set $q_1 = 0$; otherwise set $q_1 = a_1 /\bb{a_1^*a_1}^{1/2}$. For each
$k=2,3,\dots,n$, compute
\be
y_k = a_k - \sum^{k-1}_{i=1}\bb{q_i^* a_k}q_i
\ee
just as in the ordinary Gram-Schmidt orthonormalization. If $y_k = 0$ (which happens if and only if $a_k$ is a linear combination of $a_1,a_2,\dots,a_{k-1}$), set $q_k = 0$; otherwise set $q_k = y_k
/ \bb{y_k^*y_k}^{1/2}$. The vectors $q_1,\dots,q_n$ are then orthogonal set (in $\C$), each element of which is either a unit vector or the zero vector. Each vector $q_i$ is a linear combination of
$a_1,\dots,a_i$, and the construction ensures that, conversely, each column $a_i$ is linear combination of $q_1,\dots,q_i$. Thus, scalars $r_{ki}$ exist such that
\be
a_i = \sum^i_{k=1}r_{ki}q_i,\qquad i = 1,2,\dots, n.\qquad (*)
\ee

If we set $r_{ki}=0$ for all $k>i$ and set $r_{ji} = 0$ for all $i = 1,2,\dots,n$ for each $j$ such that $q_j = 0$, the upper triangular matrix $R = (r_{ij})\in M_n(\C)$ and the vectors
$q_1,q_2,\dots,q_n$ are determined, via the outlined procedure, by $a_1,\dots,a_n$. The matrix $Q := \bb{q_1,\dots,q_n}\in M_{m,n}(\C)$ has orthogonal columns (in $\C$ and some of which may be
zero), and ($*$) says that $A = QR$.

If $\rank(A) = n$, $Q$ has orthonormal columns, and hence a factorization of the desired form has been achieved.

If the columns of $A$ are not independent ($\rank(A) < n$), take the (orthonormal) set of non-zero columns of $Q$ and extend it to an orthonormal basis of $\C^m$ by Steinitz exchange theorem
(Theorem \ref{thm:steinitz_exchange}). Denote the new vectors obtained in this way by $z_1,\dots,z_p$. Now replace the first zero column of $Q$ by $z_1$, replace the second zero column by $z_2$, and
so on until all zero columns have been replaced in this way. Denote the resulting matrix by $Q'$. Then $Q'$ has orthonormal columns and $QR = Q'R$ because the new columns of $Q'$ are matched by
zero rows of $R$. Then $A = Q'R$ is a factorization of the desired form.

In particular, if $m=n$ and $A$ is nonsingular, then $Q$ must be unitary by Theorem \ref{thm:unitary_matrix_property} and the diagonal entries of the nonsingular matrix $R = Q^*A$ must be non-zero.
In this event, because $R$ is required to be upper triangular, $q_1$ is a scalar multiple of $a_1$, and for $i=2,3,\dots,n$, $q_i$ lies in the one-dimensional space that is the orthogonal complement
of the span $a_1,\dots,a_{i-1}$ with respect to the span of $a_1,\dots,a_i$. Therefore, each $q_i$ is uniquely determined up to a factor of scale of absolute value 1. Thus, replacement of $R$ by
\be
R' := \diag\bb{\frac{\abs{r_{11}}}{r_{11}},\dots, \frac{\abs{r_{nn}}}{r_{nn}}} R
\ee
and replacement of $Q$ by
\be
Q' := Q\diag\bb{\frac{r_{11}}{\abs{r_{11}}},\dots, \frac{r_{nn}}{\abs{r_{nn}}}}
\ee
gives the unique factorization $A = Q'R'$ promised in the statement of the theorem.


If $A$ is real, notice that all the necessary operations may be carried out in real arithmetic, so the factors obtained are real.
\end{proof}

We next state the $QR$ algorithm for eigenvalue calculation and briefly indicate some of its features.

\begin{algorithm}
Let $A_0\in M_n(\C)$ be given. Write $A_0 = Q_0R_0$, where $Q_0$ and $R_0$ are as guaranteed in $QR$ factorization (Theorem \ref{thm:qr_factorization}), and define $A_1 = R_0 Q_0$. Again, write $A_1
= Q_1 R_1$, with $Q_1$ unitary and $R_1$ upper triangular, and continue.

In general, factor $A_k = Q_kR_k$ and define $A_{k+1} = R_kQ_k$.
\end{algorithm}

\begin{remark}
Since $A_{k+1} = R_kQ_k$, we have
\be
Q_k A_{k+1} Q_k^* = Q_k R_k Q_k Q_k^* = Q_k R_k = A_k \ \ra\ A_k, A_{k+1}\ \text{ are unitarily equivalent.}
\ee

Thus, we can have each $A_k$ is unitarily equivalent to $A_0$.

Under certain circumstance (for example, if all the eigenvalues of $A_0$ have distinct absolute values), the $QR$ iterates $A_k$ will converge to an upper triangular matrix as
$k\to\infty$.\footnote{proof needed.} Since this upper triangular matrix is unitarily equivalent to $A_0$, the eigenvalues of $A_0$ are revealed.

If $A_0$ is real, then the $QR$ iterates $A_k$ may be calculated using real arithmetic. If $A_0$ has any non-real eigenvalues, there is no hope that the $QR$ iterates will converges to an upper
triangular matrix, since this upper triangular limit must be real (by real case of $QR$ factorization (Theorem \ref{thm:qr_factorization})).

Under certain circumstances, however, the iterates $A_k$ may be chosen so that they converge to a real block upper triangular matrix with $1\times 1$ and $2\times 2$ main diagonal
blocks.\footnote{proof needed.} A sufficient condition for this to occur is that all the eigenvalues of $A_0$ have distinct moduli, expect for the two eigenvalues in each non-real complex conjugate
pair, which have the same modulus.

Since the eigenvalues of a block triangular matrix are the union of the sets of eigenvalues of the diagonal blocks, the eigenvalues of $A_0$ are revealed as the $1\times 1$ diagonal entries of the
block triangular limit of the $QR$ iterates $A_k$, together with the (complex conjugate pairs of) eigenvalues of the $2\times 2$ diagonal blocks of the limit, which may be calculated easily using
the real arithmetic and the quadratic formula.
\end{remark}

\section{Canonical Forms}

\subsection{The Jordan canonical form}

Since two matrices that look very different can still be similar, one approach to determining whether two given matrices are similar is to have some set of `simple' matrices of prescribed form and
then see if both given matrices can be reduced by similarity to one of these `simple' forms.

Every complex $A$ is unitarily equivalent (similar) to an upper triangular matrix whose diagonal entries (the eigenvalues of $A$) may be arranged in any given order (by Schur's theorem (Theorem
\ref{thm:schur_unitary_triangularization})), so two matrices are similar if they are similar to the same upper triangular matrix.

However, two upper triangular matrices with the same main diagonal entries and different off-diagonal entries can still be similar. Thus, if we have succeeded in reducing the two given matrices to
two unequal upper triangular matrices with the same main diagonal, we cannot conclude that the matrices are not similar.

As the class triangular matrices is too large for our purposes, we turn to the class diagonal matrices. Unfortunately, not every complex matrix is similar to a diagonal matrix.

\begin{example}
Consider the matrices in Example \ref{exa:same_eigenvalues_do_not_imply_similarity},
\be
A = \bepm 0 & 1 \\ 0 & 0 \eepm,\qquad B = \bepm 0 & 0 \\ 0 & 0 \eepm.
\ee

We can seet that they have the same trace, determinant, characteristic polynomial and eigenvalues ($\bra{0,0}$) and $B$ is diagonal. However, $A$ is not diagonalizable. If it is, we have that the
diagonal matrix $\Lambda$ should be $B$ and
\be
A = P\Lambda P^{-1} = PB P^{-1} = \bepm 0 & 0 \\ 0 & 0 \eepm \qquad \ra \quad \text{Contradiction.}
\ee
\end{example}

To trade off between feasibility and complexity, we want to search for an upper triangular form that is as nearly diagonal as possible but is still attainable by similarity for every matrix.

The result is Jordan canonical form. Once one knows the Jordan canonical form of a matrix, all the linear algebraic information about the given matrix (i.e., linear transformation) is known at a
glance.

\begin{definition}[Jordan block\index{Jordan block}]\label{def:jordan_block}
A Jordan block $J_k(\lm)$ is a $k\times k$ upper triangular matrix of the form
\be
J_k(\lm) = \bepm \ba{ccc} \lm & 1 & \\ & \lm & 1 \\ & & \lm \ea & \text{\Large 0} \\ \text{\Large 0} & \ba{ccc} \ddots & \ddots & \\ & \lm  & 1  \\ & & \lm \ea\eepm.
\ee

There are $k-1$ terms `+1' in the superdiagonal; the scalar $\lm$ appears $k$ times on the main diagonal. All other entries are zero, and $J_1(\lm) = \bb{\lm}$.
\end{definition}


\begin{definition}[Jordan matrix\index{Jordan matrix}]\label{def:jordan_matrix}
A Jordan matrix $J\in M_n(\C)$ is direct sum (see Definition \ref{def:block_diagonal_matrix}) of Jordan blocks
\be
J = \bepm \ba{cc} J_{n_1}(\lm_1) & \\ & J_{n_2}(\lm_2) \ea & \text{\Large 0} \\ \text{\Large 0} & \ba{cc} \ddots & \\ & J_{n_k}(\lm_k) \ea\eepm,\qquad n_1 + n_2 + \dots + n_k = n
\ee
in which the order $n_i$ may not be distinct and the values $\lm_i$ need not be distinct.
\end{definition}

\begin{remark}\label{rem:jordan_matrix_diagonalizable}
Note that if each Jordan block $J_{n_i}(\lm_i)$ is one-dimensional, that is, all $n_i =1$ and $k=n$, then the Jordan matrix $J$ is diagonal.

If any Jordan block $J_m(\lm)$ has $m>1$, then $J$ is not only not diagonal, it is not even diagonalizable. If $J_m(\lm)$ is diagonalizable such that $J_m(\lm) = P\Lambda P^{-1}$ with $\Lambda$
diagonal, then necessarily $\Lambda = \diag\bb{\lm,\lm,\dots,\lm} = \lm I$. Thus,
\be
J_m(\lm) - \lm I = P\Lambda P^{-1} - \lm I = P \lm I P^{-1} - \lm I = \lm I - \lm I = 0,
\ee
which is not the case if $m>1$.
\end{remark}

\begin{lemma}\label{lem:jordan_block_property}
Let $k\geq 1$ be given, and consider the Jordan block
\be
J_k(0) = \bepm \ba{ccc} 0 & 1 & \\ & 0 & 1 \\ & & 0 \ea & \text{\Large 0} \\ \text{\Large 0} & \ba{ccc} \ddots & 1 & \\ & \ddots & 1 \\ & & 0 \ea\eepm.
\ee

Then
\be
J_k^T(0) J_k(0) = \bepm 0 & 0 \\ 0 & I_{k-1}\eepm,\qquad J_k(0)^p = 0,\quad p \geq k.
\ee

Moreover,
\be
J_k(0)e_{i+1} = e_i,\quad i = 1,2,\dots,k-1
\ee
and
\be
\bb{I - J_k^T(0) J_k(0)}x = \bb{x^T e_1}e_1
\ee
where $I_{k-1}\in M_{k-1}(\C)$ is the identity matrix, $e_i$ is the $i$th standard unit basis vector in $\C^k$, and $x\in \C^k$.
\end{lemma}

\begin{proof}[\bf Proof]
Let $W = J_k^T(0) J_k(0)$, then for $J_k(0) := \bb{x_{ij}}$,
\be
w_{ij} = \sum^k_{m=1} x_{mi}x_{mj}.
\ee

If $i=1$ or $j=1$, then $x_{mi} = x_{mj} = 0$. For $i,j\geq 2$,
\be
w_{ij} = x_{mi}x_{mj}\delta_{ij} = \delta_{ij}
\ee
as required. Also, we can have
\be
J_k(0)^2 = \bepm \ba{ccc} 0 & 1 & \\ & 0 & 1 \\ & & 0 \ea & \text{\Large 0} \\ \text{\Large 0} & \ba{ccc} \ddots & 1 & \\ & \ddots & 1 \\ & & 0 \ea\eepm
\bepm \ba{ccc} 0 & 1 & \\ & 0 & 1 \\ & & 0 \ea & \text{\Large 0} \\ \text{\Large 0} & \ba{ccc} \ddots & 1 & \\ & \ddots & 1 \\ & & 0 \ea\eepm.
= \bepm \ba{ccc} 0 & 0 & 1 \\ & 0 & 0 \\ & & 0 \ea & \text{\Large 0} \\ \text{\Large 0} & \ba{ccc} \ddots & \ddots & 1 \\ \ddots & 0 & 0 \\ & 0 & 0 \ea\eepm
\ee
which has `+1' entries only at $(i,j)$ with $j - i = 2$. Similarly, we can have $J_k(0)^{k-1}$ has `+1' entries only at $(i,j)$ with $j - i = k-1$, which is the top right element.
Accordingly, it is easy to see that $J_k(0)^p = 0$ for any $p\geq k$.

Moreover, for $i=1,2,\dots,k-1$, $j$th entry of vector $J_k(0)e_{i+1}$ is
\be
\sum^k_{m=1}\bb{J_k(0)}_{jm}\bb{e_{i+1}}_m = \bb{J_k(0)}_{j,i+1} = \delta_{ij} \ \ra \ J_k(0)e_{i+1} = e_i.
\ee

Also, for $x = \bb{x_1,\dots,x_k}^T\in \C^n$,
\be
\bb{I - J_k^T(0) J_k(0)}x = \bepm \ba{cc} 1 & 0 \\ 0 & 0 \ea & \text{\Large 0} \\ \text{\Large 0} &  \ba{cc} 0 & 0 \\ 0 & 0 \ea \eepm \bepm x_1 \\ x_2 \\ \vdots \\ x_k \eepm
=  \bepm x_1 \\ 0 \\ \vdots \\ 0 \eepm = x_1 \bepm 1 \\ 0 \\ \vdots \\ 0 \eepm =  \bb{x^T e_1}e_1
\ee
as required.
\end{proof}

Now we give the reduction of a strictly upper triangular matrix (which is a triangular matrix with zero entries on the main diagonal).

\begin{theorem}\label{thm:strictly_upper_triangular_jordan_canonical_form}
Let $A\in M_n\bb{\C}$ be strictly upper triangular. Then there is a nonsingular $P \in M_n(\C)$ and integers $n_1 \geq n_2 \geq \dots \geq n_m \geq 1$ and $n_1 + n_2 + \dots + n_m = n$ such that
\be
A = P \bepm \ba{cc} J_{n_1}(0) & \\ & J_{n_2}(0) \ea & \text{\Large 0} \\ \text{\Large 0} &  \ba{cc} \ddots &  \\ & J_{n_m}(0) \ea \eepm P^{-1}\qquad (*)
\ee
where $J_{n_i}(0)$, $i = 1,\dots,m$ is the Jordan block with scalar 0.

If $A$ is real, the matrix $P$ may be chosen to be real.
\end{theorem}

\begin{proof}[\bf Proof]
If $n=1$, $A = \bb{0}$ and the result is trivial. We now proceed by induction on $n$ and assume that $n>1$ and that the result has been proved for all strictly upper triangular matrices of order
less than $n$. Then we can partition $A$ as
\be
A = \bepm 0 & a^T \\ 0 & A_1 \eepm
\ee
where $a\in \C^{n-1}$ and $A_1\in M_{n-1}(\C)$ is strictly upper triangular. By the induction hypothesis, there is a nonsingular matrix $P_1\in M_{n-1}(\C)$ such that $P_1^{-1}A_1P_1$ has the desired form ($*$), that is
\be
P_1^{-1}A_1P_1 = \bepm \ba{cc} J_{k_1}(0) & \\ & J_{k_2}(0) \ea & \text{\Large 0} \\ \text{\Large 0} &  \ba{cc} \ddots &  \\ & J_{k_s}(0) \ea \eepm = \bepm J_{k_1}(0) & 0 \\ 0 & J \eepm
\ee
where $k_1 \geq k_2 \geq \dots \geq k_s \geq 1$, $k_1 + k_2 + \dots + k_s = n-1$, and
\be
J := \bepm \ba{cc} J_{k_2}(0) & \\ & \ddots \ea & \text{\Large 0} \\ \text{\Large 0} &  \ba{cc} \ddots &  \\ & J_{k_s}(0) \ea \eepm \in M_{n-k_1 -1}(\C).
\ee

Note that no diagonal Jordan block in $J$ has order greater than $k_1$ ($k_1 \geq k_2,\dots,k_m$), so $J^{k_1} = 0$ by Lemma \ref{lem:jordan_block_property}. A simple computation shows that
\be
\bepm 1 & 0 \\ 0 & P_1^{-1} \eepm A \bepm 1 & 0 \\ 0 & P_1 \eepm = \bepm 1 & 0 \\ 0 & P_1^{-1} \eepm  \bepm 0 & a^T \\ 0 & A_1 \eepm \bepm 1 & 0 \\ 0 & P_1 \eepm
= \bepm 0 & a^T \\ 0 & P_1^{-1}A_1 \eepm \bepm 1 & 0 \\ 0 & P_1 \eepm = \bepm 0 & a^TP_1 \\ 0 & P_1^{-1}A_1 P_1\eepm.
\ee

Partition $a^T P_1 = \bb{a_1^T,a_2^T}$ where $a_1\in \C^{k_1}$ and $a_2 \in \C^{n-k_1-1}$ such that
\be
\bepm 1 & 0 \\ 0 & P_1^{-1} \eepm A \bepm 1 & 0 \\ 0 & P_1 \eepm = \bepm 0 & a_1^T & a_2^T \\ 0 & J_{k_1}(0) & 0 \\ 0 & 0 & J \eepm.
\ee

Now consider the following similarly of this matrix,
\beast
& & \bepm 1 & -a_1^TJ^T_{k_1}(0) & 0 \\ 0 & I_{k_1} & 0 \\ 0 & 0 & I_{n-k_1 -1} \eepm \bepm 0 & a_1^T & a_2^T \\ 0 & J_{k_1}(0) & 0 \\ 0 & 0 & J \eepm \bepm 1 & a_1^TJ^T_{k_1}(0) & 0 \\ 0 & I_{k_1} & 0 \\ 0 & 0 & I_{n-k_1 -1} \eepm
\\
& = &  \bepm 0 & a_1^T\bb{I_{k_1}- J^T_{k_1}(0)J_{k_1}(0)} & a_2^T \\ 0 & J_{k_1}(0) & 0 \\ 0 & 0 & J \eepm \bepm 1 & a_1^TJ^T_{k_1}(0) & 0 \\ 0 & I_{k_1} & 0 \\ 0 & 0 & I_{n-k_1 -1} \eepm
=  \bepm 0 & a_1^T\bb{I_{k_1}- J^T_{k_1}(0)J_{k_1}(0)} & a_2^T \\ 0 & J_{k_1}(0) & 0 \\ 0 & 0 & J \eepm .
\eeast

By Lemma \ref{lem:jordan_block_property}, we have it is
\be
\bepm 0 & \bb{a_1^T e_1}e_1^T & a_2^T \\ 0 & J_{k_1}(0) & 0 \\ 0 & 0 & J \eepm   \qquad (\dag)
\ee
where $e_i$ is the $i$th standard unit basis vector in $\C^{k_1}$. Then we have the following two cases.

{\bf Case 1.} If $a_1^T e_1 \neq 0$, then the similarity of this matrix \beast & & \bepm 1/\bb{a_1^T e_1} & 0 & 0 \\ 0 & I_{k_1} & 0 \\ 0 & 0 & 1/\bb{a_1^T e_1} I_{n-k_1 -1} \eepm \bepm 0 &
\bb{a_1^T e_1}e_1^T & a_2^T \\ 0 & J_{k_1}(0) & 0 \\ 0 & 0 & J \eepm  \bepm a_1^T e_1 & 0 & 0 \\ 0 & I_{k_1} & 0 \\ 0 & 0 & \bb{a_1^T e_1} I_{n-k_1 -1} \eepm
\\
& = & \bepm 0 & e_1^T & a_2^T/\bb{a_1^T e_1} \\ 0 & J_{k_1}(0) & 0 \\ 0 & 0 & J/\bb{a_1^T e_1} \eepm  \bepm a_1^T e_1 & 0 & 0 \\ 0 & I_{k_1} & 0 \\ 0 & 0 & \bb{a_1^T e_1} I_{n-k_1 -1} \eepm
= \bepm 0 & e_1^T & a_2^T \\ 0 & J_{k_1}(0) & 0 \\ 0 & 0 & J \eepm := \bepm \wt{J} & e_1'a_2^T \\ 0 & J \eepm
\eeast
where
\be
\wt{J} = \bepm 0 & e_1^T \\ 0 & J_{k_1}(0) \eepm = J_{k_1 + 1}(0)
\ee
and $e_i'$ is the $i$th standard unit basis vector in $\C^{k_1+1}$. Using the property that $\wt{J}e'_{i+1} = e'_{i}$ for $i=1,2,\dots,k_1$, we can easily show that the similarity of this matrix
\beast
& & \bepm I_{k_1+1} & e_2'a_2^T \\ 0 & I_{n-k_1 -1} \eepm  \bepm \wt{J} & e_1'a_2^T \\ 0 & J \eepm \bepm I_{k_1+1} & -e_2'a_2^T \\ 0 & I_{n-k_1 -1} \eepm \\
& = & \bepm \wt{J} & e_1'a_2^T + e_2'a_2^TJ \\ 0 & J \eepm \bepm I_{k_1+1} & -e_2'a_2^T \\ 0 & I_{n-k_1 -1} \eepm  = \bepm \wt{J} & - \wt{J}e_2'a_2^T + e_1'a_2^T + e_2'a_2^TJ \\ 0 & J \eepm
= \bepm \wt{J} & e_2'a_2^TJ \\ 0 & J \eepm.
\eeast

Similarly, we can have the similarity of these matrices, for $i = 1,2,\dots$,
\beast
& & \bepm I_{k_1+1} & e_{i+1}'a_2^T J^{i-1} \\ 0 & I_{n-k_1 -1} \eepm  \bepm \wt{J} & e_i'a_2^TJ^{i-1} \\ 0 & J \eepm \bepm I_{k_1+1} & -e_{i+1}'a_2^T J^{i-1} \\ 0 & I_{n-k_1 -1} \eepm \\
& = & \bepm \wt{J} & e_{i}'a_2^T J^{i-1} + e_{i+1}'a_2^TJ^i \\ 0 & J \eepm \bepm I_{k_1+1} & -e_{i+1}'a_2^T J^{i-1} \\ 0 & I_{n-k_1 -1} \eepm  \\
& = & \bepm \wt{J} & - \wt{J}e_{i+1}'a_2^T J^{i-1} + e_i 'a_2^TJ^{i-1} + e_{i+1}'a_2^TJ^i \\ 0 & J \eepm = \bepm \wt{J} & e_{i+1}'a_2^TJ^i \\ 0 & J \eepm.
\eeast

Since $J^{k_1} =0$, we see that after $k_1$ steps in this series of similarities, the off-diagonal term will finally vanish. We can then conclude that $A$ is similar to the matrix
\be
\bepm \wt{J} & 0 \\ 0 & J \eepm
\ee
which is strictly upper triangular Jordan matrix of the required form.

{\bf Case 2.} If $a_1^Te_1 = 0$, then ($\dag$) shows that $A$ is similar to the matrix
\be
\bepm 0 & 0 & a_2^T \\ 0 & J_{k_1}(0) & 0 \\ 0 & 0 & J \eepm
\ee

which is permutation similar to the matrix

\beast
& & \bb{\ba{c;{2pt/2pt}c} \ba{ccccc} 0 & 1 & & & \\ & 0 & 1 & &  \\ & & \ddots & 1 &  \\ 0 & & & \ddots & 1 \\ 1 & 0 & \dots & &  0 \ea & \text{\Large 0} \\ \hdashline[2pt/2pt] & \\ \text{\Large 0} & I_{n-k_1 -1} \ea}
\bepm 0 & 0 & a_2^T \\ 0 & J_{k_1}(0) & 0 \\ 0 & 0 & J \eepm
\bb{\ba{c;{2pt/2pt}c} \ba{ccccc} 0 & & & 0 & 1 \\ 1 & 0 &  & & 0 \\ & 1 & \ddots & &  \\ & & & \ddots &  \\ 0 & 0 & \dots & 1 &  0 \ea & \text{\Large 0} \\ \hdashline[2pt/2pt] & \\ \text{\Large 0} & I_{n-k_1 -1} \ea}
\\
& = & \bepm 0 & J_{k_1}(0) & 0 \\ 0 & 0 & a_2^T \\  0 & 0 & J \eepm \bb{\ba{c;{2pt/2pt}c} \ba{ccccc} 0 & & & 0 & 1 \\ 1 & 0 &  & & 0 \\ & 1 & \ddots & &  \\ & & & \ddots &  \\ 0 & 0 & \dots & 1 &  0
\ea & \text{\Large 0} \\ \hdashline[2pt/2pt] & \\ \text{\Large 0} & I_{n-k_1 -1} \ea}
= \bepm J_{k_1}(0) & 0 & 0  \\ 0 & 0 & a_2^T \\  0 & 0 & J \eepm.      \qquad (\dag\dag)
\eeast

By the induction hypothesis, there is a nonsingular $P_2\in M_{n-k_1}(\C)$ such that
\be
P_2^{-1}\bepm 0 & a_2^T \\ 0 & J \eepm P_2 = \wh{J} \in M_{n-k_1}(\C)
\ee
is a Jordan matrix with zero main diagonal. Thus,
\be
\bepm I_{k_1} & 0 \\ 0 & P_2^{-1} \eepm  \bepm J_{k_1}(0) & 0 & 0  \\ 0 & 0 & a_2^T \\  0 & 0 & J \eepm  \bepm I_{k_1} & 0 \\ 0 & P_2 \eepm  = \bepm J_{k_1}(0) & 0 \\ 0 & \wh{J} \eepm
\ee

which is a Jordan matrix of the required form, except that the diagonal Jordan blocks might not be arranged in non-increasing order. A block permutation similarity\footnote{block permutation matrix
needed.} (like ($\dag\dag$)), if necessary, will produce the required form.

Finally, observe that if $A$ is real then all the similarities used in this proof can be chosen to be real (as we can find invertible $P_1\in M_{n-1}(\R),P_2\in M_{n-k_1}(\R)$), so $A$ is similar
via a real similarity to the required Jordan matrix.%\bb{\ba{c;{2pt/2pt}c} \ba{ccc} * & & * \\ & \ddots & \\ 0 & & * \ea & \text{\large $*$} \\ \hdashline[2pt/2pt]
\end{proof}


\begin{corollary}\label{cor:upper_triangular_jordan_canonical_form}
For an upper triangular matrix with all diagonal entries equal to $\lm$,
\be
A = \bepm \ba{cc} \lm & \\ & \lm \ea & \text{\Large $*$} \\ \text{\Large 0} &  \ba{cc} \ddots &  \\ & \lm \ea \eepm ,
\ee

there exists nonsingular matrix $P$ such that
\be
A = P \bepm \ba{cc} J_{n_1}(\lm) & \\ & J_{n_2}(\lm) \ea & \text{\Large 0} \\ \text{\Large 0} &  \ba{cc} \ddots &  \\ & J_{n_m}(\lm) \ea \eepm P^{-1}
\ee
where $J_{n_i}(\lm)$ are Jordan blocks with scalar $\lm$.

If $A$ is real, the matrix $P$ may be chosen to be real.
\end{corollary}

\begin{proof}[\bf Proof]
By assumption, we know that $A_0 = A - \lm I$ is a strictly upper triangular matrix, thus by Theorem \ref{thm:strictly_upper_triangular_jordan_canonical_form} we can find a nonsingular matrix $P$ such that
\be
A_0 = P \bepm \ba{cc} J_{n_1}(0) & \\ & J_{n_2}(0) \ea & \text{\Large 0} \\ \text{\Large 0} &  \ba{cc} \ddots &  \\ & J_{n_m}(0) \ea \eepm P^{-1} = PJP^{-1}.
\ee

Also, we can have that
\be
A = A_0 + \lm I = PJP^{-1} + \lm I PP^{-1} = P\bb{J+\lm I}P^{-1} = P \bepm \ba{cc} J_{n_1}(\lm) & \\ & J_{n_2}(\lm) \ea & \text{\Large 0} \\ \text{\Large 0} &  \ba{cc} \ddots &  \\ & J_{n_m}(\lm) \ea \eepm P^{-1},
\ee
as required. The case for real $A$ is also the direct result from Theorem \ref{thm:strictly_upper_triangular_jordan_canonical_form}.
\end{proof}

\begin{theorem}[Jordan canonical form\index{Jordan canonical form}]\label{thm:jordan_canonical_form}
Let $A\in M_n(\C)$ be a given complex matrix. Then there is a nonsingular matrix $P\in M_n(\C)$ such that
\be
A = P \bepm \ba{cc} J_{n_1}(\lm_1) & \\ & J_{n_2}(\lm_2) \ea & \text{\Large 0} \\ \text{\Large 0} &  \ba{cc} \ddots &  \\ & J_{n_k}(\lm_k) \ea \eepm P^{-1}  = PJP^{-1}
\ee
and $n_1+n_2 + \dots + n_k = n$.

The Jordan matrix $J$ of $A$ is unique up to permutations of the diagonal Jordan blocks.

The eigenvalues $\lm_i$, $i= 1,\dots,k$ are not necessarily distinct.

If $A$ is real matrix with only real eigenvalues $\lm_1,\dots,\lm_k$, then the similarity matrix $P$ can be taken to be real.
\end{theorem}

\begin{proof}[\bf Proof]
We shall proceed to this conclusion in three steps:
\ben
\item [(i)] Observe that every complex matrix is similar to an upper triangular matrix whose eigenvalues appear on the main diagonal in a prescribed order; this is the Schur's unitary triangular
    theorem (Theorem \ref{thm:schur_unitary_triangularization} ).

\item [(ii)] Then we have that an upper triangular matrix can be transformed by similarity into a block diagonal matrix in which each individual diagonal block has all its diagonal entries equal;
    this is Theorem \ref{thm:every_complex_matrix_similar_to_diagonal_block_upper_triangular}.

\item [(iii)] Finally, we can show that an upper triangular matrix whose main diagonal entries are all equal is similar to a direct sum of Jordan blocks; this is Corollary
    \ref{cor:upper_triangular_jordan_canonical_form}.
\een

Also, if $A$ is real with only real eigenvalues, then the similarity matrix $P$ can be taken to be real. This is obvious by combining Theorem
\ref{thm:every_complex_matrix_similar_to_diagonal_block_upper_triangular} and Corollary \ref{cor:upper_triangular_jordan_canonical_form}.

Hence, we have proved everything except the uniqueness assertion.

If $A,B\in M_n(\F)$ are similar, then for any scalar $\lm \in \F$ and any exponent $m=1,2,\dots$, the matrices $\bb{A - \lm I}^m$ and $\bb{B - \lm I}^m$ are also similar. This is, if $B =
Q^{-1}AQ$ for nonsingular matrix $Q$,
\be
\bb{B - \lm I}^m = \bb{Q^{-1}AQ - \lm I}^m = \bb{Q^{-1}\bb{A - \lm I}Q}^m = Q^{-1}\bb{A - \lm I}^m Q.
\ee

In particular, $\bb{A - \lm I}^m$ and $\bb{B - \lm I}^m$ have the same rank by Proposition \ref{pro:equivalent_rank}. Thus, it suffices to show that the set of Jordan blocks (including repetitions)
lying on the diagonal of a Jordan matrix $J\in M_n(\F)$ is uniquely determined by the finitely many integer $\rank\bb{\bb{J-\lm I}^m}$ (equivalently, by $\rank\bb{\bb{A-\lm I}^m}$ as $A$ is already
known), $m = 1,2,\dots$, $\lm \in \sigma(J)$.

First, we consider a Jordan block $J_k(\lm) \in M_k(\F)$ of Definition \ref{def:jordan_block} for some $\lm \in \F$ and $m\geq 1$.

If $\lm \neq 0$, then $\rank\bb{J_k(\lm)^m} = \rank\bb{J_k(\lm)^{m+1}} = k$, so $\rank\bb{J_k(\lm)^m} - \rank\bb{J_k(\lm)^{m+1}} = 0$.

If $\lm = 0$ and $m\geq k$, then $\rank\bb{J_k(0)^m} = \rank\bb{J_k(0)^{m+1}} = 0$, so $\rank\bb{J_k(\lm)^m} - \rank\bb{J_k(\lm)^{m+1}} = 0$ again.

If $\lm = 0$ and $m\leq k-1$, then $\rank\bb{J_k(0)^m} - \rank\bb{J_k(0)^{m+1}} = 1$ as $J_k(0)^0 = I$.

Now let $J \in M_n(\F)$ be a Jordan matrix of Definition \ref{def:jordan_matrix}. Let $\lm\in \sigma(J)$. Define
\be
r_m(\lm) := \rank\bb{\bb{J-\lm I}^m},\qquad m=1,2,\dots
\ee
and set $r_0(\lm) = n$. By Proposition \ref{pro:block_diagonal_matrix_property}.(iii), it follows from the preceding analysis of the case of one block that the difference
\be
d_m(\lm) = r_{m-1}(\lm) - r_m(\lm)
\ee
is equal to the total number of Jordan blocks $J_k(\lm)$ in $J$ of sizes $k\geq m$ and $d_m(\lm) = 0$ for all $m\geq n$. Thus, the number of Jordan blocks in $J$ with exact size $k=m$ is therefore equal to
\be
d_m(\lm) - d_{m+1}(\lm) = r_{m-1}(\lm) - 2r_m(\lm) + r_{m+1}(\lm) ,\qquad m=1,2,\dots,n.
\ee

Hence, the set of Jordan blocks are uniquely determined by $\rank\bb{\bb{J-\lm I}^m}$, for $\lm\in \sigma(J)$ and $m=1,2,\dots,n$.
\end{proof}


\begin{corollary}
Let $A\in M_n(\C)$ be a given complex matrix, and let $\ve >0$ be given. Then there exists a nonsingular matrix $P=P(\ve)\in M_n(\C)$ such that
\be
A = P \bepm \ba{cc} J_{n_1}(\lm_1,\ve) & \\ & J_{n_2}(\lm_2,\ve) \ea & \text{\Large 0} \\ \text{\Large 0} &  \ba{cc} \ddots &  \\ & J_{n_k}(\lm_k,\ve) \ea \eepm P^{-1} ,\qquad (*)
\ee
with $n_1 + n_2 + \dots + n_k = n$ and
\be
J_m(\lm,\ve) = \bepm \ba{ccc} \lm & \ve & \\ & \lm & \ve \\ & & \lm \ea & \text{\Large 0} \\ \text{\Large 0} & \ba{ccc} \ddots & \ddots & \\ & \lm  & \ve  \\ & & \lm \ea\eepm.
\ee

If $A$ is real with real eigenvalues, then $P$ may be taken to be real.
\end{corollary}

\begin{remark}
This means that the off-diagonal entries can be chosen to be arbitrarily small and the form is very `close' to a diagonal matrix.
\end{remark}

\begin{proof}[\bf Proof]
First, by Theorem \ref{thm:jordan_canonical_form}, we find a nonsingular matrix $P_1\in M_n(\C)$ such that $P_1^{-1}AP_1$ is in Jordan canonical form (with a real $P_1$ if $A$ is real and has real eigenvalues). Then take
\be
D_\ve = \diag\bb{1,\ve,\ve^2,\dots,\ve^{n-1}}
\ee
and compute $X = D^{-1}_\ve\bb{P_1^{-1}AP_1}D_\ve = D^{-1}_\ve J D_\ve$. For the first Jordan block, we have
\be
\diag\bb{1,1/\ve,\dots,1/\ve^{n_1-1}} J_{n_1}(\lm_1)\diag\bb{1,\ve,\dots,\ve^{n_1-1}}.
\ee

Similarly, for the $i$th Jordan block, $i =1,\dots,k$,
\be
\diag\bb{\ve^{-\sum^{k-1}_{i=1} n_i},\ve^{-\sum^{k-1}_{i=1} n_i -1 },\dots,\ve^{-\sum^{k}_{i=1} n_i+1}}  J_{n_i}(\lm_i)\diag\bb{\ve^{\sum^{k-1}_{i=1} n_i},\ve^{\sum^{k-1}_{i=1} n_i +1 },\dots,\ve^{\sum^{k}_{i=1} n_i-1}}.
\ee
which is
\beast
& & \bepm \ve^{-\sum^{k-1}_{i=1} n_i} & & & \\ & \ve^{-\sum^{k-1}_{i=1} n_i -1 } & & \\ & & \ddots & \\ & & & \ve^{-\sum^{k}_{i=1} n_i+1} \eepm
\bepm \lm_i \ve^{\sum^{k-1}_{i=1} n_i} & \ve^{\sum^{k-1}_{i=1} n_i +1} & & \\ & \lm_i\ve^{\sum^{k-1}_{i=1} n_i +1 } & \ve^{\sum^{k-1}_{i=1} n_i + 2} & \\ & & \ddots & \\ & & & \lm_i\ve^{\sum^{k}_{i=1} n_i-1}\eepm
\\
& = &  \bepm \lm_i  & \ve & & \\ & \lm_i  & \ve & \\ & & \ddots & \ve \\ & & & \lm_i \eepm .
\eeast

Thus, the matrix $D^{-1}_\ve J D_\ve$ is of the form ($*$). Thus, $P = P(\ve) = P_1D_\ve$ meets the requirements.
\end{proof}

%\subsection{Applications of Jordan canonical form}

\subsection{Minimal polynomial and companion matrix}

The vital role of the characteristic polynomial has already been observed, but there are other polynomials associated with a square matrix. One of these is the minimal polynomial.

Cayley-Hamilton theorem (Theorem \ref{thm:cayley_hamilton}) guarantees that for each $A\in M_n(\C)$, the characteristic polynomial at $A$, $p_A(A) = 0$. Now we want to find a polynomial of smaller
degree to annihilates $A$, which is actually the minimal polynomial of $A$.

\begin{theorem}[minimal polynomial\index{minimal polynomial}]\label{thm:minimal_polynomial_existence_uniqueness}
Let $A\in M_n(\C)$ be given. There exists a unique monic polynomial $q_A(t)$ of minimum degree that annihilates $A$ (i.e., $q_A(A) = 0$). The degree of this polynomial is at most $n$.

If $p(t)$ is any polynomial such that $p(A) = 0$, then $q_A(t)$ divides $p(t)$.

$q_A(t)$ is called minimal polynomial of matrix $A$.
\end{theorem}

\begin{proof}[\bf Proof]
The characteristic polynomial is an example of a polynomial of degree $n$ that annihilates $A$, so there is a minimum positive integer $m\leq n$ such that there exists a monic polynomial $q(t)$ of
degree $m$ with $q(A) = 0$.

If there is a polynomial $p(t)$ annihilates $A$, and if $q(t)$ is a monic polynomial of minimum degree that annihilates $A$, then the degree of $q(t)$ must be less than or equal to the degree of
$p(t)$. By Euclidean algorithm for polynomial (Proposition \ref{pro:euclidean_algorithm_polynomial}), there exists a polynomial $h(t)$ and a polynomial $r(t)$ of degree less than that of $q(t)$ such
that
\be
p(t) = q(t)h(t) + r(t).
\ee

But
\be
0 = p(A) = q(A)h(A) + r(A) = 0 h(A) + r(A) \ \ra \ r(A) = 0.
\ee

If $r(t)\neq 0$, we could normalize it and obtain a monic polynomial of degree less than that of $q(t)$ that annihilates $A$. Since this would contradict the minimal property of $q(t)$, we conclude
that $r(t) \equiv 0$, and hence $q(t)$ divides $p(t)$ with quotient $h(t)$.

If there are two monic polynomials of minimum degree that annihilates $A$, this argument shows that each divides the other;since the degrees are the same, one must be a scalar multiple of the other.
But since both are monic, the scalar factor must be $+1$ and they are identical.
\end{proof}

\begin{corollary}\label{cor:similar_matrices_have_same_minimal_polynomial}
Similar matrices have the same minimal polynomial. That is, if $A\sim B$, $q_A(t) = q_B(t)$.
\end{corollary}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
If $A,B,P\in M_n(\C)$ and $A$ and $B$ are similar with $A = PBP^{-1}$ for nonsingular $P$, then the minimal polynomial of $B$,
\be
q_B(A) = q_B\bb{PBP^{-1}} = P q_B(B)P^{-1} = 0
\ee
by Cayley-Hamilton theorem (Theorem \ref{thm:cayley_hamilton}). So the degree of $q_B(t)$ is not less than the degree of $q_A(t)$. Similarly, we have that the degree of $q_A(t)$ is not less than the degree of $q_B(t)$.

Thus, these two monic minimal polynomials have the same degree and both annihilate $A$, so they must be identical by Theorem \ref{thm:minimal_polynomial_existence_uniqueness}.
\end{proof}



\begin{corollary}\label{cor:characteristic_polynomial_minimal_polynomial_same_roots}
For every $A\in M_n(\C)$, the minimal polynomial $q_A(t)$ divides the characteristic polynomial $p_A(t)$.

Moreover, $q_A(\lm) = 0$ if and only if $p_A(\lm) = 0$. That is, every root of $p_A(t) = 0$ (every eigenvalue of $A$) is a root of $q_A(t) = 0$.
\end{corollary}



\begin{proof}[\bf Proof]
Since $p_A(A) = 0$ by Cayley-Hamilton theorem (Theorem \ref{thm:cayley_hamilton}), the fact that there is a polynomial $h(t)$ such that $p_A(t) = h(t)q_A(t)$ follows from Theorem \ref{thm:minimal_polynomial_existence_uniqueness}.

This factorization makes it clear that every root of $q_A(t) = 0$ is a root of $p_A(t) = 0$, and hence every root of $q_A(t) = 0$ is an eigenvalue of $A$ .

If $\lm$ is an eigenvalue of $A$ (such that $p_A(\lm) = 0$), and $x\neq 0$ is an associated eigenvector, then $Ax = \lm x$ and
\be
0 = q_A(A) x = q_A(\lm) x
\ee
by Proposition \ref{pro:polynomial_matrix_eigenvalue_eigenvector}. Since $x\neq 0$, we have that $q_A(\lm) = 0$ and thus every root of $p_A(t) = 0$ is a root of $q_A(t) = 0$.
\end{proof}

\begin{remark}
The above theorem and corollaries show that if the characteristic polynomial $p_A(t)$ has been completely factors as
\be
p_A(t) = \prod^m_{i=1}\bb{t-\lm_i}^{s_i},\qquad 1\leq s_i\leq n,\quad s_1 + s_2 + \dots + s_m = n\qquad (*)
\ee
with $\lm_1,\lm_2,\dots,\lm_m$ distinct, then the minimal polynomial $q_A(t)$ must have the form
\be
q_A(t) = \prod^m_{i=1}\bb{t-\lm_i}^{r_i},\qquad 1\leq r_i\leq s_i. \qquad (\dag)
\ee

In principle, this gives an algorithm for finding the minimal polynomial of a given matrix $A$:
\ben
\item [(i)] First compute the eigenvalues of $A$, together with their algebraic multiplicities, perhaps by finding the characteristic polynomial and factoring it completely. By some means, determine the factorization ($*$).

\item [(ii)] There are finitely many polynomials of the form of the product in ($\dag$). Starting with the product in which all $r_i = 1$, determine by explicit calculation the one of minimal degree that annihilates $A$. This will be the minimal polynomial.
\een

Numerically, this is not a good algorithm if it involves factoring the characteristic polynomial of a large matrix, but it can be very effective for handy calculations involving small matrices of simple form.

Another approach to computing the minimal polynomial that does not involve knowing either the characteristic polynomial or the eigenvalues is outlined in \cite{Horn_Johnson_1990}.$P_{148}\footnote{details needed.}$.
\end{remark}

The following theorem reveals an intimate connection between the Jordan canonical form of $A$ and the minimal polynomial of $A$.

\begin{theorem}\label{thm:jordan_block_order_minimal_polynomial}
Let $A\in M_n(\C)$ be a given matrix whose distinct eigenvalues are $\lm_1,\lm_2,\dots,\lm_m$. The minimal polynomial of $A$ is
\be
q_A(t) = \prod^m_{i=1}\bb{t-\lm_i}^{r_i},
\ee
where $r_i$ is the order of the largest Jordan block of $A$ corresponding to the eigenvalue $\lm_i$.
\end{theorem}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
Suppose $A = PJP^{-1}$ is the Jordan canonical form of $A$ (by Theorem \ref{thm:jordan_canonical_form}) and suppose first that
\be
J = \bepm \ba{ccc} \lm & 1 & \\ & \lm & 1 \\ & & \lm \ea & \text{\Large 0} \\ \text{\Large 0} & \ba{ccc} \ddots & \ddots & \\ & \lm  & 1  \\ & & \lm \ea\eepm \in M_n(\C).
\ee
is the only a single Jordan block. The characteristic polynomial of $J$ is $(t-\lm)^n$ and thus the minimal polynomial of $J$ is of the form $(t-\lm)^m$ for some $m\leq n$ by Corollary \ref{cor:characteristic_polynomial_minimal_polynomial_same_roots}.
Since $(J-\lm I)^k \neq 0$ if $k< n$, the minimal polynomial of J (and of $A$) is also $(t-\lm)^n$.

If
\be
J = \bepm J_{n_1}(\lm) & 0 \\ 0 & J_{n_2}(\lm)\eepm \in M_n(\C)
\ee
with $n_1\geq n_2$, then the characteristic polynomial of $J$ is still $(t-\lm)^n$, but now $(J-\lm I)^{n_1} = 0$ and no lower power vanishes. The minimal polynomial is therefore $(t-\lm)^{n_1}$.

If there are more blocks, the result is the same: The minimal polynomial of $J$ is $(t-\lm)^r$, where $r$ is the order of the largest Jordan block corresponding to $\lm$.

If $J$ is a general Jordan matrix, the minimal polynomial of $J$ must contain a factor $(t-\lm_i)^{r_i}$ for each distinct eigenvalue $\lm_i$, and $r_i$ must be the order of the largest Jordan block
corresponding to $\lm_i$. For each $i$, $\bb{J - \lm_i}^{r_i}$ will only annihilate the Jordan blocks corresponding to $\lm_i$. No smaller power will annihilate all the Jordan blocks corresponding to
$\lm_i$, and no greater power is needed.

Thus, we need $q(t) = \prod^m_{i=1} (t-\lm_i)^{r_i}$ to annihilate all the Jordan blocks for $m$ distinct eigenvalues and this is the minimal polynomial of $J$.

Then by Corollary \ref{cor:similar_matrices_have_same_minimal_polynomial}, similar matrices have the same minimal polynomial, i.e., $q_J(t) = q_A(t)$.
\end{proof}


\begin{remark}
In practice, this result is not very helpful in computing the minimal polynomial since it is usually harder to determine the Jordan canonical form of a matrix than it is to determine its minimal polynomial.

After all, if only the eigenvalues of a matrix are known, its minimal polynomial can be determined by simple trial and error.
\end{remark}


\begin{corollary}\label{cor:distinct_eigenvalue_minimal_polynomial}
Let $A\in M_n(\C)$ be a given matrix whose distinct eigenvalues are $\lm_1,\lm_2,\dots,\lm_m$. Then $A$ is diagonalizable if and only if $q(A) = 0$, where
\be
q(t) = (t-\lm_1)(t-\lm_2)\dots (t-\lm_m).
\ee
\end{corollary}

\begin{proof}[\bf Proof]
Since a matrix is diagonalizable if and only if all its Jordan blocks have order 1 (Jordan block with order $k\geq 2$ is not diagonalizable by Remark \ref{rem:jordan_matrix_diagonalizable}.), a
necessary and sufficient condition for diagonalizability it that all $r_i =1$ in Theorem \ref{thm:jordan_block_order_minimal_polynomial}.
\end{proof}

\begin{remark}
Note that this corollary gives the necessary and sufficient condition for diagonalizability. Recall Theorem \ref{thm:distinct_eigenvalues_implies_diagonalizable}, which gives the sufficient
condition for diagonalizability, we can see that distinct eigenvalues case is the special case of the statement in Corollary \ref{cor:distinct_eigenvalue_minimal_polynomial}.
\end{remark}

It is sometimes useful to have this result formulated in several equivalent ways (without proof):

\begin{corollary}
Let $A\in M_n(\C)$ be given. Each of the following is necessary and sufficient condition for $A$ to be diagonalizable:
\ben
\item [(i)] The minimal polynomial $q_A(t)$ has distinct linear factors, $(t-\lm_i)$.
\item [(ii)] Every root of $q_A(t) = 0$ has multiplicity 1.
\item [(iii)] For all $t$ such that $q_A(t) = 0$, the derivative $q_A'(t) = 0$.
\een
\end{corollary}

We have been considering the problem of finding, for a given matrix $A\in M_n(\C)$, a monic polynomial of minimum degree (minimal polynomial) that annihilates $A$. But what about the converse? Given
a monic polynomial, is there a matrix $A$ for which $p(t)$ is the minimal polynomial? If so, the size of $A$ must be at least $n\times n$. Actually, it is not hard to find such a matrix $A\in
M_n(\C)$.

\begin{definition}[companion matrix\index{companion matrix}]\label{def:companion_matrix}
Given monic polynomial
\be
p(t) = t^n + a_{n-1}t^{n-1} + a_{n-2}t^{n-2} + \dots + a_1 t + a_0,
\ee

The companion matrix of the polynomial $p(t)$ is defined by
\be
A = \bepm 0 & & & 0 & -a_0 \\ 1 & 0 & & & -a_1 \\ & 1 & \ddots & & \vdots \\ & & \ddots & 0 & -a_{n-2} \\ 0 & & & 1 & -a_{n-1} \eepm \in M_n(\C).
\ee
\end{definition}

\begin{theorem}\label{thm:characteristic_minimal_polynomial_identical_for_companion_matrix}
Every monic polynomial is both the minimal polynomial and the characteristic polynomial of its companion matrix.
\end{theorem}

\begin{proof}[\bf Proof]
Let $e_i$ be the $i$th of natural basis of $\C^n$, then
\beast
I e_1 & = &  e_1 = A^0 e_1 \\
A e_1 & = & e_2 = A^1 e_1 \\
A e_2 & = & e_3 = A^2 e_1 \\
& \vdots & \\
A e_{n-1} & = & e_n = A^{n-1}e_1 \\
A e_n & = & -a_{n-1}e_n - a_{n-2}e_{n-1} - \dots -a_1 e_2 - a_0 e_1 \\
& = & -a_{n-1}A^{n-1}e_1 - a_{n-2}A^{n-2}e_1 - \dots -a_1 A e_1 - a_0 e_1 = A^n e_1.
\eeast

But we know that $p(A)e_1 = A^n e_1 + a_{n-1}A^{n-1}e_1 + a_{n-2}A^{n-2}e_1 + \dots + a_1 A e_1 + a_0 e_1$, so $p(A)e_1 = 0$. Furthermore, for any $k=1,2,\dots,n$,
\be
p(A)e_k = p(A)A^{k-1} e_1 = A^{k-1}p(A)e_1 = 0 \ \ra \ p(A) I = 0 \ \ra \ p(A) = 0.
\ee
by combining all the columns. Thus, $p(t)$ is a monic polynomial of degree $n$ that annihilates $A$.

%Thus, \be p(A) e_1 = \bb{a_0 e_1 + a_1 A e_1 + a_2 A^2 e_1 + \dots + a_{n-1}A^{n-1}e_1} + A^n e_1 = \ee

If there were a polynomial
\be
q(t) = t^m + b_{m-1} t^{m-1} + \dots + b_1t + b_0
\ee
of lower degree $m<n$ that annihilates $A$, then
\be
0 = q(A)e_1 = A^m e_1 + b_{m-1} A^{m-1} e_1 + \dots + b_1 A e_1 + b_0 e_1 = e_{m+1} + b_{m-1}e_m + \dots + b_1 e_2 + b_0 e_1
\ee
which would imply that the basis vector $e_{m+1}$ is linearly dependent on the basis vectors $e_1,\dots,e_m$. Since this is impossible, we conclude that $p(t)$ is the unique monic polynomial of minimum order that annihilates $A$.

Moreover, since $p(t)$ has degree $n$, $A\in M_n(\C)$, and the characteristic polynomial $p_A(t)$ is a monic polynomial of degree $n$ that also annihilates $A$, $p(t)$ must be the characteristic
polynomial of $A$ (since minimal polynomial divides characteristic polynomial by Corollary \ref{cor:characteristic_polynomial_minimal_polynomial_same_roots}).
\end{proof}

\begin{theorem}
A matrix $A\in M_n(\C)$ is similar to the companion matrix $A'$ of its characteristic polynomial $p_A(t)$ if and only if the minimal and characteristic polynomial of $A$ are identical.
\end{theorem}

\begin{proof}[\bf Proof]
($\ra$). By Theorem \ref{thm:characteristic_minimal_polynomial_identical_for_companion_matrix}, we let monic polynomial $p_{A'}(t)$ be both the minimal and characteristic polynomial of its companion
matrix $A'$, i.e., $p_{A'}(t) = q_{A'}(t)$. Since $A$ is similar to $A'$, they have the same characteristic and minimal polynomials by Theorem
\ref{thm:similar_matrices_have_same_characteristic_polynomial} and Corollary \ref{cor:similar_matrices_have_same_minimal_polynomial}. Therefore, $p_A(t) = q_A(t)$ as required.

($\la$). If for matrix $A$, its characteristic polynomial $p_A(t)$ and minimal polynomial $q_A(t)$ are identical, then the Jordan canonical form of $A$ must contain exactly one Jordan block for each
distinct eigenvalue (by Theorem \ref{thm:jordan_block_order_minimal_polynomial}). The size of each Jordan block is equal to the multiplicity of the corresponding eigenvalue as a zero of
characteristic (minimal) polynomial of $A$. That is, the characteristic (minimal) polynomial has the form
\be
p_A(t) = \prod^m_{i=1}(t-\lm_i)^{r_i}
\ee
where $lm_1,\dots,\lm_m$ are distinct eigenvalues and $r_i$, $i=1,\dots,m$ are the corresponding sizes of Jordan blocks. But the Jordan canonical form $J$ of the companion matrix $A'$ has the same characteristic and minimal polynomials, i.e.,
\be
p_J(t) = p_{A'}(t),\qquad q_J(t) = q_{A'}
\ee
\end{proof}


\subsection{Triangular factorizations}



\section{Positive Definite and Positive Semi-definite Matrices}

\subsection{Definitions and basic properties}

\begin{definition}[positive definite matrix\index{positive definite matrix}, positive semi-definite matrix\index{positive semi-definite matrix}]\label{def:positive_definite_matrix}
A Hermitian matrix $A\in M_n(\C)$ is said to be positive definite, denoted as $A \succ 0$,  if
\be
x^*A x >0,\qquad \forall\ x\in \C^n,\ x \neq 0. \qquad\qquad (*)
\ee

If the strict inequality is weakened to $x^*A x \geq 0$, then $A$ is said to be positive semi-definite (or non-negative definite) and denoted as $A \succcurlyeq 0$.
\end{definition}

\begin{remark}
\ben
\item [(i)] Since $A$ is Hermitian, the left-hand side of ($*$) is always a real number.
\item [(ii)] Of course, if $A$ is positive definite, then it is also positive semi-definite.
\item [(iii)] When $n=1$, positive definite matrices are positive real numbers and positive semi-definite matrices are non-negative real numbers.
\een
\end{remark}

\begin{example}
Identity matrix $I$ is positive definite.
\end{example}

\begin{proposition}\label{pro:positive_definite_principal_submatrix_is_positive_definite}
Any principal submatrix (see Definition \ref{def:principal_submatrix}) of a positive definite matrix is positive definite.
\end{proposition}

\begin{remark}
It is easy to see that the diagonal entries of a positive definite matrix are positive real numbers by the above proposition.
\end{remark}

\begin{proof}[\bf Proof]
Let $\alpha$ be a proper subset of $\bra{1,2,\dots,n}$ and $A_{\alpha}$ be the corresponding principal submatrix.

Let $x\in\C^n$ be a non-zero vector with arbitrary entries in the components indicated by $\alpha$ and zero entries elsewhere.Let $x_\alpha$ denote the vector obtained from $x$ by deleting the
(zero) components complementary to $\alpha$, and observe that
\be
x_\alpha^* A_\alpha x_\alpha = x^*A x >0.
\ee

Since $x_\alpha \neq 0$ is arbitrary, the means that $A_\alpha$ is positive definite.
\end{proof}

\begin{proposition}
The sum of any two positive definite matrices of the same size is positive definite. More generally, any positive linear combination of positive definite matrices is positive definite and any
non-negative linear combination of positive semi-definite matrices is positive semi-definite.
\end{proposition}

\begin{remark}
This means that the set of positive definite matrices is a positive cone in the vector space of all matrices.
\end{remark}

\begin{proof}[\bf Proof]
Let $A$ and $B$ be positive semi-definite and $a,b\geq 0$, and observe that
\be
x^*(aA+bB)x = ax^*A x + bx^*Bx \geq 0,\qquad \forall x\in \C^n.
\ee

The case of more than two summands is treated in the same way. If the coefficients are positive, $A$ and $B$ are positive definite, and the vector $x$ is non-zero, then every term in the sum is
positive, so a positive linear combination of positive definite matrices is positive definite.
\end{proof}

\begin{proposition}\label{pro:positive_definite_matrix_positive_eigenvalue}
Each eigenvalue of a positive definite matrix is a positive real number. %Each eigenvalue of a positive semi-definite matrix is a non-negative real number.

It is similar for positive semi-definite matrices as they have non-negative eigenvalues.
\end{proposition}

\begin{proof}[\bf Proof]
Let $A$ be positive definite and $\lm \in \sigma(A)$. Let $x$ be an eigenvector of $A$ associated with $\lm$ (such that $x^*A x >0$) and get
\be
x^*A x = x^*\lm x = \lm x^*x \ \ra \ \lm = \bb{x^*A x}/\bb{x^*x} > 0,
\ee
which is a ratio of two positive numbers as $x\neq 0$.
\end{proof}



\begin{proposition}
The trace, determinant, and all principal submatrices' determiants of a positive definite matrix are positive.
\end{proposition}

\begin{proof}[\bf Proof]
The trace and determinant are just the sum and product of the eigenvalues by Theorem \ref{thm:elementary_symmetric_function_sum_of_determinant_principal_submatrix_equivalent}. The rest follows from Proposition \ref{pro:positive_definite_principal_submatrix_is_positive_definite}
\end{proof}


\begin{proposition}\label{pro:product_rank_positive_definite}
Let $A\in M_m(\C)$ be positive definite. If $C\in M_{m,n}(\C)$, then $C^*AC$ is positive semi-definite.

Furthermore, $\rank\bb{C^*AC} = \rank(C)$, so that $C^*AC$ is positive definite if and only if $C$ has rank $n$.
\end{proposition}


\begin{proof}[\bf Proof]    %\footnote{proof needed.}
First note that $C^*AC$ is Hermitian. For any $x\in \C^n$, we have
\be
x^*C^*A C x = y^* A y \geq 0,
\ee
where $y = Cx$ and the inequality follows from the positive definiteness of $A$. Thus, $C^*AC$ is positive semi-definite.

Furthermore, note that $x^*C^*ACx >0$ if and only if $Cx \neq 0$ because $A$ is positive definite. The statement about rank (and thus about the positive definiteness of $C^*AC$) would follow if we knew that
\be
C^*AC x = 0 \ \lra \ Cx = 0
\ee
because this would mean that $C^*AC$ and $C$ have the same null space (and hence they also have the same rank by rank-nullity theorem\footnote{matrix version needed.} (Theorem \ref{thm:rank_nullity})).

If $Cx = 0$, then obviously $C^*ACx = 0$. Conversely, if $C^*ACx = 0$, then $x^*C^*AC x = 0$ and we conclude that $Cx = 0$ by using the positive definiteness of $A$.
\end{proof}


\subsection{Characterizations}

\begin{theorem}\label{thm:positive_definite_matrix_iff_positive_eigenvalue}
A Hermitian matrix $A\in M_n(\C)$ is positive semi-definite if and only if all of its eigenvalues are non-negative. It is positive definite if and only if all of its eigenvalues are positive.
\end{theorem}

\begin{proof}[\bf Proof]
If each eigenvalue of $A$ is positive, then for any non-zero $x\in \C^n$, we have (by spectral theorem for Hermitian matrices (Theorem \ref{thm:spectral_hermitian_matrices}))
\be
x^*A x = x^*U^* DU x = y^* D y = \sum^n_{i=1}d_{i}\abs{y_i}^2 >0
\ee
where $D = \diag\bb{d_1,\dots,d_n}$ is the diagonal matrix of eigenvalues of $A$ (by Corollary \ref{cor:similarity_same_eigenvalues}) and $y = Ux$ with unitary $U$.

The reverse implication is contained in Proposition \ref{pro:positive_definite_matrix_positive_eigenvalue} and the positive semi-definite case is similar.
\end{proof}

\begin{corollary}\label{cor:power_of_positive_semidefinite_matrix}
If $A\in M_n(\C)$ is positive semi-definite, then so are all the powers $A^k$, $k=1,2,\dots$.

Similarly, the conclusion holds for positive definite matrix $A$, respectively,
\end{corollary}

\begin{proof}[\bf Proof]
Since $A$ is positive semi-definite, all the eigenvalues of $A$ are non-negative by Theorem \ref{thm:positive_definite_matrix_iff_positive_eigenvalue}. Since $A$ is Hermitian, there exists a unitary matrix $U$ such that $U^*AU = \Lambda$ where $\Lambda$ is the diagonal matrix $\diag\bb{\lm_1,\dots,\lm_n}$. Thus,
\be
U^*A^k U = \underbrace{U^* A U U^* A U \dots U^*A U}_{k\text{-fold}} = \Lambda^k = \diag\bb{\lm_1^k,\dots,\lm_n^k}.
\ee

Thus, all the eigenvalues of $A^k$ are non-negative. Therefore, we can say $A^k$ is positive semi-definite by Theorem \ref{thm:positive_definite_matrix_iff_positive_eigenvalue}.
\end{proof}



\begin{proposition}\label{pro:positive_definite_matrix_inverse}
Let $A\in M_n(\C)$ be a positive definite. Then $A^{-1}$ is also a positive definite matrix.
\end{proposition}

\begin{proof}[\bf Proof]
Since all eigenvalues $\lm_1,\dots,\lm_n$ of $A$ are positive by Theorem \ref{thm:positive_definite_matrix_iff_positive_eigenvalue}, we have that for unitary matrix $U$ and $\Lambda := \diag\bb{\lm_1,\dots,\lm_n}$,
\be
\det A = \det\bb{U\Lambda U^*} = \det \Lambda \det \bb{U^*U} = \det\Lambda >0.
\ee

Thus, $A^{-1}$ is well-defined. We can then define the Hermitian matrix $A^{-1} := U\Lambda^{-1}U^*$ such that
\be
A^{-1}A =  U\Lambda^{-1}U^* U\Lambda U^* = UU^* = I = UU^* = U\Lambda\Lambda^{-1}U^* = U\Lambda U^*U\Lambda^{-1}U = AA^{-1}.
\ee

For any $x\neq 0$, we have that
\beast
x^* A x & = & x^* U\Lambda{-1}U^*x = x^*U\Lambda^{-1}\Lambda \Lambda^{-1} U^*x \\
& = & x^*U\Lambda^{-1} U^* U\Lambda U^*U \Lambda^{-1} U^*x = \bb{U \Lambda^{-1} U^*x}^* A \bb{U \Lambda^{-1} U^*x} >0
\eeast
since $A$ is positive definite and $U \Lambda^{-1} U^*x \neq 0$. Thus, $A^{-1}$ is positive definite.
\end{proof}





\begin{corollary}
If $A = \bb{a_{ij}} \in M_n(\C)$ is Hermitian and strictly diagonally dominant\footnote{definition needed.} and if $a_{ii} >0$ for all $i=1,2,\dots,n$, then $A$ is positive definite.
\end{corollary}

\begin{proof}[\bf Proof]
\footnote{proof needed.}
\end{proof}


\begin{corollary}
Let $A\in M_n(\C)$ be Hermitian, and let
\be
p_A(t) = t^n + a_{n-1}t^{n-1} + \dots + a_{n-m}t^{n-m}
\ee
be the characteristic polynomial of $A$. Suppose that $0\leq m\leq n$ and $a_{n-m} \neq 0$.

Then $A$ is positive semi-definite if and only if $a_k \neq 0$ for all $n-m\leq k\leq n$ and $a_k a_{k+1} <0$ for $k=n-m,\dots,n-1$.
\end{corollary}

\begin{proof}[\bf Proof]
\footnote{proof needed.}
\end{proof}

\begin{theorem}\label{thm:positive_definite_iff_leading_principal_submatrices_determinants}
Let $A\in M_n(\C)$ be Hermitian and $A_i$ be its leading principal submatrix of first $i$ rows and columns.

Then $A$ is positive definite if and only if $\det A_i > 0$ for $i=1,2,\dots,n$.

More generally, the positivity of any nested sequence of $n$ principal submatrices' determinants of $A$ (not just the leading principal submatrix determinants) is necessary and sufficient for $A$ to be positive definite.
\end{theorem}

\begin{proof}[\bf Proof]
\footnote{proof needed.}
\end{proof}


Every positive real number has a unique positive $k$th root for all $k=1,2,\dots$. A similar result holds for positive definite matrices.

\begin{theorem}[existence and uniqueness of $k$th root matrix of positive semi-definite matrix]\label{thm:existence_uniqueness_kth_root_matrix_of_positive_semi-definite_matrix}
Let $A\in M_n(\C)$ be positive semi-definite and let $k\geq 1$ be a given integer. Then there exists a unique positive semi-definite Hermitian matrix $B$ such that $B^k = A$, i.e., $B$ is the $k$th
root matrix of $A$. We also have
\ben
\item [(i)] $BA = AB$ and there is a polynomial $p(t)$ such that $B = p(A)$.
\item [(ii)] $\rank(B) = \rank(A)$, so $B$ is positive definite if $A$ is.
\item [(iii)] $B$ is real if $A$ is real.
\een
\end{theorem}

\begin{proof}[\bf Proof]
We know that the Hermitian matrix $A$ can be unitarily diagonalizable (by spectral theorem for Hermitian matrices (Theorem \ref{thm:spectral_hermitian_matrices})) as $A = U\Lambda U^*$ with $\Lambda
= \diag\bb{\lm_1,\dots,\lm_n}$ and all $\lm_i \geq 0$ as $A$ is positive semi-definite.

We define $B = U\Lambda^{1/k} U^*$ where $\Lambda^{1/k} := \diag\bb{\lm_1^{1/k},\dots,\lm_n^{1/k}}$, and the unique non-negative $k$th root is taken in each case.

Clearly, $B^k = U\Lambda^{1/k}U^* \dots U\Lambda^{1/k}U^* = U\Lambda U^* = A$ and $B$ is Hermitian and positive semi-definite, i.e.,
\be
B^* = \bb{U\Lambda^{1/k} U^*}^* = U\Lambda^{1/k} U^* = B\qquad \text{(by Proposition \ref{pro:matrix_multiple_hermitian})}
\ee
and
\be
x^* B x = x^* U\Lambda^{1/k}U^* x = \bb{\Lambda^{1/(2k)}U^*x}^* \bb{\Lambda^{1/(2k)}U^*x} \geq 0,\qquad \forall x\neq 0.
\ee

Also,
\be
AB = U\Lambda U^* U\Lambda^{1/k} U^* = U\Lambda^{1/k}\Lambda U^* = U\Lambda^{1/k} U^*U\Lambda U^* = BA.
\ee

The rank of $B$ is just the number of non-zero $\lm_i$ terms, which is also the rank of $A$ by Proposition \ref{pro:rank_equalities}.(ii). Also, if $A$ is positive definite and thus, all its
eigenvalues $\lm_i>0$ by Theorem \ref{thm:positive_definite_matrix_iff_positive_eigenvalue}, so all eigenvalues of $B$, $\lm_i^{1/k} >0$ and thus $B$ is positive definite by Theorem
\ref{thm:positive_definite_matrix_iff_positive_eigenvalue}.

If $A$ is real and positive definite, we know that $U$ can be chosen to be a real orthogonal matrix (by Theorem \ref{thm:spectral_hermitian_matrices}). So it is clear that $B$ can be chosen to be
real in case.

It remains only to consider the question of uniqueness of $B$ and to find a polynomial such that $p(A) = B$. Recalling Lagrange interpolation polynomial (Definition
\ref{def:lagrange_interpolation_polynomial}), we have
\be
L(t,X,Y) = \sum^m_{i=1} \bb{y_i\ \frac{\prod^m_{j\neq i}\bb{t - x_j}}{\prod^m_{j\neq i}\bb{x_i - x_j}}}
\ee
where $X = \bb{x_1,\dots,x_m}$ and $Y= \bb{x_1^{1/k},\dots,x_m^{1/k}}$ for $m$ distinct non-zero eigenvalues $x_1,\dots,x_m$ among all $n$ eigenvalues (as $m\leq n$). Thus, we define for any $i$, $i=1,\dots,m$,
\be
p_i(A) := \frac{\prod^m_{j\neq i}\bb{A - x_jI}}{\prod^m_{j\neq i}\bb{x_i - x_j}} = \bb{\prod^m_{j\neq i}\bb{x_i - x_j}}^{-1}  \prod^m_{j\neq i}\bb{A - x_jI}
\ee

Therefore,
\beast
p_i(\Lambda) & = & \bb{\prod^m_{j\neq i}\bb{x_i - x_j}}^{-1}  \prod^m_{j\neq i}\bb{\Lambda - x_jI} \\
& = &  \bb{\prod^m_{j\neq i}\bb{x_i - x_j}}^{-1} \bepm 0 & 0 & \dots & 0 \\ 0 & \prod^m_{i\neq j}\bb{x_i-x_j} & & \\ & & \ddots & \\ 0 & 0 & \dots & 0 \eepm
=  \bepm 0 & 0 & \dots & 0 \\ 0 & 1 & & \\ & & \ddots & \\ 0 & 0 & \dots & 0 \eepm
\eeast
with entries 1 in the diagonal if $\lm_{jj} = x_i$ for $j=1,\dots,n$. Thus, we can have that $L(t,X,Y) = p(t)$ and
\be
p(\Lambda) =  \sum^m_{i=1} \bb{y_i p_i(\Lambda)} = \sum^m_{i=1} \bb{x_i^{1/k} p_i(\Lambda)} = \Lambda^{1/k}.
\ee

Thus, by property of unitary matrix,
\be
p(A) = p\bb{U\Lambda U^*} = Up(\Lambda) U^* = U\Lambda^{1/k} U^* = B.
\ee

If $C$ is any positive semi-definite matrix such that $C^k = A$, we have
\be
B = p(A) = p\bb{C^k}  \ \ra \ CB = Cp\bb{C^k} = p\bb{C^k}C = BC.
\ee

Since $B$ and $C$ are commuting Hermitian matrices (thus diagonalizable), they are simultaneously diagonalizable (by Theorem \ref{thm:commute_iff_simultaneously diagonalizable}). That is, there is
some unitary matrix $V$ and diagonal matrices $\Lambda_1$ and $\Lambda_2$ with non-negative diagonal entries such that $B = V\Lambda_1 V^*$ and $C = V\Lambda_2 V^*$.

Then from the fact that $B^k = A = C^k$ we deduce that $\Lambda_1^k = \Lambda_2^k$. But since the non-negative $k$th root of a non-negative number is unique, we conclude that $\Lambda_1 =
\bb{\Lambda_1^k}^{1/k} = \bb{\Lambda_2^k}^{1/k} = \Lambda_2$ and $B =C$.
\end{proof}

\begin{example}
Let $A = \bepm 5 & 3 \\ 3 & 2 \eepm$. It is easy to check that $A$ is positive definite by Theorem \ref{thm:positive_definite_iff_leading_principal_submatrices_determinants}. Thus, we can have
unique positive definite matrix $B = A^{1/2}$. The eigenvalues of $A$ are given by
\be
\det \bb{A - \lm I} = 0 \ \ra\ \lm^2  - 7\lm + 1 = 0 \ \ra \ \lm = \frac 12 \bb{7 \pm 3\sqrt{5}}
\ee
and their roots are $\frac 12 \bb{3\pm \sqrt{5}}$. So we can use Lagrange interpolation polynomial to evaluate $B$,
\be
p(t) = \frac 12 \bb{3 + \sqrt{5}} \frac{t-\frac 12 \bb{7 - 3\sqrt{5}}}{3\sqrt{5}}  - \frac 12 \bb{3 - \sqrt{5}} \frac{t-\frac 12 \bb{7 + 3\sqrt{5}}}{3\sqrt{5}} = \frac 13 (t + 1).
\ee

Thus,
\be
B = p(A) = \frac 13 \bb{A + I} = \frac 13 \bepm 6 & 3 \\ 3 & 3 \eepm = \bepm 2 & 1 \\ 1 & 1 \eepm.
\ee

Also, it is easy to check that
\be
B^2 = \bepm 2 & 1 \\ 1 & 1 \eepm \bepm 2 & 1 \\ 1 & 1 \eepm = \bepm 5 & 3 \\ 3 & 2 \eepm = A.
\ee
\end{example}

\begin{proposition}
If $A$ is positive definite, then for integer $k\geq 1$, $\bb{A^{1/k}}^{-1} = \bb{A^{-1}}^{1/k}$ and it can be written by $A^{-1/k}$, which is also a positive definite.
\end{proposition}

\begin{proof}[\bf Proof]
We can uniquely define positive definite matrix $B = A^{1/k}$ by Theorem \ref{thm:existence_uniqueness_kth_root_matrix_of_positive_semi-definite_matrix}. Also, since all eigenvalues of $A$ and $B$
are positive, $A$ and $B$ are invertible. Therefore, $B^{-1}$ is well-defined and $A^{-1}$ is also a positive definite matrix by Proposition \ref{pro:positive_definite_matrix_inverse}. Let $C :=
\bb{A^{-1}}^{1/k}$ and it is uniquely determined by Theorem \ref{thm:existence_uniqueness_kth_root_matrix_of_positive_semi-definite_matrix}.

Since $A$ is Hermitian, we can have $A = U\Lambda U^*$ by spectral theorem for Hermitian matrices (Theorem \ref{thm:spectral_hermitian_matrices}) where $U$ is unitary and $\Lambda$ is diagonal
matrix of eigenvalues of $A$. Therefore, by the same step in proof of Proposition \ref{pro:positive_definite_matrix_inverse} and Theorem
\ref{thm:existence_uniqueness_kth_root_matrix_of_positive_semi-definite_matrix}, we have
\be
B = U\Lambda^{1/k} U^*,\ A^{-1} = U \Lambda^{-1} U^*,\ C = U \Lambda^{-1/k} U^* \ \ra\ BC = I,\ CB = I.
\ee

%\be
%\left\{\ba{l}
%C^k B^k = A^{-1} A = I\\
%B^k C^k = A A^{-1} = I \ea\right.
%\ \ra \
%\left\{\ba{l}
%C^k B^k = A^{-1} A = I\\
%B^k C^k = A A^{-1} = I \ea\right.
%\ee
%

Thus, $C = B^{-1}$. Furthermore, $A^{-1/k}$ is also a positive definite by Theorem \ref{thm:existence_uniqueness_kth_root_matrix_of_positive_semi-definite_matrix} as $A^{-1}$ is positive definite.
\end{proof}



\begin{theorem}\label{thm:positive_definite_product_nonsingular_matrix}
A matrix $B\in M_n(\C)$ is positive definite if and only if there is a nonsingluar matrix $C\in M_n(\C)$ such that $B = C^*C$.
\end{theorem}

\begin{proof}[\bf Proof]
If $B$ can be so written, we have $B = C^*C = C^*I C$ is positive definite by Proposition \ref{pro:product_rank_positive_definite} since rank of $C$ is $n$ (by Theorem \ref{thm:invertible_full_rank} as $C$ is nonsingular).

If $B$ is positive definite, let $U^*B U = \Lambda$ by spectral theorem for Hermitian matrices (Theorem \ref{thm:spectral_hermitian_matrices}) where $\Lambda = \diag\bb{\lm_1,\dots,
\lm_n}$ with $\lm_1,\dots,\lm_n >0$. Thus, we can define
\be
C := U \Lambda^{1/2}U^* \ \ra \ C^*C = U \Lambda^{1/2}U^*U \Lambda^{1/2}U^* = U \Lambda U^* = B.
\ee

Obviously, $\det C = \det U\bb{\det \Lambda}^{1/2}\det \bb{U^*} \neq 0$ as $\det U$, $\det \bb{U^*}$ and $\det \Lambda$ are non-zero. This gives the fact that $C$ is invertible by Theorem \ref{thm:matrix_invertible_determinant_non_zero}.
\end{proof}

\begin{corollary}
A Hermitian matrix $A$ is positive definite if and only if it is $*$-congruent\footnote{definition needed.} to the identity.
\end{corollary}

\begin{corollary}[Cholesky decomposition\index{Cholesky decomposition}]\label{cor:cholesky_decomposition}
A matrix $A\in M_n(\C)$ is positive definite if and only if there exists an invertible lower triangular matrix $L \in M_n(\C)$ with positive diagonal entries such that $A = LL^*$.

If $A$ is real, $L$ may be taken to be real.
\end{corollary}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
According to Theorem \ref{thm:positive_definite_product_nonsingular_matrix}, we can have that $A$ is positive definite if and only if there is a nonsingular matrix $C$ such that $B =C^*C$. By $QR$
factorization (Theorem \ref{thm:qr_factorization}), we have $C = QR$ where $Q$ is unitary and $R$ is an upper triangular matrix with positive diagonal entries (and thus nonsingular). Then
\be
A = C^*C = \bb{QR}^*QR = R^* Q^*Q R = R^*R.
\ee

Define $L := R^*$, we can have the required form.

Now assume $A$ is real.

If there exists such lower triangular matrix $L$, we can take real $Q$ and $R$ to form nonsingular real $C$ (by $QR$ factorization (Theorem \ref{thm:qr_factorization})), which will imply the
positive definiteness of $A$.

Conversely, if $A$ is positive definite, we can replicate the method in the proof of Theorem \ref{thm:positive_definite_product_nonsingular_matrix} by applying spectral theorem for Hermitian
matrices (Theorem \ref{thm:spectral_hermitian_matrices}) in the real case to get the real $C$. Accordingly, the lower triangular matrix $L$ can be given by applying the real case of $QR$
factorization (Theorem \ref{thm:qr_factorization}).
\end{proof}


%\begin{remark}
%This is true for real and complex $A$.
%\end{remark}

%\section{PoNon-negative Definite Matrices}

\subsection{The polar form and the singular value decomposition}

\begin{lemma}\label{lem:any_matrix_product_of_unitary_diagonal_orthonormal}
Let $A\in M_{m,n}(\C)$ with $m\leq n$ and $\rank (A) = k\leq m$.

Then there exists a unitary matrix $X\in M_m(\C)$, a diagonal matrix $\Lambda \in M_m(\C)$ with non-negative diagonal entries $\lm_1 \geq \lm_2 \geq \dots \geq \lm_k > \lm_{k+1} = \dots = \lm_m =
0$, and a matrix $Y\in M_{m,n}(\C)$ with orthonormal rows (and thus $YY^* = I_m$) such that
\be
A = X\Lambda Y.
\ee

The matrix $\Lambda = \diag\bb{\lm_1,\dots,\lm_n}$ is always uniquely determined and $\bra{\lm_1^2,\dots,\lm_n^2}$ are the eigenvalues of $AA^*$. The columns of the matrix $X$ are eigenvectors of
$AA^*$.

If $AA^*$ has distinct eigenvalues, then $X$ is determined up to a right diagonal factor $D = \diag\bb{e^{i\theta_1},\dots,e^{i\theta_n}}$ with all $\theta_i\in \R$. That is, if $A = X'\Lambda Y' =
X''\Lambda Y''$, then $X'' = X'D$.

Given $X$, the matrix $Y$ is uniquely determined if $\rank(A) = m$.

If $A$ is real, then $X$ and $Y$ can be taken to be real.
\end{lemma}

\begin{proof}[\bf Proof]
First we consider Hermitian matrix $AA^*$. It is obvious that $AA^*$ is positive semi-definite. Thus, by spectral theorem for Hermitian matrices (Theorem \ref{thm:spectral_hermitian_matrices}) we
can find a unitary matrix $X$ such that
\be
AA^* = X\Lambda^2 X^*
\ee
where $\Lambda^2 = \diag\bb{\lm_1^2,\dots,\lm_n^2}$ with eigenvalues of $AA^*$ as all its eigenvalues $\lm_1^2,\dots,\lm_n^2$ are non-negative. Because the diagonal entries of $\Lambda$ are to be
non-negative and are to be arranged in non-increasing order, $\Lambda$ is uniquely determined by $AA^*$. Also, we have
\be
AA^* X_i = \lm_i^2 X_i,\quad i = 1,2,\dots,m
\ee
where $X_i$ is the $i$th column of $X$.

If the numbers $\bra{\lm_i^2}$ are distinct, the corresponding normalized eigenvectors of $AA^*$ are each determined up to a complex scalar factor of modulus
1. This is because that if there exists two eigenvectors of $AA^*$ associated with the same $\lm_i^2$, denoted as $X_i$ and $X_i'$, then each of them can form a basis by combining the other
eigenvectors associated with other eigenvalues. This means that $X_1,\dots,X_i,\dots,X_m$ and $X_1,\dots,X_i',\dots,X_m$ are both bases. Thus, $X_i$ can be expressed by the linear combination of
$X_1,\dots,X_i,\dots,X_m$, i.e.,
\be
X_i' = a_1 X_1 + \dots + a_m X_m \ \ra\ \inner{X_i'}{X_j} = a_j\inner{X_j}{X_j} = a_j.
\ee

Thus, $a_j = 0$ for any $i\neq j$ and $X_i' = a_i X_i$ with $\abs{a_i} = 1$. Therefore, if $X'$ and $X''$ are unitary matrices whose columns are eigenvectors of $AA^*$, we must have $X'' = X'D$
with $D = \diag\bb{d_1,\dots,d_m}$ and all $\abs{d_i} = 1$ for $i = 1,\dots,m$. Eigenvectors of $AA^*$ corresponding to a multiple eigenvalue are not uniquely determined.%but once they are chosen and orthonormalized so that

Now assume $X$ is given. If $A$ is full rank, i.e., $k = \rank(A) = m$. Then $\rank\bb{AA^*} = \rank (A) =m$ by Proposition \ref{pro:rank_equalities}.(i)\&(iv). Then $\rank(\Lambda) =
\rank\bb{\Lambda^2} = m$ by Proposition \ref{pro:rank_equalities}.(ii)\&(iv). Thus $\Lambda$ is nonsingular by Proposition \ref{pro:invertible_non_singular_equivalent}. Then $Y$ is uniquely determined by
\be
Y = \Lambda^{-1}X^*A.
\ee

It is easy to check that
\be
YY^* = \Lambda^{-1}X^*A A^* X \Lambda^{-1} = \Lambda^{-1} X^*X\Lambda^2 \Lambda^{-1} =  \Lambda^{-1} X^*X\Lambda = I_m,
\ee
so this matrix $Y$ has orthonormal rows.

It remains only to handle the case in which $\rank(A) = k<m$ ($\lm_i>0$ for $i\leq k$ and $\lm_i =0$ for $i>k$). %Since we want $Y = \Lambda^{-1} X^* A$
We define the $i$th row vector $Y_i^*$ where
\be
Y_i := \lm_i^{-1}\bb{A^*X_i},\qquad i = 1,\dots,k.
\ee

Then for $i,j = 1,\dots,k$,
\be
Y_i^* Y_j = \bb{ \lm_i^{-1}\bb{A^*X_i}}* \lm_j^{-1}\bb{A^*X_j} = \frac{1}{\lm_i\lm_j} X_i^* AA^* X_j = \frac{1}{\lm_i\lm_j} X_i^* \lm_j^2 X_j = \frac{\lm_j}{\lm_i} X_i^* X_j
\ee
which is 0 if $i\neq j$ and is $1$ if $i=j$ since the vectors $\bb{X_i}$ are orthonormal. Thus, $\bb{Y_1,\dots,Y_k}$ are orthonormal set in $\C^n$, and $n\geq m> k$, so there exists $m-k$ additional
(but not uniquely determined) orthonormal vectors $Y_{k+1},\dots,Y_m$ (by Theorem \ref{thm:steinitz_exchange}) such that the matrix $Y^* := \bb{Y_1,\dots,Y_k,Y_{k+1},\dots,Y_m} \in M_{n,m}(\C)$ has $m$ orthonormal columns.

Now notice that $X^*A = \Lambda Y$. The first $k$ rows of both sides of this identity are equal by construction of the vector $Y_i$ for $i=1,\dots,k$. The last $m-k$ rows are all 0 on the right
because the last $m-k$ diagonal entries $\Lambda$ are 0. If $AA^* X_i = 0 = \lm_i^2 X_i$ for $i=k+1,\dots,m$ ($X_i$ is the eigenvector associated with $\lm_i^2$), then $\bb{X_{k+1},\dots,X_m}$ are
the last $m-k$ columns of $X$. Thus, we have for $i= k+1,\dots,m$,
\be
AA^* X_i = 0 \ \ra\ \ X_i^* AA^*X_i = 0 \ \ra\ \bb{A^*X_i}^*A^*X_i = 0 \ \ra \ A^*X_i = 0 \ \ra \ X_i^*A = 0,
\ee
which means the last $m-k$ rows are all 0 on the left. Thus, $X^*A = \Lambda Y$ gives that the statement that
\be
A = \bb{X^*}^{-1} \Lambda Y = X \Lambda Y.
\ee

Finally, if $A$ is real, then $AA^*$ is real and has real eigenvalues, and hence the eigenvectors $X$ can be taken to be real (by spectral theorem for Hermitian matrices (Theorem
\ref{thm:spectral_hermitian_matrices})). The first $k$ rows of $Y$ which are determined by $X$, are real by construnction, the $m-k$ orthonormal vectors that are added may be taken to be real. Thus
all the factors can be taken to be real if $A$ is real.
\end{proof}


Every non-zero complex number $z$ has a unique ``polar representation'',
\be
z = pu,
\ee
where $p$ is a positive real number and $u$ is complex number of modulus 1. Now we generalize this property to the follow polar decomposition theorem.

\begin{theorem}[polar decomposition\index{polar decomposition}]\label{thm:polar_decomposition}
Let $A \in M_{m,n}(\C)$ with $m\leq n$. Then $A$ can be written as
\be
A = PU
\ee
where $P\in M_m(\C)$, called polar matrix\index{polar matrix}, is positive semi-definite, $\rank (P) = \rank (A)$, and $U\in M_{m,n}(\C)$ has orthonormal rows (i.e., $UU^* = I_m$).

The matrix $P$ is always uniquely determined as $P = \bb{AA^*}^{1/2}$, and $U$ is uniquely determined when $A$ has rank $m$.

If $A$ is real, then both $P$ and $U$ can be taken to be real.
\end{theorem}

\begin{remark}
Note that both factors are unique if $A$ has full rank.
\end{remark}

\begin{proof}[\bf Proof]
By Lemma \ref{lem:any_matrix_product_of_unitary_diagonal_orthonormal}, we can write
\be
A = X\Lambda Y = X\Lambda X^*X Y = PU
\ee
where $P := X\Lambda X^*$ and $U = XY$. Then $P$ is positive semi-definite since
\be
x^* P x = \bb{\Lambda^{1/2} X^*x}^*  \Lambda^{1/2} X^*x \geq 0,\qquad \forall x\neq 0.
\ee

Also, since $Y$ has orthonormal rows,
\be
UU^* = XY (XY)^* = XYY^* X^* = X I_m X^* = I_m,
\ee
so $U$ has orthonormal rows as well. By the construction in Lemma \ref{lem:any_matrix_product_of_unitary_diagonal_orthonormal}, we have
\be
P^2 = \bb{X\Lambda X^*}^2 = X\Lambda X^* X\Lambda X^* = X\Lambda X^* = AA^*.
\ee

Therefore, $P = \bb{AA^*}^{1/2}$ is the unique positive semi-definite square root matrix of $AA^*$ by Theorem \ref{thm:existence_uniqueness_kth_root_matrix_of_positive_semi-definite_matrix}.

If $A$ has rank $m$, then $P$ is full rank and thus nonsingular ($\rank\bb{AA^*} = \rank (A) =m$ by Proposition \ref{pro:rank_equalities}.(i)\&(iv), $\rank(P) = \rank\bb{AA^*}$ by Theorem
\ref{thm:existence_uniqueness_kth_root_matrix_of_positive_semi-definite_matrix}). Thus, %Thus, $Y$ is uniquely determined by Lemma \ref{lem:any_matrix_product_of_unitary_diagonal_orthonormal} and
\be
A = PU \ \ra \ U = P^{-1}A,
\ee
which means $U$ is uniquely determined as well.

However, if $\rank (A) < m$, then the row of $Y$ corresponding to the 0 eigenvalues of $P$ are not uniquely determined, so $U=XY$ need not be uniquely determined when $\rank(A) < m$.
\end{proof}


\begin{corollary}
If $A\in M_n(\C)$, then it can be written in the form
\be
A = PU
\ee
where $P$ is positive semi-definite and $U$ is unitary. The matrix $P$ is always uniquely determined as $P := \bb{AA^*}^{1/2}$.

If $A$ is nonsingular, then $U$ is uniquely determined as $U = P^{-1}A$.

If $A$ is real, then $P$ and $U$ can be taken to be real.
\end{corollary}

\begin{proof}[\bf Proof]
Direct result by letting $m=n$ from Theorem \ref{thm:polar_decomposition}.
\end{proof}


\begin{theorem}
Let $A\in M_n(\C)$ and $A = PU$ be a polar decomposition where $P\in M_n(\C)$ is the positive semi-definite matrix and $U\in M_n(\C)$ is unitary matrix. Then $A$ is normal if and only if $P$ and $U$ commute, i.e., $PU = UP$.
\end{theorem}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
If $P$ and $U$ commute, then
\be
AA^* = PUU^*P^* = PP^* = P^2,\quad A^*A = U^*P^*PU = U^* P^2 U = U^*U P^2 = P^2 \ \ra \ A\text{ is normal.}
\ee

If $A$ is normal, then
\be
P^2 = PUU^*P^* = AA* = A^*A = U^*P^*PU = U^* P^2 U.
\ee

Since $P^2$ and $U^*P^2 U$ are positive semi-definite by Corollary \ref{cor:power_of_positive_semidefinite_matrix}, both of them have the unique positive semi-definite square root matrices by
Theorem \ref{thm:existence_uniqueness_kth_root_matrix_of_positive_semi-definite_matrix}.
Thus,
\be
P = \bb{P^2}^{1/2} = \bb{U^*P^2 U}^{1/2} = U^*P U  \ \ra\ UP = PU,
\ee
as required.
\end{proof}

The next goal is to deduce the singular value decomposition (SVD) of an arbitrary (not necessarily square) matrix.

\begin{theorem}[singular value decomposition\index{singular value decomposition}]\label{thm:singular_value_decomposition}
If $A\in M_{m,n}(\C)$ has rank $k$, then it may be written in the form
\be
A = V\Sigma W^*
\ee
where $V\in M_m(\C)$ and $W\in M_n(\C)$ are unitary. The matrix $\Sigma = \bb{\sigma_{ij}} \in M_{m,n}(\C)$ has $\sigma_{ij} =0$ for all $i\neq j$, and
\be
\sigma_{11} \geq \sigma_{22} \geq \dots \geq \sigma_{kk} > \sigma_{k+1,k+1} = \dots = \sigma_{qq} = 0, \qquad  q = \min\bra{m,n}.
\ee

The numbers $\bra{\sigma_{ii}} := \bra{\sigma_i}$, called singular values of $A$, are the non-negative square roots of the eigenvalues of $AA^*$, and hence are uniquely determined.

The columns of $V$, called left singular vectors of $A$, are eigenvectors of $AA^*$. The columns of $W$, called right singular vectors of $A$, are eigenvectors of $A^*A$ (arranged in the same order
as the corresponding eigenvalues $\sigma^2_i$).

If $m\leq n$ and if $AA^*$ has distinct eigenvalues, then $V$ is determined up to a right diagonal factor $D = \diag\bb{e^{i\theta_1},\dots,e^{i\theta_n}}$ with all $\theta_i \in \R$. That is, if $A
= V_1\Sigma W_1^* = V_2 \Sigma W_2^*$, then $V_2 = V_1 D$. If $m<n$, then $W$ is never uniquely determined. If $n=m=k$ and $V$ is given, then $W$ is uniquely determined.

If $n\leq m$, the uniqueness of $V$ and $W$ is determined by considering $A^*$.

If $A$ is real, then $V$, $\Sigma$ and $W$ may all be taken to be real.
\end{theorem}

\begin{remark}
Note that the singular value decomposition is a natural generalization to arbitrary matrices of the unitary diagonalization of normal matrices. For this reason, it is often the case that facts about
eigenvalues of normal matrices generalize to statements about singular values of general matrices.
\end{remark}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
We assume without loss of generality that $m\leq n$ (otherwise, we can replace $A$ by $A^*$).

Note that $AA^*$ and $A^*A$ have the same non-zero eigenvalues, so the eigenvalues are still the same when we replace $A$ by $A^*$.

If $\lm$ is a non-zero eigenvalue of $AA^*$, then
\be
AA^*x = \lm x \neq 0 \quad\text{for some }x\neq 0 \ \ra\ A^*AA^*x = \lm A^*x.
\ee

Since $A^*x \neq 0$ (otherwise, $AA^*x = 0 = \lm x$ implies a contradiction), we have that $\lm$ is also an eigenvalue of $A^*A$.

Applying Lemma \ref{lem:any_matrix_product_of_unitary_diagonal_orthonormal}, $A = X\Lambda Y$ with $X,\Lambda\in M_m(\C)$ and $Y \in M_{m,n}(\C)$. Set
\be
V:=X,\qquad \Sigma := \bb{ \Lambda | 0 }\in M_{m,n}(\C),\qquad W:= \bb{Y^*| S^*}\in M_n(\C),
\ee
by requiring that the columns of $W$ be an orthonormal set in $\C^n$. The columns of $Y^*$ are already orthonormal by Lemma \ref{lem:any_matrix_product_of_unitary_diagonal_orthonormal}. So if $m<n$,
the columns of $S^* \in M_{n,n-m}(\C)$ may be chosen (but not uniquely) to make $W$ be unitary.

%Also, by Lemma \ref{lem:any_matrix_product_of_unitary_diagonal_orthonormal}, if $AA^*$ has distinct eigenvalues, $V$ is determined up to a right diagonal factor $D = \diag\bb{e^{i\theta_1},\dots,e^{i\theta_n}}$ with all $\theta_i \in \R$.

It is immediate that
\be
V \Sigma W^* = X \bepm \Lambda & 0 \eepm \bepm Y \\ S \eepm = X\Lambda Y = A.
\ee

The statements about uniqueness follow from Lemma \ref{lem:any_matrix_product_of_unitary_diagonal_orthonormal}.
Also, $V$, $\Sigma$ and $W$ may all be taken to be real by Lemma \ref{lem:any_matrix_product_of_unitary_diagonal_orthonormal}.
\end{proof}


\section{Groups of Matrices}


\section{Problems}


\section{Summary}

\subsection{Square matrices}

We can combine Propositions and Theorems (see \cite{Horn_Johnson_1990}.$P_{14}$)\footnote{more equivalent forms needed.} to get that for $A\in M_n(\F)$ the following statements are equivalent: \ben
\item [(i)] $A$ is nonsingular.
\item [(ii)] $0\in \sigma(A)$ (i.e., 0 is an eigenvalue of $A$). \hspace{2cm} (Proposition \ref{pro:zero_eigenvalue_singular_equivalent})
\item [(iii)] $A$ is invertible, i.e. $A^{-1}$ exists. \hspace{3cm} (Proposition \ref{pro:invertible_non_singular_equivalent})
\item [(iv)] $\det A \neq 0$. \hspace{6cm} (Theorem \ref{thm:matrix_invertible_determinant_non_zero})
\item [(v)] The columns of $A$ are linearly independent. \hspace{1cm} (Corollary \ref{cor:invertible_column_linearly_independent})
\item [(vi)] $r(A) = n$, i.e., $A$ is full-rank. \hspace{3cm} (Lemma \ref{lem:full_rank_linearly_independent})
\item [(vii)] The rows of $A$ are linearly independent. \hspace{2cm} (Theorem \ref{thm:rank_matrix_transpose})
\een


\subsection{Relation of matrices}

\begin{center}
\begin{tabular}{ccllll}
\hline
 & matrix  & rank & determinant & trace & eigenvalue \\\hline
equivalence & $m \times n$ & $\surd$ (Proposition \ref{pro:equivalent_rank}) & - & - & -  \\
similarity & $n\times n$ & $\surd$ (Proposition \ref{pro:similarity_implies_equivalent_matrices}) & $\surd$ & $\surd$  & $\surd$ (Corollary \ref{cor:similarity_same_eigenvalues})  \\
unitary equivalence & $n\times n$ & $\surd$ (Proposition \ref{pro:unitary_equivalence_implies_similarity}) & $\surd$ & $\surd$ & $\surd$  \\
\hline
\end{tabular}
\end{center}


\begin{center}
\begin{tabular}{ccllllll}
\hline
 & matrix  & characteristic  & Jordan canonical & minimal & & &  \\
 &  & polynomial & form & polynomial & & &  \\
 \hline
similarity & $n\times n$ & $\surd$ (Theorem \ref{thm:similar_matrices_have_same_characteristic_polynomial}) & $\surd$ (Theorem \ref{thm:jordan_canonical_form} ) & $\surd$ (Corollary \ref{cor:similar_matrices_have_same_minimal_polynomial}) & & & \\
unitary equivalence & $n\times n$ & $\surd$ & $\surd$ & $\surd$ & & & \\
\hline
\end{tabular}
\end{center}

\beast
\text{unitary equivalence} & \subset & \text{similarity} \qquad (\text{Proposition \ref{pro:unitary_equivalence_implies_similarity}})\\
& \subset & \text{equivalence} \qquad (\text{Proposition \ref{pro:similarity_implies_equivalent_matrices}})  \\
& \subset & \text{equivalent relation} \qquad (\text{Proposition \ref{pro:equivalence_matrix_is_equivalent_relation}})
\eeast

unitary diagonalizability $\subset$ diagonalizability $\subset$ similarity


unitary equivalence preserves normality (Proposition \ref{pro:unitary_equivalence_preserves_normality}).

Class of Hermitian matrices is closed under unitary equivalence. (Proposition \ref{pro:hermitian_matrices_closed_under_unitary_equivalence}).

Hermitian $\subset$ normal

skew-Hermitian $\subset$ normal

unitary $\subset$ normal

%\subsection{Subsets of matrices}
