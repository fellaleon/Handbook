


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Analysis}

\ben
\item [(i)] If $f_n \to f$, then $\abs{f_n} \to \abs{f}$. The converse does not hold.
\item [(ii)] If $f_n \to f$, then $f_n^p \to f^p$.
\een

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%\subsection{Series}

%\begin{problem}
%Any problem on sums $\sum_{j=1}^\infty a_j$ can be converted into one on sequences by considering the sequence $s_n = \sum_{j=1}^n a_j$. Show conversely that a sequence $s_n$ converges if and only if, when we set $a_1 = s_1$ and $a_n = s_n - s_{n-1}$ $[n \geq 2]$ we have $\sum_{j=1}^\infty a_j$ convergent. What can you say about  $\lim_{n \rightarrow \infty} \sum_{j=1}^n a_j$ and $\lim_{n \rightarrow \infty} s_n$ if both exist?
%\end{problem}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Continuity}

\begin{problem}
Prove the following results.

(i) Suppose that $E$ is a subset of $\mathbb{R}$ and that $f : E \rightarrow \mathbb{R}$ is continuous at $x \in E$. If $x \in  E' \subset E$ then the restriction $f|_{E'}$ of $f$ to $E'$ is also continuous at $x$.

(ii) If $J : \mathbb{R} \rightarrow \mathbb{R}$ is defined by $J(x) = x$ for all $x \in \mathbb{R}$, then $J$ is continuous on $\mathbb{R}$.

(iii) Every polynomial $P$ is continuous on $\mathbb{R}$.

(iv) Suppose that $P$ and $Q$ are polynomials and that $Q$ is never zero on some subset $E$ of $\mathbb{R}$. Then the rational function $P/Q$ is continuous on $E$ (or, more precisely, the restriction of $P/Q$ to $E$ is continuous.)
\end{problem}


\begin{problem}
Show that any real polynomial of odd degree has at least one root. Is the result true for polynomials of even degree? Give a proof or counterexample.
\end{problem}

\begin{proof}
Let $p(x) = x^3 + ax^2 + bx + c = x^3(\frac{a}{x} + \frac{b}{x^2} + \frac{c}{x^3}) \geq x^3 (1 - \frac{|a|}{x} - \frac{|b|}{x^2} - \frac{|c|}{x^3}) > \frac{x^5}{4}$

If we set $x \geq 4(2 + |a| + |b| + |c|), p(x) \geq 0$ and for $x$ sufficiently large and negative $p(x)$ is negative, so by continuity has a root.
\end{proof}

\begin{problem}
Suppose that $g : [0,1] \rightarrow [0,1]$ is a continuous function. By considering $f(x) = g(x)-x$, or otherwise, show that there exists a $c \in [0,1]$ with $g(c) = c$. (Thus every continuous map of $[0,1]$ into itself has a fixed point.) Give an example of a bijective (but, necessarily, non-continuous) function $h : [0,1] \rightarrow [0,1]$ such that $h(x) \ neq x$ for all $x \in [0,1]$.

\noindent [Hint: First find a function $H : [0,1] \setminus \{0, 1, 1/2 \} \rightarrow [0,1] \setminus \{0, 1, 1/2 \}$ such that $H(x) \neq x$.]
\end{problem}

\begin{proof}
Let $f(x) = g(x) - x$

$f(0) \geq 0$ (if $f(0) = 0, 0 = c$ and done)

$f(1) \leq 0$ (if $f(1) = 0, 0 = c$ and done)
Therefore using the Intermediate Value Theorem $\exists c$ such that $f(c) = 0$ and thus $g(c) = c$
\end{proof}

\begin{problem}
Every mid-summer day at six o'clock in the morning, the youngest monk from the monastery of Damt starts to climb the narrow path up Mount Dipmes. At six in the evening he reaches the small temple at the peak where he spends the night in meditation. At six o'clock in the morning on the following day he starts downwards, arriving back at the monastery at six in the evening. Of course, he does not always walk at the same speed. Show that, none the less, there will be some time of day when he will be at the same place on the path on both his upward and downward journeys.
\end{problem}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analysis I}



\begin{problem}
Let $[a_n,b_n], \ n=1,2,\dots$ be closed intervals with $[a_n,b_n]\cap[a_m,b_m]\neq \emptyset$ for all $n,m$. Prove that $\cap^\infty_{n=1}[a_n,b_n]\neq \emptyset$.
\end{problem}

\begin{solution}[\bf Solution.]
First, we can not use induction to prove the statements about infinity! For example, consider the sets $A_n = [n,\infty),n\in \N$. We have
\be
\bigcap^k_{n=1} A_n = A_k \neq \emptyset = \bigcap^\infty_{n=1}A_n.
\ee
\end{solution}

\begin{problem}
The real sequence $a_n$ is bounded but does not converge. Prove that it has two convergent subsequences with different limits.
\end{problem}

\begin{solution}[\bf Solution.]
By Balzano-Weierstrass Theorem, $a_n$ is real and bounded, so $a_n$ contains a convergent subsequence. Let its limit be $l$. Because $a_n$ is not convergent, there exist $\ve>0$ s.t. there exists infinityly many terms of $a_n$ which satisfy 
\be
\abs{a_n -l} > \ve.
\ee
Now apply Balzano-Weierstrass Theorem again for these $a_n$. There exists another subsequence converges to another limit $m \neq l$. Thus, this real sequence $a_n$ has two convergent subsequences with different limits.
\end{solution}

\begin{problem}
Investigate the convergence of the following series. For those expressions containing the complex number $z$, find those $z$ for which convergence occurs.
\be
\sum_n\frac{\sin n}{n^2}\quad \quad \sum_n\frac{n^2z^n}{5^n}\quad\quad \sum_n\frac{(-1)^n}{4+\sqrt{n}}\quad\quad \sum_n \frac{z^n(1-z)}{n}
\ee
\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] It's obvious that $\abs{\frac {\sin n}{n^2}} \leq \frac 1{n^2}$ and $\sum_n \frac 1{n^2}$ converges. By comparison test, $\sum_n\frac{\sin n}{n^2}$ is absolutely convergent.

\item [(ii)] Using ratio test, we check
\be
\abs{\frac {a_{n+1}}{a_n}} = \abs{\frac{z(n+1)^2}{5n^2}} \to \abs{\frac z5}  \quad \text{as }n \to \infty.
\ee

Thus, we have
\be
\sum_n\frac{n^2z^n}{5^n}\quad
\left\{\ba{ll}
\text{converges absolutely }\quad \quad & \abs{z}<5\\
\text{inconclusive} & \abs{z} = 5\\
\text{diverges} & \abs{z} >5
\ea\right..
\ee

\item [(iii)] We know that $\frac 1{4+\sqrt{n}}$ is decreasing. Thus, by alternative test, $\sum_n\frac{(-1)^n}{4+\sqrt{n}}$ converges.

\item [(iv)] Use ratio test again, 
\be
\abs{\frac {a_{n+1}}{a_n}} = \abs{\frac{zn}{n+1}} \to \abs{z}  \quad \text{as }n \to \infty.
\ee
Thus, we have
\be
\sum_n \frac{z^n(1-z)}{n}\quad
\left\{\ba{ll}
\text{converges absolutely }\quad \quad & \abs{z}<1\\
\text{inconclusive} & \abs{z} = 1\\
\text{diverges} & \abs{z} >1
\ea\right..
\ee
\een
\end{solution}

\begin{problem}
Show that $\sum 1/(n\log^\alpha n)$ converges if $\alpha>1$ and diverges otherwise. Does $\sum 1/(n\log n\log\log n)$ converge?
\end{problem}

\begin{solution}[\bf Solution.]
Cauchy's condensation test. $a_n$ is decreasing sequence of positive terms, then $\sum^\infty_{n=1}a_n$ converges iff $\sum^\infty_{n=1}2^n a_{2^n}$ converges.

We know that $\frac 1{n\log^\alpha n}$ is decreasing. Using Cauchy' condensation test, we have
\be
\sum \frac 1{n\log^\alpha n} \text{ converges } \ \lra \ \sum \frac {2^n}{2^n\log^\alpha 2^n} \text{ converges } \ \lra \ \frac 1{\log^\alpha 2}\sum \frac {1}{n^\alpha } \text{ converges }
\ee

Thus $\sum_n \frac {1}{n^\alpha }$ converges iff $\alpha >1$, diverges otherwise.

For the decreasing sequence $\frac 1{n\log n\log\log n}$, we apply Cauchy's condensation test again,
\beast
\sum \frac 1{n\log n\log\log n} \text{ converges }& \lra & \sum \frac {2^n}{2^n\log 2^n\log\log 2^n} \text{ converges } \\
& \lra & \frac 1{\log 2}\sum \frac {1}{n\bb{\log n + \log\log 2}} \text{ converges }\\
& \lra & \sum \frac {2^n}{2^n\bb{n\log 2 + \log\log 2}} \text{ converges }\quad\quad \text{Cauchy's condensation test once more}\\
& \lra & \sum \frac {1}{n\log 2 + \log\log 2} \text{ converges }
\eeast

It's obvious that
\be
\frac {1}{n\log 2 + \log\log 2} \text{ diverges } \ \ra \ \sum \frac 1{n\log n\log\log n} \text{ diverges.}
\ee
\end{solution}

\begin{problem}
Let $a_n\in\C$ and let $b_n = \frac 1n\sum^n_{i=1}a_i$. Show that, if $a_n\to a$ as $n\to\infty$, then $b_n\to a$ also.
\end{problem}

\begin{solution}[\bf Solution.]
\end{solution}

\begin{problem}
Consider the two series $1-\frac 12 + \frac 13 - \frac 14+ \frac 15- \frac 16+\cdots$ and $1+ \frac 13 -\frac 12 + \frac 15 + \frac 17 - \frac 14+ \cdots$ having the same terms but taken in a different order. Let $s_n$ and $t_n$ be the corresponding partial sums to $n$ terms. Show that $s_{2n} = H_{2n}-H_n$ and $t_{3n} = H_{4n} - \frac 12H_{2n} -\frac 12H_{n}$, where $H_n = 1 + \frac 12 + \frac 13 + \frac 14 + \frac 15 + \cdots + \frac 1n$. Show that $s_n$ converges to a limit $s$ and that $t_n$ converges to $3s/2$.
\end{problem}

\begin{solution}[\bf Solution.]
We have
\beast
s_{2n} & = &  1 - \frac 12 + \frac 13 - \frac 14 \dots + \frac 1{2n-1} - \frac 1{2n}\\
& = & 1 + \frac 12 + \frac 13 + \frac 14 + \dots + \frac 1{2n-1} + \frac 1{2n} - 2\bb{\frac 12 + \frac 14 + \dots + \frac 1{2n}}\\
& = & H_{2n} - H_n.
\eeast

\beast
t_{3n} & = &  \bb{1 + \frac 13 - \frac 12 } + \bb{\frac 15 + \frac 17 - \frac 14 } + \bb{ \frac 1{4n-3} + \frac 1{4n-1}- \frac 1{2n}}\\
& = & \bb{1 + \frac 12 + \frac 13 + \frac 14 + \dots + \frac 1{4n-1} + \frac 1{4n}} - \bb{\frac 12 + \frac 14 + \dots + \frac 1{4n-2} + \frac 1{4n}} - \bb{\frac 12 + \frac 14 + \dots + \frac 1{2n}}\\
& = & H_{4n} - \frac 12 H_{2n} - \frac 12 H_n.
\eeast

Also,
\be
s_{n} \to 1 - \frac 12 + \frac 13 - \frac 14 \dots + \dots = \sum \frac {(-1)^{n+1}}{n} = \log(1+1) = \log 2.
\ee

\beast
t_{n} & \to & \sum \bb{ \frac 1{4n-3} + \frac 1{4n-1}- \frac 1{2n}} = \sum \bb{ \frac 1{4n-3} + \frac 1{4n-1}- \frac 1{4n-2} - \frac 1{4n}} +  \sum \bb{ \frac 1{4n-2} + \frac 1{4n}- \frac 1{2n}}\\
& = & \sum \frac {(-1)^{n+1}}{n} + \frac 12 \sum \frac {(-1)^{n+1}}{n} = \frac 32 \sum \frac {(-1)^{n+1}}{n} = \frac 32 \log 2.
\eeast
\end{solution}

\begin{problem}
Suppose that $\sum a_n$ diverges and $a_n>0$. Show that there exist $b_n$ with $b_n/a_n\to 0$ and $\sum b_n$ divergent.
\end{problem}

\begin{solution}[\bf Solution.]
Since $\sum a_n$ is increasing and divergent, we take a subsequence $N_k$ s.t. $N_0 = 0$, $N_1$ is the first integer where $\sum a_n$ crosses 1, similarly
\be
N_k = \inf\left\{n:\sum^n_{i=N_{k-1}+1}a_i \geq 1,\ n\in \N\right\}.
\ee

Then we define
\be
b_n := \frac {a_n}{k},\quad N_{k-1} < n \leq N_k
\ee
so that $b_n/a_n$ converges to 0 as $n\to \infty$. Also,
\be
\sum b_n = \sum^\infty_{k=1} \sum^{N_{k}}_{N_{k-1}+1} b_n = \sum^\infty_{k=1} \sum^{N_{k}}_{N_{k-1}+1} \frac {a_n}k = \sum^\infty_{k=1} \frac 1k \sum^{N_{k}}_{N_{k-1}+1}a_n \geq \sum^\infty_{k=1} \frac 1k = \infty.
\ee

Hence, $b_n$ is divergent.
\end{solution}

\begin{problem}[Abel's test]
Let $a_n$ and $b_n$ be two sequences and let $S_n=\sum^n_{j=1}a_j$ and $S_0=0$. Show that for any $1\leq m\leq n$ we have
\be
\sum^n_{j=m}a_jb_j = S_nb_n - S_{m-1}b_m + \sum^{n-1}_{j=m}S_j(b_j-b_{j+1}).
\ee
Suppose now that $b_n$ is a decreasing sequence of positive terms tending to zero. Moreover, suppose that $S_n$ is a bounded sequence. Prove that $\sum^\infty_{j=m}a_jb_j$ converges. Deduce the alternative series test. Does the series $\sum^\infty_{n=1}\frac{\cos(n)}{n}$ converge or diverge?
\end{problem}

\begin{solution}[\bf Solution.]
First, we have $S_n b_n = a_1b_n + a_2b_n + \dots + a_nb_n$, $S_{m-1}b_m = a_1b_m + a_2b_m + \dots + a_{m-1}b_m$ and
\be
\sum^{n-1}_{j=m}S_j(b_j-b_{j+1}) = (a_1 + a_2+ \dots + a_m)b_m + (a_{m+1}b_{m+1} + \dots + a_{n-1}b_{n-1}) - (a_1 + a_2 + \dots + a_{n-1})b_n
\ee

Thus, we sum these up to get
\be
S_nb_n - S_{m-1}b_m + \sum^{n-1}_{j=m}S_j(b_j-b_{j+1}) = a_m b_m + (a_{m+1}b_{m+1} + \dots + a_{n-1}b_{n-1}) + a_n b_n = \sum^n_{j=m}a_jb_j.
\ee

Also, we know that $S_n$ is bounded, $\abs{S_n} \leq k$ for some $k$ and $\forall n\in \N$
\beast
\abs{\sum^n_{j=m} a_j b_j} & \leq & \abs{S_nb_n} + \abs{S_{m-1}b_m} + \abs{\sum^{n-1}_{j=m}S_j(b_j-b_{j+1})}\\
& \leq & k b_n + k b_m + k \abs{b_m - b_{n-1}} = 2k b_m.
\eeast

Since $\lim_{m\to\infty}b_m = 0$, $2kb_m$ can be arbitrarily small so by Cauchy's criterion, $\abs{\sum^n_{j=1} a_j b_j}$ converges, so does $\abs{\sum^n_{j=m} a_j b_j}$.

Now let $a_n = (-1)^{n+1}$ then $S_n = 0 \text{ or }1$ which is bounded. Then by Abel's test, the alternative series test holds. 

For the series $\sum^\infty_{n=1}\frac{\cos(n)}{n}$, we have $b_n = \frac 1n$, and 
\be
\sum^n_{k=1} a_k = \sum^n_{k=1} \cos k = \Re\bb{\sum^n_{k=1} e^{ki}} = \Re\bb{\frac{e^{(n+1)i}-1}{e^i - 1}}
\ee
is bounded. Thus, with Abel's test, we know that the series converges.
\end{solution}

\begin{problem}
For $n\geq 1$, let 
\be
a_n = \frac 1{\sqrt{n}} + \frac{(-1)^{n-1}}{n}.
\ee
Show that each $a_n$ is positive and that $\lim a_n =0$. Show also that $\sum^\infty_{n=1}(-1)^{n-1}a_n$ diverges. [This shows that, in the alternative series test, it is essential that the moduli of the terms decrease as $n$ increases.]
\end{problem}

\begin{solution}[\bf Solution.]
Since $n\geq 1$, $\frac 1{\sqrt{n}}\geq \frac 1n$. Thus, we have $a_n \geq 0$. Also,
\be
\lim_{n\to\infty}a_n = \lim_{n\to\infty}\frac 1{\sqrt{n}} + \lim_{n\to\infty}\frac{(-1)^{n-1}}{n} = 0 + 0 = 0.
\ee

Assume $\sum^\infty_{n=1}(-1)^{n-1}a_n$ converges, we have
\be
\sum^\infty_{n=1}(-1)^{n-1}a_n = \sum^\infty_{n=1}(-1)^{n-1}\frac 1{\sqrt{n}} + \sum^\infty_{n=1}\frac 1n.
\ee
The first part converges by alternative test and second part diverges. Hence, this contradiction gives the divergence of the series.
\end{solution}

\begin{problem}
Let $z\in \C$. Show that the series
\be
\frac{z}{1-z^2} + \frac{z^2}{1-z^4} + \frac{z^4}{1-z^8} + \frac{z^8}{1-z^{16}} + \cdots
\ee
converges to $z/(1-z)$ if $|z|<1$, converges to $1/(1-z)$ if $|z|>1$, and diverges if $|z|=1$.
\end{problem}

\begin{solution}[\bf Solution.]
Let $S_n$ be the partial sum of the $n$th partial sum and assume
\be
S_n = \frac 1{1-z} - \frac 1{1-z^{2^n}}.
\ee

Thus,
\be
S_1 = \frac 1{1-z} - \frac 1{1-z^{2}} = \frac z{1-z^2}
\ee
holds. Then by induction, 
\be
S_k = \frac 1{1-z} - \frac 1{1-z^{2^k}}  \ \ra \ S_{k+1} =  \frac 1{1-z} - \frac 1{1-z^{2^k}} + \frac {z^{2^k}}{1-z^{2^{k+1}}} = \frac 1{1-z} - \frac 1{1-z^{2^{k+1}}}. 
\ee
So $S_n$ is true for all $n\in \N$.

If $|z|<1$, as $n\to\infty$, $z^{2^n} \to 0$, thus $S_n \to \frac 1{1-z}-1 = \frac z{1-z}$ as $n\to\infty$.

If $|z|>1$, $\frac 1{|z^{2^n}|}\to 0$ as $n\to\infty$, so $\frac 1{1-z^{2^n}}\to 0$ and $S_n \to \frac 1{1-z}$ as $n\to\infty$.

If $|z|=1$, $S_n$ converges iff $\frac 1{1-z^{2^n}}\to 0$. We have
\be
\abs{\frac 1{1-z^{2^n}}-0} = \abs{\frac 1{1-z^{2^n}}} = \frac 1{\abs{1-z^{2^n}}} \geq \frac 1{1+\abs{z^{2^n}}} = \frac 12 \neq 0
\ee
Thus, we can say that $S_n$ diverges.
\end{solution}


\begin{problem}
Can we write the open interval $(0,1)$ as a disjoint union of closed intervals of positive length?
\end{problem}

\begin{solution}[\bf Solution.]
\end{solution}


\begin{problem}
Is there an enumeration of $\Q$ as $q_1,q_2,q_3,\dots$ such that $\sum(q_n-q_{n+1})^2$ converges?
\end{problem}

\begin{solution}[\bf Solution.]
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}
Define $f : \R \to \R$ by $f(x) = x$ if $x \in \Q$ and $f(x) = 1-x$ otherwise. Find $\{a : f \text{ is continuous at }a\}$.
\end{problem}

\begin{solution}[\bf Solution.]
$\forall \ve >0$, let $\delta = \ve$. Then for $x$ s.t. $\abs{x-\frac 12} < \delta$, 
\be
\abs{f(x)-f\bb{\frac 12}} = \left\{\ba{ll}
\abs{x-\frac 12} < \delta = \ve & \text{if }x\in \Q\\
\abs{1-x-\frac 12} = \abs{x-\frac 12}  < \delta = \ve  \quad\quad & \text{if }x\in \R\backslash\Q
\ea\right.
\ee
Thus $f(x)$ is continuous at $\frac 12$.

\emph{Approach 1}. Now if $f(x)$ is also continuous at $a>\frac 12$. Let $a=\frac 12 + \ve$, $\ve >0$ and $x,y >a$, $\exists \delta >0$ 
\be
\abs{x-a} < \delta, \ x \in \Q,\quad\quad \abs{y-a} < \delta, \ y \in \R\backslash\Q
\ee
such that
\be
\abs{f(x)-f(a)} < \ve,\quad \abs{f(y)-f(a)} < \ve \ \ra \ \abs{f(x)-f(y)} < 2\ve.
\ee

However,
\be
\abs{f(x)-f(y)} = \abs{x+y-1} = x+y-1 > 2a-1 = 2\bb{\frac 12+\ve} -1 = 2\ve \ \ra \ \text{Contradiction.}
\ee

\emph{Approach 2}. If $f(x)$ is continuous at $a\neq \frac 12$, then for any sequence $x_n \to a$, $f(x_n)\to f(a)$. Thus, we pick the sequence $y_n \in \R\backslash\Q$ s.t. $y_n \to a$, then $f(y_n) \to a$.
\be
a =  \lim_{n\to \infty}f(y_n) =  \lim_{n\to \infty}1-y_n = 1 -  \lim_{n\to \infty}y_n = 1-a \ \ra \ a = \frac 12. \quad \text{Contradiction.}
\ee

Hence, $f(x)$ is only continuous at $\frac 12$.
\end{solution} 


\begin{problem}
Write down the definition of "$f(x) \to \infty$ as $x \to \infty$". Prove that $f(x) \to \infty$ as $x \to\infty$ if, and only if, $f(x_n) \to \infty$ for every sequence such that $x_n \to\infty$.
\end{problem}

\begin{solution}[\bf Solution.]
Definition. $f(x) \to \infty$ as $x \to \infty$ if $\forall \ve >0$, $\exists \delta >0$ s.t. $\forall x> \delta$, $f(x)>\ve$.

Proof. '$\Longrightarrow$'.
\beast
\left\{\ba {l}
f(x) \to \infty \text{ as }x \to \infty \ \ra \ \forall \ve >0,\ \exists \delta >0 \text{ s.t. }\forall x> \delta,\ f(x)>\ve\\
x_n \to \infty \ra \forall \delta >0,\ \exists N>0 \text{ s.t. } \forall n\geq N,\ x_n > \delta
\ea\right.\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\\
\ra
\left\{\ba{l}
\forall \ve >0,\ \exists N >0 \text{ s.t. }\forall n \geq N,\ f(x_n)>\ve\\
\text{ which means }f(x_n) \to \infty.
\ea\right.
\eeast

'$\Longleftarrow$'. We have
\be
f(x) \nrightarrow \infty \text{ as }x \to \infty\ \ra \ \forall \delta >0,\ \exists \ve>0 \text{ s.t. }\exists x > \delta,\ f(x)<\ve.
\ee

Choose $\delta_n = n$, thus there is a sequence $x_n > \delta_n$, so $x_n \to \infty$. As $n=\delta_n \to\infty$,
\be
\exists \ve>0, n>0, \ f(x_n) <\ve\ \ra \ f(x_n) \nrightarrow \infty.
\ee
\end{solution}

\begin{problem}
Suppose that $f(x) \to l$ as $x \to a$ and $g(y) \to k$ as $y \to l$. Must it be true that $g(f(x)) \to k$ as $x \to a$?
\end{problem}

\begin{solution}[\bf Solution.]
No. If $f(x)$ is continuous at $a$ and $g(y)$ is not continuous at $l$, let's say
\be
f(x) = 2,\quad\quad g(y) = \left\{\ba{ll}
1 \quad\quad & y \neq 2\\
3 & y = 2
\ea\right.
\ee
which means that $l = 2$, $k=1$. However,
\be
g(f(x)) \to g(2) = 3 \neq 1 \ (k) \quad\quad \text{as }x\to a.
\ee
\end{solution}

\begin{problem}
Let $f_n : [0, 1] \to [0, 1]$ be continuous, $n \in \N$. Let $h_n(x) = \max\{f_1(x), f_2(x), \dots, f_n(x)\}$. Show that $h_n$ is continuous on $[0, 1]$ for each $n \in \N$. Must $h(x) = \sup\{f_n(x) : n \in \N\}$ be continuous?
\end{problem}

\begin{solution}[\bf Solution.]
Realizing for two continuous functions $f$ and $g$
\be
\max\{f,g\} = \frac{(f+g)+\abs{f-g}}2
\ee

We have $f+g$ and $abs{f-g}$ as continuous functions as well. Thus, $\max\{f,g\}$ is continuous. Also, we know that

\be
\max\{f_1(x), f_2(x), \dots, f_n(x)\} = \max\{f_1(x),\max\{f_2(x),\dots,f_n(x) \}\}.
\ee

By induction, we have that $h_n(x)$ is continuous.

Now consider $f_n(x) = \min\{nx,1\},\ x\in[0,1]$. Obviously, $f_n(x)$ is continuous. For any $x\in(0,1]$, there exists $N\in \N$ s.t. $Nx > 1$. Thus, $\forall n\geq N$, $f_n(x) = 1$. This implies that 
\be
h(x) = \sup\{f_n(x) : n \in \N\} = 1,\quad x\in (0,1].
\ee

Then consider $x=0$, we have $f_n(x) = 0$ for all $n$. Hence we have
\be
h(x) = \left\{\ba{ll}
1 \quad\quad & x\neq 0\\
0 & x= 0
\ea\right.
\ee
which is NOT continuous on $[0,1]$.
\end{solution}

\begin{problem}
The unit circle in $\C$ is mapped to $\R$ by a map $e^{i\theta} \mapsto f(\theta)$, where $f : [0, 2\pi] \to \R$ is continuous and $f(0) = f(2\pi)$. Show that there exist two diametrically opposite points that have the same image.
\end{problem}

\begin{solution}[\bf Solution.]
Consider $g(x) = f(x+\pi)-f(x)$, $x\in [0,\pi]$. Then we have three cases:
\ben
\item [(i)] If $f(0) = f(\pi)$, 0 and $\pi$ are diametrically oppositte points, done.
\item [(ii)] If $f(0) > f(\pi)$, 
\be
g(0) = f(\pi) -f(0) < 0, \quad g(\pi) = f(2\pi) - f(\pi) = f(0) - f(\pi) > 0.
\ee
By I.V.T., $\exists x^* $ such that $g(x^*) = 0$ which implies that $f(x^*+\pi) = f(x^*)$.
\item [(iii)] If $f(0) < f(\pi)$, use the same argument.
\een
\end{solution}

\begin{problem}
Let $f(x) = \sin^2 x + \sin^2(x + \cos^7 x)$. Assuming the familiar features of $\sin$ without justification, prove that there exists $k > 0$ such that $f(x) \geq k$ for all $x \in \R$.
\end{problem}

\begin{solution}[\bf Solution.]
First, we have $f(x)$ is non-negative and continuous on $[0,2\pi]$. Thus, there exists $k \geq 0$ such that $f(x)\geq k$. If $f(x) = 0$, we have 
\be
\left\{\ba{l}
\sin^2 x = 0\\
\sin^2(x + \cos^7 x) = 0
\ea\right. \ \ra \ 
\left\{\ba{l}
x = k\pi,\ k\in \Z \\
x + \cos^7 x = m\pi,\ m\in \Z
\ea\right. \ \ra \ \cos^7 x = (m-k)\pi
\ee
which gives
\beast
 -1\leq (m-k)\pi\leq 1 & \ra & k-\frac 1{\pi} \leq m \leq k+ \frac 1{\pi} \\
& \ra &  m=k \ \ra \ \cos^7x = 0 \ \ra \ \cos x = 0 \ \ra \ x = n\pi + \frac {\pi}2, \ n\in\Z \quad \text{contradiction}.
\eeast
Thus, there is no $x$ satisfying $f(x) = 0$ and $f(x) >0$. So $k$ can not be 0, then we have $k>0$.
\end{solution}


\begin{problem}
Suppose that $f : [0, 1] \to \R$ is continuous, that $f(0) = f(1) = 0$, and that for every $x \in (0, 1)$ there exists $0 < \delta < \min\{x, 1 - x\}$ with $f(x) = (f(x - \delta) + f(x + \delta))/2$. Show that $f(x) = 0$ for all $x$.
\end{problem}

\begin{solution}[\bf Solution.]
Since $f:[0,1]\mapsto \R$ is continuous, $\exists K$ s.t. $f(x)\leq K$ and $K$ can be achieved. ($K\geq 0$)

Let $S =\{x:f(x) = K, x\in [0,1]\}$. Since $S$ is bounded below (by zero), $\inf S$ exists. Let $r= \inf S$. By definition of infimum, $\exists x_n \in [0,1]$ s.t. 
\be
r+\frac 1n \geq x_n \geq r,\quad \quad f(x_n) = K.
\ee

By Bolzano-Weierstrass Theorem, $\exists x_{n_j} \to a$ as $j\to \infty$, thus
\be
r \leq x_{n_j} \leq r+ \frac1{n_j} \to r \text{ as }j\to \infty\ \ra \ r=a \ \ra \ \lim_{j}f(x_{n_j}) = f(a) =f(r) = K.
\ee

If $0<r<1$, $f(r-\delta) <K$ (since $r$ is infimum of $S$), we have
\be
f(x) = (f(x - \delta) + f(x + \delta))/2 \ \ra \ f(r) = (f(r - \delta) + f(r + \delta))/2 \ \ra \ f(r+\delta) > K.
\ee
This contradiction gives $r=0$, so $K = 0$. 

Similarly, $\exists L$ s.t. $f(x)\geq L$ and $L$ can be achieved. ($L\leq 0$). Using the same argument, we have $K=L=0$ which means that $f(x)=0$ for all $x$.
\end{solution}

\begin{problem}
Let $f : [a, b] \to \R$ be bounded. Suppose that $f((x + y)/2) \leq (f(x) + f(y))/2$ for all $x, y \in [a, b]$. Prove that $f$ is continuous on $(a, b)$. Must it be continuous at $a$ and $b$ too?
\end{problem}

\begin{solution}[\bf Solution.]
For $x\in (a,b)$, we consider $x,x+h,x+2h,\dots$. With the given inequality, we have
\beast
2f(x+h) \leq f(x) + f(x+2h) & \ra & 2\bb{f(x+h)-f(x)} \leq f(x+2h)-f(x) \\
& \ra & 2^k\bb{f(x+h)-f(x)} \leq f(x+2^k h)-f(x)
\eeast

We require that
\be
a \leq x+ 2^k h \leq b \ \ra \ h \leq 2^{-k}\min\{\abs{x-a},\abs{x-b}\}.
\ee

Also, we know that $f(x)$ is bounded, thus, $\exists M \geq 0$ such that
\be
\abs{f(x)} \leq M, \quad f(y)-f(x)\leq 2M, \ \forall x,y \in (a,b)
\ee

Thus, $\forall \ve >0$, if $f(x+h)-f(x) \geq \ve$, we have
\be
2^k\ve \leq 2^k\bb{f(x+h)-f(x)} \leq f(x+2^k h)-f(x) \ \ra \ f(x+2^k h)-f(x) \geq 2^k\ve
\ee

If we pick $k$ s.t.
\be
2^k\ve > 2M \ \bb{\text{i.e. }2^{-k} < \frac{\ve}{2M}},
\ee
then this is a contradiction for the bounded condition then implies $f(x+h)-f(x) < \ve$. So $\forall \ve>0$, we can have $f(x+h)-f(x) < \ve$ by choosing
\be
h \leq 2^{-k}\min\{\abs{x-a},\abs{x-b}\} < \frac{\ve}{2M} \min\{\abs{x-a},\abs{x-b}\}.
\ee

Hence, $f(x)$ is continuous on $(a,b)$. 

Now consider the case
\be
f(x) = \left\{\ba{ll}
1 \quad\quad & x=a,b\\
0 & x\in (a,b)
\ea\right.
\ee
The inequality 
\be
f((x + y)/2) \leq (f(x) + f(y))/2, \ \forall x, y \in [a, b]
\ee
is satisfied but $f(x)$ is NOT continuous at $a$ and $b$.
\end{solution}

\begin{problem}
Prove that $2x^5 +3x^4 +2x +16 = 0$ has no real solutions outside $[-2,-1]$ and exactly one inside.
\end{problem}

\begin{solution}[\bf Solution.]
First we have
\be
f(-1) = 15,\quad f(-2) = -4.
\ee

With I.V.T., $\exists c \in [-2,-1]$ s.t. $f(c) = 0$. Also, we have
\be
f'(x) = 10x^4+ 12x^3 + 2 = 2x^3(5x+6) + 2 =  2(x+1)(5x^3+x^2-x+1).% 10x^3 (x+1) + 2x^3 + 2=
\ee

Let $g(x) = 5x^3+x^2-x+1$. Thus,
\be
g'(x) = 15x^2 +2x -1 \ \ra g'(x) > 0, \ x \leq -1
\ee

With $g(-1) = -2$, we have $g(x) <0$, $x\leq -1$. Thus,
\be
f'(x) = 2(x+1)g(x) \geq 0,\ x\leq -1.
\ee

Hence, there is only one solution in $[-2,-1]$ and no solution in $(-\infty,-2]$. Furthermore, for $x\geq -1$, 
\be
f(x) = 2x^5 +3x^4 +2x +16 \geq -2 + 0 + -2 + 16 = 12 > 0.
\ee
Thus, there is no solution outline $[-2,-1]$.
\end{solution}

\begin{problem}
Let $f : [a, b] \to \R$ be continuous on $[a, b]$ and differentiable on $(a, b)$. Which of (1)-(4) must be true?
\ben
\item [(1)] If $f$ is increasing then $f'(x) \geq 0$ for all $x \in (a, b)$.
\item [(2)] If $f'(x) \geq 0$ for all $x \in (a, b)$ then $f$ is increasing.
\item [(3)] If $f$ is strictly increasing then $f'(x) > 0$ for all $x \in (a, b)$.
\item [(4)] If $f'(x) > 0$ for all $x \in (a, b)$ then $f$ is strictly increasing.
\een
[Increasing means $f(x) \leq f(y)$ if $x < y$, and strictly increasing means $f(x) < f(y)$ if $x < y$.]
\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(1)] True. $\forall x\in(a,b)$, 
\be
f'(x) = \lim_{h\to 0} \frac{f(x+h)-f(x)}{h}
\ee
since
\be
\forall h>0,\ \frac{f(x+h)-f(x)}{h} \geq 0 \quad \text{(definition of increasing function)}
\ee
Thus,
\be
\lim_{h\to 0} \frac{f(x+h)-f(x)}{h} \geq 0 \ \ra \ f'(x) \geq, \ \forall x\in(a,b).
\ee
\item [(2)] True. Let $x,y\in [a,b]$ with $x<y$. The mean value theorem says 
\be
f(y)-f(x) = f'(c)(y-x), \quad \exists c\in (x,y).
\ee

Then $f'(c)\geq 0 \ \Rightarrow \ f(y)-f(x) = f'(c)(y-x) \geq 0 \ \ra \ f(y) \geq f(x)  \ra f(x) \text{ is increasing}$.

\item [(3)] False. Let $f(x) = x^3,\ x\in[-1,1]$. $f(x)$ is strictly increasing, but $f'(0) = 0$.
\item [(4)] True. Similar argument with (1).
\een
\end{solution}

\begin{problem}
Let $f : \R \to \R$ be differentiable for all $x$. Prove that if $f'(x) \to l$ as $x \to \infty$ then $f(x)/x \to l$. If $f(x)/x \to l$ as $x \to \infty$, must $f'(x)$ tend to a limit?
\end{problem}

\begin{solution}[\bf Solution.]
Without loss of generality, we assume $l=0$, since
\be
g(x) = f(x) - lx \ \ra\ g'(x) = f'(x) -l,\quad \frac{g(x)}{x} = \frac{f(x)}{x} -l.
\ee

With condition provided, we get $\forall \ve >0$, $\exists K >0$ s.t. 
\be
\abs{g'(x)} < \ve,\quad \forall x \geq K.
\ee

Now for any $y<x$,
\be
\abs{\frac{g(x)}{x}} \leq \abs{\frac{g(x)-g(y)}{x}} + \abs{\frac{g(y)}{x}}
\ee

Thus, $\forall \ve>0$, 
\be
\abs{\frac{g(x)-g(y)}{x}} \leq \abs{\frac{g(x)-g(y)}{x-y}} = \abs{g'(x^*)},\ x^*\in (y,x)\quad \bb{\text{by M.V.T. since $g(x)$ is differentiable}}
\ee
So $\exists K_1 >0$ s.t. 
\be
\abs{g'(x^*)} < \frac{\ve}2,\quad \forall x^* \geq K_1.
\ee

Also, $\exists K_2$ s.t.
\be
\abs{\frac{g(K_1)}{x}} < \frac {\ve}2, \quad \forall x\geq K_2
\ee

Thus, $\forall \ve>0$, $\exists K = \max\{K_1,K_2\}$ s.t.
\be
\abs{\frac{g(x)}{x}} \leq \abs{\frac{g(x)-g(K_1)}{x}} + \abs{\frac{g(K_1)}{x}} \leq \abs{g'(x^*)} + \abs{\frac{g(K_1)}{x}} < \frac{\ve}2 + \frac{\ve}2 = \ve, \quad \forall x\geq K.
\ee

It can be proved by L'H\^opital rule which is proved by using M.V.T..
\be
f(x)\to \infty,\ x \to \infty \ \ra \ \lim \frac{f'(x)}{x'} = \lim \frac{f(x)}{x} = l.
\ee

However, the inverse does not hold. For example, 
\be
f(x) = \sin x \ \ra \ \frac{f(x)}{x} = \frac{\sin x}{x} \to 0
\ee
but $f'(x) = \cos x$ does not converge.
\end{solution}

\begin{problem}
Let $f(x) = x + 2x^2 \sin(1/x)$ for $x \neq 0$ and $f(0) = 0$. Show that $f$ is differentiable everywhere and that $f'(0) = 1$, but that there is no interval around 0 on which $f$ is increasing.
\end{problem}

\begin{solution}[\bf Solution.]
For the function $f(x)$
\be
f'(x) = 1 + 4x\sin (1/x) - 2\cos(1/x),\quad x\neq 0
\ee
and
\be
f'(0) = \lim_{h\to 0} \frac{f(h)-f(0)}{h-0} = \lim_{h\to 0} \frac{f(h)}h = \lim_{h\to 0} \frac{h+ 2h^2 \sin(1/h)}{h} = 1+ 2\lim_{h\to 0}h \sin(1/h) = 1.
\ee

Thus, $f$ is differentiable everywhere. 

Now assume there is an interval around 0 on which $f$ is increasing. By the previous result, we have $f'(x)\geq 0$ for all $x$ on this interval.

However, $\forall \ve >0$, $\exists n$ s.t. $x = \frac 1{2n\pi} < \ve$ and
\be
f'(x) = 1 + 4x\sin (1/x) - 2\cos(1/x) = 1 + \frac2{n\pi} \sin (2n\pi) - 2\cos (2n\pi) = 1 -2 = -1 < 0.
\ee
The contradiction gives that there is no interval around 0 on which $f$ is increasing.
\end{solution}

\begin{problem}
Let $f :\R \to \R$ be a function which has the intermediate value property: If $f(a) < c < f(b)$, then $f(x) = c$ for some $x$ between $a$ and $b$. Suppose also that for every rational $r$, the set $S_r$ of all $x$ with $f(x) = r$ is closed, that is, if $x_n$ is any sequence in $S_r$ with $x_n \to a$, then $a \in S_r$. Prove that $f$ is continuous.
\end{problem}

\begin{solution}[\bf Solution.]
Proof by contradiction. Assume all the assumptions hold and $f(x)$ is NOT continuous at some points. That is saying there exists a convergent sequence $\{x_n\}$, $x_n \to a$ s.t. 
\be
\forall \delta >0, \ \exists \ve>0,\ \exists i \in \N \text{ s.t. } \abs{x_i -a}< \delta,\ \abs{f(x_i)-f(a)}>\ve.
\ee

Now without loss of generality, let $\{x_{n_j}\} =\{y_{j}\}$ be the subsequence s.t. $y_j<a$ satisfying the above condition. Then by the intermediate value property, $\forall k \in \N$, $\exists r \in (f(a),f(y_k))$ (or $(f(y_k),f(a))$) s.t. $f(z_k)=r$ for some $y_k < z_k < a$. 

Thus, we obtain a convergent sequence $\{z_k\}$ with $z_k \to a$ (since $y_k \to a$ and $y_k < z_k < a$). Also, $f(z_k) = r$, $\forall k\in\N$, $\lim_{k\to\infty}f(z_k) \neq f(a)$. Thus, $a\notin S_r$, so $S_r$ is not closed.

Hence, $f(x)$ must be continuous everywhere.
\end{solution}

\begin{problem}
Suppose that $f : \R \to \R$ satisfies $|f(x)-f(y)| \leq |x-y|^2$ for all $x, y \in \R$. Show that $f$ is constant.
\end{problem}

\begin{solution}[\bf Solution.]
Suppose $f$ is not constant. Take $a,b\in \R$ with $a<b$ and $f(a)\neq f(b)$, then choose $n\in\N$ with 
\be
n>\frac{(b-a)^2}{\abs{f(b)-f(a)}}
\ee
and for $i=0,1,\dots,n$ set 
\be
c_i = a + \frac in(b-a)
\ee
so that $c_0 = a$ and $c_n = b$. Then
\be
\abs{f(b)-f(a)} = \abs{f(c_n)- f(c_0)} \leq \sum^n_{i=1}\abs{f(c_i) -f(c_{i-1})} \leq \sum^n_{i=1}\abs{c_i - c_{i-1}}^2 = \sum^n_{i=1}\bb{\frac{b-a}n}^2 = \frac {(b-a)^2}n < \abs{f(b)-f(a)}.
\ee

The contradiction implies that $f$ must be constant. An alternative way to prove is to use $f'(x)$.
\end{solution}

\begin{problem}
Given $\alpha \in \R$, define $f_\alpha : [-1, 1] \to \R$ by $f_\alpha(x) = x^\alpha \sin(1/x)$ for $x \neq 0$ and $f_\alpha(0) = 0$. Is $f_0$
continuous? Is $f_1$ differentiable? Draw a table, with 4 columns labelled 0, 1, 2, 3 and with 6 rows labelled "$f_\alpha$ bounded", "$f_\alpha$ continuous2, "$f_\alpha$ differentiable", "$f'_\alpha$ bounded", "$f'_\alpha$ continuous", "$f'_\alpha$ differentiable". Place ticks and crosses at appropriate places in the table. 

Does $|x|^\alpha \sin(1/x)$ behave the same way? Complete 5 extra columns, for $\alpha = -\frac 12 , \frac 12 , \frac 32 , \frac 52 , \frac 72$.
\end{problem}

\begin{solution}[\bf Solution.]
The table is
\begin{center}
\begin{tabular}{l|ccccccccc}
 & \ $-\frac 12 $ \ & \ 0 \ & \ $\frac 12 $ \ & \ 1 \ & \ $\frac 32 $ \ & \ 2 \ & \ $\frac 52 $ \ & \ 3 \  & \ $\frac 72$ \ \\ \hline
$f_\alpha$ bounded & $\times$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$  \\ 
$f_\alpha$ continuous & $\times$ & $\times$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$  \\ 
$f_\alpha$ differentiable & $\times$ & $\times$ & $\times$ & $\times$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$  \\ 
$f_\alpha'$ bounded & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\surd$ & $\surd$ & $\surd$ & $\surd$  \\ 
$f_\alpha'$ continuous & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\surd$ & $\surd$ & $\surd$  \\ 
$f_\alpha$ differentiable & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\surd$  
\end{tabular}
\end{center}

Both functions behave in the same way for $\alpha\in\Z$. 
\beast
|x|^\alpha \sin(1/x)\text{ is bounded } & \lra & \alpha \geq 0,\\
|x|^\alpha \sin(1/x)\text{ is continuous } & \lra & \lim_{h\to 0} \abs{h}^\alpha \sin \frac 1h = 0 \lra \alpha >0, \\
|x|^\alpha \sin(1/x)\text{ is differentiable} & \lra & \lim_{h\to 0}\frac{\abs{h}^\alpha \sin \frac 1h}{h} \text{ exists} \lra \alpha >1. 
\eeast

Thus, we obtain the table shown.
\end{solution}

\begin{problem}
By applying the mean value theorem to $\log(1 + x)$ on $[0, a/n]$ with $n > |a|$, prove carefully that $(1 + a/n)^n \to e^a$ as $n \to \infty$.
\end{problem}

\begin{solution}[\bf Solution.]
Set $f(x) = \log (1+x)$ and fix $a\in\R$. If $a =0$, then
\be
\bb{1+\frac an}^n = 1 = e^0 = e^a, \ \forall n \in \N.
\ee

So we assume $a\neq 0$, given $n > |a|$, apply mean value theorem (Theorem \ref{thm:mean_value}) to either $\bsb{0,\frac an}$ or $\bsb{\frac an, 0}$ according as $a>0$ or $a<0$, there exists $x\in \bb{0,\frac an}$ with 
\be
f'(x) = \frac{f\bb{\frac an}-0}{\frac an - 0} \quad \text{i.e.},\quad \frac 1{1+x} = \frac na \log \bb{1+\frac an}.
\ee

As $n\to \infty$, since $|x| < \frac {\abs{a}}n$ we have $x\to 0$, so
\be
\frac 1{1+x} \to 1 \ \ra \ \frac na \log \bb{1+\frac an} \to 1 \ \log \bb{1+\frac an}^n \to a.
\ee

Finally, as $\exp$ is continuous, 
\be
\bb{1 + \frac an}^n \to e^a.
\ee
\end{solution}

\begin{problem}
Find $\lim_{n\to\infty} n(a^{1/n} - 1)$, where $a > 0$.
\end{problem}

\begin{solution}[\bf Solution.]
Set $f(x) = a^x$ and apply mean value theorem (Theorem \ref{thm:mean_value}) to $\bsb{0,\frac 1n}$, there exists $x\in \bb{0,\frac 1n}$ with
\be
f'(x) = \frac{f\bb{\frac 1n} - f(0)}{\frac 1n - 0} \ \ra \ a^x \log a = n\bb{a^{\frac 1n} - 1}.
\ee

As $n\to \infty$, we have $x\to 0$. Thus,
\be
\lim_{n\to \infty} n\bb{a^{\frac 1n} - 1} = \lim_{x\to 0} a^x \log a = \log a.
\ee
\end{solution}

\begin{problem}
"Let $f'$ exist on $(a, b)$ and let $c \in (a, b)$. If $c + h \in (a, b)$ then $(f(c + h) - f(c))/h = f'(c + \theta h)$. Let $h \to 0$; then $f'(c + \theta h) \to f'(c)$. Thus $f'$ is continuous at $c$." Is this argument correct?
\end{problem}

\begin{solution}[\bf Solution.]
The argument is NOT correct as it can be seen by taking 
\be
f(x) = x^2\sin \frac 1x, \ x\neq 0,\quad f(0) = 0 \ \ra \ f'(x) = 2x\sin \frac 1x - \cos \frac 1x.
\ee

Thus, $\lim_{h\to 0} f'(c+\theta h) \neq f'(c)$ because $f'(c)$ contains $\cos \frac 1c$. 

Although as $h\to 0$ the values $c+\theta h$ tend to $c$, the $\theta$ may vary, so one cannot conclude that for all sequences $(x_n)$ tending to $c$ we have $f'(x_n) \to f'(c)$.
\end{solution}

\begin{problem}
Let $f : \R \to \R$ be defined by $f(x) = \exp(-1/x^2)$ for $x \neq 0$ and $f(0) = 0$. Show that $f$ is continuous and differentiable. Show that $f$ is twice differentiable. Indeed, show that $f$ is infinitely differentiable, and that $f^{(n)}(0) = 0$ for all $n\in \N$. Comment, in the light of what you know about Taylor series.
\end{problem}

\begin{solution}[\bf Solution.]
We prove by induction that for all $n\in \N$ the function $f$ is $n$ times differentiable, with $f^{(n)}(0)=0$ and $f^{(n)}(x) = P_n\bb{\frac 1x}f(x)$ for $x\neq 0$, where $P_n$ is some polynomial. Since 
\be
f'(0) = \lim_{h\to 0} \frac{f(h)-f(0)}{h} = \lim_{h\to 0} \frac{e^{-\frac 1{h^2}}-f(0)}{h} = \lim_{y\to \infty} \frac{y}{e^{y^2}} = 0,
\ee
while for $x\neq 0$ we have
\be
f'(x) = \frac 2{x^3}e^{-1/x^2} = P_1\bb{\frac 1x} f(x),
\ee
where $P_1(t) = 2t^3$, thus the statement is true if $n=1$. (The differentiability implies the continuity.)

Proof. $f$ is differentiable at $x_0$, which implies
\be
\lim_{x\to x_0} = \frac{f(x)-f(x_0)}{x-x_0} = f'(x_0).
\ee

We want to prove that $\lim_{x\to x_0}f(x) = f(x_0)$.
\be
\lim_{x\to x_0} f(x)-f(x_0) = \lim_{x\to x_0} (x-x_0) \frac{f(x)-f(x_0)}{x-x_0} = \lim_{x\to x_0} (x-x_0) \lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0} = 0 \cdot f'(x_0) = 0.
\ee
Thus, $f$ is continuous at $x_0$.

Now assume it is true for $n=k$, then 
\be
f^{(k+1)}(0) = \lim_{h\to 0} \frac{P_k\bb{\frac 1h}f(h)-0}h = \lim_{h\to 0} \frac 1h P_k\bb{\frac 1h}e^{-1/h^2} = \lim_{y\to \infty} \frac{yP_k(y)}{e^{y^2}} = 0,
\ee
while for $x\neq 0$ we have 
\be
f^{(k+1)}(x) = -\frac 1{x^2} P_k'\bb{\frac 1x} f(x) + \frac 2{x^3} P_k\bb{\frac 1x}f(x) = P_{k+1} \bb{\frac 1x}f(x),
\ee
where
\be
P_{k+1}(t) = -t^2 P_k'(t) + 2t^3 P_k(t).
\ee
So it is true for $n= k+1$. Thus, by induction $f$ is infinitely differentiable and $f^{(0)}(0) =0$ for all $n\in \N$. It follows that the Taylor expansion of $f$ about 0 is valid at non-zero points.
\end{solution}

\begin{problem}
Find the radius of convergence of each of these power series.
\be
\sum_{n\geq 0} \frac{ 2 \cdot 4 \cdot 6 \dots (2n + 2) }{1 \cdot 4 \cdot 7 \dots (3n + 1)} z^n,\quad \quad \sum_{n\geq 1} \frac{z^{3n}}{n2^n},\quad\quad \sum_{n\geq 0} \frac {n^nz^n}{n!},\quad\quad \sum_{n\geq 1} n^{\sqrt{n}} z^n
\ee
\end{problem}

\begin{solution}[\bf Solution.]
Let the $n$th term of each power series be denoted $A_n$.
\ben
\item \be
\frac{A_{n+1}}{A_n} = \frac{2n+4}{3n+4}z \to \frac 23 z \quad \text{as }n\to \infty \quad \ra \ \text{ radius of convergence is }\frac 32.
\ee
\item \be
\frac{A_{n+1}}{A_n} = \frac{nz^3}{2(n+1)} \to \frac 12 z^3 \quad \text{as }n\to \infty \quad \ra \ \text{ radius of convergence is }\sqrt[3]{2}.
\ee
\item \be
\frac{A_{n+1}}{A_n} = \frac{(n+1)^{n+1} n!}{(n+1)!n^n}z = \bb{\frac {n+1}{n}}^n z = \bb{1 + \frac 1n}^n z \to ez \quad \text{as }n\to \infty \quad \ra \ \text{ radius of convergence is }\frac 1e.
\ee
\item \be
A_n^{1/n} = n^{1/\sqrt{n}}z \to z \quad \text{as }n\to \infty \quad \ra \ \text{ radius of convergence is 1}.
\ee
Note that $n^{1/\sqrt{n}} = \bb{\sqrt{n}^{1/\sqrt{n}}}^2$, and $m^{1/m} \to 1$ as $m\to \infty$. 
\een
\end{solution}

\begin{problem}[L'H\^opital's rule]
Suppose that $f, g : [a, b] \to \R$ are continuous and differentiable on $(a, b)$. Suppose that $f(a) = g(a) = 0$, that $g'(x)$ does not vanish near $a$ and $f'(x)/g'(x) \to l$ as $x \to a$. 

Show that $f(x)/g(x) \to l$ as $x \to a$. Use the rule with $g(x) = x - a$ to show that if $f'(x) \to l$ as $x \to a$, then $f$ is differentiable at a with $f'(a) = l$.

Find a pair of functions $f$ and $g$ as above for which $\lim_{x\to a} f(x)/g(x)$ exists, but $\lim_{x\to a} f'(x)/g'(x)$ does not.

Investigate the limit as $x \to 1$ of
\be
\frac{x - (n + 1)x^{n+1} + nx^{n+2}}{(1 - x)^2}.
\ee
\end{problem}

\begin{solution}[\bf Solution.]
The generalized M.V.T. shows that for all $x\in (a,b)$ there exists $y\in (a,x)$ with 
\be
\frac{f'(y)}{g'(y)} = \frac{f(x)-f(a)}{g(x)-g(a)} = \frac {f(x)}{g(x)}.
\ee
Thus, as $x\to a$, we have
\be
y \to a \ \ra \ \frac{f(x)}{g(x)} = \frac{f'(x)}{g'(y)} \to l
\ee
as required. If we take $g(x) = x-a$, then $g(a) = 0$, while $g'(x)=1$ so that $g'(x)$ does not vanish near $a$. Thus if $f'(x)\to l$ as $x\to a$, then as $x\to a$ we have
\be
\frac{f'(x)}{g'(x)} \to l,
\ee
so that by the rule,
\be
\frac{f(x)-f(a)}{x-a} = \frac{f(x)}{g(x)} \to l \quad \text{i.e., $f$ is differentiable at $a$ with }f'(a) = l.
\ee

Now set $f = x^2\sin\frac 1x$, $g(x) =x$ and $a=0$. Then $f$ and $g$ are both continuous and differentiable on $\R$, with $f(0)=g(0)=0$, and $g'(x)$ does not vanish near $a$. We have 
\be
\lim_{x\to 0} \frac{f(x)}{g(x)} = \lim_{x\to 0}x\sin \bb{\frac 1x} = 0,
\ee
but if $x\neq 0$, then
\be
\frac{f'(x)}{g'(x)} = f'(x) = 2x\sin\frac 1x - \cos\frac 1x,
\ee
which does not tend to a limit as $x\to 0$.

Set 
\be
f(x) = x-(n+1)x^{n+1} +nx^{n+2},\quad\quad g(x) = (1-x)^2.
\ee

Then $f$ and $g$ are infinitely differentiable on $\R$, and we have
\be
\left\{\ba{l}
f'(x) = 1-(n+1)^2x^n + n(n+2)x^{n+1}\\
f''(x) = -n(n+1)^2x^{n-1}+n(n+1)(n+2)x^n
\ea\right.\quad\quad 
\left\{\ba{l}
g'(x) = - 2(1-x)\\
g''(x) = 2
\ea\right.
\ee

Thus,
\be
\frac{f''(x)}{g''(x)} = \frac 12 n(n+1)x^{n-1}\bb{(n+2)x - (n+1)} \to \frac 12 n(n+1) \quad \text{as }x\to 1.
\ee
so as $f'(1) = g'(1) = 0$, applying the rule to $f'$ and $g'$ gives
\be
\frac{f'(x)}{g'(x)} \to \frac 12 n(n+1) \quad \text{as }x\to 1.
\ee

Then as $f(1)=g(1) = 0$, applying the rule to $f$ and $g$ gives 
\be
\frac{f(x)}{g(x)} \to \frac 12 n(n+1) \quad \text{as }x\to 1.
\ee
\end{solution}

\begin{problem}
Find the derivative of $\tan x$. How do you know there is a differentiable inverse function $\arctan x$ for $x \in \R$? What is its derivative? Now let $g(x) = x-x^3/3+x^5/5+\dots$ for $|x| < 1$. By considering $g'(x)$, explain carefully why $\arctan x = g(x)$ for $|x| < 1$.
\end{problem}

\begin{solution}[\bf Solution.]
$\tan x = \frac{\sin x}{\cos x}$, so
\be
\tan'x = \frac{\cos x \cos x - \sin x(-\sin x)}{\cos^2 x} = \frac 1{\cos^2 x} = \sec^2 x.
\ee

Since $\tan:\ \bb{-\frac {\pi}2, \frac {\pi}2} \mapsto \R$ is strictly increasing (as the derivative is positive), bijective and differentiable with non-zero derivative, by the inverse function theorem the inverse map $\arctan:\ \R \mapsto \bb{-\frac {\pi}2, \frac {\pi}2} $ is also differentiable, and we have
\be
\bb{\arctan}'(\tan x) = \frac 1{\tan'x} = \frac 1{\sec^2 x} = \frac 1{1+\tan^2 x}, \quad \text{i.e., } \bb{\arctan}'y = \frac 1{1+y^2}.
\ee

If $\abs{x} <1$, then
\be
\frac 1{1+x^2} = 1-x^2 + x^4 - x^6 + \dots
\ee
which is a power series with radius of convergence 1, so if $\abs{x}<1$ we may integrate term by term to obtain
\be
\arctan x = c+x - \frac {x^3}3 + \frac {x^5}5 - \frac {x^7}7 + \dots
\ee
for some constant $c$. Evaluating both sides at 0, we have $c=0$, so
\be
\arctan x = x - \frac {x^3}3 + \frac {x^5}5 - \frac {x^7}7 + \dots,\quad \text{for }\abs{x}<1.
\ee
\end{solution}

\begin{problem}
The infinite product $\prod^\infty_{n=1}(1 + a_n)$ is said to converge if the sequence $p_n = (1 + a_1) \dots (1 + a_n)$ converges. Suppose that $a_n \geq 0$ for all $n$. Putting $s_m = a_1+\dots+a_m$, prove that $s_n \leq p_n \leq e^{s_n}$, and deduce that $\prod^\infty_{n=1}(1+a_n)$ converges if and only if $\sum^\infty_{n=1} a_n$ converges. Evaluate $\prod^\infty_{n=2} (1+1/(n^2-1))$.
\end{problem}

\begin{solution}[\bf Solution.]
If $p_n = (1 + a_1) \dots (1 + a_n)$ then
\be
p_n = 1 + \sum_i a_i + \sum_{i<j}a_ia_j + \dots > \sum_i a_i  = s_n.
\ee

Also,
\be
e^{a_i} =  1+a_i + \frac 12 a_i^2 + \geq 1+a_i \ \ra \ e^{s_n} = e^{a_1}\dots e^{a_n} \geq (1 + a_1) \dots (1 + a_n) = p_n.
\ee

Thus, $s_n \leq p_n \leq e^{s_n}$. Therefore, if $\sum a_n$ converges then $p_n$ is an increasing series bounded above by $e^{\sum^\infty_{n=1}a_n}$, so $\prod^\infty_{n=1}(1 + a_n)$ converges. 

Conversely if $\prod^\infty_{n=1}(1 + a_n)$ converges then $s_n$ is an increasing sequence bounded above by $\prod^\infty_{n=1}(1 + a_n)$, so $\sum a_n$ converges.

Now consider $a_n = \frac 1{n^2-1}$, then 
\be
p_n = \prod^n_{i=2} \frac {i^2}{i^2-1} = \prod^n_{i=2} \frac {i\cdot i}{(i-1)(i+1)} = \frac 21 \frac 23 \cdot \frac 32 \frac 34 \cdot \dots \frac n{n-1}\frac n{n+1} = \frac {2n}{n+1} \to 2 \quad \text{as }n \to \infty.
\ee

Hence, $\sum^\infty_{n=2} \frac 1{n^2-1}$ converges.
\end{solution}

\begin{problem}
Let $f$ be continuous on $[-1, 1]$ and twice differentiable on $(-1, 1)$. Let $\phi(x) = (f(x) - f(0))/x$ for $x \neq 0$ and $\phi(0) = f'(0)$. Show that $\phi$ is continuous on $[-1, 1]$ and differentiable on $(-1, 1)$. Using a second order mean value theorem for $f$, show that $\phi'(x) = f''(\theta x)/2$ for some $0 < \theta < 1$. Hence prove that there exists $c \in (-1, 1)$ with $f''(c) = f(-1) + f(1) - 2f(0)$.
\end{problem}

\begin{solution}[\bf Solution.]
For continuity on $[-1,1]$ and differentiability on $(-1,1)$, it clearly suffices to check the behaviour of $\phi$ at 0, since each other point lies in an interval on which $\phi$ is defined as a composition of continuous and differentiable functions.

We have 
\be
\lim_{x\to 0} \phi(x) = \lim_{x\to 0} \frac {f(x)-f(0)}x = f'(0) = \phi(0),
\ee
so $\phi$ is continuous at 0. 

Now take $z\neq 0$ and write $f(z) = f(0) + zf'(0) + \frac 12 z^2 A$. Set $g(u) = f(u)-f(0)-uf'(0) -\frac 12u^2A$, then $g(0)=g(z) = 0$. So by Rolle's theorem, there exists $y\in (0,z)$ with 
\be
g'(y) = 0,\quad \text{i.e., } f'(y)-f'(0) - Ay = 0, \ \ra \ A = \frac{f'(y)-f'(0)}{y}.
\ee

Then we have
\be
f(z) = f(0) + zf'(0) + \frac 12 z^2 \frac{f'(y)-f'(0)}{y} \ \ra \ \phi(z) = \frac{f(z)-f(0)}{z} = f'(0) + \frac 12z \frac{f'(y)-f'(0)}{y}
\ee

Thus,
\be
\frac{\phi(z)-\phi(0)}z = \frac 12 \frac{f'(y)-f'(0)}{y} \ \ra \ \lim_{z\to 0}\frac{\phi(z)-f(0)}{z} = \frac 12 \lim_{y\to 0}\frac{f'(y)-f'(0)}{y} = \frac 12 f''(0).
\ee
Thus, $\phi$ is differentiable at 0, and $\phi'(x) = \frac 12 f''(\theta x)$ holds for $x=0$. 

Now take $x\neq 0$, we have
\beast
f(0) = f(x+(-x)) & = & f(x) + (-x)f'(x) + \frac 12 (-x)^2f''(x + \theta(-x)) \quad \text{for some }\theta' \in (0,1)\\
& = & f(x) -xf'(x) + \frac 12 x^2f''(\theta x ) \quad \theta = 1-\theta' \in (0,1)\\
\eeast

Thus.
\be
\phi(x) = \frac{f(x)-f(0)}x \ \ra\ \phi'(x) = \frac 1{x^2}\bb{f'(x)x-(f(x)-f(0))} = \frac 12 f''(\theta x)
\ee
as required.

To conclude, apply mean value theorem (Theorem \ref{thm:mean_value}) to $\phi$ on $[-1,1]$, there exists $a\in (-1,1)$ with $\phi'(a) = \frac{\phi(1)-\phi(-1)}{1-(-1)}$, so
\be
\frac 12 f''(\theta a) = \frac 12 \bb{\phi(1)-\phi(-1)} = \frac 12 \bb{f(1)-f(0) + f(-1) - f(0)}
\ee

Let $c=\theta a$, we have
\be
f''(c) = f(-1)+f(1)-2f(0).
\ee
\end{solution}

\begin{problem}
Prove the theorem of Darboux: that if $f : \R \to \R$ is differentiable then $f'$ has the "property of Darboux". (That is to say, if $a < b$ and $f'(a) < z < f'(b)$ then there exists $c,\ a < c < b$, with $f'(c) = z$.)
\end{problem}

\begin{solution}[\bf Solution.]
Take $a<b$ and suppose $f'(a)<z<f'(b)$. Set $g(x) = f(x)-zx$, then $g$ is differentiable and 
\be
g'(x) = f'(x) - z \ \ra \ g'(a) < 0 < g'(b).
\ee

We must show that there exists $c\in (a,b)$ with $g'(c) = 0$, since then $f'(c) =z$.

By replacing $g(x)$ by $g(a+b-x)$ if necessary (which preserves the assumption $g'(a) <0< g'(b)$) we may assume $g(a)\geq g(b)$. Take $\epsilon = \frac 12 g'(b)>0$, then $\exists \delta >0$ such that $\abs{x-b}<\delta$ s.t. 
\be
\abs{\frac{g(x)-g(b)}{x-b}-g'(b)} < \epsilon \ \ra \ \frac{g(x)-g(b)}{x-b} > \frac 12g'(b)
\ee

Set $x=b-\frac 12 \delta$, then $g(b)-g(x) > \frac 12 \delta g'(b)$, so $g(x)<g(b)$. Now by I.V.T. there exists $y\in (a,x)$ with $g(y)=g(b)$, then Rolle's theorem shows that there exists $c\in(y,b)$ with $g'(c)=0$ as required.
\end{solution}

\begin{problem}
Let $h : \R \mapsto \R$ be defined by $h(x) = \exp(-1/x^2)$ for $x \neq 0$ and $h(0) = 0$. Construct a function $g : \R \to \R$ that is infinitely-differentiable, positive on a given interval $(a, b)$ and zero elsewhere. Now set 
\be
f(x) = \frac{\int^x_{-\infty} g}{\int^\infty_{-\infty} g}.
\ee
Show that $f$ is infinitely-differentiable, $f(x) = 0$ for $x < a$, $f(x) = 1$ for $x > b$ and $0 < f(x) < 1$ for $x \in (a, b)$. [For this part of the question you may assume standard properties of integration, including that $f'(x) = g(x)/\int^\infty_{-\infty} g$.]

Finally, construct a function from $\R$ to $\R$ that is infinitely-differentiable, but is identically 1 on $[-1, 1]$ and identically 0 outside $(-2, 2)$.
\end{problem}

\begin{solution}[\bf Solution.]
From the previous question, we know 
\be
h:\ \R\mapsto \R, \quad h(x) = \left\{\ba{ll}
e^{-1/x^2} \quad\quad & x > 0\\
0 & x \leq 0
\ea\right.
\ee
is infinitely differentiable. Next, define 
\be
g:\ \R\mapsto \R, \quad g(x)= h(x-a)h(b-x).
\ee

Then $g$ is also infinitely differentiable, is positive on $(a,b)$ and zero elsewhere. Now define 
\be
f:\ \R\mapsto \R, \quad f(x) = \frac{\int^x_{-\infty} g(t)dt}{\int^\infty_{-\infty} g(t)dt}.
\ee

Then as 
\be
f'(x) = \frac{g(x)}{\int^\infty_{-\infty} g(t)dt}
\ee
we see that $f$ is infinitely differentiable.

If $x<a$, then
\be
f(x) = \frac{\int^x_{-\infty} g(t)dt}{\int^\infty_{-\infty} g(t)dt} = \frac{\int^x_{-\infty}0 dt}{\int^\infty_{-\infty} g(t)dt} = 0.
\ee

If $x>b$, then
\be
f(x) = \frac{\int^x_{-\infty} g(t)dt}{\int^\infty_{-\infty} g(t)dt} = \frac{\int^\infty_{-\infty}g(t) dt}{\int^\infty_{-\infty} g(t)dt} = 1.
\ee

If $a<x<b$, then
\be
0 < \int^x_{-\infty} g(t)dt < \int^\infty_{-\infty} g(t)dt \ \ra \ 0<f(x)<1.
\ee

Finally take $a=1$, $b=2$ and define
\be
k:\ \R\mapsto \R\quad k(x) = 1-f(\abs{x}).
\ee

Then $k$ is infinitely differentiable, and is identically 1 on $[-1,1]$ and identically 0 outside $[-2,2]$.
\end{solution}

\begin{problem}
Show directly from the definition of an integral that $\int^a_0 x^2 = a^3/3$ for $a > 0$.
\end{problem}

\begin{solution}[\bf Solution.]
Consider 
\be
D = \left\{0,\frac an, \frac {2a}n, \dots, \frac {(n-1)a}n, a\right\}
\ee

Then
\be
S(x^2,D) = \sum^n_{j=1} \sup_{x\in [x_{j-1},x_j]}x^2(x_j - x_{j-1}) = \sum^n_{j=1} \bb{\frac{aj}n }^2 \frac an = \frac {a^3}{n^3}\frac 16 n(n+1)(2n+1)= \frac {(n+1)(2n+1)}{6n^2}a^3,
\ee
\be
s(x^2,D) = \sum^n_{j=1} \inf_{x\in [x_{j-1},x_j]}x^2(x_j - x_{j-1}) = \sum^n_{j=1} \bb{\frac{a(j-1)}n }^2 \frac an = \frac {a^3}{n^3}\frac 16 (n-1)n(2n-1)= \frac {(n-1)(2n-1)}{6n^2}a^3.
\ee

Thus, we have
\be
I^*(x^2) = \inf_D S(x^2,D) = \frac {a^3}3 = \inf_D s(x^2,D) = I_*(x^2).
\ee

Hence, $x^2$ is integrable, (which can be achieved by monotonicity of $x^2$ (in the lecture notes)) and 
\be
\int^a_0 x^2 = \frac {a^3}3.
\ee
\end{solution}

\begin{problem}
Let $f(x) = \sin(1/x)$ for $x \neq 0$ and $f(0) = 0$. Does $\int^1_0 f$ exist?
\end{problem}

\begin{solution}[\bf Solution.]
Since $f(x)$ is continuous at $x\neq 0$, $\forall \ve >0$, $\exists$ a partition $D_1$ of $[\ve,1]$ s.t.
\be
S(f,D_1) - s(f,D_1) < \ve
\ee
by Riemann Theorem. Also, for any $D_2$ of $[0,\ve]$ s.t. 
\be
S(f,D_2) \leq \ve \sup_{x\in[0,\ve]} \sin \frac 1x \leq \ve, \quad\quad s(f,D_2) \geq \ve \inf_{x\in[0,\ve]} \sin \frac 1x \geq -\ve
\ee
with $f(0) = 0$. Thus,
\be
S(f,D_2) - s(f,D_2) \leq 2\ve.
\ee

Now let $D = D_1 \cup D_2$, we have
\be
S(f,D) - s(f,D) = S(f,D_1) - s(f,D_1) + S(f,D_2) - s(f,D_2) < \ve + 2\ve = 3\ve.
\ee

Thus, $f$ is integrable by Riemann Theorem and $\int^1_0 f$ exists.
\end{solution}

\begin{problem}
Give an example of a continuous function $f : [0,\infty) \to [0,\infty)$, such that $\int^\infty_0 f$ exists but $f$ is unbounded.
\end{problem}

\begin{solution}[\bf Solution.]
See the diagram. For each step, we double the height of the triangles and shrink the width to $\frac 14$ of previous one.

Thus, the area is 
\be
1\times \frac 12 + \frac 14 \times 1 + \frac 16 \times 2 + \dots = \frac 12 + \frac 14 + \frac 18 + \dots \to 1.
\ee

Thus, $\int^\infty_0 f$ exists but $f$ is unbounded.

\centertexdraw{
\drawdim in

\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0

\move (0.5 0) \lvec(1 0.25) \lvec(1.5 0) 
\move (1.75 0) \lvec(2 0.5) \lvec(2.25 0) 
\move (2.875 0) \lvec(3 1) \lvec(3.125 0) 

\htext(3.5 0.5){$\dots$}
\move (-0.2 0) \avec(4.3 0)
\move (0 -0.2) \avec(0 1.3)
}
\end{solution}

\begin{problem}\label{ques:vanish_identically} 
Give an example of an integrable function $f : [0, 1] \to \R$ with $f \geq 0$, $\int^1_0 f = 0$, and $f(x) > 0$ for some value of $x$. Show that this cannot happen if $f$ is continuous.
\end{problem}

\begin{solution}[\bf Solution.]
Here we give two examples
\be
f_1(x) = \left\{\ba{ll}
1 \quad\quad & x=\frac 12\\
0 & x \neq \frac 12
\ea\right.,\quad\quad\quad\quad 
f_2(x) = \left\{\ba{ll}
\frac 1q \quad\quad & x = \frac pq \in [0,1]\\
0 & x \text{ is irrational in }[0,1]
\ea\right..
\ee

Now consider a function $f$ which continuous at $[0,1]$ and $f(x)>0$ for some $x$, let's say, $x=a$. Choose $\ve>0$ s.t. $f(a) = 2\ve$. Since $f$ is continuous, $\exists \delta >0$ s.t.
\be
\forall \abs{x-a} < \delta \ \ra \ \abs{f(x)-f(a)}< \ve \ \ra \ f(x) > \frac 12 f(a)
\ee

Thus,
\be
\int^1_0 f = \int^{a-\delta}_0 f + \int^{a+\delta}_{a-\delta}f + \int^1_{a+\delta}f > \int^{a-\delta}_0 0 + \frac 12f(a) \int^{a+\delta}_{a-\delta}dx + \int^1_{a+\delta}0 = \delta f(a) > 0.
\ee

Hence, the contradiction gives the fact that $f$ is NOT continuous.
\end{solution}

\begin{problem}
Let $f : \R \to \R$ be monotonic. Show that $\{x \in \R : f \text{ is discontinuous at }x\}$ is countable. Let $x_n,\ n \geq 1$ be a sequence of distinct points in $(0, 1]$. Let $f_n(x) = 0$ if $0 \leq x < x_n$ and $f_n(x) = 1$ if $x_n \leq x \leq 1$. Let $f(x) = \sum^\infty_{n=1} 2^{-n}f_n(x)$. Show that this series converges for every $x \in [0, 1]$. Show that $f$ is increasing (and so is integrable). Show that $f$ is discontinuous at every $x_n$.
\end{problem}

\begin{solution}[\bf Solution.]
Without loss of generality, we have $f$ increasing. Let $S=\{x\in\R: \ x \text{ is a discontinuity of }f\}$, take $y\in S$ and define
\be
Ly = \{f(x):\ x<y\},\quad\quad Ry= \{f(x):\ x>y\}
\ee
then we have
\be
\sup Ly \leq f(y) \leq \inf Ry.
\ee

Since $f(x)$ is discontinuous at $y$, $\sup Ly < \inf Ry$. Since rational number is dense in $\R$, we can take any rational number $y_0 \in (\sup Ly , \inf Ry)$ then the map $y\mapsto y_0$ is an injection. Thus, rationals are countable implies that $S$ is countable.

Now consider the function $g_m(x) = \sum^m_{n=1} 2^{-n}f_n(x)$, then by fundamental axiom, $\forall x\in [0,1]$, the sequence $\{g_m(x)\}$ is non-decreasing and bounded above by $f(1)=\sum^\infty_{n=1}2^{-n} = 1$. So the sequence converges and the series converges for every $x\in [0,1]$.

If $x_1\leq x_2$, we have $f_n(x_1)\leq f_n(x_2)$, then
\be
f(x_1) = \sum^\infty_{n=1}2^{-n}f_n(x_1) \leq \sum^\infty_{n=1}2^{-n}f_n(x_2) = f(x_2).
\ee

Thus, $f$ is increasing and therefore integrable (since it is monotonic). 

Consider $y<x_n$, then
\be
f(x_n)-f(y) = \sum^\infty_{m=1}2^{-m} \bb{f_m(x_n)-f_m(y)} = \sum_{m\neq n} 2^{-m}\underbrace{\bb{f_m(x_n)-f_m(y)}}_{\geq 0\text{ since increasing}} + 2^{-n}\bb{\underbrace{f_n(x_n)}_{=1}-\underbrace{f_n(y)}_{=0}} = 2^{-n} >0
\ee

Thus, $\forall \delta >0$ s.t. $\abs{y-x_n}<\delta$, $\exists \ve= 2^{-n}$ s.t. $\abs{f(y)-f(x_n)}\geq \ve$, which means that $f$ is discontinuous at $x_n$.
\end{solution}

\begin{problem}
Let $f(x) = \log(1-x^2)$. Use the mean value theorem to show that $|f(x)| \leq 8x^2/3$ for $0 \leq x \leq 1/2$. Now let 
\be
I_n = \int^{n+1/2}_{n-1/2} \log x dx - \log n
\ee
for $n \in \N$. Show that 
\be
I_n = \int^{1/2}_0 f(t/n) dt
\ee
and hence that $|I_n| < 1/9n^2$. By considering $\sum^n_{j=1} I_j$, deduce that $n!/n^{n+1/2}e^{-n} \to l$ for some constant $l$.

[The bounds $8x^2/3$ and $1/9n^2$ are not best possible; they are merely good enough for the conclusion.]
\end{problem}

\begin{solution}[\bf Solution.]
First we know that $g(x) = \log x$ and $h(x) = 1-x^2$ are differentiable on $\left[0,\frac 12\right]$, then we have $f(x)$ is differentiable. Thus, $\exists c \in \bb{0,x}$ s.t.
\be
f(x)-f(0) = f'(c)(x-0) \ \ra \ \frac 1x f(x) = \frac {-2c}{1-c^2} \ \ra \ \abs{f(x)} = \abs{\frac{2cx}{1-c^2}} \leq \abs{\frac{2cx}{1-\bb{\frac12}^2}} \leq \abs{\frac{2x^2}{\frac 34}} = \frac 83 x^2.  
\ee

Now consider $I_n$,
\beast
I_n & = & \int^{n+1/2}_{n-1/2} \log x dx - \log n = \int^{n+1/2}_{n-1/2} \log \frac xn dx \\
& = & \int^{n+1/2}_n \log \frac xn dx + \int^n_{n-1/2} \log \frac xn dx = \int^{1/2}_0 \log \bb{1+\frac tn} dt + \int^{1/2}_0 \log \bb{1-\frac tn} dt\\
& = & \int^{1/2}_0 \log \bb{1-\frac {t^2}{n^2}} dt = \int^{1/2}_0 f(t/n) dt.
\eeast

Thus,
\be
\abs{I_n} = \abs{\int^{1/2}_0 f\bb{\frac tn} dt} \leq \frac 83 \abs{\int^{1/2}_0  \bb{\frac tn}^2 dt} = \frac 1{9n^2}.
\ee

Therefore, we have $\sum^n_{j=1} I_j$ is absolutely convergent and 
\beast
\sum^n_{j=1} I_j & = & \sum^n_{j=1}\bb{\int^{j+1/2}_{j-1/2} \log x dx - \log j} = \int^{n+1/2}_{1/2}\log xdx - \log n!\\
& = & \left. x\log x\right|^{n+1/2}_{1/2} - n -\log n! = \bb{n+ \frac 12}\log \bb{n+ \frac 12} - \frac 12 \log \frac 12 -n - \log n!\\
& = & \log \bb{\frac{\sqrt{2}\bb{n+ \frac 12}^{n+ \frac 12}}{e^n n!}} \to c \ \ra \ \frac{e^n n!}{\sqrt{2}\bb{n+ \frac 12}^{n+ \frac 12}} \to e^{-c}.
\eeast

Furthermore, we have
\be
\frac{e^n n!}{\sqrt{2}\bb{n+ \frac 12}^{n+ \frac 12}}= \frac{e^n n!}{\sqrt{2}\bb{n\bb{1+ \frac 1{2n}}}^{n+ \frac 12}} \to \frac{e^n n!}{\sqrt{2}n^{n+ \frac 12}e^{\frac 12}} \ \ra \ \frac{e^n n!}{n^{n+ \frac 12}} \to \sqrt{2}e^{\frac 12}e^{-c} = l.
\ee
\end{solution}

\begin{problem}
Let 
\be 
I_n = \int^{\pi/2}_0 \cos^n x dx.
\ee
Prove that $nI_n = (n - 1)I_{n-2}$, and hence $\frac{2n}{2n+1}\leq I_{2n+1}/I_{2n} \leq 1$. Deduce Wallis’s Product:
\be
\frac {\pi}2 = \lim_{n\to\infty}\frac{2 \cdot 2 \cdot 4 \cdot 4 \dots 2n \cdot 2n}{1 \cdot 3 \cdot 3 \cdot 5 \dots (2n - 1) \cdot (2n + 1)} = \lim_{n\to\infty} \frac{2^{4n}}{2n + 1} \binom{2n}{n}^{-2}.
\ee
By taking note of the previous exercise, prove that 
\be
n!/n^{n+1/2}e^{-n} \to \sqrt{2\pi}\quad\quad \text{(Stirling's formula)}.
\ee
\end{problem}

\begin{solution}[\bf Solution.]
We have
\beast
I_n & = & \int^{\pi/2}_0 \cos^n x dx =  \int^{\pi/2}_0 (1-\sin^2x)\cos^{n-2} x dx \\
& = & I_{n-2} + \int^{\pi/2}_0 \sin x\cos^{n-2} x d\cos x = I_{n-2} + \frac 1{n-1} \int^{\pi/2}_0 \sin x d \cos^{n-1} x\\
& = & I_{n-2} + \frac 1{n-1} \bb{\left. \sin x \cos^{n-1} x \right|^{\pi/2}_0 - \int^{\pi/2}_0  \cos^{n-1} x d\sin x}\\
& = & I_{n-2} - \frac 1{n-1} I_n \ \ra \ nI_n = (n - 1)I_{n-2}.
\eeast

Also,

\be
I_{2n+1} = \int^{\pi/2}_0 \cos^{2n+1} x dx \leq \int^{\pi/2}_0 \cos^{2n} x dx = I_{2n} \ \ra \ \frac{I_{2n+1}}{I_{2n}}\leq 1,
\ee
\be
\frac{I_{2n+1}}{I_{2n}} = \frac{2n I_{2n-1}}{(2n+1)I_{2n}} \geq \frac{2n }{2n+1} \cdot 1 = \frac{2n }{2n+1}.
\ee
So 
\be
\frac{I_{2n+1}}{I_{2n}} \to 1\quad \text{as }n\to \infty.
\ee

Thus,
\be
\lim_{n\to\infty}\frac{2 \cdot 2 \cdot 4 \cdot 4 \dots 2n \cdot 2n}{1 \cdot 3 \cdot 3 \cdot 5 \dots (2n - 1) \cdot (2n + 1)} = \lim_{n\to \infty} \frac{I_0}{I_2} \frac{I_3}{I_1} \dots \frac{I_{2n-2}}{I_{2n}} \frac{I_{2n+1}}{I_{2n-1}} = \lim_{n\to \infty} \frac{I_0I_{2n+1}}{I_1I_{2n}} \to \frac{I_0}{I_1} = \frac {\pi/2}{1} =  \frac {\pi}2.
\ee

Using the previous results, we have
\be
l = \lim_{n\to\infty} \frac{e^n n!}{n^{n+ \frac 12}} = \lim_{n\to\infty}\frac{e^{2n} (2n)!}{(2n)^{2n+ \frac 12}}
\ee
Thus,
\be
l = \frac{l^2}l = \left.\lim_{n\to\infty} \bb{\frac{e^n n!}{n^{n+ \frac 12}}}^2 \right/\lim_{n\to\infty}\frac{e^{2n} (2n)!}{(2n)^{2n+ \frac 12}} = \lim_{n\to \infty}\frac{2^{2n+\frac 12} (n!)^2}{n^{\frac 12}(2n)!}.
\ee

Then
\be
l^2 = \lim_{n\to \infty}\bb{\frac{2^{2n+\frac 12} (n!)^2}{n^{\frac 12}(2n)!}}^2 = \lim_{n\to \infty}\frac{2^{4n+1} (n!)^4}{n((2n)!)^2} = = \lim_{n\to\infty} \frac{2^{4n}}{2n + 1} \binom{2n}{n}^{-2} \lim_{n\to\infty} \frac{2(2n + 1)}n = \frac {\pi}2 \cdot 4 = 2\pi.
\ee

Thus, we have $l = \sqrt{2\pi}$.
\end{solution}

\begin{problem}
Do these improper integrals converge? 
\be
\text{(i)} \quad \int^\infty_1 \sin^2(1/x)dx,\quad\quad \text{(ii)}\quad \int^\infty_0 x^p \exp(-x^q)dx \quad \text{where }p, q > 0.
\ee
\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] We know that $\forall t\geq 0$, $\sin t \leq t$ (which can be checked by Taylor expansion), 
\be
\sin t = t+ \bb{-\frac {z^3}{3!} + \frac {z^5}{5!}} + \bb{-\frac {z^7}{7!} + \frac {z^9}{9!}} + \dots
\ee

For $t\geq 1$, the inequality is obvious, so for $t<1$, we have each term in the bracket,
\be
-\frac{t^{4n-1}}{(4n-1)!} + \frac{t^{4n+1}}{(4n+1)!} = \frac{t^{4n-1}}{(4n-1)!}\bb{-1 + \frac{t^2}{4n(4n+1)}} <0,\quad\quad n=1,2,\dots
\ee
So
\be
\int^\infty_1 \sin^2(1/x)dx \leq \int^\infty_1 \bb{\frac 1x}^2dx = -\int^\infty_1 d\frac 1x = 1.
\ee
Thus, the integral converges.

\item [(ii)] We have $x^{p+2} \leq \exp(x^q)$ for some $x\geq n$. Thus,
\be
\int^\infty_0 x^p \exp(-x^q)dx = \int^n_0 x^p \exp(-x^q)dx + \int^\infty_n x^p \exp(-x^q)dx \leq \int^n_0 x^p \exp(-x^q)dx + \int^\infty_n \frac 1{x^2}dx
\ee
So the integral converges.
\een
\end{solution}

\begin{problem}
Show that 
\be
\frac 1{n+1} + \frac 1{n+2} + \dots + \frac 1{2n} \to \log 2 \quad \text{as }n \to \infty,
\ee
and find 
\be
\lim_{n\to\infty} \frac 1{n+1} - \frac 1{n+2} + \dots + \frac{(-1)^{n-1}}{2n}.
\ee
\end{problem}

\begin{solution}[\bf Solution.]
Let  $g(x) =  \frac 1x$ on $[1,2]$, $\int^2_1 f = \log 2$. We choose partition
\be
D = \left\{1+\frac in:\ 0\leq i\leq n\right\} \ \ra \ L(g,D) = \sum^n_{i=1} \frac 1{1+ \frac in} \cdot \frac 1n = \sum^n_{i=1} \frac 1{n+i}.
\ee

Let $f(x) =  \frac 1{n+x}$, $f$ is decreasing and $\forall x\in [n-1,n]$,
\be
f(n-1) \geq f(x) \geq f(n) \ \ra \ f(n-1) \geq \int^n_{n-1} f(x)dx \geq f(n) \ \ra \ \sum^{n-1}_{i=1} f(i) \geq \int^n_1 f(x)dx \geq \sum^n_{i=2} f(i).
\ee

Thus, we have
\be
\frac 1{n+1} + \dots + \frac 1{2n-1} \geq \log \bb{\frac{2n}{n+1}} \geq \frac 1{n+2} + \dots + \frac 1{2n}.
\ee

Let $S_n =\frac 1{n+1} + \dots + \frac 1{2n-1}$, $T_n = \frac 1{n+2} + \dots + \frac 1{2n}$. We have $\forall \ve > 0$, $\exists N\geq 0$ s.t. $\abs{S_n - T_n} < \ve$ as $n\geq N$. Thus,
\be
S_n =\frac 1{n+1} + \dots + \frac 1{2n-1} \to \log 2 \ \ra \ \frac 1{n+1} + \frac 1{n+2} + \dots + \frac 1{2n} \to \log 2.
\ee

Alternatively, we know 
\be
1 + \frac 12 + \dots + \frac 1n - \log n \to \gamma \quad \text{as }n \to \infty, \quad\quad 1 + \frac 12 + \dots + \frac 1{2n} - \log 2n \to \gamma \quad \text{as }n \to \infty, 
\ee
So
\be
\frac 1{n+1} + \frac 1{n+2} + \dots + \frac 1{2n} - \log 2n + \log n \to 0 \quad \text{as }n \to \infty \ \ra \ \frac 1{n+1} + \frac 1{n+2} + \dots + \frac 1{2n} \to \log 2.
\ee

For $\lim_{n\to\infty} \frac 1{n+1} - \frac 1{n+2} + \dots + \frac{(-1)^{n-1}}{2n}$, if $n$ be even
\beast
\lim_{n\to\infty} \frac 1{n+1} - \frac 1{n+2} + \dots + \frac{(-1)^{n-1}}{2n} & = & \lim_{n\to\infty} \frac 1{(n+1)(n+2)} + \dots + \frac 1{(2n-1)2n} \\
& \leq & \frac 12 \lim_{n\to\infty} \frac n{(n+1)(n+2)} = 0.
\eeast

For $n$ odd it's similar. Alternatively, we let
\be
S_n = \frac 1{n+1} - \frac 1{n+2} + \dots + \frac{(-1)^{n-1}}{2n},\quad\quad T_n = \frac 1{n} - \frac 1{n+1} + \dots + \frac 1{2n} \ \ra \ T_{2n}-T_n = -\frac 1{2n} + S_{2n}
\ee

We know $T_n \to \log 2$, so $S_{2n} \to 0$, also $S_{2n+1} \to 0$, thus $S_n \to 0$.
\end{solution}

\begin{problem}
Let $f : [a, b] \to \R$ be continuous and suppose that $\int^b_a f(x)g(x) dx = 0$ for every continuous function $g : [a, b] \to \R$ with $g(a) = g(b) = 0$. Must $f$ vanish identically?
\end{problem}

\begin{solution}[\bf Solution.]
Let $g(x) = f(x)(x-a)^2(x-b)^2$. Then $fg \geq 0$ and $\int^b_a fg = 0$. Since $fg$ is continuous, we use the previous result (question \ref{ques:vanish_identically}) and conclude that $fg = 0$ on $(a,b)$. Thus $f=0$ on $(a,b)$ and $f=0$ at $a$ and $b$ by continuity.
\end{solution}

\begin{problem}
Suppose that $f :\ \R \mapsto \R$ has a continuous derivative, $f(0) = 0$ and $|f'(x)| \leq M$ for $x \in [0, 1]$. Show that $\abs{\int^1_0 f} \leq M/2$. Show that if, in addition, $f(1) = 0$ then $\abs{\int^1_0 f} \leq M/4$. What could you say if $\abs{f'(x)} \leq Mx$?
\end{problem}

\begin{solution}[\bf Solution.]
By the definition, we have
\be
\abs{f(x)} = \abs{f(x)-f(0)} = \abs{\int^x_0 f'(t)dt} \leq \abs{\int^x_0 M dt} = Mx.
\ee

Thus,
\be
\abs{\int^1_0f}  \leq \int^1_0 \abs{f} \leq \int^1_0 Mt dt = \frac M2.
\ee

Additionally, if $f(1)=0$, we have $x\geq \frac 12$,
\be
\abs{f(x)} = \abs{f(1)-f(x)} = \abs{\int^1_x f'(t)dt} \leq \int^1_x \abs{f'(t)}dt \leq \int^1_x M dt = M(1-x).
\ee

Then,
\beast
\abs{\int^1_0f} = \abs{\int^{\frac 12}_0 f + \int^1_{\frac 12} f } \leq \int^{\frac 12}_0 \abs{f} + \int^1_{\frac 12} \abs{f} \leq \int^{\frac 12}_0 Mt dt + \int^1_{\frac 12} M(1-t) dt = \frac M8 + \frac M8 = \frac M4.
\eeast

If $\abs{f'(x)} \leq Mx$,
\be
\abs{f(x)} = \abs{f(x)-f(0)} = \abs{\int^x_0 f'(t)dt} \leq \abs{\int^x_0 Mt dt} = \frac 12 Mx^2.
\ee

Thus,
\be
\abs{\int^1_0f}  \leq \int^1_0 \abs{f} \leq \frac 12 \int^1_0 Mt^2 dt = \frac M6.
\ee

Additionally, if $f(1)=0$, we have $x\geq \frac 12$,
\be
\abs{f(x)} = \abs{f(1)-f(x)} = \abs{\int^1_x f'(t)dt} \leq \int^1_x \abs{f'(t)}dt \leq \int^1_x Mt dt = \frac 12 M(1-x^2).
\ee

Then,
\beast
\abs{\int^1_0f} = \abs{\int^{\frac 12}_0 f + \int^1_{\frac 12} f } \leq \int^{\frac 12}_0 \abs{f} + \int^1_{\frac 12} \abs{f} \leq \frac 12\int^{\frac 12}_0 Mt^2 dt + \frac 12 \int^1_{\frac 12} M(1-t^2) dt = \frac M{48} + \frac {5M}{48} = \frac M8.
\eeast
\end{solution}

\begin{problem}
Let $f : [0, 1] \to \R$ be continuous. Let $G(x, t) = t(x - 1)$ for $t \leq x$ and $G(x, t) = x(t - 1)$ for $t \geq x$. Let $g(x) = \int^1_0 f(t)G(x, t)dt$. Show that $g''(x)$ exists for $x \in (0, 1)$ and equals $f(x)$.
\end{problem}

\begin{solution}[\bf Solution.]
First, we have
\beast
g(x) & = & \int^1_0 f(t)G(x,t)dt = \int^x_0 f(t)G(x,t)dt + \int^1_x f(t)G(x,t)dt = (x-1) \int^x_0 tf(t)dt + x\int^1_x (t-1)f(t)dt.
\eeast
Thus,
\be
g'(x) = \int^x_0 tf(t)dt + (x-1)xf(x) + \int^1_x (t-1)f(t)dt - x(x-1)f(x) = \int^x_0 tf(t)dt + \int^1_x (t-1)f(t)dt,
\ee
\be
g''(x) = xf(x) - (x-1)f(x) = f(x).
\ee
\end{solution}

\begin{problem}
Let 
\be
I_n(\theta) = \int^1_{-1} (1 - x^2)^n \cos(\theta x)dx.
\ee

Prove that 
\be
\theta^2 I_n = 2n(2n - 1)I_{n-1} - 4n(n - 1)I_{n-2} 
\ee
for $n \geq 2$, and hence that $\theta^{2n+1}I_n(\theta) = n!(P_n(\theta) \sin \theta +Q_n(\theta) \cos \theta)$, where $P_n$ and $Q_n$ are polynomials
of degree at most $2n$ with integer coefficients. Deduce that $\pi$ is irrational.
\end{problem}

\begin{solution}[\bf Solution.]
With the definition, we have
\be
I_n(\theta) = \int^1_{-1} (1 - x^2)^n \cos(\theta x)dx = \frac 1{\theta} \underbrace{\sin \theta x (1-x^2)^n|^1_{-1}}_{=0} + \frac n{\theta} \int^1_{-1}\sin \theta x (1-x^2)^{n-1}2x dx.
\ee

Then
\beast
\theta^2 I_n & = & 2\theta n\int^1_{-1}\sin \theta x (1-x^2)^{n-1}2x dx \\
& = & -2n\cos \theta x (1-x^2)^{n-1}|^1_{-1} + 2n\int^1_{-1}\cos \theta x \bb{(1-x^2)^{n-1}+ x(1-x^2)^{n-2}(n-1)(-2x)}dx\\
& = & 2n\int^1_{-1} (1-x^2)^{n-1}\cos \theta x dx - 4n(n-1) \int^1_{-1} \cos \theta x (1-x^2)^{n-2}x^2dx\\
& = & 2nI_{n-1} - 4n(n-1)\bb{I_{n-2}-I_{n-1}} = 2n(2n-1)I_{n-1} - 4n(n-1)I_{n-2}.
\eeast

For $\theta^{2n+1}I_n(\theta) = n!(P_n(\theta) \sin \theta +Q_n(\theta) \cos \theta)$, we show by induction,
\be
n=0:\ I_0 = \int^1_{-1}\cos \theta x dx = \frac 2{\theta}\sin\theta \ \ra \ \theta I_0 = 0!\bb{2\sin\theta + 0 \cos \theta}, \quad P_0(\theta) = 2,\ Q_0(\theta) = 0.
\ee
\beast
n=1:\ I_1 & = & \int^1_{-1}(1-x^2)\cos \theta x dx = \frac 1{\theta}\bb{(1-x^2)\sin\theta x|^1_{-1} - \int^1_{-1} \sin\theta x (-2x)dx }\\
& = & \frac 2{\theta} \int^1_{-1} \sin\theta x x dx = \frac 2{\theta^2} \bb{ - x \cos\theta x|^1_{-1} + \int^1_{-1} \cos\theta x dx}\\
& = & \frac 4{\theta^3} \sin\theta - \frac 4{\theta^2} \cos \theta \ \ra \ \theta^3 I_1 = 1!\bb{4\sin\theta - 4\theta \cos \theta},\quad P_1(\theta) = 4,\ Q_0(\theta) = -4\theta.
\eeast

Now if it holds for $I_k$ and $I_{k+1}$, then it is true for $I_{k+2}$,
\beast
& & I_k: \ \theta^{2k+1} I_k(\theta) = k! \bb{P_k(\theta)\sin\theta + Q_k(\theta)\cos\theta},\\
& & I_{k+1}: \ \theta^{2k+3} I_{k+1}(\theta) = (k+1)! \bb{P_{k+1}(\theta)\sin\theta + Q_{k+1}(\theta)\cos\theta},
\eeast
and 
\beast
I_{k+2}: \quad & & \theta^{2k+5} I_{k+2}(\theta) = \theta^{2k+3} \bb{\theta^2 I_{k+2}(\theta)}\\
& = & \theta^{2k+3} \bb{2(k+2)(2k+3)I_{k+1}(\theta) - 4(k+2)(k+1)I_k(\theta)}\\
& = & 2(k+2)(k+1)! \bb{(2k+3)P_{k+1}(\theta)\sin\theta + Q_{k+1}(\theta)\cos\theta} - 4\theta^2 (k+2)(k+1)k! \bb{P_k(\theta)\sin\theta + Q_k(\theta)\cos\theta}\\
& = & (k+2)! \bb{\bb{(4k+6) P_{k+1}(\theta)\sin\theta + (4k+6) Q_{k+1}(\theta)\cos\theta} - 4\theta^2 \bb{P_k(\theta)\sin\theta + Q_k(\theta)\cos\theta}}\\
& = & (k+2)! \bb{[(4k+6) P_{k+1}(\theta) - 4\theta^2 P_k(\theta)]\sin\theta  + [(4k+6) Q_{k+1}(\theta) - 4\theta^2 Q_k(\theta)]\cos\theta}
\eeast
as required.

If $\pi$ is rational, write $\pi = \frac ab$ and let $\theta = \frac {\pi}2 = \frac a{2b}$, then
\be
\bb{\frac{a}{2b}}^{2n+1} I_n\bb{\frac {\pi}2} = n! P_n\bb{\frac{a}{2b}} \ \ra \ \frac{a^{2n+1}}{n!} I_n\bb{\frac {\pi}2} = (2b)^{2n+1}P_n\bb{\frac{a}{2b}}.
\ee

$P_n\bb{\frac{a}{2b}}$ has at most order $2n$ so we have
\be
(2b)^{2n+1}P_n\bb{\frac{a}{2b}} \in \Z \ \ra \ \frac{a^{2n+1}}{n!} I_n\bb{\frac {\pi}2} \in \Z.
\ee

However, $a$ is fixed and $\exists n$ s.t. 
\be
\frac{a^{2n+1}}{n!} < \frac 12
\ee
and 
\be
I_n\bb{\frac {\pi}2} = \int^1_{-1} (1-x^2)^n \cos \frac{\pi x}2 dx \leq \int^1_{-1} dx = 2 \ \ra \ \frac{a^{2n+1}}{n!} I_n\bb{\frac {\pi}2} < 1
\ee

It is obvious that $\frac{a^{2n+1}}{n!} I_n\bb{\frac {\pi}2} > 0$, so this contradiction implies that $\pi$ is an irrational number.
\end{solution}

\begin{problem}
Let $f_1, f_2 : [-1, 1] \to \R$ be increasing and $g = f_1 - f_2$. Show that there exists $K$ such that, for any dissection $D = x_0 < \dots < x_n$ of $[-1, 1]$, 
\be
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| \leq K.
\ee

Now let $g(x) = x \sin(1/x)$ for $x \neq 0$ and $g(0) = 0$. Show that $g$ is integrable but is not the difference of two increasing functions.
\end{problem}

\begin{solution}[\bf Solution.]
For $g = f_1 - f_2$, 
\beast
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| & = & \sum^n_{j=1} \abs{f_1(x_j)-f_2(x_j)-f_1(x_{j-1})+f_2(x_{j-1})} \leq \sum^n_{j=1} \abs{f_1(x_j)-f_1(x_{j-1})}+ \abs{f_2(x_j) - f_2(x_{j-1})} \\
& \leq & \sum^n_{j=1} f_1(x_j)-f_1(x_{j-1}) + f_2(x_j) - f_2(x_{j-1})\quad\quad \text{since $f_1,f_2$ increasing}\\
& = & f_1(x_n)-f_1(x_0) + f_2(x_n) - f_2(x_0) = K.
\eeast

Since $g(x) =x \sin(1/x)$, it is continuous and thus integrable. Now we want to prove it is not the difference of two increasing functions. It suffices to show that there exist a partition $D = x_0 < \dots < x_n$ of $[-1, 1]$ s.t. 
\be
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| \to \infty.
\ee

Since $g$ is an even function, we only consider the interval $[-1,0]$. We choose the partition $D$ of $[-1,0]$ s.t.
\be
x_0= -1,\quad x_n = -\frac 2{n\pi},\ n\geq 1.
\ee

Thus,
\be
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| = \abs{\sin 1 - \frac 2{\pi}} + \sum^n_{j=2} |g(x_j)-g(x_{j-1})|
\ee

If $n$ is even,
\be
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| = \sin 1 - \frac 4{\pi} + \frac 4{\pi}\sum^{n-1}_{j \text{ odd}}\frac 1j
\ee

If $n$ is odd,
\be
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| = \sin 1 - \frac 4{\pi} + \frac 4{\pi}\sum^{n-2}_{j \text{ odd}}\frac 1j + \frac 2{n\pi}
\ee

Since
\be
\sum^\infty_{j \text{ odd}}\frac 1j = \frac 12 \sum^\infty_{j \text{ odd}}\frac 2j \geq \frac 12 \sum^\infty_{j \text{ odd}}\bb{\frac 1j+ \frac 1{j+1}} = \frac 12 \sum^\infty_{j=1}\frac 1j
\ee
which is divergent. Thus, $\sum^n_{j=1} |g(x_j)-g(x_{j-1})|$ is unbounded as $n\to \infty$, as required.
\end{solution}

\begin{problem}
Show that if $f : [0, 1] \to \R$ is integrable then $f$ has infinitely many points of continuity.
\end{problem}

\begin{solution}[\bf Solution.]
\end{solution}


\section{Analysis II exercise}


\begin{problem}
Which of the following sequences of functions converge uniformly on $X$?
\ben
\item [(a)] $f_n(x) = x^n$ on $X = \bb{0, \frac 12}$;
\item [(b)] $f_n(x) = \sin(n^2x)/ \log n$ on $X = \R$;
\item [(c)] $f_n(x) = x^n$ on $X = (0, 1)$;
\item [(d)] $f_n(x) = x^n - x^{2n}$ on $X = [0, 1]$;
\item [(e)] $f_n(x) = xe^{-nx}$ on $X = [0,\infty)$;
\item [(f)] $f_n(x) = e^{-x^2}\sin(x/n)$ on $X = \R$.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(a)] Yes. $\forall \ve >0$, take $N>\log_2\frac 1{\ve}$, then $\forall n\geq N$, $\forall x \in \bb{0,\tfrac 12}$, we have $f(x)=0$
\be
\abs{f_n(x)-f(x)} = \abs{x^n-0} = x^n < \frac 1{2^n} < \frac 1{2^N} < \ve.
\ee

\item [(b)] Yes. $\forall \ve >0$, take $N>e^{\frac 1{\ve}}$, then $\forall n\geq N$, $\forall x \in \R$, we have $f(x)=0$
\be
\abs{f_n(x)-f(x)} = \abs{\frac{\sin(n^2x)}{\log n}-0} \leq \frac 1{\log n} < \frac 1{\log N} < \ve.
\ee

\item [(c)] No. $\forall x\in(0,1)$, $\lim_{n\to\infty}f_n(x) = 0$, but $\forall n\in \N$, 
\be
f_n\bb{2^{-\frac 1n}} =\frac 12,
\ee
so the convergence is not uniform.

\item [(d)] No. $\forall x\in(0,1)$, $\lim_{n\to\infty}f_n(x) = 0$, but $\forall n\in \N$, 
\be
f_n\bb{2^{-\frac 1n}} =\frac 14,
\ee
so the convergence is not uniform. 

\item [(e)] Yes. Each $f_n$ is differentiable, with 
\be
f_n'(x) = e^{-nx} - nxe^{-nx} = (1-nx)e^{-nx},
\ee
so the maximum value occurs at $\frac 1n$ and $f_n\bb{\tfrac 1n} = \tfrac 1{ne}$, thus, the maximum value tends to 0 as $n\to \infty$.

\item [(f)] Yes. $\forall \ve >0$, choose $X>0$ such that $e^{-X^2}<\ve$, and take $N> \tfrac X{\ve}$, then $\forall n\geq N$, $\forall x \in \R$, if $\abs{x} > X$ we have 
\be
\abs{f_n(x)} = \abs{e^{-x^2}\sin(x/n)} \leq \abs{e^{-x^2}} < \abs{e^{-X^2}} < \ve,
\ee
while if $\abs{x} \leq X$, we have
\be
\abs{f_n(x)} = \abs{e^{-x^2}\sin(x/n)} \leq \abs{\sin \frac xn} < \sin \frac Xn < \frac Xn \leq \frac XN < \ve.
\ee
\een
\end{solution}


\begin{problem}
Suppose that $f : [0, 1] \mapsto \R$ is continuous. Show that the sequence $(x^nf(x))$ is uniformly convergent on $[0, 1]$ if and only if $f(1) = 0$.
\end{problem}

\begin{solution}[\bf Solution.]
"$\Longrightarrow$". Since a uniform limit of continuous functions is continuous, and the limit as $n\to \infty$ of $x^nf(x)$ is 0 if $x<1$ and $f(1)$ if $x=1$, for $x^n f(x)$ to converge uniformly requires $f(1)=0$.

"$\Longleftarrow$". Conversely, suppose $f(1)=0$, as $f$ is continuous on a closed bounded interval, it is bounded. So $\exists M$ s.t. $\abs{f(x)} \leq M$ for all $x\in [0,1]$. Given $\ve >0$, $\exists\delta>0$ s.t.
\be
\abs{x-1}<\delta, \ \abs{f(x)-f(1)} = \abs{f(x)} < \ve \quad(\text{definition of continuity at 1}).
\ee

Choose $N$ with 
\be
\bb{1-\frac 12 \delta}^N < \frac {\ve}M,
\ee

then $\forall n\geq N$, $\forall x\in [0,1]$, if $x< 1 -\tfrac 12 \delta $ we have
\be
\abs{x^nf(x)} < \bb{1-\frac 12 \delta}^nM \leq \bb{1-\frac 12 \delta}^NM < \ve,
\ee
while if $x\geq 1 -\tfrac 12 \delta$ we have
\be
\abs{x^nf(x)} \leq \abs{f(x)} < \ve.
\ee
\end{solution}


\begin{problem}\label{ques:x^2_uniform_continuous} 
Let $f$ and $g$ be uniformly continuous real-valued functions on a set $E \subseteq \R$. Show that the pointwise sum $f + g$ is uniformly continuous on $E$, and so is $\lm f$ for each real constant $\lm$. Give an example showing that the (pointwise) product $fg$ need not be uniformly continuous on $E$. Is
it possible to find such an example with $f$ bounded?
\end{problem}

\begin{solution}[\bf Solution.]
Since $f$ ang $g$ are uniformly continuous, $\forall \ve >0$, then $\exists \delta_1>0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta_1$,
\be
\abs{f(x)-f(y)} < \frac {\ve}2,
\ee
$\exists \delta_2>0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta_2$,
\be
\abs{g(x)-g(y)} < \frac {\ve}2.
\ee

Take $\delta = \min\{\delta_1,\delta_2\}$, we have $\forall \ve>0$, $\exists \delta>0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta$,
\be
\abs{f(x)+g(x)-f(y)-g(y)} \leq \abs{f(x)-f(y)} + \abs{g(x)-g(y)} < \frac {\ve}2 + \frac {\ve}2 = \ve.
\ee

so $f+g$ is uniformly continuous.

Now given $\lm \in \R$, if $\lm = 0$ the result is trivial, so assume $\lm \neq 0$. $\forall \ve >0$, $\exists \delta >0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta$,
\be
\abs{f(x)-f(y)} < \frac {\ve}{\abs{\lm}} \ \ra \ \abs{(\lm f)(x)-(\lm f)(y)} = \abs{\lm} \cdot \abs{f(x)-f(y)} < \abs{\lm}\cdot \frac {\ve}{\abs{\lm}} = \ve.
\ee
so $\lm f$ is uniformly continuous.

Let $E = \R$ and $f(x)=g(x)=x$, then $f$ and $g$ are both uniformly continuous (given $\ve>0$, we may let $\delta = \ve$), but we have
\be
fg(x) = x^2,\quad x\in \R.
\ee

Take $\ve = 1$, then $\forall \delta >0$, $\exists x > \frac 1{\delta}$ and set $y = x+ \frac {\delta}2$, then $\abs{x-y} = \frac {\delta}2 < \delta$ but
\be
\abs{fg(x)-fg(y)} = \abs{x^2 - \bb{x+\tfrac {\delta}2}^2} = \delta \abs{x+\tfrac{\delta}4} > \delta \abs{x} > \delta \frac 1{\delta} = 1 = \ve.
\ee
so $fg$ need not be uniformly continuous.

Now consider $E\in \R$ and $f(x)=\sin x$, $g(x) = x$ (note that $f$ is periodic with period $2\pi$, and on $[0,2\pi]$ (closed and bounded), it is continuous and hence uniformly continuous). Take $\ve =1$, then $\forall \delta >0$, $\exists$
\be
n > \frac 1{2\pi \abs{\sin \frac{\delta}2}},\ n \in \N, \quad x = 2n\pi,\quad y = 2n\pi + \tfrac {\delta}2,
\ee
then $\abs{x-y}=\frac{\delta}2 < \delta$ but
\beast
\abs{fg(x)-fg(y)} & = & \abs{x\sin x - \bb{x+\tfrac {\delta}2}\sin \bb{x+\tfrac {\delta}2}} = \abs{\bb{x+\tfrac {\delta}2}\sin \bb{x+\tfrac {\delta}2}} \\
& = & \abs{\bb{2n\pi +\tfrac {\delta}2}\sin \bb{2n\pi+\tfrac {\delta}2}} = \abs{\bb{2n\pi +\tfrac {\delta}2}\sin \tfrac {\delta}2} = \abs{\bb{2n\pi +\tfrac {\delta}2}}\abs{\sin \tfrac {\delta}2} > \delta \abs{x} > 1 = \ve.
\eeast
so $fg$ need nto be uniformly continuous if $f$ is bounded but $g$ is not.
\end{solution}

\begin{problem}
Let $(f_n)$ be a sequence of continuous real-valued functions on a closed, bounded interval $[a, b]$, and suppose that $f_n$ converges pointwise to a continuous function $f$.

Show that if $f_n \to f$ uniformly on $[a, b]$ and $(x_n)$ is a sequence of points in $[a, b]$ with $x_n \to x$, then $f_n(x_n) \to f(x)$. [Careful -- this is not quite as easy as it looks!]

On the other hand, show that if $f_n$ does not converge uniformly to $f$, then we can find a convergent sequence $x_m \to x$ in $[a, b]$ such that $f_n(x_n)$ does not converge to $f(x)$. 
\end{problem}

\begin{solution}[\bf Solution.]
Since $f$ is continuous and $x_n \to x$, we have $f(x_n) \to f(x)$ i.e. $\forall \ve>0$, $\exists N$ s.t. $n\geq N$,
\be
\abs{f(x_n)-f(x)} < \frac {\ve}2.
\ee

Also, if $f_n \to f$ uniformly on $[a, b]$, we have given $\ve>0$, $\exists N'$ s.t. 
\be
\abs{f_n(y) - f(y)} < \frac {\ve}2,
\ee
for all $y\in [a,b]$. Thus, $\forall \ve>0$, we take $n \geq \max\{N,N'\} $,
\be
\abs{f_n(x_n)-f(x)} \leq \abs{f(x_n)-f(x)} + \abs{f_n(x_n) - f(x_n)} < \frac {\ve}2 + \frac {\ve}2 = \ve.
\ee
so $f_n(x_n) \to f(x)$.

If $f_n$ does not converge uniformly to $f$, for each $n\in \N$ the continuous function $f_n-f$ is bounded on $[a,b]$ and attains its bounds, so we take $y_n$ such that $\abs{f_n(y_n) - f(y_n)}$ is maximal. Then
\be
f_n(y_n) - f(y_n) \nrightarrow 0 \quad \text{as }n\to \infty, \quad (\text{otherwise it converges uniformly})
\ee
so $\exists \ve>0$ s.t. $\forall N \in\N$, $\exists n \geq N$ with 
\be
\abs{f_n(y_n) - f(y_n)} > \ve.
\ee

Thus, there exists a sequence $n_1<n_2< \dots$ such that $\forall k \in \N$ we have
\be
\abs{f_{n_k}(y_{n_k}) - f(y_{n_k})} > \ve.
\ee

With Bolzano-Weierstrass Theorem, the bounded sequence $(y_{n_k})$ has a convergent subsequence $(y_{n_{k_i}})$ converging to $x$, say. For $m\in \N$, define
\be
x_m = \left\{\ba{ll}
y_{n_{k_i}} \quad\quad & m= n_{k_i}\\
x & m \neq \{ n_{k_i}:i\in \N\}
\ea\right.
\ee
then $x_m \to x$, so as $f$ is continuous $f(x_m)\to f(x)$, and thus given $\ve>0$, $\exists N'\in \N$ such that $\forall m \geq N'$,
\be
\abs{f(x_m) -f(x)} < \frac{\ve}2.
\ee

Thus, $\forall N$, there exist $\ve>0$ and $N'$, and then $\exists i \in\N$ s.t. $n_{k_i}\geq \max\{N,N'\}$ with
\be
\abs{f_{n_{k_i}}(y_{n_{k_i}}) - f(y_{n_k})} > \ve,\quad\quad \abs{f(y_{n_{k_i}}) -f(x)} < \frac{\ve}2.
\ee

Hence, 
\beast
\abs{f_{n_{k_i}}(y_{n_{k_i}}) - f(x)} = \abs{f_{n_{k_i}}(y_{n_{k_i}}) - f(y_{n_k}) + f(y_{n_k}) - f(x)} \geq \abs{f_n(y_{n_{k_i}}) - f(y_{n_k})} - \abs{f(y_{n_{k_i}}) -f(x)} > \ve - \frac{\ve}2 =  \frac{\ve}2.
\eeast

so we have $f_n(x_n)\nrightarrow f(x)$.
\end{solution}



\begin{problem}
Suppose that $f :\ [0,\infty) \mapsto \to \R$ is continuous and that $f(x)$ tends to a (finite) limit as $x \to \infty$. Is $f$ necessarily uniformly continuous on $[0,\infty)$? Give a proof or a counter-example as appropriate.
\end{problem}

\begin{solution}[\bf Solution.]
Let $f(x)\to \ell$ as $x\to \infty$. Given $\ve>0$, $\exists M >0$ s.t. $x > M$ with
\be
\abs{f(x) - \ell} < \frac{\ve}2.
\ee

On $[0,M+1]$ (closed and bounded) $f$ is continuous and hence uniformly continuous, so $\exists \delta >0$ (we may assume $\delta <1$) such that if $0\leq x<y \leq M+1$ then
\be
\abs{y-x} \ \ra \ \abs{f(y)-f(x)} < \ve.
\ee

If $y>M+1$, then $\abs{y-x}<\delta$ implies $x > M$, thus,
\be
\abs{f(y)-f(x)} \leq \abs{f(y)-\ell} + \abs{f(x)-\ell} < \frac{\ve}2 + \frac{\ve}2 = \ve.
\ee

Thus, $f$ is uniformly continuous on $[0,\infty)$.
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}
Which of the following functions $f$ on $[0,\infty)$ are (a) uniformly continuous, (b) bounded?
\ben
\item [(i)] $f(x) = \sin x^2$;
\item [(ii)] $f(x) = \inf\{\abs{x - n^2}:\ n \in \N\}$;
\item [(iii)] $f(x) = (\sin x^3)/(x + 1)$.
\een
\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] (a) Take $\ve = \frac 12$, then $\forall \delta >0$ (we may assume that $\delta < \frac{\sqrt{\pi}}2$) and choose
\be
N > \frac{\sqrt{\pi}}{4\delta \sqrt{2}},\ N\in \N \quad\quad \delta' = \frac{\sqrt{\pi}}{4N\sqrt{2}}<\delta
\ee

Take $x = N\sqrt{2\pi}$ and $y = x + \delta'$ we have $\abs{x-y}< \delta$, but
\beast
\abs{f(x)-f(y)} & = & \abs{\sin y^2} = \abs{\sin (2\pi N^2 + 2\delta' \sqrt{2\pi} N + \delta'^2)} = \abs{\sin (\tfrac {\pi}2 + \delta'^2)}\\
& > & \sin \tfrac {3\pi}4 = \frac 1{\sqrt{2}} > \frac 12 = \ve. \quad\quad (\text{since $\delta'^2 < \delta^2 < \frac {\pi}4$})
\eeast
So it is not uniformly continuous. (b) It is bounded by $\pm 1$.

\item [(ii)] (a) $f$ is piecewise linear with linear sections of gradient $\pm 1$, so $\forall \ve >0$ we may take $\delta = \ve$ to prove that $f$ is uniformly continuous. (b) $\forall n\in\N$, we have $f(n^2 +n) = n$, which implies that $f$ is unbounded.

\item [(iii)] (a) Since $f:\ [0,\infty) \mapsto \to \R$ is continuous and that $f(x)$ tends to a limit 0 as $x \to \infty$, we can see that it is uniformly continuous by previous question. (b) Obviously, it is bounded by $\pm 1$.
\een
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}
Show that if $(f_n)$ is a sequence of uniformly continuous functions on $\R$, and $f_n \to f$ uniformly on $\R$, then $f$ is uniformly continuous. Give an example of a sequence of uniformly continuous functions $f_n$ on $\R$, such that $f_n$ converges pointwise to a continuous function $f$, but $f$ is not uniformly continuous. [Hint for the last part: choose the limit function $f$ first.]
\end{problem}

\begin{solution}[\bf Solution.]
$\forall \ve >0$, by uniform convergence of $f_n$, we have $\exists N \in \N$ s.t. $\forall n\geq N$, $\forall x \in\R$, we have
\be
\abs{f_n(x)-f(x)} < \frac{\ve}3.
\ee

Then by uniform continuity of $f_N$, given $\ve$, $\exists \delta >0$ s.t. $\abs{x-y}<\delta $ with
\be
\abs{f_N(x) - f_N(y)} < \frac{\ve}3.
\ee

Thus, $\forall \ve>0$, $\exists \delta >0$ s.t. $\abs{x-y}<\delta$ with
\be
\abs{f(x)-f(y)} \leq \abs{f(x)-f_N(x)} + \abs{f_N(x) - f_N(y)} + \abs{f(y)-f_N(y)} < \frac{\ve}3 + \frac{\ve}3 + \frac{\ve}3 = \ve.
\ee

Hence, $f$ is uniform continuous.

Now consider function
\be
f_n(x) = \left\{\ba{ll}
x^2 \quad\quad & \abs{x} \leq n\\
n^2 & \abs{x} >n
\ea\right.,
\ee
then $f_n$ is uniformly continuous on $\R$, as its restriction to $[-n,n]$ is continuous and hence uniformly continuous, and it is constant outside $[-n,n]$. 

Let $f(x)=x^2$, then $f_n \to f$ pointwise, so the limit is continuous but not uniformly continuous by previous question (question \ref{ques:x^2_uniform_continuous}).
\end{solution}

\begin{problem}
Let $f_n(x) = n^\alpha x^n(1 - x)$, where $\alpha$ is a real constant.
\ben
\item [(i)] For which values of $\alpha$ does $f_n(x) \to 0$ pointwise on $[0, 1]$?
\item [(ii)] For which values of $\alpha$ does $f_n(x) \to 0$ uniformly on $[0, 1]$?
\item [(iii)] For which values of $\alpha$ does $\int^1_0 f_n(x)dx \to 0$?
\item [(iv)] For which values of $\alpha$ does $f'_n(x) \to 0$ pointwise on $[0, 1]$?
\item [(v)] For which values of $\alpha$ does $f'_n(x) \to 0$ uniformly on $[0, 1]$?
\een
\end{problem}

\begin{solution}[\bf Solution.]
 \ben
\item [(i)] First $f_n(0)=f_n(1)=0$ for all $n\in\N$. Now for $x\in (0,1)$ we have
\be
\frac{f_{n+1}(x)}{f_n(x)} = \bb{\frac {n+1}n}^\alpha x = \bb{1+\frac 1n}^\alpha x,
\ee
so for large enough $n$ we have 
\be
\frac{f_{n+1}(x)}{f_n(x)} < \sqrt{x} < 1,
\ee
and thus $f_n(x)\to 0$. Thus $f_n(x) \to 0$ pointwise on $[0,1]$ for all $\alpha \in \R$.

\item [(ii)] Since 
\be
f_n'(x) = n^\alpha \bb{nx^{n-1} - (n+1)x^n},
\ee
the maximum value take by $f_n$ occurs at $\frac n{n+1}$ and is 
\be
f_n\bb{\tfrac n{n+1}} = n^\alpha \bb{\frac n{n+1}}^n\bb{1-\frac n{n+1}} = \frac{n^{n+\alpha}}{(n+1)^{n+1}}.
\ee

Thus, if $\alpha <1$ then given $\ve >0$ take $N\in \N$ s.t. $N^{\alpha -1} < \ve$, then we have $\forall n\geq N$ and $\forall x\in [0,1]$
\be
\abs{f_n(x)} \leq \frac{n^{n+\alpha}}{(n+1)^{n+1}} < n^{\alpha -1} < N^{\alpha -1} < \ve.
\ee
as required for uniform convergence. However, if $\alpha \geq 1$ then
\be
f_n\bb{\tfrac n{n+1}} = \frac{n^{n+\alpha}}{(n+1)^{n+1}} \geq \bb{\frac n{n+1}}^{n+1} = \bb{1+ \frac 1n}^{-n} \cdot \frac 1{1+ \frac 1n} \to \frac 1e \quad\text{as }n\to \infty,
\ee
so we do not have uniform convergence. Thus, $f_n \to 0$ uniformly on $[0,1]$ for $\alpha < 1$.

\item [(iii)] We have
\be
\int^1_0 f_n(x)dx = \int^1_0 n^\alpha x^n(1 - x) dx = \frac{n^\alpha}{(n+1)(n+2)},
\ee
so $\int^1_0 f_n(x)dx\to 0$ for $\alpha < 2$.

\item [(iv)] We already have
\be
f_n'(x) = n^\alpha \bb{nx^{n-1} - (n+1)x^n} = n^{\alpha+1} x^{n-1}\bb{1 - \frac {n+1}nx},
\ee
for $x \in [0,1)$, for large enough $n$ we have $\frac{n+1}n x <1$ so that $1-\frac {n+1}nx >0$, and then
\be
0\leq f_n'(x)\leq n^{\alpha+1} x^{n-1}.
\ee

So $f_n'(x) \to 0$ for any $\alpha\in \R$. However, $f_n'(1) = -n^{\alpha}$, so for $f_n'(1) \to 0$ we need $\alpha <0$. Thus $f_n'(x)\to 0$ pointwise on $[0,1]$ for $\alpha <0$.


\item [(v)] The minimum value taken by $f_n'$ is $f_n'(1)=-n^\alpha$, the maximum value occurs when $f_n''(x) = 0$, i.e., 
\be
n^\alpha \bb{n(n-1)x^{n-2} - n(n+1)x^{n-1}} = n^{\alpha+1}(n-1)x^{n-2}\bb{1 - \frac{n+1}{n-1}x} = 0 \ \ra \ x = \frac {n-1}{n+1}.
\ee
So the maximum value is 
\be
f_n'\bb{\tfrac {n-1}{n+1}} = n^{\alpha+1} \bb{\frac {n-1}{n+1}}^{n-1}\bb{1 - \frac {n+1}n \frac {n-1}{n+1}} = n^{\alpha} \bb{\frac {n-1}{n+1}}^{n-1}.
\ee

Thus, if $\alpha <0 $ then given $\ve >0$ take $N\in \N$ s.t. $N^{\alpha} < \ve$, then we have $\forall n\geq N$ and $\forall x\in [0,1]$
\be
\abs{f_n'(x)} \leq n^{\alpha} \bb{\frac {n-1}{n+1}}^{n-1} < n^{\alpha} < N^{\alpha} < \ve.
\ee
as required for uniform convergence. However, if $\alpha \geq 0$ then
\be
f_n'\bb{\tfrac {n-1}{n+1}} = n^{\alpha} \bb{\frac {n-1}{n+1}}^{n-1} \geq \bb{\frac {n-1}{n+1}}^{n-1}  = \bb{1+ \frac 2{n-1}}^{-(n-1)} \to e^{-2} \quad\text{as }n\to \infty,
\ee
so we do not have uniform convergence. Thus, $f_n' \to 0$ uniformly on $[0,1]$ for $\alpha < 0$.
\een
\end{solution}




\begin{problem}
Consider the sequence of functions $f_n :\ \R\backslash \Z \to \R$ defined by $f_n(x) = \sum^n_{m=-n}(x-m)^{-2}$. Show that $f_n$ converges pointwise on $\R\backslash \Z$ to a function $f$. Show that $f_n$ does not converge uniformly on $\R \backslash \Z$. Why can we nevertheless conclude that the limit function $f$ is continuous, and indeed differentiable, on $\R \backslash \Z$?
\end{problem}

\begin{solution}[\bf Solution.]Given $x\in \R\bs\Z$ and $\ve>0$, choose $M\in\N$ s.t. 
\be
\sum^\infty_{k=M+1} \frac 1{k^2} < \frac{\ve}2
\ee
and take $N > \abs{x} + M, N\in\N$. Then if $\abs{m} = N+\ell$ with $\ell \in \N$, we have
\be
\abs{m-x} \geq \abs{m}-\abs{x} = N+\ell -\abs{x} > M + \ell,
\ee
so that if $n' > n>N$, we have
\beast
\abs{f_{n'}(x)-f_n(x)} & = & \sum^{-(n+1)}_{m=-n'} (x-m)^{-2} + \sum^{n'}_{m=n+1} (x-m)^{-2} <  \sum^{-(N+1)}_{m=-n'} (x-m)^{-2} + \sum^{n'}_{m=N+1} (x-m)^{-2}\\
& \leq & 2\sum^{n'-N}_{\ell=1} (M+\ell)^{-2} < 2\sum^{\infty}_{k=M+1} k^{-2} < \ve,
\eeast
so the sequence $\bb{f_n(x)}$ is Cauchy and hence converges. Thus, $f_n$ converges pointwise on $\R\bs\Z$ to function $f$.

Now we prove that the sequence $(f_n)$ does not converge uniformly to $f$ on $\R\bs\Z$. Take $\ve =1$, then $\forall N\in\N$ and take $x=N+\frac 12$, we have
\be
\abs{f_N(x)-f(x)} = \sum_{\abs{m}>N}(x-m)^{-2} > \bb{\frac 12}^{-2} = 4 > \ve.
\ee
Thus, $f_n$ does not converge to a function $f$. 

However, $f$ is differentiable and therefore continuous: take $x\in \R\bs\Z$ and choose $N>\abs{x}+2$, then as $f_N$ is a finite sum of differentiable functions, it suffices to show $f-f_N$ is differentiable at $x$. Take $y\in \R\bs\Z$ with $\abs{y-x}<1$, then we have
\beast
& & \frac{(f-f_N)(y)-(f-f_N)(x)}{y-x} + 2\sum_{\abs{m}>N}(x-m)^{-3} \\
& = & \frac 1{y-x}\sum_{\abs{m}>N}\bb{(y-m)^{-2}-(x-m)^{-2}} + 2\sum_{\abs{m}>N}(x-m)^{-3} \\
& = & \sum_{\abs{m}>N}\frac {(x-m)^3- (y-m)^2(x-m) + 2 (y-x)(y-m)^2 }{(y-x)(y-m)^2(x-m)^3} = \sum_{\abs{m}>N}\frac {(x-m)\bb{(x-m)^2- (y-m)^2} + 2 (y-x)(y-m)^2 }{(y-x)(y-m)^2(x-m)^3} \\
& = & \sum_{\abs{m}>N}\frac { 2(y-m)^2 - (x-m)(x+y-2m)}{(y-m)^2(x-m)^3} = \sum_{\abs{m}>N}\frac { (2(y-m)+ (x-m))((y-m)-(x-m))}{(y-m)^2(x-m)^3}\\
& = & (y-x)\sum_{\abs{m}>N}\frac {2(y-m)+ (x-m)}{(y-m)^2(x-m)^3} = (y-x)\sum_{\abs{m}>N}\frac {2}{(y-m)(x-m)^3} + \frac {1}{(y-m)^2(x-m)^2}.
\eeast

since $\abs{x-m}>2$ and $\abs{y-m} \geq \abs{x-m}-\abs{x-y} > 1$ if $\abs{m}>N$, we have
\be
\sum_{\abs{m}>N}\frac {2}{(y-m)(x-m)^3} + \frac {1}{(y-m)^2(x-m)^2} < 2 \sum_{\abs{m}>N}\frac 1{(x-m)^2}
\ee
which converges, so as $y\to x$ we have 
\be
\frac{(f-f_N)(y)-(f-f_N)(x)}{y-x} + 2\sum_{\abs{m}>N}(x-m)^{-3} \to 0 \ \ra \ \frac{(f-f_N)(y)-(f-f_N)(x)}{y-x} \to - 2\sum_{\abs{m}>N}(x-m)^{-3}.
\ee
Thus, $f-f_N$ is differentiable at $x$ as required.
\end{solution}






\begin{problem}Let $f$ be a differentiable, real-valued function on a (bounded or unbounded) interval $E \subseteq \R$, and suppose that $f'$ is bounded on $E$. Show that $f$ is uniformly continuous on $E$.

Let $g :\ [-1, 1] \mapsto \R$ be the function defined by $g(x) = x^2 \sin(1/x^2)$, for $x \neq 0$ and $g(0) = 0$. Show that $g$ is differentiable, but its derivative is unbounded. Is $g$ uniformly continuous on $[-1, 1]$?
\end{problem}

\begin{solution}[\bf Solution.]Suppose $\forall x\in\R$, $\abs{f'(x)}<M$, given $\ve>0$ set $\delta = \frac{\ve}M$, then if $\abs{y-x}<\delta$ and with Mean Value Theorem, we have
\be
\abs{f(y)-f(x)} = \abs{y-x}\cdot \abs{f'(z)}
\ee
for some $z\in (x,y)$, so 
\be
\abs{f(y)-f(x)} < \delta M = \ve.
\ee
Thus, $f$ is uniformly continuous.

Now consider function $g$,
\be
\lim_{h\to 0} \frac{g(h)-g(0)}{h} = \lim_{h\to 0} \frac{h^2\sin \tfrac 1{h^2}}{h} = 0,
\ee
we see that $g$ is differentiable at 0. At $x\neq 0$, $g$ is clearly differentiable with 
\be
g'(x) = 2x\sin \tfrac 1x - \frac 2x \cos \tfrac 1x.
\ee

In particular,
\be
g'\bb{\tfrac 1{\sqrt{2n\pi}}}= -2\sqrt{2n\pi},
\ee
so $g'$ is unbounded. However, as $g$ is continuous on a closed bounded interval it is uniformly continuous.



\end{solution}

\begin{problem}Suppose that a function $f$ has a continuous derivative on $(a, b) \subseteq \R$ and
\be
f_n(x) = n\bb{f\bb{x +\tfrac 1n} - f(x)}.
\ee
Show that $f_n$ converges uniformly to $f'$ on each interval $[\alpha , \beta] \subset (a, b)$.



\end{problem}

\begin{solution}[\bf Solution.]Take an interval $[\alpha , \beta] \subset (a, b)$. Let
\be
c = \frac 12 (a + \alpha),\quad\quad d = \frac 12 (b+\beta)
\ee
so that $a<c<\alpha$ and $\beta <d<b$. Since $f'$ is continuous on $[c,d]$, it is uniformly continuous there. Thus, $\forall \ve>0$, $\exists \delta >0$ (and we may assume $\delta < \min(a-c,d-\beta)$) such that if $y,x\in [c,d]$, then
\be
\abs{y-x} < \delta \ \ra \ \abs{f'(y)-f'(x)}< \ve.
\ee

Take $N > \frac 1{\delta}$, then for all $x\in[\alpha,\beta]$, $\forall n> N$ we may write (since $f$ is continuous and differentiable on $(c,d)$),
\be
f_n(x) = n\bb{f\bb{x +\tfrac 1n} - f(x)} = \frac{f\bb{x +\tfrac 1n} - f(x)}{x+\frac 1n -x} = f'(y)
\ee
for some $y\in (x,x+\tfrac 1n)$ by M.V.T. (where $x+ \frac 1n \in (\alpha -\delta, \beta + \delta)\subseteq (c,d)$). Then as $\abs{y-x} < \frac 1n < \delta$, we have $y\in [c,d]$ and so
\be
\abs{f_n(x)-f'(x)} = \abs{f'(y)-f(x)} < \ve. 
\ee

Thus, $f_n$ converges uniformly to $f'$ on $[\alpha, \beta]$.



\end{solution}

\begin{problem}Let $\sum^\infty_{n=1} a_n$ be an absolutely convergent series of real numbers. Define a sequence $(f_n)$ of functions on $[-\pi, \pi]$ by 
\be
f_n(x) = \sum^n_{m=1} a_m \sin mx
\ee
and show that each $f_n$ is differentiable with 
\be
f'_n(x) = \sum^n_{m=1} m a_m \cos mx.
\ee
Show further that 
\be
f(x) = \sum^\infty_{m=1} a_m \sin mx
\ee
defines a continuous function on $[-\pi, \pi]$, but that the series 
\be
\sum^\infty_{m=1} m a_m \cos mx
\ee
need not converge.



\end{problem}




\begin{solution}[\bf Solution.]As $x\mapsto \sin mx$ is a differentiable function on $[-\pi,\pi]$ with derivative $x\mapsto m\cos mx$, it follows that $f_n$, being a finite sum of such functions, is differentiable with 
\be
f_n'(x) = \sum^n_{m=1} ma_m \cos mx.
\ee

As $\sum^\infty_{n=1}a_n$ is absolutely convergent, $\forall \ve>0$, $\exists N\in \N$ s.t. 
\be
\sum_{m>N}\abs{a_m} < \ve.
\ee

Thus, $\forall n\geq N$, $\forall x\in [-\pi,\pi]$,
\be
\abs{f_n(x)-f(x)} = \abs{\sum_{m>n}a_m \sin mx} \leq \sum_{m>n}\abs{a_m \sin mx} \leq \sum_{m>n} \abs{a_m} \leq \sum_{m>N}\abs{a_m} < \ve
\ee
so $f_n$ converges uniformly to $f$, thus $f$ is a uniform limit of continuous functions, hence is continuous. However, if we take $a_m = \frac 1{m^2}$, then $\sum a_m$ is absolutely convergent but 
\be
\sum m a_m \cos mx = \sum \frac 1m \cos mx,
\ee
which does not converge at 0.



\end{solution}

\begin{problem}Let $f$ be a bounded function defined on a set $E \subseteq \R$, and for each positive integer $n$ let $g_n$ be a function defined on $E$ by
\be
g_n(x) = \sup\{\abs{f(y) - f(x)} :\ y \in E, \abs{y - x} < 1/n\}.
\ee

Show that $f$ is uniformly continuous on $E$ if and only if $g_n \to 0$ uniformly on $E$ as $n \to \infty$.



\end{problem}

\begin{solution}[\bf Solution.]"$\Longrightarrow$". Suppose $f$ is uniformly continuous on $E$ and take $\ve>0$. Then $\exists \delta >0$ such that for all $x,y\in E$, $\abs{y-x}< \delta$, with $\abs{f(y)-f(x)}< \ve$. So $\forall \ve >0$, $\exists \delta >0$ and $n> \frac 1{\delta} \ \ra \ \abs{y-x}<\frac 1n < \delta$, we have $g_n(x) < \ve$, which means that $g_n \to 0$ uniformly on $E$.

"$\Longleftarrow$". Suppose $g_n \to 0$ uniformly on $E$ and take $\ve>0$. Then there exists $N\in \N$ s.t. $\forall x\in E$ and $\forall n \geq N$, we have
\be
\abs{g_n(x)} < \ve \ \ra g_n(x) < \ve.
\ee

So we set $\delta = \frac 1{N+1}$, then for $\forall x,y \in E$ with $\abs{x-y} < \delta = \frac 1{N+1} < \frac 1N$,
\be
\abs{f(y)-f(x)} \leq g_N(x) < \ve.
\ee

Thus, $f$ is uniformly continuous on $E$.



\end{solution}

\begin{problem}(Dini's theorem) Let $f_n :\ [0, 1] \mapsto \R$ be a sequence of continuous functions converging pointwise to a continuous function $f :\ [0, 1] \mapsto \R$. Suppose that $f_n(x)$ is a decreasing sequence $f_n(x) \geq f_{n+1}(x)$ for each $x \in [0, 1]$. Show that $f_n \to f$ uniformly on $[0, 1]$.

[If you have done Metric and Topological Spaces then you may prefer to find a topological proof.]



\end{problem}

\begin{solution}[\bf Solution.]Here we use two approaches.

\vspace{2mm}

{\bf Non-topological proof.} 

By replacing each $f_n$ by the continuous function $g_n = f_n -f$, we may assume $g=0$. Then $\forall n\in \N$ and $x\in [0,1]$ we have $g_n$ is decreasing sequence and $g_n(x) \geq 0$.

If $g_n$ does not converge to 0 uniformly, $\forall N\in \N$, $\exists \ve >0$ and $\exists n\geq N, \exists x_n\in [0,1]$ s.t.
\be
\abs{g_n(x_n)} \geq \ve.
\ee  
Then the bounded sequence $(x_n)$ has a convergent subsequence $(x_{n_i})$ with limit $\ell \in [0,1]$. 

But with the above argument, $\forall \delta >0$, $\exists \ve >0, i\in \N$ with
\be
\abs{x_{n_i}-\ell}<\delta,\quad\quad \abs{g_{n_i}(x_{n_i})} \geq \ve. \quad\quad (*)
\ee

Since $g_n \to 0$ pointwise, there exists $N\in \N$, $\forall n\geq N$ with $g_n(\ell) < \frac{\ve}2$. 

Since $g_n$ is continuous, there exists $\delta > 0$ with $\abs{y-\ell} < \delta \ \ra \ \abs{g_n(y)-g_n(\ell)} < \frac {\ve}2$. Thus,
\be
\abs{g_n(y)} \leq \abs{g_n(y) - g_n(\ell)} + \abs{g_n{\ell}} <  \frac{\ve}2 +  \frac{\ve}2 = \ve.
\ee

So $\forall m \geq n$ we have 
\be
\abs{y-\ell}< \delta \ \ra \ g_m(y) < \ve.
\ee

i.e., $\forall \ve >0$, $\exists \delta >0, N\in\N$, $\forall m \geq N$ with $\abs{y-\ell}< \delta$,
\be
g_m(y) < \ve.
\ee

This is a contradiction to $(*)$. Thus $g_n\to 0$ uniformly.

\vspace{4mm}

{\bf Topological proof.} 

Given $\ve>0$, let $E_n= \{x\in [0,1]: f_n(x)-f(x) < \ve\}$. For all $n\in\N$, since $f_n-f$ is continuous $E_n$ is open, and as $f_n(x)\geq f_{n+1}(x)$ for all $x$ we have $E_n\subseteq E_{n+1}$. Since $f_n\to f$, for all $x$ there exists $n\in \N$ with $x\in E_n$, so the $E_n$ form an open cover of the compact set $[0,1]$, and thus there is a finite subcover, and so there exists $N\in \N$ with $E_N=[0,1]$. Then $n> N$, $\forall x\in [0,1]$ we have
\be
\abs{f_n(x)-f(x)} = f_n(x)-f(x) < \ve.
\ee
So $f_n \to f$ uniformly on $[0,1]$.



\end{solution}

\begin{problem}(Abel's test) Let $a_n$ and $b_n$ be real-valued functions on $E \subseteq \R$. Suppose that $\sum^\infty_{n=0} a_n(x)$ is uniformly convergent on $E$. Suppose further that the $b_n(x)$ are uniformly bounded on $E$ (this means there is a constant $K$ with $\abs{b_n(x)} \leq K$ for all $n$ and all $x \in E$), and that $b_n(x) \geq b_{n+1}(x)$ for all $n$ and all $x \in E$. Show that the sum 
\be
\sum^\infty_{n=0} a_n(x)b_n(x)
\ee
is uniformly convergent on $E$. 

[Hint: show first that 
\be
\sum^m_{k=n} a_k b_k = \sum^{m-1}_{k=n} (b_k-b_{k+1}) A_k + b_mA_m - b_nA_{n-1},
\ee
where $A_n = \sum^n_{k=0} a_k$.]

Deduce that if $a_n$ are real constants and $\sum^\infty_{n=0} a_n$ is convergent, then $\sum^\infty_{n=0} a_nx^n$ is uniformly convergent on $[0, 1]$. (But note that $\sum^\infty_{n=0} a_nx^n$ need not be convergent at $x = -1$; you almost certainly know a counterexample!)



\end{problem}

\begin{solution}[\bf Solution.]Set 
\be
A_n =\sum^n_{k=0} a_k \ \ra \ a_n = A_n - A_{n-1}.
\ee

then if $m>n$, we have
\beast
& & \sum^{m-1}_{k=n}(b_k-b_{k+1}) A_k + b_mA_m - b_nA_{n-1} \\
& = & -b_nA_{n-1} + (b_n - b_{n+1})A_n + (b_{n+1}-b_{n+2})A_{n+1} + \dots + (b_{m-1}-b_m)A_{m-1} + b_mA_m \\
& = & b_n(A_n - A_{n-1}) + b_{n+1} (A_{n+1}-A_n) + \dots + b_m(A_m - A_{m-1}) = \sum^m_{k=n} a_k b_k.
\eeast

Thus,
\beast
\sum^m_{k=n} a_k b_k & = & \sum^{m-1}_{k=n}(b_k-b_{k+1}) A_k + b_mA_m - b_nA_{n-1} \\
& = & \sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + \sum^{m-1}_{k=n}(b_k-b_{k+1}) A_m + b_mA_m - b_nA_{n-1}\\
& = & \sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + (b_n-b_m) A_m + b_mA_m - b_nA_{n-1}\\
& = & \sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + b_n(A_m - A_{n-1}).
\eeast

Now take $\ve >0$, as $\sum^\infty_{n=0} a_n(x)$ uniformly convergent on $E$, there exists $N\in \N$ s.t. if $m>n\geq N$, then for all $x\in E$ we have
\be
\abs{A_m(x)-A_n(x)} < \frac {\ve}{3K}.
\ee

Then, if $m>n\geq N+1$, we have
\beast
\abs{\sum^m_{k=n} a_k b_k} & = & \abs{\sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + b_n(A_m - A_{n-1})} \leq \sum^{m-1}_{k=n}(b_k-b_{k+1})\abs{A_k - A_m} + \abs{b_n(A_m - A_{n-1})}\\
& \leq & \sum^{m-1}_{k=n}(b_k-b_{k+1})\frac{\ve}{3K} + \abs{b_n}\abs{A_m - A_{n-1})} \leq 2K \frac{\ve}{3K} + K \frac{\ve}{3K} = \ve.
\eeast
Thus, $\sum^\infty_{n=0} a_n(x)b_n(x)$ is uniformly convergent on $E$. 

Set $E =[0,1]$, $a_n(x)=a_n$ and $b_n(x) = x^n$. Since $a_n$ are real constants and $\sum^\infty_{n=0} a_n$ is convergent, then it is uniformly convergent. Thus, applying the Abel's test, we have $\sum^\infty_{n=0} a_nx^n$ is uniformly convergent on $[0,1]$.



\end{solution}






\begin{problem}
Define $\varphi(x) = |x|$ for $x\in [-1, 1]$ and extend the definition of $\varphi(x)$ to all real $x$ by requiring that
\be
\varphi(x + 2) = \varphi(x).
\ee
\ben
\item [(i)] Show that $\abs{\varphi(s) - \varphi(t)} \leq \abs{s - t}$ for all $s$ and $t$.
\item [(ii)] Define $f(x) = \sum^\infty_{n=0} \bb{\frac 34}^n \varphi(4^nx)$. Prove that $f$ is well-defined and continuous.
\item [(iii)] Fix a real number $x$ and positive integer $m$. Put $\delta_m = \pm \frac 12 4^{-m}$, where the sign is so chosen that no integer lies between $4^m x$ and $4^m(x + \delta_m)$. Prove that
\be
\abs{\frac {f(x + \delta_m) - f(x)}{\delta_m} } \geq \frac 12 (3^m + 1).
\ee
\item [(iv)] Conclude that $f$ is not differentiable at $x$. Hence there exists a real continuous function on the real line which is nowhere differentiable.
\een
\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] We may assume $s>t$. Since $\varphi$ has period 2 and takes values in $[0,1]$, it suffices to consider $s,t$ s.t. 
\be
\abs{t} \leq 1,\quad\quad \abs{s-t} \leq 1. \quad (\text{the inequality is obvious when }\abs{s-t}> 1)
\ee
If $\abs{s} \leq 1$, then
\be
\abs{\varphi(s)-\varphi(t)} = \abs{\abs{s}-\abs{t}} \leq \abs{s-t} \quad\quad (\text{by triangle inequality}).
\ee

If $\abs{s} >1$, then $s\in (1,2]$ and hence $t\in (0,1]$. Hence,
\be
\abs{\varphi(s)-\varphi(t)} = \abs{(2-s) - t} \leq \abs{s-1}+\abs{t-1} = s-1 + 1-t = s-t = \abs{s-t}.
\ee

Thus for all $s$ and $t$, the inequality holds.

\item [(ii)] For all $x$, the partial sum of $\sum^\infty_{n=N+1} \bb{\frac 34}^n \varphi(4^nx)$ are bounded by 
\be
\sum^\infty_{n=0}\bb{\tfrac 34}^n = \frac 1{1-\frac 34} = 4.
\ee
so as the terms are all non-negative, the sum converges, thus $f$ is well-defined. For $m\in\N$, set
\be
f_m(x) = \sum^m_{n=0}\bb{\tfrac 34}^n \varphi(4^nx).
\ee

Given $\ve>0$, choose $N\in\N$ s.t. $\sum^\infty_{n=N+1}\bb{\frac 34}^n < \frac {\ve}3$, then $f_N$ is a finite sum of continuous functions, so is continuous. Thus given $x$ there exists $\delta>0$ s.t. 
\be
\abs{y-x}< \delta,\quad\quad\abs{f_N(y)-f_N(x)}< \frac {\ve}3.
\ee
Thus,
\beast
\abs{f(y)-f(x)} & \leq & \abs{f(x)- f_N(x)} + \abs{f(y)- f_N(y)} + \abs{f_N(y)-f_N(x)} \leq \sum^\infty_{n=N+1}\bb{\tfrac 34}^n \bb{\varphi(4^nx)+\varphi(4^ny)} + \abs{f_N(y)-f_N(x)} \\
& \leq & 2 \sum^\infty_{n=N+1}\bb{\tfrac 34}^n + \abs{f_N(y)-f_N(x)} < \frac {2\ve}3 + \frac {\ve}3 = \ve.
\eeast

Thus, $f$ is continuous.

\item [(iii)] If $n>m$, then $4^n(x+\delta_m) = 4^nx \pm 2\cdot 4^{n-m-1}$, so that
\be
\varphi(4^nx) = \varphi(4^n(x+\delta_m)) \ \ra \ (f-f_m)(x+\delta_m) = (f-f_m)(x) \ \ra \ f(x+\delta_m)-f(x) = f_m(x+\delta_m) - f_m(x).
\ee

If $n\leq m$, as no integer lies between $4^nx$ and $4^n(x+\delta_m)$, the function $y\mapsto \bb{\tfrac 34}^n \varphi(4^ny)$ is linear between $x$ and $x+\delta_m$, and its gradient is $\pm \bb{\frac 34}^n 4^n = 3^n$. Thus $f_m$ is linear between $x$ and $x+\delta_m$ and the absolute value of its gradient is at least 
\be
3^m - 3^{m-1} - \dots - 3 -1 = 3^m - \frac 12 (3^m-1) = \frac 12\bb{3^m + 1}
\ee

Thus, we have
\be
\abs{\frac {f(x + \delta_m) - f(x)}{\delta_m} } = \abs{\frac {f_m(x + \delta_m) - f_m(x)}{\delta_m} } \geq \frac 12 (3^m + 1).
\ee

\item [(iv)] If $f$ is differentiable at $x$ with $f'(x) = M$, there exists $\delta>0$ s.t. $\abs{h}<\delta$, 
\be
\abs{\frac{f(x+h)-f(x)}{h}-M} < 1 \ \ra \ \abs{\frac{f(x+h)-f(x)}{h}} < \abs{M}+1,
\ee
but we may choose $m$ s.t. $\abs{\delta_m}< \delta$ and $\frac 12 (3^m+1)>\abs{M}+1$, and we have a contradiction. Thus $f$ is not differentiable at $x$. Since $x$ is arbitrary, $f$ is a real continuous function which is nowhere differentiable.

\een
\end{solution}


\begin{problem}
Let $(x^{(m)})$ and $(y^{(m)})$ be sequences in $\R^n$ converging to $x$ and $y$ respectively. Show that $x^{(m)}\cdot y^{(m)}$ converges to $x \cdot y$. Deduce that if $f :\ \R^n \to \R^p$ and $g :\ \R^n \to \R^p$ are continuous at $x \in \R^n$, then so is the pointwise scalar product function $f \cdot g:\ \R^n \to \R$.
\end{problem}

\begin{solution}[\bf Solution.]First observe that if $a,b\in \R^n$ then 
\be
\abs{a\cdot b}^2 = \bb{\sum a_i b_i}^2 \leq \bb{\sum a_i^2}\cdot \bb{\sum b_i^2} = \dabs{a}^2 \cdot\dabs{b}^2 \ \ra \ \abs{a\cdot b} \leq \dabs{a}\cdot \dabs{b}.
\ee 

Now given $\ve>0$, $\exists N_1 \in\N$ s.t. $m>N_1$ 
\be
\dabs{x^{(m)}-x} < \frac {\ve}{2\bb{\dabs{y}+1}}
\ee
and $\exists N_2 \in\N$ s.t. $m>N_2$ 
\be
\dabs{y^{(m)}-y} < \frac {\ve}{2\bb{\dabs{x}+\frac{\ve}{2\bb{\dabs{y}+1}}}}.
\ee

Thus, $m> \max\{N_1,N_2\}$, we have
\beast
\abs{x^{(m)}\cdot y^{(m)} - x\cdot y} & \leq & \abs{x^{(m)}\cdot y^{(m)} - x^{(m)}\cdot y} + \abs{x^{(m)}\cdot y - x\cdot y} \leq \dabs{x^{(m)}}\dabs{y^{(m)} - y} + \dabs{y}\dabs{x^{(m)} - x}\\
 & < & \bb{\dabs{x} + \frac {\ve}{2\bb{\dabs{y}+1}}}\dabs{y^{(m)} - y} + \dabs{y}\frac {\ve}{2\bb{\dabs{y}+1}} < \frac {\ve}2 + \frac {\ve}2 = \ve.
\eeast

Thus, $x^{(m)}\cdot y^{(m)}$ converges to $x \cdot y$. 

Now if $f$, $g$ are continuous at $x$, take any sequence $x^{(m)}$ converging to $x$, then by continuity we have
\be
f(x^{(m)}) \to f(x),\quad \quad g(x^{(m)}) \to g(x) \quad  \ra\quad  (f\cdot g)(x^{(m)}) = f(x^{(m)}) \cdot g(x^{(m)}) \to f(x)\cdot g(x) = f\cdot g(x) 
\ee
by the previous result. Thus, $f\cdot g$ is continuous at $x$.



\end{solution}

\begin{problem}
Show that $\dabs{x}_1 = \sum^n_{i=1} \abs{x_i}$ defines a norm on $\R^n$. Show directly that it is Lipschitz equivalent to the Euclidean norm.
\end{problem}

\begin{solution}[\bf Solution.]For $\dabs{x}_1 = \sum^n_{i=1} \abs{x_i}$, we check \ben
\item [(i)] Clearly $\dabs{\cdot}_1:\R^n \to \R$ takes non-negative values,
\item [(ii)] $\dabs{x}_1 = 0 \ \ra\ \sum^n_{i=1} \abs{x_i} = 0 \ \ra \ \forall i,\ x_i = 0 \ \ra \ x = (0,\dots,0)^T$,
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm x}_1 = \sum^n_{i=1} \abs{\lm x_i} = \abs{\lm} \sum^n_{i=1} \abs{x_i} = \abs{\lm}\cdot \dabs{x}_1$,
\item [(iv)] and
\be
\dabs{x+y}_1 = \sum^n_{i=1} \abs{(x+y)_i} = \sum^n_{i=1} \abs{x_i+y_i} \leq \sum^n_{i=1} \abs{x_i}+\abs{y_i} = \sum^n_{i=1} \abs{x_i} +\sum^n_{i=1} \abs{y_i} = \dabs{x}_1 + \dabs{y}_1.
\ee
\een
So $\dabs{\cdot}_1$ is a norm. 

We have
\be
\dabs{x}^2 = \sum^n_{i=1} x_i^2 \leq \bb{\sum^n_{i=1} \abs{x_i}}^2= \dabs{x}_1^2 \ \ra \ \dabs{x} \leq \dabs{x}_1.
\ee
and
\be
\dabs{x}_1^2 = \bb{\sum^n_{i=1} \abs{x_i}}^2 \leq \bb{\sum^n_{i=1} \abs{x_i}^2}\bb{\sum^n_{i=1} 1^2} = n\dabs{x}^2 \ \ra \ \dabs{x}_1 \leq \sqrt{n}\dabs{x}.
\ee
Thus, $\dabs{\cdot}_1$ is Lipschitz equivalent to the Euclidean norm $\dabs{\cdot}$.



\end{solution}


\begin{problem}\label{ques:norm_one} \ben
\item [(a)] Show that $\dabs{f}_1 = \int^1_0 \abs{f(x)} dx$ defines a norm on the space $C[0, 1]$. Is it Lipschitz equivalent to the uniform norm?
\item [(b)] Let $R[0, 1]$ denote the vector space of all integrable functions on $[0, 1]$. Does $\dabs{f} = \int^1_0 \abs{f(x)} dx$ define a norm on $R[0,1]$?
\een
\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(a)] For $\dabs{f}_1 = \int^1_0 \abs{f(x)} dx$ on the space $C[0,1]$, we check \ben
\item [(i)] Clearly $\dabs{\cdot}_1:C[0,1] \to \R$ takes non-negative values,
\item [(ii)] $\dabs{f}_1 = 0 \ \ra\ \int^1_0 \abs{f} = 0 \ \ra \ f = 0$, (Analysis I Example Sheet 4)
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm f}_1 = \int^1_0 \abs{\lm f} = \abs{\lm } \int^1_0 \abs{f} = \abs{\lm}\cdot \dabs{f}_1$,
\item [(iv)] and
\be
\dabs{f+g}_1 = \int^1_0 \abs{f+g} \leq \int^1_0 \abs{f}+\abs{g} = \int^1_0 \abs{f} + \int^1_0 \abs{g} = \dabs{f}_1 + \dabs{g}_1.
\ee
\een
so $\dabs{\cdot}_1$ is a norm. Given $M>0$, choose $n\in \N$ with $n>M+1$ and define $f \in C[0,1]$ by
\be
f(x) = \left\{\ba{ll}
nx & 0\leq x< \frac 1n\\
2-nx \quad\quad & \frac 1n \leq x < \frac 2n\\
0 & \frac 2n \leq x \leq 1
\ea\right..
\ee 
then
\be
\dabs{f}_1 = \int^1_0 \abs{f(x)}dx = \int^{\frac 1n}_0 nx dx + \int^{\frac 2n}_{\frac 1n}(2-nx)dx = \frac n2\frac 1{n^2} + 2\frac 1n - \frac n2 \bb{\frac 4{n^2}-\frac 1{n^2}} = \frac 1n.
\ee
\be
\dabs{f}_\infty = \sup_{x\in [0,1]}\abs{f(x)} = 1.
\ee
so $\dabs{f}_\infty = n\dabs{f}_1 > M\dabs{f}_1$, so $\dabs{\cdot}_1$ is not Lipschitz equivalent to $\dabs{\cdot}_\infty$.

\item [(b)] No. Define $f\in R[0,1]$ by 
\be
f(x) = \left\{\ba{ll}
1 & x=0\\
0 \quad\quad & x>0
\ea\right. \ \ra \ f\neq 0, \text{ but }\dabs{f} = \int^1_0 \abs{f(x)} dx = 0.
\ee 

\een



\end{solution}

\begin{problem}Which of the following subsets of $\R^2$ are open? Which are closed? (And why?)
\ben
\item [(i)] $\{(x, 0) :\ 0 \leq x \leq 1\}$;
\item [(ii)] $\{(x, 0) :\ 0 < x < 1\}$;
\item [(iii)] $\{(x, y) :\ y \neq 0 \}$;
\item [(iv)] $\{(x, y) :\ x \in \Q \text{ or }y \in \Q\}$;
\item [(v)] $\{(x, y) :\ y = nx \text{ for some }n \in \N \} \cup \{(x, y) :\ x = 0\}$;
\item [(vi)] $\{(x, f(x)) :\ x \in \R\}$, where $f :\ \R \to \R$ is a continuous function.
\een



\end{problem}

\begin{solution}[\bf Solution.]In each case we denote the subset concerned by $S$.
\ben
\item [(i)] Not open. $S$ contains no open ball about any point. 

Closed. If $(x,y)\notin S$ then either $y\neq 0$ or $y=0$ and $x<0$, or $y=0$ and $x>1$. The open ball about $(x,y)$ of radius $\frac {\abs{y}}2$ or $-\frac x2$, or $\frac{x-1}2$ as appropriate lies outside $S$.

\item [(ii)] Not open. as in (i). 

Not closed. $(0,0)\notin S$ but any open ball about $(0,0)$ meets $S$.

\item [(iii)] Open. If $(x,y)\in S$ the open ball about $(x,y)$ of radius $\frac{\abs{y}}2$ lies in $S$.

Not closed. as in (ii).

\item [(iv)] Not open. as in (i).

Not closed. If $(x,y)\notin S$ then any open ball about $(x,y)$ meets $S$.

\item [(v)] Not open. $(1,1)\in S$ but $S$ contains no open ball about $(1,1)$.

Closed. If $(x,y)\notin S$ then $x\neq 0$ and $\frac yx \notin \N$. If $\frac yx <1$ set
\be
S_1 = \{(x',0):x'\in \R\},\quad\quad S_2 = \{(x',x'):x'\in \R\},
\ee
while if $\frac yx >1$ choose $n\in\N$ such that $n< \frac yx < n+1$ and set
\be
S_1 = \{(x',nx'):x'\in \R\},\quad\quad S_2 = \{(x',(n+1)x'):x'\in \R\}.
\ee

As in (iii), $S_1$ and $S_2$ are closed, so $S_1\cup S_2$ is closed and thus $\R^2\bs (S_1\cup S_2)$ is open, and as this set contains $(x,y)$ it contains an open about $(x,y)$, which then lies outside $S$.


\item [(vi)] Not open. as in (i). 

Closed. If $(x,y)\not\in S$ then $f(x) \neq y$. Let $\ve=\frac 12 \abs{f(x)-y}$, then $\exists \delta >0$ such that
\be
\abs{z-x}< \delta \ \ra \ \abs{f(z)-f(x)} < \ve \ \ra \ \abs{y-f(z)} \geq \abs{y-f(x)}-\abs{f(x)-f(z)} > 2\ve - \ve = \ve,
\ee
so the open ball about $(x,y)$ of radius $\min\{\delta, \ve\}$ lies outside $S$.

\centertexdraw{
\drawdim in
\def\bdot {\fcir f:0 r:0.02 }

\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0

\move (-0.2 0) \avec(2.3 0)
\move (0 -0.2) \avec(0 1.8)

\move (2.8 0) \avec(5.3 0)
\move (3 -0.2) \avec(3 1.8)

\lpatt( )
\move (0 0) \lvec (1.8 1.8)
\move (0 0) \lvec (0.9 1.8)

\move (3.2 0.5) \clvec (3.5 0.8)(4 0)(4.5 1.2)
\move (4.5 1.2) \clvec (4.8 2)(4.9 1.5)(5 1.3)

\htext (0.1 1.9){$x=0$}
\htext (0.8 1.9){$y = 2x$}
\htext (1.4 1.9){$y = x$}


\htext (3.2 0.6){$f$}
\htext (1 -0.2){(v)}
\htext (4 -0.2){(vi)}

\lpatt(0.05 0.05)

\move (1.5 1.2) \lcir r:0.2
\move (1.5 1.2) \bdot

\move (4.5 1.2) \lcir r:0.2
\move (4.5 1.2) \bdot
\move (0 2.2)
}

\een



\end{solution}

\begin{problem}Is the set $\{f : f(1/2) = 0\}$ closed in the space $C[0, 1]$ with the uniform norm? What about the set $\{f : \int^1_0 f(x)dx = 0\}$? In each case, does the answer change if we replace the uniform norm with the norm $\dabs{\cdot}_1$ defined in Question \ref{ques:norm_one}?



\end{problem}

\begin{solution}[\bf Solution.]Let $S=\{f : f(1/2) = 0\}$. Given $f\in C[0,1]\bs S$, set $r = \frac 12 \abs{f(\tfrac 12)} \ (\neq 0)$. Given $g\in C[0,1]$, if we have
\be
\dabs{g-f}_\infty < r \ \ra \ \sup_{x\in [0,1]}\abs{g(x)-f(x)} < r \ \ra \ g(\tfrac 12) - f(\tfrac 12) < r.
\ee

Thus,
\be
\abs{g(\tfrac 12)}  \geq \abs{f(\tfrac 12)} - \abs{g(\tfrac 12) - f(\tfrac 12)} > 2r -r = r \ \ra \ g \notin S.
\ee
So $C[0,1]\bs S$ is open and thus $S$ is closed in $C[0,1]$ with the uniform norm.

Now define $f\in C[0,1]\bs S$ by $f(x)=1$ and $g_1, g_2, \dots \in C[0,1]$ by
\be
g_n(x) = \left\{\ba{ll}
n\abs{x-\frac 12} \quad\quad & \abs{x-\frac 12}\leq \frac 1n\\
1 & \text{otherwise}
\ea\right. \ \ra \ g_n \in S.
\ee
and $\dabs{h}_1 = \int^1_0 \abs{h(x)}dx$
\be
\dabs{g_n - f}_1 = \int^1_0 \abs{g_n - f} = \frac 2n - n\int^{\frac 12 + \frac 1n}_{\frac 12 - \frac 1n}\abs{x-\frac 12}dx  = \frac 2n - 2n\int^{\frac 1n}_0 x dx = \frac 1n.
\ee

So $S$ meets every open ball about $f$, and hence is not closed on $C[0,1]$ with norm $\dabs{\cdot}_1$.

Now consider $S'= \{f : \int^1_0 f(x)dx = 0\}$. Given $f\in C[0,1]\bs S'$, set $r = \frac 12 \abs{\int^1_0 f} \ (\neq 0)$. Given $g\in C[0,1]$, if we have
\be
\dabs{g-f}_\infty < r \ \ra \ \sup_{x\in [0,1]}\abs{g(x)-f(x)} < r \ \ra \ \int^1_0 \abs{g-f} < r.
\ee
Thus,
\be
\abs{\int^1_0 g} \geq \abs{\int^1_0 f} - \abs{\int^1_0 (g-f)} \geq 2r -\int^1_0  \abs{g-f} > 2r -r = r \ \ra \ g \notin S'.
\ee
So $C[0,1]\bs S'$ is open and thus $S'$ is closed in $C[0,1]$ with the uniform norm.

Similarly, if $\dabs{g-f}_1 < r$ then $\dabs{h}_1 = \int^1_0 \abs{h(x)}dx$
\be
\int^1_0 \abs{g-f} < r \ \ra \ \abs{\int^1_0 g} \geq r \ \ra \ g\notin S'.
\ee
So $C[0,1]\bs S'$ is open and thus $S'$ is closed in $C[0,1]$ with the norm $\dabs{\cdot}_1$.



\end{solution}

\begin{problem}Which of the following functions $f$ are continuous?
\ben
\item [(i)] The linear map $f :\ \ell_\infty \to \R$ defined by $f(x) = \sum^\infty_{n=1} x_n/n^2$.
\item [(ii)] The identity map from the space $C[0, 1]$ with the uniform norm to the space $C[0, 1]$ with the norm $\dabs{\cdot}_1$ defined in Question \ref{ques:norm_one}.
\item [(iii)] The identity map from $C[0, 1]$ with the norm $\dabs{\cdot}_1$ to $C[0, 1]$ with the uniform norm.
\item [(iv)] The linear map $f :\ \ell_0 \to \R$ defined by $f(x) = \sum^\infty_{i=1} x_i$.
\een



\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] Continuous. Given $x\in \ell_\infty$ and $\ve>0$, set $\delta = \frac {6\ve}{\pi^2}$, then 
\be
\dabs{y-x}_\infty < \delta \ \ra \ \forall n \in \N, \ \abs{y_n - x_n} < \frac {6\ve}{\pi^2}.
\ee

Thus,
\be
\abs{f(y)-f(x)} = \abs{\sum^\infty_{n=1} \frac{y_n-x_n}{n^2}} \leq \abs{y_n - x_n} \sum^\infty_{n=1} \frac 1{n^2} < \frac {6\ve}{\pi^2}\sum^\infty_{n=1} \frac 1{n^2} = \ve.
\ee

\item [(ii)] Continuous. Given $f\in C[0,1]$ and $\ve>0$, set $\delta = \ve$, then 
\be
\dabs{g-f}_\infty < \delta \ \ra \ \sup\abs{g(x)-f(x)} < \delta \ \ra \ \forall x \in [0,1], \ \abs{g(x)-f(x)} < \delta = \ve
\ee
Then
\be
\dabs{g-f}_1 = \int^1_0 \abs{g-f} < \int^1_0 \ve dx = \ve.
\ee

\item [(iii)] Not continuous. Take $f\in C[0,1]$ defined by $f(x)=1$ and set $\ve = \frac 12$. So $\forall \delta >0$ choose $n\in\N$ with $n> \frac 1{\delta}$, and define $g\in C[0,1]$ by 
\be
g(x) = \left\{\ba{ll}
n\abs{x-\frac 12} \quad\quad & \abs{x-\frac 12}\leq \frac 1n\\
1 & \text{otherwise}
\ea\right.
\ee
Then 
\be
\dabs{g-f}_1 = \int^1_0 \abs{g-f} = \frac 1n < \delta \quad (\text{by previous result})
\ee
but 
\be
\dabs{g-f}_\infty = \sup\abs{g(x)-f(x)} = 1 > \ve.
\ee

\item [(iv)] Not continuous. Take $x=0\in \ell_0$ and set $\ve= \frac 12$. Given $\delta>0$ choose $N>\frac 1{\delta}$ and take $y=(y_n)$ where 
\be
y_n = \left\{\ba{ll}
\frac 1N \quad\quad & n\leq N\\
0 & n>N
\ea\right. \ \ra \ \dabs{y-x} = \frac 1N< \delta, \quad \quad \text{but } \abs{f(y)-f(x)} = 1 > \ve.
\ee

\een



\end{solution}

\begin{problem}If $A$ and $B$ are subsets of $\R^n$, we write $A + B$ for the set $\{a + b :\ a \in A,\ b \in B\}$. Show that if $A$ and $B$ are both closed and one of them is bounded then $A + B$ is closed. Give an example in $\R^1$ to show that the boundedness condition cannot be omitted. If $A$ and $B$ are both open, is $A + B$ necessarily open? Justify your answer.



\end{problem}

\begin{solution}[\bf Solution.]If say $A$ is bounded, suppose $c\in \R^n$ is such that for all $\ve>0$, there exists $a\in A$, $b\in B$ with 
\be
\dabs{c-(a+b)} < \ve.
\ee

Taking $\ve = 1,\frac 12,\frac 13,\dots$, we obtain sequences $(a_m)$ in $A$ and $(b_m)$ in $B$ with $a_m+b_m\to c$. As the sequence $(a_m)$ is bounded, it has a convergent subsequence $(a_{m_i})$ (by Bolzano-Weierstrass Theorem) with limit $a$, and as $A$ is closed we have $a\in A$. Then
\be
\lim (a_{m_i} + b_{m_i}) = c \ \ra \ \lim b_{m_i} = c - \lim a_{m_i} = c-a, 
\ee
and as $B$ is closed we have $c-a \in B$, thus $c=a+(c-a) \in A+B$ and so $A+B$ is closed.

Let $A=\Z$ and $B=\{n+\frac 1n:n\geq 2,n\in \N\}$. Both sets are clearly closed. We have
\be
A+B = \{m+\tfrac 1n,\ m\in \Z,\ n\geq 2,n\in\N\}
\ee
So $0\notin A+B$ but $\forall n\geq 2$, $\frac 1n\in A+B$, so any open ball about 0 meets $A+B$, thus $A+B$ is not closed.

If $A,B\subset \R^n$ are open, $\forall c\in A+B$, then $\exists a\in A$, $b\in B$ with $c=a+b$. As $A$ is open, $\exists r>0$ s.t.
\be
\dabs{a'-a} < r \ra \ a' \in A, 
\ee
So 
\be
\dabs{c'-c} < r \ \ra \ \dabs{(c'-b)-a}<r \ \ra \ c'-b \in A \ \ra \ c'-b = a', a'\in A \ \ra \ c' = a'+b \in A+B.
\ee
Thus, $A+B$ is open.



\end{solution}

\begin{problem}\ben
\item [(a)] Show that the space $\ell_\infty$ is complete. Show also that $c_0 = \{x\in \ell_\infty:\ x_n \to 0\}$, the vector subspace of $\ell_\infty$ consisting of all sequences converging to 0, is complete.
\item [(b)] Is the space $R[0, 1]$ of integrable functions on $[0, 1]$, equipped with the uniform norm, complete?
\een



\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(a)] Take a Cauchy sequence $x^{(1)}, x^{(2)}, \dots, $ in $\ell_\infty$. Given $\ve>0$, $\exists N\in\N$ such that $n,m>N$,
\be
\dabs{x^{(n)}- x^{(m)}}_\infty < \ve \ \ra \ \forall t \in \N, \quad \abs{x^{(n)}_t - x^{(m)}_t} < \ve.
\ee
Thus, $\forall t\in \N$ the sequence $x^{(1)}_t, x^{(2)}_t,\dots$ is Cauchy and hence converges to some $x_t \in \R$. Set $x=(x_1, x_2,\dots)$. $\exists N\in \N$ such that $n>N$,
\be
\dabs{x^{(n)}- x^{(N)}} < 1, \quad \text{i.e., }\forall t\in\N,\quad \abs{x^{(n)}_t - x^{(N)}_t} < 1 \ \ra \ \abs{x_t-x^{(N)}_t} < 1 \ \ra\ \abs{x_t} < \abs{x^{(N)}_t}  + 1.
\ee

Thus, as $x^{(N)}$ is a bounded sequence, so is $x$, and $x\in \ell_\infty$. As above, $\forall \ve >0$, $\exists N\in \N$ such that $n,m>N$,
\be
\forall t\in \N,\quad \abs{x_t^{(n)}-x_t^{(m)}} < \ve,
\ee
taking the limit as $m\to\infty$ gives $\forall t\in \N$, 
\be
\abs{x_t^{(n)}-x_t} < \ve \ \ra \ \dabs{x^{(n)}-x}_\infty < \ve \ \ra\ x^{(n)}\to x. \ \ra \ \ell_\infty \text{ is complete.}
\ee

Now suppose $x^{(1)}, x^{(2)},\dots$ is a Cauchy sequence in $c_0$, then $x^{(n)}\to x$ for some $x\in \ell_\infty$. Given $\ve>0$, $\exists N\in\N$ such that $n,m>N$
\be
\forall t\in \N,\quad \abs{x^{(n)}_t -x^{(m)}_t} < \frac {\ve}2,
\ee
taking the limit as $m\to\infty$ gives 
\be
\forall t\in \N,\quad \abs{x^{(n)}_t -x_t} < \frac {\ve}2.
\ee
Then $\exists T\in\N$ such that $\forall t>T$,
\be
\abs{x^{(n)}_t} < \frac{\ve}2 \quad\quad (\text{by }x_n\to 0)
\ee
Thus,
\be
\abs{x_t} \leq \abs{x_t^{(n)}-x_t} + \abs{x_t^{(n)}} < \frac {\ve}2 + \frac {\ve}2 = \ve \ \ra \ x\in c_0 \ \ra \ c_0 \text{ is complete}.
\ee

\item [(b)] Take a Cauchy sequence $f_1,f_2,\dots$ in $R[0,1]$. Given $\ve >0$, $\exists N\in\N$ such that $n,m>N$
\be
\dabs{f_n - f_m}_\infty < \ve \ \ra \ \sup_{x\in [0,1]}\abs{f_n(x)-f_m(x)} < \ve.
\ee
Then $\forall x \in [0,1]$, the sequence $f_1(x),f_2(x),\dots$ is Cauchy and hence converges in $\R$. Define $f:[0,1]\to \R$ by $f(x)=\lim_{n\to\infty}f_n(x)$. Thus, given $\ve>0$, $\exists N\in\N$ such that 
\be
n>N,\ \forall x \in [0,1], \quad \abs{f_n(x)-f(x)} < \frac {\ve}3.
\ee
Choose $n>N$, then exists a dissection $\sD$ of $[0,1]$ such that 
\be
S_{\sD}(f_n) - s_{\sD}(f_n) < \frac {\ve}3.
\ee

On any interval $[a,b]\subseteq [0,1]$ we have
\be
\sup_{[a,b]}f \leq \sup_{[a,b]}f_n + \frac {\ve}3,\quad\quad \inf_{[a,b]}f \geq \inf_{[a,b]}f_n - \frac {\ve}3,
\ee
then
\be
S_{\sD}(f) \leq S_{\sD}(f_n) + \frac {\ve}3, \quad s_{\sD}(f) \geq s_{\sD}(f_n) - \frac {\ve}3 \ \ra \ S_{\sD}(f) - s_{\sD}(f) < \frac {\ve}3 + \frac {\ve}3 + \frac {\ve}3 = \ve \ \ra \ f\in R[0,1].
\ee

As above, $\forall \ve>0$, $\exists N\in\N$ such that $n,m > N$
\be
\forall x\in [0,1],\quad \abs{f_n(x)-f_m(x)} < \ve.
\ee
Taking the limit as $m\to \infty$ gives $\forall x\in [0,1]$, 
\be
\abs{f_n(x) -
 f(x)} < \ve \ \ra \ \dabs{f_n - f}_{\infty} < \ve \ \ra \ f_n \to f\ \ra \ R[0,1]\text{ is complete.}
\ee
\een



\end{solution}






\begin{problem}Let $\alpha:\R^n \to \R^m$ be a linear map. Show that $\dabs{x}' = \dabs{x}+\dabs{\alpha x}$ defines a norm on $\R^n$. Using the fact that all norms on a finite-dimensional space are Lipschitz equivalent, deduce that $\alpha$ is continuous.
\end{problem}

\begin{solution}[\bf Solution.]For $\dabs{x}' = \dabs{x}+\dabs{\alpha x}$ we check
\ben
\item [(i)] Clearly, $\dabs{\cdot}':\R^n\to\R$ takes non-negative values
\item [(ii)]
\be
\dabs{x}' = 0 \ \ra \ \dabs{x} = \dabs{\lm x} = 0 \ \ra \ x = 0,
\ee
\item [(iii)] For all $\lm \in\R$
\be
\dabs{\lm x}' = \dabs{\lm x} +\dabs{\alpha \lm x} = \abs{\lm} \dabs{x} + \abs{\lm \alpha}\dabs{x} = \abs{\lm}\dabs{x}',
\ee
\item [(iv)] and
\be
\dabs{x+y}' = \dabs{x+y}+\dabs{\alpha (x+y)} \leq \dabs{x} + \dabs{y} + \abs{\alpha} \dabs{x+y} = \dabs{x}' + \dabs{y}'.
\ee
\een
Thus, $\dabs{x}'$ defines a norm on $\R^n$. 

As $\dabs{\cdot}$ and $\dabs{\cdot}'$ are Lipschitz equivalent, $\exists M>0$ such that $\forall x\in\R^n$
\be
\dabs{x}' \leq M\dabs{x} \ \ra \ \dabs{x} + \dabs{\alpha x}\leq M\dabs{x} \ \ra \ \dabs{\alpha x} \leq (M-1)\dabs{x}
\ee

Given $\ve>0$, set $\delta = \frac {\ve}{M-1}$, then
\be
\dabs{y-x} < \delta \ \ra \ \dabs{\alpha(y)-\alpha(x)} = \abs{\alpha(y-x)} \leq (M-1)\dabs{y-x} < \ve,
\ee
thus, $\alpha$ is continuous.



\end{solution}

\begin{problem}Which of the following vector spaces of functions, considered with the uniform norm, are complete? (Justify your answer.)
\ben
\item [(i)] The space $C_b(\R)$ of bounded continuous functions $f : \R \to \R$.
\item [(ii)] The space $C_0(\R)$ of continuous functions $f : \R \to \R$ such that $f(x) \to 0$ as $\abs{x} \to \infty$.
\item [(iii)] The space $C_c(\R)$ of continuous functions $f : \R \to \R$ such that $f(x) = 0$ for $\abs{x}$ sufficiently large.
\een



\end{problem}

\begin{solution}[\bf Solution.]In each case, let $S$ be the space and $(f_n)$ be a Cauchy sequence in $S$. We know that the space of continuous function $\R\to\R$ is complete with respect to the uniform norm, so $f_n \to f$ for some continuous function $f$. It thus suffices to determine whether or not we must have $f\in S$.
\ben
\item [(i)] If $S=C_b(\R)$, there exists $N\in\N$ such that $\forall m>n\geq N$,
\be
\dabs{f_n-f_m}_\infty = \sup\{\abs{f_n - f_m}:x\in\R\} <1.
\ee
Since $f_N$ is bounded, there exists $K\in\R$ such that for all $x\in\R$, we have $\abs{f_N(x)}<K$, and then $\forall m>N$ and $x\in\R$, 
\be
\abs{f_m(x)} \leq \abs{f_m(x)-f_N(x)} + \abs{f_N(x)} < K+1 \ \ra \ \abs{f(x)}\leq \abs{f(x)-f_m(x)} + \abs{f_m(x)} 
\ee
we take $\ve = 1$, $\exists N'\in\N$ such that $\forall n > N, x\in\R$, $\abs{f_n(x) -f(x)} < 1$, then take $N'' = \max\{N,N'\}$, then $\forall n>N$
\be
\abs{f(x)}\leq \abs{f(x)-f_n(x)} + \abs{f_n(x)} < 1 + K+1 = K+2 \ \ra \ f \in C_b(\R).
\ee

\item [(ii)] If $S=C_0(\R)$, given $\ve>0$, there exists $N\in\N$ such that $\forall m>n\geq N,\ x\in S$,
\be
\sup\{\abs{f_n(x)-f_m(x)}:x\in\R\} < \frac {\ve}4, \ \ra \ \abs{f_N(x)-f_m(x)} < \frac {\ve}4,
\ee
and then by the definition of $C_0(\R)$ and $f_N\in C_0(\R)$, $\exists M>0$ such that $\abs{x}>M$, 
\be
\abs{f_N(x)} < \frac{\ve}4.
\ee
Take $x\in\R$ with $\abs{x}>M$, then 
\be
\abs{f_m(x)} \leq \abs{f_m(x)-f_N(x)} + \abs{f_N(x)} < \frac {\ve}4 + \frac {\ve}4 = \frac {\ve}2.
\ee
then since $f_m \to f$, $\exists N'\in\N$ such $\forall n \geq N'$, 
\be
\abs{f_m(x)-f(x)} < \frac {\ve}2.
\ee

Thus, $\forall \ve>0$, $\exists M>0$ such that $\abs{x}>M$ and $\exists N'' = \max\{N,N'\}$ such that $\forall n\geq N''$, 
\be
\abs{f(x)} \leq \abs{f_m(x)-f(x)} + \abs{f_m(x)} < \frac {\ve}2 + \frac {\ve}2 = \ve \ \ra \ f\in C_0(\R).
\ee

\item [(iii)] If $S=C_c(\R)$, define $f_n$ by 
\be
f_n(x)= \left\{\ba{ll}
1 & \abs{x}\leq 1 \\
\frac 1{\abs{x}} & 1<\abs{x}\leq n\\
\frac 1n\bb{n+1-\abs{x}} \quad\quad & n< \abs{x} \leq n+1\\
0 & n+1 < \abs{x}
\ea\right.
\ee

It is obvious that $f_n \in S$. Given $\ve>0$ choose $N > \frac 1{\ve}$, then if $m>n\geq N$ the function $f_m$ and $f_n$ agree on $[-n,n]$, while $\abs{x}>n$, we have
\be
\abs{f_m(x)-f_n(x)} \leq \frac 1n < \ve \ \ra \ (f_n) \text{ is a Cauchy sequence in }S.
\ee

Also, we know that $f_n \to f$ with 
\be
f(x) = \left\{\ba{ll}
1 & \abs{x}\leq 1 \\
\frac 1{\abs{x}} \quad\quad & \abs{x}>1
\ea\right. \quad \ra \ f\notin S \ \ra \ C_c(\R) \text{ is not complete.}
\ee

\een



\end{solution}

\begin{problem}In lectures we proved that if $E$ is a closed and bounded set in $\R^n$, then any continuous function defined on $E$ has bounded image. Prove the converse: if every continuous real-valued function on $E \subseteq \R^n$ is bounded, then $E$ is closed and bounded.



\end{problem}

\begin{solution}[\bf Solution.]Take $a\in E$ and define $f:E\to\R$ by $f(x) = \dabs{x-a}$, then $f$ is continuous and therefore bounded (by the assumption), there exists $r>0$ such that for all $x\in E$ we have $\abs{f(x)} \leq r$. So $E$ lies in the ball of radius $r$ about $a$, thus $E$ is bounded.

Given $b\in \R^n\bs E$ define $f:E\to \R$ by $f(x) = \frac 1{\dabs{x-b}},\ x\in E$, then $f$ is continuous therefore bounded (by the assumption), there exists $\ve>0$ such for all $x\in E$ we have 
\be
\abs{f(x)} < \frac 1{\ve} \ \ra \ \dabs{x-b} > \ve.
\ee
So the open ball of radius $\ve$ about $b$ does not meet $E$, thus $E$ is closed.



\end{solution}

\begin{problem}Let $(x^{(m)})_{m\geq 1}$ be a bounded sequence in $\ell_\infty$. Show that there is a subsequence $(x^{(m_j)})_{j\geq 1}$ which converges in every coordinate; that is to say, the sequence $(x^{(m_j)}_i)_{j\geq 1}$ of real numbers converges for each $i$. Why does this not show that every bounded sequence in $\ell_\infty$ has a convergent subsequence?



\end{problem}

\begin{solution}[\bf Solution.]Choose a subsequence $(x^{(m_i)})$ of $x^{(m)}$ as follow. The value $x_1^{(m)}$ form a bounded sequence, so they have a convergent subsequence $x_1^{(l_{1,m})}$. 
\begin{center}
\begin{tabular}{c|ccc}
 & bounded & bounded &  \\ 
 & sequence & sequence  & $\ra$ \\ \hline
$x^{(1)}$ & $x_1^{(1)}$ & $x_2^{(1)}$ & $\dots$  \\ 
$x^{(2)}$ & $x_1^{(2)}$ & $x_2^{(2)}$ & $\dots$  \\ 
$\vdots$ & & &\\
$x^{(m)}$ & $x_1^{(m)}$ & $x_2^{(m)}$ & $\dots$  
\end{tabular} 
\begin{tabular}{c|ccc}
 & convergent  & bounded  &  \\ 
 & sequence  & sequence  & $\ra$ \\ \hline
$x^{(l_{1,1})}$ & $x_1^{(l_{1,1})}$ & $x_2^{(l_{1,1})}$ & $\dots$  \\ 
$x^{(l_{1,2})}$ & $x_1^{(l_{1,2})}$ & $x_2^{(l_{1,2})}$ & $\dots$  \\ 
$\vdots$ & & & \\
$x^{(l_{1,m})}$ & $x_1^{(l_{1,m})}$ & $x_2^{(l_{1,m})}$ & $\dots$  
\end{tabular} 
\begin{tabular}{c|ccc}
 & convergent & convergent &  \\ 
 & sequence  & sequence & \\ \hline
$x^{(l_{2,1})}$ & $x_1^{(l_{2,1})}$ & $x_2^{(l_{2,1})}$ & $\dots$  \\ 
$x^{(l_{2,2})}$ & $x_1^{(l_{2,2})}$ & $x_2^{(l_{2,2})}$ & $\dots$  \\ 
$\vdots$ & & & \\
$x^{(l_{2,m})}$ & $x_1^{(l_{2,m})}$ & $x_2^{(l_{2,m})}$ & $\dots$  
\end{tabular}
\end{center}
Then for the bounded sequence $x_2^{(l_{1,m})}$, we have a convergent subsequence $x_2^{(l_{2,m})}$. Repeating these steps we can find a convergent subsequence $x_i^{(l_{i,m})}$ converges in the 1st, 2nd, $\dots,\ i$th co-ordinates. Thus, there is subsequence as required. 

This does not show that every bounded sequence in $l_{\infty}$ has a convergent subsequence. We have shown the statement
\be
\forall i \in \N,\ \forall \ve>0, \ \exists N\in\N \text{ such that }\forall m\geq N,\quad \abs{x_i^{(l_{i,m})}-\lim_{m\to\infty}x_i^{(l_{i,m})}}<\ve,
\ee
but this does not imply
\be
\forall \ve >0, \ \exists N\in\N \text{ such that }\forall m\geq N,\ \forall i\in \N,\quad \abs{x_i^{(l_{i,m})}-\lim_{m\to\infty}x_i^{(l_{i,m})}}<\ve.
\ee

For instance, take $\ve= \frac 12$,
\be
\left\{\ba{l}
x^{(m_1)} = \bb{1,1,1,\dots}\\
x^{(m_2)} = \bb{0,\frac 12,\frac 23,\dots}\\
\vdots\\
x^{(m_n)} = \bb{0,0,\dots, \frac 1n, \frac 2{n+1},\dots,}
\ea\right. \ \ra \ x^{(m_n)} = \bb{0,0,\dots, \frac 1n, \frac 2{n+1},\dots,\underbrace{\frac {n+1}{2n}}_{>\frac 12},\dots}.
\ee



\end{solution}

\begin{problem}Show that $\dabs{x}_1 = \sum^\infty_{i=1} \abs{x_i}$ defines a norm on $\ell_0$ and that this norm is not Lipschitz equivalent to the uniform norm $\dabs{\cdot}$. Find a third norm on $\ell_0$ which is neither Lipschitz equivalent to $\dabs{\cdot}_1$, nor to $\dabs{\cdot}$. Is it possible to find uncountably many norms on $\ell_0$ such that no two are Lipschitz equivalent?



\end{problem}

\begin{solution}[\bf Solution.]For $\dabs{x}_1 = \sum^\infty_{i=1} \abs{x_i}: \ell_0 \to\R$, we check \ben
\item [(i)] Clearly $\dabs{\cdot}_1:\ell_0 \to \R$ takes non-negative values,
\item [(ii)] $\dabs{x}_1 = 0 \ \ra\ \sum^\infty_{i=1} \abs{x_i} = 0 \ \ra \ \forall i,\ x_i = 0 \ \ra \ x = 0$,
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm x}_1 = \sum^\infty_{i=1} \abs{\lm x_i} = \abs{\lm} \sum^\infty_{i=1} \abs{x_i} = \abs{\lm}\cdot \dabs{x}_1$,
\item [(iv)] and
\be
\dabs{x+y}_1 = \sum^\infty_{i=1} \abs{(x+y)_i} = \sum^\infty_{i=1} \abs{x_i+y_i} \leq \sum^\infty_{i=1} \abs{x_i}+\abs{y_i} = \sum^\infty_{i=1} \abs{x_i} +\sum^\infty_{i=1} \abs{y_i} = \dabs{x}_1 + \dabs{y}_1.
\ee
\een
so $\dabs{\cdot}_1$ is a norm. 

For $n\in \N$ define $x^{(n)}\in \ell_0$ by 
\be
x_t^{(n)} = \left\{\ba{ll}
1 \quad\quad & t\leq n\\
0 & t>n
\ea\right. \ \ra \ \dabs{x^{(n)}}_\infty = 1,\quad \dabs{x^{(n)}}_1 = n.
\ee
Thus, $\forall M>0$, we may choose $n\in \N$ with $n>M$, and then
\be
\dabs{x^{(n)}}_1 > M \dabs{x^{(n)}}_\infty
\ee
So $\dabs{\cdot}_1$ is not Lipschitz equivalent to the uniform norm $\dabs{\cdot}_\infty$.

Note that if $x\in\ell_0$, then $\exists k\in\N$ such that 
\be
\sum_{i>k} \abs{x_i} < 1 \ \ra\ \forall i>k, \ \abs{x_i} <1 \ \ra \ \forall i>k,\ \abs{x_i}^2 < \abs{x_i} \ \ra \ \sum\abs{x_i} \text{ converges (increasing and bounded).}
\ee

We may therefore define the norm $\dabs{\cdot}_2: \ell_0 \to \R$ by
\be
\dabs{x}_2 = \bb{\sum^\infty_{i=1} x_i^2}^{\frac 12}.
\ee
We check \ben
\item [(i)] Clearly $\dabs{\cdot}_2:\ell_0 \to \R$ takes non-negative values,
\item [(ii)] $\dabs{x}_2 = 0 \ \ra\ \sum^\infty_{i=1} x_i^2 = 0 \ \ra \ \forall i,\ x_i = 0 \ \ra \ x = 0$,
\item [(iii)] For all $\lm \in\R$, 
\be
\dabs{\lm x}_2 = \bb{\sum^\infty_{i=1} (\lm x_i)^2}^{\frac 12} = \abs{\lm} \bb{\sum^\infty_{i=1} x_i^2}^{\frac 12} = \abs{\lm}\cdot \dabs{x}_2,
\ee
\item [(iv)] and
\beast
\dabs{x+y}_2 & = & \bb{\sum^\infty_{i=1} ((x+y)_i)^2}^{\frac 12} = \bb{\sum^\infty_{i=1} (x_i+y_i)^2}^{\frac 12} = \bb{\sum^\infty_{i=1} x_i^2 +y_i^2 + 2x_iy_i}^{\frac 12}\\
& \leq & \bb{\sum^\infty_{i=1} x_i^2 +y_i^2 + 2\sqrt{\sum^\infty_{i=1}x_i^2 \sum^\infty_{i=1}y_i^2}}^{\frac 12} \quad\quad (\text{by Cauchy-Schwarz inequality}) \\
& = & \bb{\bb{\sqrt{\sum^\infty_{i=1}x_i^2} + \sqrt{\sum^\infty_{i=1}y_i^2} }^2}^{\frac 12} = \sqrt{\sum^\infty_{i=1}x_i^2} + \sqrt{\sum^\infty_{i=1}y_i^2} = \dabs{x}_2 + \dabs{y}_2.
\eeast
\een
so $\dabs{\cdot}_2$ is a norm. Given $M>0$ we may choose $n\in\N$ with $n>M^2$, since $\dabs{x^{(n)}}_2 = \sqrt{n}$ we have 
\be
\dabs{x^{(n)}}_1 > M\dabs{x^{(n)}}_2,\quad\quad \dabs{x^{(n)}}_2 > M\dabs{x^{(n)}}_\infty
\ee
so $\dabs{\cdot}_2$ is Lipschitz equivalent to neither $\dabs{\cdot}_1$ nor $\dabs{\cdot}_\infty$.

In fact, for all $p\geq 1$ we observe as above that $\abs{x_i}^p$ converges (since $\abs{x_i}<1$), and hence we may define the map $\dabs{\cdot}_p:\ell_0\to\R$ by
\be
\dabs{x}_p = \bb{\sum^\infty_{i=1}\abs{x_i}^p}^{\frac 1p}.
\ee
We claim that $\dabs{\cdot}_p$ is a norm. Assuming this, whenever $p>q\geq 1$, Given $M>0$, we may choose $n\in\N$ with $n> M^{pq/(p-q)}$, then we have
\be
\dabs{x^{(n)}}_p = n^{\frac 1p}, \quad \dabs{x^{(n)}}_q = n^{\frac 1q} \ \ra \ \dabs{x^{(n)}}_q/\dabs{x^{(n)}}_p = n^{\frac 1q - \frac 1p} = n^{\frac {p-q}{pq}} > M \ \ra \ \dabs{x^{(n)}}_q > M \dabs{x^{(n)}}_p.
\ee
Thus $\dabs{\cdot}_p$ is not Lipschitz equivalent to $\dabs{\cdot}_q$. Thus the uncountably many norms $\dabs{\cdot}_p$ for $p\geq 1$ have the property that no two are Lipschitz equivalent.

It remains to check that $\dabs{\cdot}_p $ is a norm. The only property which is not obvious is the triangle inequality. Take $p$ and write $q=\frac p{p-1}$ so that $\frac 1p + \frac 1q = 1$. We first show the Young inequality:

Since the function $\log$ is concave, for all $a,b>0$ we have
\be
\log ab = \tfrac 1p \log a^p + \tfrac 1q \log b^q \leq \log \bb{\tfrac 1p a^p + \tfrac 1q b^q} \ \ra \ ab \leq \tfrac 1p a^p + \tfrac 1q b^q.
\ee

Next we use this to prove H\"older's inequality: given $x,y\in \ell_0$ we have
\be
\frac{\abs{\sum^\infty_{k=1} x_ky_k}}{\dabs{x}_p\dabs{y}_q} \leq \frac{\sum^\infty_{k=1} \abs{x_k}\cdot\abs{y_k}}{\dabs{x}_p\dabs{y}_q} \leq \sum^\infty_{k=1} \frac{\abs{x_k}}{\dabs{x}_p} \cdot \frac{\abs{y_k}}{\dabs{y}_q} \leq \frac 1p \sum^\infty_{k=1} \frac{\abs{x_k}^p}{\dabs{x}_p^p} + \frac 1q\frac{\abs{y_k}^q}{\dabs{y}_q^q} = \frac 1p + \frac 1q = 1.
\ee
Thus,
\be
\abs{\sum^\infty_{k=1}x_ky_k}\leq \dabs{x}_p\dabs{y}_q = \bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}\bb{\sum^\infty_{k=1}\abs{y_k}^q}^{\frac 1q}
\ee

Finally, given $x,y\in \ell_0$, for all $k\in\N$ we have
\be
\abs{x_k+y_k}^p = \abs{x_k + y_k} \cdot \abs{x_k + y_k}^{p-1} \leq \abs{x_k}\cdot \abs{x_k + y_k}^{p-1} + \abs{y_k}\cdot \abs{x_k + y_k}^{p-1}.
\ee

By H\"older's inequality, we have
\be
\sum^\infty_{k=1}\abs{x_k}\cdot \abs{x_k + y_k}^{p-1} \leq \bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p} \bb{\sum^\infty_{k=1}\abs{x_k+y_k}^{(p-1)q}}^{\frac 1q},
\ee
\be
\sum^\infty_{k=1}\abs{y_k}\cdot \abs{x_k + y_k}^{p-1} \leq \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p} \bb{\sum^\infty_{k=1}\abs{x_k+y_k}^{(p-1)q}}^{\frac 1q}.
\ee
Thus,
\beast
\sum^\infty_{k=1}\abs{x_k + y_k}^{p} & \leq & \bb{\bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}+ \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p}} \bb{\sum^\infty_{k=1}\abs{x_k+y_k}^p}^{\frac 1q}\\ 
\ra \ \bb{\sum^\infty_{k=1}\abs{x_k + y_k}^{p}}^{1-\frac 1q} & \leq & \bb{\bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}+ \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p}} \\
\ra \ \bb{\sum^\infty_{k=1}\abs{x_k + y_k}^{p}}^{\frac 1p} & \leq & \bb{\bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}+ \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p}} \\
\ra \ \dabs{x+y}_p & \leq & \dabs{x}_p + \dabs{y}_p
\eeast
as require.



\end{solution}

\begin{problem}Let $V$ be a normed space in which every bounded sequence has a convergent subsequence.
\ben
\item [(a)] Show that $V$ must be complete.
\item [(b)] Show further that $V$ must be finite-dimensional.
\een
[Hint for (b): Show first that for every finite-dimensional subspace $V_0$ of $V$ there exists an $x \in V$ with $\dabs{x + y} > \dabs{x}/2$ for each $y \in V_0$.]



\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(a)] Let $x_1,x_2,\dots$ be a Cauchy sequence in $V$. Take $\ve=1$, then $\exists N\in\N$ such that 

\be
\forall n,m>N, \quad \dabs{x_n-x_m} <1 \ \ra\ \forall n >N,\quad \dabs{x_n} \leq \dabs{x_n-x_{N+1}} + \dabs{x_{N+1}} < \dabs{x_{N+1}} + 1.
\ee
Thus, the sequence $(x_n)$ is bounded by $\max\{\dabs{x_1},\dots,\dabs{x_N},\dabs{x_{N+1}}+1\}$, so it has a convergent subsequenct $(x_{n_i})$ with limit $x\in V$. 

Now given $\ve>0$, $\exists M\in \N$ such that 
\be
\forall n,m>M,\quad \dabs{x_n-x_m}< \frac {\ve}2.
\ee
As $x_{n_i}\to x$, we may choose $n_i > M$ such that 
\be
\dabs{x_{n_i}-x} < \frac {\ve}2.
\ee
Then $\forall m>M$,
\be
\dabs{x_m-x} \leq \dabs{x_m - x_{n_i}} + \dabs{x_{n_i}-x} < \frac {\ve}2 + \frac{\ve}2 = \ve \ \ra \ x_m \to x \ \ra\ V \text{ is complete.}
\ee

\item [(b)] Let $V$ be an infinite-dimensional normed space. Suppose we have chosen $x_1,\dots,x_k\in V$ with $\dabs{x_i} = 1$ for all $i$ and $x_i$ has the bases
\be
\bb{1,0,0,\dots},\quad \bb{0,1,0,\dots}, \quad \dots, \quad (0,0,\dots,0,\underbrace{1}_{i\text{th}},0,\dots).
\ee
Let $S$ be the subspace spanned by $x_1,\dots,x_k$, and choose $y\in V\bs S$. Let
\be
r=\inf\{\dabs{y-s}:s\in S\},
\ee
then as $S$ is closed subspace (any finite dimensional subspace of a normed vector space is closed), we have $r>0$. Choose $s\in S$ with $r\leq \dabs{y-s}< 2r$ and set 
\be
x_{k+1} = \frac {y-s}{\dabs{y-s}} \ \ra \ \dabs{x_{k+1}} = 1.
\ee

Then, for all $i\leq k$, since $x_i \in S$ we have
\be
s+ \dabs{y-s}x_i \in S \quad \quad (\text{since $S$ is spanned by $x_1,\dots,x_k$}).
\ee
so that 
\be
\dabs{x_{k+1}-x_i} = \dabs{\frac {y-s}{\dabs{y-s}}-x_i} = \frac 1{\dabs{y-s}} \dabs{y- (s+\dabs{y-s}x_i)} \leq \frac 1{\dabs{y-s}}r > \frac 12.
\ee

Thus the sequence $(x_n)$ bounded, but for all $i<j$ we have $\dabs{x_i - x_j} > \frac 12$, so there can be no convergent subsequence. Therefore any normed space in which every bounded sequence has a convergent subsequence must be finite-dimensional.

\een



\end{solution}

\begin{problem}Recall from the lectures the normed space $\ell_2$. The Hilbert cube is the subset of $\ell_2$ consisting of all the sequences $(x_n)^\infty_{n=1}$ such that for each $n$, $\abs{x_n} \leq 1/n$. Show that the Hilbert cube is closed in $\ell_2$, and that it has the Bolzano-Weierstrass property, that is, any sequence in the Hilbert cube has a convergent subsequence. (So the Hilbert cube is \emph{compact}.)



\end{problem}

\begin{solution}[\bf Solution.]Let $H$ be the Hilbert cube, and take $(x_n)\in \ell_2\bs H$. Then there exists $k\in\N$ with $\abs{x_k}>\frac 1k$. Let 
\be
\ve = \frac 12 \bb{\abs{x_k}-\frac 1k},
\ee
then if $(y_n)\in \ell_2$ with $\abs{(y_n)-(x_n)}_2 < \ve$, then
\be
\ve^2 > \sum^\infty_{k=1}\abs{y_k -x_k}^2 \geq \abs{y_k - x_k}^2 \ \ra \ \abs{y_k - x_k} < \ve.
\ee
Thus,
\be
\abs{y_k}\geq \abs{x_k} - \abs{y_k - x_k} > \abs{x_k} -\ve = \frac 12 \bb{\abs{x_k}+\frac 1k} > \frac 1k \ \ra \ (y_n) \in \ell_2\bs H.
\ee
Therefore, $\ell_2\bs H$ is open, and hence $H$ is closed in $\ell_2$.

Let $(x_n^{(m)})$ be any sequence in $H$ (bounded). As previous question we may obtain a subsequence $(x_n^{m_j})$ which converges in every co-ordinate. For each $n\in \N$, let 
\be
\lim_{j\to \infty} x_n^{(m_j)} = x_n,
\ee
then as $\abs{x_n^{(m_j)}} \leq \frac 1n$ for all $j$, we have $\abs{x_n}\leq \frac 1n$. Thus $(x_n)\in H$.

Given $\ve>0$, take $N\in\N$ such that 
\be
\sum_{n>N} \frac 1{n^2} < \frac {\ve^2}8.
\ee

For each $n\leq N$ choose $M_n\in \N$ such that
\be
j>M_n \ \ra \ \abs{x_n^{(m_j)}-x_n} < \frac {\ve}{\sqrt{2N}},
\ee
and let $M = \max\{M_1,M_2,\dots, M_N\}$. Then if $j>M$, we have
\beast
\bb{\dabs{(x_n^{(m_j)})-(x_n)}}^2 & = & \sum^N_{n=1}\bb{x_n^{(m_j)}-x_n}^2 + \sum_{n>N}\bb{x_n^{(m_j)}-x_n}^2 \leq \sum^N_{n=1}\bb{x_n^{(m_j)}-x_n}^2 + \sum_{n>N}\bb{\abs{x_n^{(m_j)}}+\abs{x_n}}^2 \\
& < & N\cdot \bb{\frac{\ve}{\sqrt{2N}}}^2 + \sum_{n>N} \bb{\frac 2{n}}^2 < \frac {\ve^2}2 + \frac {\ve^2}2 = \ve^2.
\eeast
so
\be
\dabs{(x_n^{(m_j)})-(x_n)} < \ve \ \ra \ (x_n^{(m_j)}) \to (x_n).
\ee
Hence $H$ has the Bolzano-Weierstrass property.
\end{solution}



\begin{problem}\label{ques:norm_derivative} 
Let $\dabs{\cdot}$ denote the usual Euclidean norm on $\R^n$. Show that the map sending $x$ to $\dabs{x}^2$ is differentiable everywhere. What is its derivative? Where is the map sending $x$ to $\dabs{x}$ differentiable and what is its derivative?
\end{problem}

\begin{solution}[\bf Solution.]Define $f:\R^n\to \R$ by $f(x) = \dabs{x}^2$. Given $x,h\in\R$ we have
\be
f(x+h)-f(x) = \sum^n_{i=1}(x_i + h_i)^2 - \sum^n_{i=1}x_i^2 = \sum^n_{i=1} 2x_ih_i + \sum^n_{i=1}h_i^2.
\ee
Define $\alpha: \R^n\to \R$ by $\alpha(h) = 2x\cdot h$, then $\alpha$ is linear and 
\be
\abs{\frac {f(x+h)-f(x)-\alpha(h)}{\dabs{x+h-x}}} = \frac{\dabs{h}^2}{\dabs{h}} = \dabs{h} \to 0 \quad \text{as }h\to 0.
\ee

Thus, $f$ is differentiable at $x$ and the derivative is $D_xf(h) = 2x\cdot h$.

Now define $g:\R^n\to\R$ by $g(x) = \dabs{x}$. Then $g = j\circ f$ where $j:\R\to\R$ is defines by $j(r) = \sqrt{\abs{r}}$. We know that if $r>0$ then $j$ is differentiable at $r$ with $Dj(r)= \frac 1{2\sqrt{r}}$. Thus if $x\in\R^n\bs\{0,\dots,0\}$, by the chain rule $g$ is differentiable with
\be
D_xg(h) = D_{f(x)}j \cdot \bb{D_xf(h)} = \bb{D_{\dabs{x}^2}j} \cdot \bb{D_xf(h)} = \frac 1{2\dabs{x}} 2x\cdot h = \frac {x\cdot h}{\dabs{x}}
\ee

However, $g$ is not differentiable at $(0,\dots,0)$. If it had derivative $\alpha:\R^n\to \R$ with $\alpha(h) = \sum^n_{i=1} \lm_i \frac{h_i}{\dabs{h}}$, choosing $h= (\delta,0,\dots,0)$ gives
\be
\abs{\frac{g(h)-g(0)- \alpha(h)}{\dabs{h-0}}} = \abs{1-\lm_1 \frac{\delta}{\abs{\delta}}},
\ee
and for this to tend to 0 as $\delta \to 0^+$ we need $\lm_1 = 1$, but then it does not tend to 0 as $\delta \to 0^-$.
\end{solution}

\begin{problem}At which points of $\R^2$ are the following functions $\R^2 \mapsto \R$ differentiable?
\ben
\item [(i)] $f(x, y) = \left\{\ba{ll} x/y \quad\quad & y \neq 0,\\ 0 & y = 0.\ea\right.$
\item [(ii)] $f(x, y) = \abs{x}\abs{y}$.
\item [(iii)] $f(x, y) = xy \abs{x - y}$.
\item [(iv)] $f(x, y) = \left\{\ba{ll} xy / \sqrt{x^2 + y^2}\quad\quad & (x, y) \neq (0, 0),\\ 0 & (x, y) = (0, 0). \ea\right.$
\item [(v)] $f(x, y) = \left\{\ba{ll} xy \sin(1/x) \quad\quad & x \neq 0, \\ 0 & x = 0.\ea\right.$
\een



\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] At $(x,y)$ with $y\neq 0$, $f$ is clearly differentiable, at $(x,0)$ with $x\neq 0$, $f(x,h) = \frac xh \nrightarrow 0$ as $h\to 0$, so $f$ is not even countinuous. At $(0,0)$, $f(h,h)=1 \nrightarrow 0$ as $h\to 0$, so again $f$ is not even continuous.
\item [(ii)] At $(x,y)$ with $x,y\neq 0$, write $\abs{x} = ex$, $\abs{y} = e'y$ with $e,e'\in\{\pm 1\}$, then in a neighourhood of $(x,y)$ we have $f(x,y) = ee'xy$ so $f$ is clearly differentiable. At $(0,y)$ with $y\neq 0$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to\R$ where $\alpha(h,k) = \lm h + \mu k$, then
\be
f(h,y) - f(0,y) = \abs{h}\abs{y}= \lm h + \epsilon(h,0)\abs{h} \ \ra\ \abs{y}-\lm \frac{h}{\abs{h}} = \epsilon(h,0).
\ee
If $h>0$, this forces $\lm = \abs{y}$ while if $h<0$, it forces $\lm = -\abs{y}$, a contradiction. Similarly, at $(x,0)$ with $x\neq 0$, $f$ is not differentiable. At $(0,0)$ it is differentiable with derivative 0, since 
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{f(h,k)}{\dabs{(h,k)}}} = \frac{\abs{h}\abs{k}}{\dabs{(h,k)}} = \frac{\abs{h}\abs{k}}{\sqrt{h^2+k^2}} \leq \frac 12 \bb{\frac{\abs{h}\abs{k}}{\sqrt{h^2}} + \frac{\abs{h}\abs{k}}{\sqrt{k^2}}} = \frac 12\bb{\abs{h}+\abs{k}} \to 0 
\ee
as $(h,k)\to (0,0)$.
\item [(iii)] At $(x,y)$ with $a \neq b$, there is neighbourhood on which $f(x,y) = exy(x-y)$ for some $e\in\{\pm 1\}$, so $f$ is clearly differentiable. At $(x,x)$ with $x\neq 0$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to\R$ where $\alpha (h,k) = \lm h + \mu k$, then
\be
f(x+h,x)-f(x,x) = x(x+h)\abs{x+h-x} - 0 = x(x+h)\abs{h} = \lm h + \epsilon(h,0)\abs{h}.
\ee
So
\be
x^2 + xh - \lm \frac {h}{\abs{h}} = \epsilon(h,0).
\ee
If $h>0$ this forces $\lm = x^2$ while if $h<0$ it forces $\lm = -x^2$, a contradiction. At $(0,0)$ it is differentiable with derivative 0, since
\beast
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} & = & \abs{\frac{hk\abs{h-k}}{\dabs{(h,k)}}} = \frac{\abs{h}\abs{k}\abs{h-k}}{\dabs{(h,k)}} = \frac{\abs{h}\abs{k}\abs{h-k}}{\sqrt{h^2+k^2}} \leq \frac 12 \abs{h-k} \bb{\frac{\abs{h}\abs{k}}{\sqrt{h^2}} + \frac{\abs{h}\abs{k}}{\sqrt{k^2}}} \\
& = & \frac 12\bb{\abs{h}+\abs{k}}\abs{h-k} \to 0 \quad \quad \text{as }(h,k)\to (0,0).
\eeast

\item [(iv)] At $(x,y)\neq (0,0)$, $f$ is clearly differentiable. At $(0,0)$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to \R$ where $\alpha(h,k)=\lm h + \mu k$, then
\be
f(h,0) - f(0,0) = 0 = \lm h + \epsilon(h,0)\abs{h}
\ee
forces $\lm = 0$, and similarly we must have $\mu = 0$, so that $\alpha(h,k)=0$, but
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{hk/\sqrt{h^2+k^2}}{\dabs{(h,k)}}} = \frac{\abs{h}\abs{k}}{h^2+k^2}.
\ee
Without loss of generality, we have $\abs{h}= M\abs{k}$. Thus,
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \frac{\abs{h}\abs{k}}{h^2+k^2} = \frac{M\abs{k}^2}{(1+M^2)k^2} = \frac M{1+M^2} \nrightarrow 0=\alpha(h,k).
\ee
\item [(v)] At $(x,y)$ with $x\neq 0$, $f$ is clearly differentiable. At $(0,y)$ with $y \neq 0$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to\R$ at $(0,y)$, where $\alpha(h,k)= \lm h + \mu k$, then
\be
f(h,y)-f(0,y) = hy\sin \tfrac 1h = \lm h + \epsilon(h,0)\abs{h} \ \ra \ \bb{y\sin \tfrac 1h - \lm}\frac{h}{\abs{h}} = \epsilon(h,0) \ \ra \ \lm = y\sin\tfrac 1h
\ee
which is a contradiction. At $(0,0)$ it is differentiable with derivative 0, since
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{hk\sin \tfrac 1h}{\dabs{(h,k)}}} \leq \frac{\abs{h}\abs{k}}{\dabs{(h,k)}} = \frac{\abs{h}\abs{k}}{\sqrt{h^2+k^2}} \leq \frac 12 \bb{\frac{\abs{h}\abs{k}}{\sqrt{h^2}} + \frac{\abs{h}\abs{k}}{\sqrt{k^2}}} = \frac 12\bb{\abs{h}+\abs{k}} \to 0 .
\ee
\een



\end{solution}

\begin{problem}Let $f(x, y) = x^2y/(x^2 + y^2)$ for $(x, y) \neq (0, 0)$, and $f(0, 0) = 0$. Show that $f$ is continuous at $(0, 0)$ and that it has directional derivatives in all directions there (i.e. for any fixed $\alpha$, the function $t \mapsto f(t \cos \alpha, t \sin \alpha )$ is differentiable at $t = 0$). Is $f$ differentiable at $(0, 0)$?



\end{problem}

\begin{solution}[\bf Solution.]Given $\ve>0$, take $\delta = \ve$, then
\be
\dabs{(x,y)-(0,0)} < \delta \ \ra \ \abs{y}<\ve \ \ra \ \abs{f(x,y)-f(0,0)}= \abs{\frac{x^2y}{x^2 + y^2}} \leq \abs{\frac{x^2y}{x^2}} = \abs{y}< \ve.
\ee
so $f$ is continuous at $(0,0)$. For fixed $\alpha$, let $f_\alpha:\R\to\R$ be the function defined by 
\be
f_\alpha(t) = f(t\cos\alpha,t\sin \alpha) = \frac{t^3\cos^2\alpha\sin\alpha}{t^2} = t\cos^2\alpha\sin\alpha
\ee
then $f_\alpha$ is linear and thus differentiable at $t=0$, i.e., $f$ has directional derivatives in all directions at $(0,0)$. Suppose $f$ had derivative $\alpha:\R^2\to \R$ at $(0,0)$ where $\alpha(h,k) = \lm h + \mu k$, then
\be
f(h,0)-f(0,0) = 0 = \lm h + \epsilon(h,0)\abs{h}
\ee
forces $\lm = 0$, and similarly we must have $\mu = 0$, thus $\alpha = 0$, but (ithout loss of generality, we have $\abs{h}= M\abs{k}$)
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{f(h,k)}{\dabs{(h,k)}}} = \abs{\frac{h^2k/(h^2+k^2)}{\dabs{(h,k)}}} = \abs{\frac{M^2}{(1+M^2)^{\frac 32}}} \nrightarrow 0 \quad\text{as }(h,k)\to (0,0).
\ee
So $f$ is not differentiable at $(0,0)$.



\end{solution}

\begin{problem}We work in $\R^3$ with the usual inner product. Consider the map $f :\R^3 \mapsto \R^3$ given by $f(x) = x/\dabs{x}$ for $x \neq 0$ and $f(0) = 0$. Show that $f$ is differentiable except at 0 and 
\be
D_xf(h) = \frac h{\dabs{x}} - \frac{x(x \cdot h)}{\dabs{x}^3},\quad\quad x \neq 0, \ h \in \R^3
\ee

Verify that $D_xf(h)$ is orthogonal to $x$ and explain geometrically why this is the case.



\end{problem}

\begin{solution}[\bf Solution.]Since $\dabs{f(x)} = 1$ for all $x\neq 0$ but $f(0)=0$, $f$ is not continuous at 0, so it is certianly not differentiable there. Define $g:\R^3\to \R^3$ by $g(x) = \dabs{x}$, $i:\R^3\to\R^3$ by $i(x)=x$, and $k:\R\bs\{0\}$ by $k(r) = \frac 1r$. Then by question \ref{ques:norm_derivative}, $g$ is differentiable on $\R^3\bs\{0\}$ with $D_xg(h) = \frac {x\cdot h}{\dabs{x}}$, and we know that $i$ and $k$ are differentiable at $x$ with
\be
D_xi(h) = h,\quad \quad D k(r) = -\frac 1{r^2}.
\ee

Since on $\R^3\to \{0\}$ we have $f=i \cdot (k\circ g)$, by the chain and product rules $f$ is differentiable at $x$ with
\beast
D_xf(h) = D_xi(h) \cdot (k\circ g)(x) + i(x) \cdot D_{g(x)}k \cdot D_xg(h) = h \frac 1{\dabs{x}} + x \bb{-\frac 1{\dabs{x}^2} \frac {x\cdot h}{\dabs{x}}} = \frac h{\dabs{x}} - \frac{x(x \cdot h)}{\dabs{x}^3}.
\eeast

Thus,
\be
x\cdot D_xf(h) = x\cdot \bb{\frac h{\dabs{x}} - \frac{x(x \cdot h)}{\dabs{x}^3}} = \frac {x\cdot h}{\dabs{x}} - \frac{\dabs{x}^2(x \cdot h)}{\dabs{x}^3} =0.
\ee
so $D_xf(h)$ is orthogonal to $x$. The geometrical reason is that for fixed $x$ the value of $f$ is constant in the direction of $x$.



\end{solution}

\begin{problem}\ben
\item [(i)] Suppose that $f : \R^2 \mapsto \R$ is such that $D_1 f= \partial f/\partial x$ is continuous in some open ball around $(a, b)$, and $D_2 f= \partial f/\partial y$ exists at $(a, b)$. Show that $f$ is differentiable at $(a, b)$.
\item [(ii)] Suppose that $f : \R^2 \mapsto \R$ is such that $\partial f/\partial x$ exists and is bounded near $(a, b)$, and that for a fixed, $f(a, y)$ is continuous as a function of $y$. Show that $f$ is continuous at $(a, b)$.
\een



\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] Take $r>0$ such that $\partial f/\partial x$ exists and is continuous at $(a+h,b+k)$ whenever $\dabs{(h,k)}<r$. Given $\ve>0$, $\exists \delta >0$ with $\delta<r$ such that
\be
\dabs{(h',k')} < \delta \ \ra \ \abs{D_1 f(a+h',b+k')-D_1 f(a,b)} < \ve. \quad (\text{by continuity of }\partial f/\partial x)
\ee
We may write 
\be
f(a,b+k) = f(a,b) + kD_2 f(a,b) + \epsilon_1(k)\abs{k}
\ee
where $\epsilon_1(k)\to 0$ as $k\to 0$. Take $(h,k)$ with $\dabs{(h,k)}<r$, then by the mean value theorem $\exists \theta\in(0,1)$ with
\be
f(a+h,b+k) - f(a,b+k) = hD_1 f(a+\theta h,b+k).
\ee

Thus provided $\dabs{(h,k)}< \delta$ we have
\beast
& & \abs{f(a+h,b+k) -f(a,b) - hD_1 f(a,b) - kD_2 f(a,b) } \\
& = & \abs{\bb{f(a+h,b+k) -f(a,b+k) - hD_1 f(a,b)} + \bb{ f(a,b+k) - f(a,b) -kD_2 f(a,b) }}\\
& = & \abs{\bb{hD_1 f(a+\theta h,b+k) - hD_1 f(a,b)} + \bb{ f(a,b+k) - f(a,b) -kD_2 f(a,b) }}\\
& \leq & \abs{hD_1 f(a+\theta h,b+k) - hD_1 f(a,b)} + \abs{ f(a,b+k) - f(a,b) -kD_2 f(a,b)}\\
& < & \ve \abs{h} + \epsilon_1(k)\abs{k}
\eeast
Since we can make $\ve$ arbitrarily small, we have 
\be
\abs{f(a+h,b+k) -f(a,b) - hD_1 f(a,b) - kD_2 f(a,b) }/\dabs{(h,k)} \to 0
\ee
as $(h,k)\to (0,0)$. Thus $f$ is differetiable at $(a,b)$ with
\be
D_{(a,b)} f(h,k) = hD_1f(a,b) + kD_2f(a,b). 
\ee

\item [(ii)] Take $r>0$ such that $D_1f$ exists and is bounded by $M$ at $(a+h,b+k)$ whenever $\dabs{h,k}<r$. By mean value theorem $\theta \in (0,1)$,
\be
f(a+h,b+k) - f(a,b+k) = h D_1 f(a+\theta h,b+k).
\ee

Given $\ve>0$, $\delta >0$ with $\delta<r$ such that $\abs{k}<\delta$
\be
\abs{f(a,b+k)-f(a,b)} < \frac {\ve}2. \quad (\text{by the continuity as a function of }y)
\ee

Then take $\delta < \frac {\ve}{2M}$, then we have $\abs{(h,k)} < \delta  \ra \ \abs{h},\abs{k}<\delta$, 
\beast
\abs{f(a+h,b+k)-f(a,b)} & \leq & \abs{f(a+h,b+k)-f(a,b+k)} + \abs{f(a,b+k)-f(a,b)} \\
& < & \abs{h D_1 f(a+\theta h,b+k)} + \frac {\ve}2 \leq  \abs{h} M + \frac {\ve}2 < \delta M + \frac {\ve}2 < \frac {\ve}2 + \frac {\ve}2 = \ve.
\eeast
Thus $f$ is continuous at $(a,b)$.
\een



\end{solution}

\begin{problem}Let $M_n = M_n(\R)$ be the space of $n \times n$ real matrices (it can be identified with $\R^{n^2}$). Show that the function $f : M_n \mapsto M_n$ defined by $f(X) = X^2$ is differentiable everywhere in $M_n$. Is it true that $D_Af = 2A$? If not, what is the derivative of $f$?



\end{problem}

\begin{solution}[\bf Solution.]Given $A,H\in M_n$ we have 
\be
f(A+H)-f(A) = (A+H)^2 -A^2 = AH + HA + H^2. 
\ee
Define $\alpha (H) = AH + HA$, then $\alpha$ is linear and
\be
\dabs{\frac{f(A+H)-f(A)-\alpha(H)}{\dabs{H}}} = \dabs{\frac{H^2}{\dabs{H}}} = \dabs{H}\to 0\quad \text{as }H\to 0.
\ee
Thus, $f$ is differentiable at $A$ and 
\be
D_Af(H)= AH + HA.
\ee
(It would not even make sense to say that $D_A f = 2A$, since $D_af$ is linear map $M_n\to M_n$.)



\end{solution}

\begin{problem}Let $A : \R^n \to \R^m$ be a linear map. Recall that the operator norm of $A$ is
\beast
\dabs{A}' = \sup \left\{ \dabs{Ax} : x\in \R^n,\ \dabs{x} \leq 1\right\} & = & \sup \left\{ \frac{\dabs{Ax}}{\dabs{x}}: 0 \neq x \in \R^n\right\} = \inf\{k\in\R: k \text{ is a Lipschitz constant for }A\}.
\eeast

Complete the proof that this defines a norm on the vector space $L(\R^n,\R^m)$ of all linear maps $\R^n \mapsto \R^m$.

Now assume $m = n$ and identify $L(\R^n,\R^n)$ with $M_n(\R)$, the space of $n\times n$ real matrices. Show that if the operator norm of $A \in M_n$ satisfies $\dabs{A}' < 1$, then the sequence $B_k = I +A+A^2+\dots + A^{k-1}$ converges (here $I$ is the identity matrix), and deduce that $I - A$ is then invertible. Deduce that the set $GL_n(\R)$ of all invertible $n\times n$ real matrices is an open subset of $M_n(\R)$.



\end{problem}

\begin{solution}[\bf Solution.]Given $A$, set
\be
S=\left\{ \dabs{Ax} : x\in \R^n,\ \dabs{x} \leq 1\right\}, \quad\quad R = \left\{ \frac{\dabs{Ax}}{\dabs{x}}: 0 \neq x \in \R^n\right\},
\ee
\be
T = \{k\in\R:k \text{ is a Lipschitz constant for }A\}.
\ee

Given $k\in T$, for all $x\in\R^n$ we have $\dabs{Ax} \leq k\dabs{x}$, so 
\be
\dabs{x}\leq 1 \ \ra \ \dabs{Ax}\leq k \ \ra \ \forall s\in S,\ s\leq k \ \ra \ \sup S\leq k
\ee
and 
\be
x\neq 0 \ \ra \ \dabs{A\frac{x}{\dabs{x}}}\leq k \ \ra \ \forall r\in R,\ r\leq k \ \ra \ \sup R\leq k.
\ee
Since this is true for all $k\in T$ and $\sup S\leq \inf T$ and $\sup R \leq \inf T$. 

On the other hand, for all $x\in \R^n\bs\{0\}$ we have $\dabs{\frac{x}{\dabs{x}}}=1$, so that $\dabs{A\frac{x}{\dabs{x}}}\in S$ and hence 
\be
\dabs{A\frac{x}{\dabs{x}}} \leq \sup S \ \ra \ \dabs{Ax} \leq (\sup S)\dabs{x},
\ee
so $\sup S$ is a Lipschitz constant for A, i.e., $\sup S\in T$, and so $\sup S \geq \inf T$. 

Similarly, $\sup R \in T$ and $\sup R\geq \inf T$. 

Thus $\sup S = \sup R = \inf T$ as required.

If we set
Define $\dabs{\cdot}': L(\R^n,\R^m)\to \R$ by $\dabs{A}' = \sup S$.
\ben
\item [(i)] Clearly $\dabs{\cdot}'$ takes non-negative values,
\item [(ii)] \beast
\dabs{A}' = 0 & \ra & \sup\{\dabs{Ax}:\dabs{x}\leq 1\}= 0 \ \ra \ \forall x\in\R^n\bs\{0\}, \ \dabs{A\bb{\frac{x}{\dabs{x}}}}=0\\
& \ra & \forall x\in\R^n, \ \dabs{Ax}=0 \ \ra \ \forall x\in\R^n,\ Ax = 0 \quad(\text{since $\dabs{\cdot}$ is a norm}) \ \ra \ A = 0.
\eeast
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm A}' = \sup\{\dabs{\lm A x}:\dabs{x}\leq 1\} = \abs{\lm} \sup\{\dabs{A x}:\dabs{x}\leq 1\} = \dabs{A}'$,
\item [(iv)] and
\beast
\dabs{A+B}' & = & \sup\{\dabs{(A+B)x}:\dabs{x}\leq 1\} \leq \sup\{\dabs{Ax} + \dabs{Bx}:\dabs{x}\leq 1\}  \\
& \leq & \sup\{\dabs{Ax}:\dabs{x}\leq 1\} + \sup\{\dabs{Bx}:\dabs{x}\leq 1\}  = \dabs{A}' + \dabs{B}'.
\eeast
\een

Now given $A,B\in M_n$, for all $x\in \R^n$, $\dabs{x}\leq 1$ we have
\beast
\dabs{(AB)x} & = & \dabs{A(Bx)} \leq \dabs{A}'\dabs{Bx} \quad\quad (\text{by the set }R) \\
& \leq & \dabs{A}' \dabs{B}' \quad\quad\quad\quad\quad\quad (\text{by the set }S)
\eeast

Thus, we have
\be
\dabs{AB}' \leq \dabs{A}' \dabs{B}'.
\ee

Hence, for all $k\in \N$, we have $\dabs{A^k}' \leq \dabs{A}'^k$. So if $\dabs{A}'<1$, then the sequence
\be
B_k = I + A + A^2 + \dots + A^{k-1} 
\ee
is a Cauchy squence, because given $\ve>0$ we may choose $N\in\N$ with $\dabs{A}'^N < \ve(1-\dabs{A}')$, and then if $m>n\geq N$ we have
\be
\dabs{B_m - B_n}' = \dabs{A^{m-1} + A^{m-1} + \dots + A^n}' \leq \sum^{m-1}_{k=n} \dabs{A^k}'  \leq \sum^{m-1}_{k=n} \dabs{A}'^k < \sum^{\infty}_{k=N} \dabs{A}'^k = \frac{\dabs{A}'^N}{1-\dabs{A}'}< \ve.
\ee

Since $M_n$ is complete it follows that $B_k$ converges and let the limit be $B$. Since 
\be
\dabs{\lim_{k\to \infty} A^k}' = \lim_{k\to\infty} \dabs{A^k}' \leq \lim_{k\to\infty}\dabs{A}'^k = 0,
\ee
we have $\lim_{k\to\infty} A^k = 0$, whence 
\be
(I-A)B = (I-A)\lim_{k\to\infty}B_k = \lim_{k\to\infty} (I-A)B_k = \lim_{k\to\infty} I - A^k = I - \lim_{k\to\infty} A^k = I.
\ee
So $I-A$ is invertible. Thus there is an open ball about $I$ in $GL_n(\R)$. Given $X\in GL_n(\R)$, left multiplication by $X^{-1}$ is a linear and hence continuous map from $M_n$ to itself. 

Thus the preimage of the open ball $\{I-A : \dabs{A}'<1\}$ is an open set, i.e.,, $\{X(I-A):\dabs{A}'<1\}$ is an open set and it contains $X$, and lies in $GL_n(\R)$. Thus each element of $GL_n(\R)$ lies in an open ball in $GL_n(\R)$, so $GL_n(\R)$ is an open subset of $M_n$.



\end{solution}

\begin{problem}We regard $GL_n(\R)$ as an open subset of $M_n(\R) \simeq \R^{n^2}$ (cf. the previous question). Define $g : GL_n(\R) \to M_n(\R)$ by $g(X) = X^{-1}$ for $X \in GL_n(\R)$. Show that $g$ is differentiable at the identity matrix $I \in GL_n(\R)$, and that its derivative there is the map $D_I g(H) = -H$.

Let $A \in GL_n(\R)$. By writing $(A + H)^{-1} = A^{-1}\bb{I + HA^{-1}}^{-1}$, or otherwise, show that $g$ is differentiable at $X = A$. What is $D_Ag$?

Show further that $g$ is twice differentiable at $I$, and find $D^2_I g$ as a bilinear map $M_n \times M_n \to M_n$.



\end{problem}

\begin{solution}[\bf Solution.]From the previous question, we know that $\dabs{H}' < 1 \ \ra \ I+H \text{ is invertible}$, then
\be
(I+H)^{-1}-I^{-1} + H = (I+H)^{-1}\bb{I-I-H+H+H^2}= \bb{I+H}^{-1} H^2 = \epsilon(H)\dabs{H}'
\ee
where $\epsilon(H)\to 0$ as $H\to 0$, so
\be
g(I+H)=g(I)-H + \epsilon(H)\dabs{H}'
\ee
and hence $g$ is differentiable at $I$ with derivative given by $D_I g(H) = -H$.

Now given $A\in GL_n(\R)$, take $\delta >0$ such that $\dabs{H}<\delta \ \ra \ A+H\in GL_n(\R)$ (since $GL_n(\R)$ is an open set), then
\beast
(A+H)^{-1} & = & A^{-1}(I+HA^{-1})^{-1} = A^{-1}\bb{I+HA^{-1}}^{-1}\bb{I-(HA^{-1})^2 + (HA^{-1})^2}\\
& = & A^{-1}(I-HA^{-1} + (I+HA^{-1})^{-1}(HA^{-1})^2) \\
& = & A^{-1} - A^{-1}HA^{-1} + A^{-1}\bb{I + HA^{-1}}^{-1} (HA^{-1})^2\\
& = & A^{-1} - A^{-1}HA^{-1} + \bb{A + H}^{-1} (HA^{-1})^2
\eeast

Since 
\be
\frac 1{\dabs{H}'} \bb{A + H}^{-1} (HA^{-1})^2 \to 0 \quad \text{as }H\to 0,
\ee

$g$ is differentiable at A with derivative 
\be
D_Ag(H) = -A^{-1}HA^{-1}.
\ee

For all $H,K \in M_n$ with $\dabs{H}'<1$ (then $g$ is differentiable at $I+H$), we have
\beast
D_{I+H}g(K) - D_I g(K) & = & -(I+H)^{-1}K(I+H)^{-1} + K = (I+H)^{-1}\bb{-K + (I+H)K(I+H)}(I+H)^{-1} \\
& = &  (I+H)^{-1}\bb{ HK + KH + HKH}(I+H)^{-1}
\eeast

Set $\psi(H,K) = HK + KH$, then
\beast
& & \frac1{\dabs{H}'}\bb{D_{I+H}g(K) - D_I g(K)-\psi(H,K)}\\
& = & \frac1{\dabs{H}'}\bb{(I+H)^{-1}\bb{ HK + KH + HKH}(I+H)^{-1} - (HK+KH)}\\
& = & \frac1{\dabs{H}'}(I+H)^{-1}\bb{ HK + KH + HKH - (I+H)(HK+KH)(I+H)}(I+H)^{-1} \\
& = & \frac1{\dabs{H}'}(I+H)^{-1}\bb{ HK + KH + HKH - (HK + H^2K + KH + HKH )(I+H)}(I+H)^{-1} \\
& = & -\frac1{\dabs{H}'}(I+H)^{-1}\bb{ H^2K + HKH + H^2KH + KH^2 + HKH^2}(I+H)^{-1} \to 0 \quad \text{as }H\to 0.
\eeast

Thus $g$ is twice differentiable at $I$ and $D^2_Ig(H,K) = HK + KH$.



\end{solution}

\begin{problem}\ben
\item [(i)] Define $f : M_n \to M_n$ by $f(X) = X^3$. Find the Taylor series of $f(A + H)$ about $A$.
\item [(ii)] Let again $g : GL_n(\R) \to GL_n(\R)$ be defined by $g(X) = X^{-1}$. Find the Taylor series of $g(I + H)$ about $I$.
\een



\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] Given $A,H\in M_n$ we have
\be
f(A+H)-f(A) = (A^2H + AHA + HA^2) + (AH^2 + HAH + H^2A + H^3).
\ee
Define $\alpha:M_n\to M_n$ by
\be
\alpha (H) = A^2H + AHA + HA^2,
\ee
then $\alpha$ is linear and 
\be
\frac 1{\dabs{H}'}\bb{f(A+H)-f(A)-\alpha (H)} \to 0 \quad \text{as }H\to 0.
\ee
Thus, $f$ is differentiable at $A$ and $D_Af = \alpha$.

Now given $A,H,K\in M_n$ we have
\beast
D_{A+H}f(K) - D_Af(K) & = & (A+H)^2K + (A+H)K(A+H) + K(A+H)^2 - \bb{A^2K + AKA + KA^2}\\
 & = & AHK + HAK + H^2K + HKA + AKH + HKH + KAH + KHA + KH^2 \\
 & = & \bb{AHK + HAK + HKA + AKH + KAH + KHA} + \bb{H^2K + KH^2 + HKH}
\eeast

Define $\psi(H,K) = AHK + HAK + HKA + AKH + KAH + KHA$, then $\psi$ is bilinear and 
\be
\frac 1{\dabs{H}'}\bb{D_{A+H}f(K) - D_Af(K)-\psi(H,K)} = \frac 1{\dabs{H}'}\bb{H^2K + KH^2 + HKH} \to 0 \quad \text{as }H\to 0.
\ee
Thus, $f$ is differentiable at $A$ and $D^2_Af = \psi$.

Finally, given $A,H,K,L\in M_n$ we have
\beast
D^2_{A+H}f(K,L) - D_Af(K,L) & = & (A+H)KL + K(A+H)L + KL(A+H) + (A+H)LK + L(A+H)K + LK(A+H)\\ 
& & \quad -\bb{AKL + KAL + KLA + ALK + LAK + LKA }\\
 & = & HKL + KHL + KLH + HLK + LHK + LKH .
\eeast

Define $\chi(H,K,L) = HKL + KHL + KLH + HLK + LHK + LKH$, then $\chi$ is trilinear and 
\be
\frac 1{\dabs{H}'}\bb{D^2_{A+H}f(K,L) - D^2_Af(K,L)-\chi(H,K,L)} = 0.
\ee
Thus, $f$ is thrice differentiable at $A$ and $D^3_Af = \chi$. Hence the Taylor series of $(A+H)^3$ about $A$ is
\beast
f(A+H) & = & f(A) + D_Af(H) + \frac 12 D^2_Af(H,H) + \frac 16 D^3_A f(H,H,H)\\
& = & f(A) + A^2H + AHA + HA^2 + AH^2 + HAH + H^2A + H^3.
\eeast

\item [(ii)] For each $k\in\N$ we define a $k$-linear map $\psi_k:M_n\times M_n\times \dots\times M_n \to M_n$ by 
\beast
\psi_k(H_1,H_2,\dots,H_k) & = & (-1)^k \sum_{\sigma\in S_k}A^{-1}H_{\sigma(1)}A^{-1}H_{\sigma(2)}\dots A^{-1}H_{\sigma(k)}A^{-1} \\
& = & (-1)^k \sum_{\sigma\in S_k}A^{-1} \prod^k_{i=1} \bb{H_{\sigma(i)}A^{-1}}
\eeast
where $\sigma(k)$ is a permutation of $S_k$. We claim that $D^k_Ag = \psi_k$ for all $k\in\N$. By previous question, we know that this is true for $k=1$. For convenience we write $B=A^{-1}$. Note that
\beast
(A+H)^{-1} & = & A^{-1}(I+HA^{-1})^{-1} = A^{-1}\bb{I+HA^{-1}}^{-1}\bb{I-(HA^{-1})^2 + (HA^{-1})^2}\\
& = & A^{-1}(I-HA^{-1} + (I+HA^{-1})^{-1}(HA^{-1})^2) \\
& = & A^{-1} - A^{-1}HA^{-1} + A^{-1}\bb{I + HA^{-1}}^{-1} (HA^{-1})^2\\
& = & A^{-1} - A^{-1}HA^{-1} + \bb{A + H}^{-1} (HA^{-1})^2 \\
& = & B - BHB + (A+H)^{-1}(HB)^2.
\eeast

Thus, ignoring terms involving at least two factors $H_{k+1}$ we have
\beast
& & D^k_{A+H_{k+1}}g(H_1,\dots,H_k) - D^k_{A}g(H_1,\dots,H_k) - \psi_{k+1}(H_1,\dots,H_k) \\
& = & (-1)^k\bb{\sum_{\sigma\in S_k}\bb{(A+H_{k+1})^{-1} \prod^k_{i=1} \bb{H_{\sigma(i)}(A+H_{k+1})^{-1}} - B \prod^k_{i=1} \bb{H_{\sigma(i)}B}} + \sum_{\tau\in S_{k+1}}B \prod^{k+1}_{i=1} \bb{H_{\tau(i)}B}}\\
& =& (-1)^k\bb{\sum_{\sigma\in S_k}\bb{(B - BH_{k+1}B)\prod^k_{i=1} \bb{H_{\sigma(i)}(B - BH_{k+1}B)} - B \prod^k_{i=1} \bb{H_{\sigma(i)}B}} + \sum_{\tau\in S_{k+1}}B \prod^{k+1}_{i=1} \bb{H_{\tau(i)}B}}.
\eeast
The final step is made since the remainders are at least two factors $H_{k+1}$. If we multiply out the products in the first sum, each first term is of the form $B \prod^k_{i=1} \bb{H_{\sigma(i)}B}$ and thus is cancelled, each term involving one factor $BH_{k+1}B$ and $k$ factors $B$ cancels with one in the final sum, and all remaining terms involve at least two factors $H_{k+1}$, so
\be
\frac 1{\dabs{H_{k+1}}'} \bb{D^k_{A+H_{k+1}}g(H_1,\dots,H_k) - D^k_{A}g(H_1,\dots,H_k) - \psi_{k+1}(H_1,\dots,H_k)} \to 0 \quad \text{as }H_{k+1} \to 0.
\ee
Thus, $D^{k+1}_A g = \psi_{k+1}$ as required. Thus the Taylor series of $g(I+H)$ about $I$ is
\beast
\sum^\infty_{k=0}\frac 1{k!}D^k_I g(\underbrace{H,\dots,H}_{k \text{ terms}}) & = & \sum^\infty_{k=0}\frac 1{k!}(-1)^k \sum_{\sigma\in S_k}I^{-1} \prod^k_{i=1} \bb{HI^{-1}} = \sum^\infty_{k=0}\frac 1{k!}(-1)^k k! H^k\\
& = & I - H+ H^2 -H^3 + \dots.
\eeast

\een



\end{solution}

\begin{problem}Show that $\det : M_n \to \R$ is differentiable at the identity matrix $I$ with $(D_I \det)(H) = \tr(H)$. 

Deduce that $\det$ is differentiable at any invertible matrix $A$ with $(D_A \det)(H) = \det(A) \tr(A^{-1}H)$. 

Show further that $\det$ is twice differentiable at $I$ and find $D^2_I \det$ as a bilinear map.



\end{problem}

\begin{solution}[\bf Solution.]Given $H\in M_n$, each term of $\det(I+H)$ other than 1 is either $h_{ii}$ for some $i$ or a product of two or more entries $h_{ij}$, 
\beast
\det\bepm
1+h_{11} & h_{12} & \dots & h_{1n}\\
h_{21} & 1+h_{22} & \dots & h_{2n}\\
\vdots & \vdots & \ddots & \vdots \\
h_{n1} & h_{n2} & \dots & 1+ h_{nn} 
\eepm 
& = & (1+h_{11}) \det\bepm
1+h_{22} & h_{23} & \dots & h_{2n}\\
h_{32} & 1+h_{33} & \dots & h_{3n}\\
\vdots & \vdots & \ddots &\\
h_{n2} & h_{n3} & \dots & 1+ h_{nn} 
\eepm + \underbrace{\sum^n_{i=2} (-1)^{1+i}h_{1i}\det\bepm
h_{21} & \dots\\
\vdots & \dots\\
h_{n1} & \dots
\eepm}_{\text{two or more entries of $h_{ij}$, $(i\neq j)$}}\\
& = & (1+h_{11})(1+h_{22}) \det\bepm
1+h_{33} & h_{34} & \dots & h_{3n}\\
h_{43} & 1+h_{44} & \dots & h_{4n}\\
\vdots & \vdots & \ddots &\\
h_{n3} & h_{n4} & \dots & 1+ h_{nn} 
\eepm + \underbrace{\text{two or more entries of $h_{ij}$}}_{i\neq j}\\
& = & (1+h_{11})(1+h_{22}) \dots (1+h_{nn}) + \underbrace{\text{two or more entries of $h_{ij}$}}_{i\neq j}\\
& = & 1+\underbrace{\bb{h_{11}+h_{22} \dots h_{nn}}}_{\tr H} + \underbrace{\text{two or more entries of $h_{ij}$}}_{i\neq j}
\eeast
thus,
\be
\frac 1{\dabs{H}'}\bb{\det(I+H)-\det I-\tr H} \to 0 \quad\text{as }H\to 0,
\ee
so as $H\to \tr H$ is a linear map we see that $\det$ is differentiable at $I$ with derivative $D_I\det(H) = \tr H$.

Thus, if $A\in GL_n(\R)$ we have
\be
\frac 1{\dabs{H}'} \bb{\det (A+H) -\det A - \det (A) \tr(A^{-1}H)} = \frac 1{\dabs{H}'} \det A \bb{\det (I+A^{-1}H) - \det I - \tr(A^{-1}H)} \to 0 
\ee
as $H\to 0$. So $\det$ is differentiable at $A$ with derivative $D_A \det (H) = \det(A) \tr(A^{-1}H) $.

Now given $H,K\in M_n$ such that $\dabs{H}'<1 \ \ra \ I+H\in GL_n(\R)$ (since $GL_n(\R)$ is open), we have
\beast
D_{I+H}\det(K) - D_I \det(K) & = & \det(I+H)\tr((I+H)^{-1}K) - \tr K \\
& = & \tr(\det(I+H)(I+H)^{-1}K) - \tr K \\
& = & \tr((I+H)^*K) - \tr K \\
& = & \tr\bb{\bb{(I+H)^* - I}K}
\eeast
where $X^*$ is the adjugate matrix of $X$. Write $H=(h_{ij})$, $K = (k_{ij})$ and work modulo terms involving 2 or more $h_{ab}$. Then if $i\neq j$, without loss of generality, we assume that $i<j$, the element corresponding to $C_{ij} = (-1)^{i+j}M$ (where $M$ is determinant of the $(n-1)\times(n-1)$ matrix that results from deleting row $i$ and column $j$ of $H$) is 
\beast
(-1)^{i+j} \det\bepm
\underline{1+h_{11}} & h_{12} & \dots & \dots & \dots & \dots & h_{1,j-1} & h_{1,j+1} & \dots & h_{1n}\\
h_{21} & \underline{1+h_{22}} & \dots & \dots & \dots & \dots & h_{2,j-1} & h_{2,j+1} & \dots & h_{2n}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots &\\
h_{i-1,1} & h_{i-1,2} & \dots & \underline{1+ h_{i-1,i-1}} & \dots & \dots & h_{i-1,j-1} & h_{i-1,j+1} & \dots & h_{i-1,n}\\
h_{i+1,1} & h_{i+1,2} & \dots & h_{i+1,i-1}  & \underline{h_{i+1,i}} & 1 + h_{i+1,i+1} &  h_{i+1,j-1} &  h_{i+1,j+1} & \dots & h_{i-1,n}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots &\\
h_{j-1,1} & h_{j-1,2} & \dots & h_{j-1,i-1} &  \dots & \dots & 1+h_{j-1,j-1} & h_{j-1,j+1} & \dots & h_{j-1,n}\\
h_{j,1} & h_{j,2} & \dots & h_{j,i-1} &  h_{ji}  & \dots & \underline{h_{j,j-1}} & h_{j,j+1} & \dots & h_{j,n}\\
h_{j+1,1} & h_{j+1,2} & \dots & h_{j+1,i-1}  & h_{i+1,i} &  \dots & h_{j+1,j-1} &  \underline{1 + h_{j+1,j+1}} & \dots & h_{j-1,n}\\
\vdots & \vdots & \ddots &\ddots &\ddots &\ddots &\ddots &\ddots &\ddots &\\
h_{n1} & h_{n2} & \dots & \dots & \dots &  \dots & \dots & \dots &\dots & \underline{1+ h_{nn}}
\eepm 
\eeast
where the underlined terms are the diagonal elements of matrix. Thus, its value is 
\be
(-1)^{i+j}(1+h_{11})\dots (1+ h_{i-1,i-1}) (1 + h_{j+1,j+1})\dots (1+ h_{nn})
\det\bepm
\underline{h_{i+1,i}} & 1 + h_{i+1,i+1} &  \dots & h_{i-1,n}\\
\vdots & \vdots & \vdots & \vdots \\
h_{j-1,i} & \dots & \dots & 1+h_{j-1,j-1} \\
h_{ji}  & \dots & \dots & \underline{h_{j,j-1}}
\eepm 
\ee
which is
\be
(-1)^{i+j} (-1)^{(j-i)+1} h_{ji} = -h_{ji}.
\ee

If $i = j$ we have
\be
C_{ii} = 1 + (h_{11}+h_{nn}) = 1 + \tr H - h_ii
\ee

so $(I+H)^* = C^T$ and
\be
\bb{(I+H)^* - I}_{ij} = \left\{\ba{ll}
\tr H - h_{ii} \quad\quad & i = j\\
-h_{ij} & i\neq j
\ea\right.
\ee

Then we have
\beast
\tr\bb{\bb{(I+H)^* - I}K} & = & \sum_{i,j} \bb{(I+H)^*-I)_{ij}k_{ji}} =  \sum_i \bb{\tr H -h_{ii}}k_{ii} - \sum_{j\neq i} h_{ij}k_{ji}\\
& = & \tr H \sum_i k_{ii} - \sum_i h_{ii}k_{ii} - \sum_{j\neq i} h_{ij}k_{ji} = \tr H \tr K - \sum_{ij} h_{ij}k_{ji} = \tr H \tr K - \tr HK.
\eeast

Therefore, define $\psi: M_n\times M_n\to M_n$ by 
\be
\psi(H,K) =  \tr H \tr K - \tr HK,
\ee
then $\psi$ is bilinear and 
\be
\frac1{\dabs{H}'}\bb{D_{I+H}\det(K) - D_I \det(K) - \psi(H,K)} \to 0 \quad\text{as }H\to 0.
\ee

Hence, $\det$ is twice differentiable at $I$ and $D^2_I\det(H,K) = \tr H \tr K - \tr HK$.



\end{solution}



\begin{problem}
Show that there is a continuous square-root function on some neighbourhood of $I$ in $M_n$; that is, show that there is an open ball $B(I; r) \subset M_n$ for some $r > 0$ and a continuous function $g : B(I; r) \to M_n$ such that $g(X)^2 = X$ for all $X \in B(I; r)$.

Is it possible to define a square-root function on all of $M_n$? What about a cube-root function?
\end{problem}

\begin{solution}[\bf Solution.]
Define $f:M_n \to M_n$ by $f(A) = A^2$. By previous question, we have 
\be
D_Af(H) = AH + HA.
\ee

Thus, the map sending $A$ to $D_Af$ is linear and hence continuous. Since $D_If(H) = 2H$, the derivative $D_If$ is an isomorphism $M_n\to M_n$. Thus, by the inverse function theorem $f$ is a local $C^1$-diffeomorphism at $I$, i.e., there exists an open set $U$ in $M_n$ containing $I$ such that $f:U\to f(U)$ is bijective, $f(U)$ is open an both $f:U\to f(U)$ and $f^{-1}:f(U)\to U$ are continuously differentiable.

Since $f(U)$ is open and $f(I) = I$, $I\in U$, $\exists r>0$ such that $B(I;r) \subseteq f(U)$. Let $g: B(I,r)\to M_n$ be the restriction of $f^{-1}$, then $g$ is certainly continuous and for all $A\in B(I,r)$ we have
\be
f(g(A)) = f(f^{-1}A) = A \ \ra \ g(A)^2 = A.
\ee

It is not possible to define either a square-root or a cube root function on the whole of $M_n$, since not all matrices have square or cube root. To see this, let
\be
A = \bepm
0 & 1 & 0 & \dots & 0 & 0\\
0 & 0 & 1 & \dots & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 1\\
0 & 0 & 0 & \dots & 0 & 0
\eepm \ \ra \ A^{n-1} = \bepm
0 & 0 & 0 & \dots & 0 & 1\\
0 & 0 & 0 & \dots & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 0\\
0 & 0 & 0 & \dots & 0 & 0
\eepm \neq 0, \quad A^n = 0.
\ee

Thus if $B^r = A$ for $r\in\{2,3\}$, then $B^{rn} = A^n=0$, so that if we work over $\C$ then all eigenvalues of $B$ are 0, whence its Jordan normal form consists of blocks with diagonal entries 0. But raising such a matrix to the $n$th power then gives 0, so as $r(n-1)\geq n$ we have $0=B^{r(n-1)} = A^{n-1}$, a contradiction.



\end{solution}

\begin{problem}Define $f : \R^2 \to \R^2$ by $f(x, y) = (x, x^3+y^3-3xy)$ and let $C = \{(x, y) \in \R^2 : x^3+y^3-3xy = 0\}$. Show that $f$ is locally invertible around each point of $C$ except $(0, 0)$ and $(2^{\frac 23}, 2^{\frac 13})$; that is, show that if $(x_0,y_0) \in C \left\backslash \left\{(0, 0), (2^{\frac 23}, 2^{\frac 13})\right\}\right.$ then there are open sets $U$ containing $(x_0, y_0)$ and $V$ containing $f(x_0, y_0)$ such that $f$ maps $U$ bijectively to $V$. What is the derivative of the local inverse function? Deduce that for each point $(x_0,y_0) \in C$ other than $(0, 0)$ and $(2^{\frac 23}, 2^{\frac 13})$ there exist open intervals $I$ containing $x_0$ and $J$ containing $y_0$ such that for each $x \in I$ there is a unique $y \in J$ with $(x, y) \in C$.



\end{problem}

\begin{solution}[\bf Solution.]By computing partial derivatives we have
\be
D_{(x,y)}f(h,k) = (h,3hx^2-3hy + 3ky^2 - 3kx) \ \ra \ D_{(x,y)}f = \bepm
1 & 0 \\
3x^2 - 3y & 3y^2 - 3x
\eepm.
\ee

Therefore 
\be
D_{(x,y)}f \text{ is invertible } \ \lra \ 3y^2 -3x \neq 0 \ \lra \ x \neq y^2.
\ee

Thus if $(x,y)\in C$ and $D_{(x,y)}f$ is not invertible, then 
\be
x^3 + y^3 = 3xy, \quad x = y^2 \ \ra \ y^6 + y^3 = 3y^3 \ \ra \ y^3(y^3-2) = 0 \ \ra \ y =0,\ 2^{\frac 13} \ \ra \ (x,y) = \bb{0,0},\bb{2^{\frac 23},2^{\frac 13}}.
\ee

Therefore by the inverse function theorem if $(x_0,y_0)\in C \left\backslash \left\{(0, 0), (2^{\frac 23}, 2^{\frac 13})\right\}\right.$ then exist open sets $U$ containing $(x_0,y_0)$ and $V$ containing $f(x_0,y_0)$ such that $f:U\to V$ is a bijection with continuously differentiable inverse.

By the chain rule, 
\be
Df^{-1}(f(x,y))= (D_{(x,y)}f)^{-1} = \frac 1{3y^2 - 3x} \bepm
3y^2 - 3x & 0\\
3y - 3x^2 & 1
\eepm.
\ee

Suppose $(x_0,y_0)\in C \left\backslash \left\{(0, 0), (2^{\frac 23}, 2^{\frac 13})\right\}\right.$ and take open sets $U$ and $V$ as above. Then $\exists r>0$ such that $B((x,y);r)\subseteq U$. Let
\be
I' = \bb{x_0-\frac r2,x_0+\frac r2},\quad\quad J = \bb{y_0-\frac r2,y_0+\frac r2}
\ee  
then
\be
(x_0,y_0) \in I' \times J \subseteq U.
\ee

The set $f(I'\times J)\subseteq V$ is open and contains $f(x_0,y_0) = (x_0,0)$, so $\exists r'>0$ such that 
\be
B((x_0,0),r') \subseteq f(I'\times J).
\ee
Let
\be
I = \bb{x_0-\frac {r'}2,x_0+\frac {r'}2}, \quad\quad K = \bb{-\frac {r'}2,\frac {r'}2}
\ee
then
\be
(x_0,0) \in I\times K \subseteq f(I'\times J).
\ee

We know that $f: I\times J \to f(I\times J)$ is a bijection and $I\times K \subseteq f(I\times J)$ (since $f$ does not change first co-ordinates). Thus, $\forall x\in I$ we have
\be
(x,0)\in I\times K \subseteq f(I\times J)
\ee
so there is a unique $y\in J$ such that $f(x,y) = (x,0)$, i.e., we have $(x,y)\in C$.



\end{solution}

\begin{problem}Let $f : \R^2 \to \R$ be a differentiable function and let $g(x) = f(x, c- x)$ where $c$ is a constant. Show that $g : \R \to \R$ is differentiable and find its derivative
\ben
\item [(i)] directly from the definition of differentiability

and also
\item [(ii)] by using the chain rule.
\een

Deduce that if $\partial f/\partial x = \partial f/\partial y$ holds throughout $\R^2$, then $f(x, y) = h(x + y)$ for some differentiable function $h$.



\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] Write 
\be
f(x+h,y+k) = f(x,y) + D_{1,(x,y)}f h + D_{2,(x,y)}f k + \epsilon(h,k)\dabs{(h,k)},
\ee
where $\epsilon(h,k)\to 0$ as $(h,k)\to (0,0)$. Then
\beast
g(x+h)-g(x) & = & f(x+h,c-x-h) - f(x,c-x) = D_{1,(x,c-x)}f h + D_{2,(x,c-x)}f(-h) + \epsilon(h,-h)\abs{h}\\
& = & \bb{D_{1,(x,c-x)}f - D_{2,(x,c-x)}f}h + \epsilon(h,-h)\abs{h},
\eeast
so as $\epsilon(h,-h) \to 0$ as $h\to 0$ we see that $g$ is differentiable and its derivative at $x$ is $D_{1,(x,c-x)}f - D_{2,(x,c-x)}f$.

\item [(ii)] Define $j:R\to \R^2$ by $j(x) = (x,c-x)$, so that $g= f\circ i$. Then $j$  is linear so it is differentiable with derivative given by 
\be
D_xj(h) = (h,-h).
\ee

Thus, $g$ is differentiable, and we have
\beast
D_xg(h) & = & D_{(x,c-x)}f D_xj(h) = D_{(x,c-x)}f(h,-h) \\
& = & D_{1,(x,c-x)}fh + D_{2,(x,c-x)}f(-h) \\
& = & \bb{D_{1,(x,c-x)}f - D_{2,(x,c-x)}f}h.
\eeast
Thus $D_xg = D_{1,(x,c-x)}f - D_{2,(x,c-x)}f$.
\een

Thus, if $D_1f = D_2f$ holds throughtout $\R^2$, then $g$ has derivative 0 and hence is constant (for each value of $c$). Define $h:\R\to\R$ by $h(t) = f(0,t)$, then $h$ is differentiable. Given $(x,y)\in\R^2$, set $c=x+y$ and let $g$ be as given then
\be
f(x,y) = f(x,c-x) = g(x) = g(0) = f(0,c) = f(0,x+y) = h(x+y).
\ee



\end{solution}

\begin{problem}Let $U \subset \R^2$ be an open set that contains a rectangle $[a, b] \times [c, d]$. Suppose that $g : U \to \R$ is continuous and that the partial derivative $\partial g/\partial y$ exists and is continuous. Set $G(y) = \int^b_a g(x, y)dx$. Show that $G$ is differentiable on $(c, d)$ with derivative
\be
G'(y) = \int^b_a (\partial g/\partial y)(x, y)dx.
\ee

Show further that 
\be
H(y) = \int^y_a g(x, y)dx
\ee
is differentiable. What is its derivative $H'(y)$?

[Hint: consider a function $F(y, z) = \int^z_a g(x, y)dx$ before dealing with $H$.]



\end{problem}

\begin{solution}[\bf Solution.]Given $x\in[a,b]$ and $y\in(c,d)$, choose $\delta >0$ such that $(y-\delta ,y+\delta) \subseteq (c,d)$. Then if $\abs{k}<\delta$ we have
\be
g(x,y+k) -g(x,y) = D_{2,(x,y)}g k + \epsilon(k)\abs{k}
\ee
where $\epsilon(k)\to 0$ as $k\to 0$. So 
\be
\frac{G(y+k)-G(y)}{k} = \frac 1{k}\bb{\int^b_a g(x,y+k)dx - \int^b_a g(x,y)dx} = \int^b_a \bb{D_2g(x,y) + \frac{\epsilon(k)\abs{k}}{k}}dx.
\ee

Thus 
\be
\lim_{k\to 0} \frac{G(y+k)-G(y)}{k} = \int^b_a D_2g(x,y) dx = \int^b_a \fp{}{y} g(x, y)dx,
\ee
i.e., $G$ is differentiable at $y$ with derivative $G'(y) = \int^b_a \fp{}{y} g(x, y)dx$.

Now assume that $[c,d]\subseteq [a,b]$ and define $F:[c,d]\times [c,d]\to \R$ by $F(y,z) = \int^z_a g(x,y)dx$. Then by above
\be
D_1F(y,z) = \int^z D_2g(x,y)dx \ \ra \ D_2F(y,z) = g(z,y) \quad\quad (\text{fundamental theorem of calculus})
\ee

Since both partial derivatives exist and are continuous on $[c,d]\times [c,d]$, it follows that $F$ is differentiable on $(c,d)\times (c,d)$.

Now define $j:\R\to \R^2$ by $j(y) = (y,y)$, so that $H=F\circ j$. Then $j$ is linear, so it is differentiable with derivative given by $D_yj(h) = (h,h)$. Thus $H$ is differentiable on $(c,d)$ and we have
\be
D_yH(h) = D_{(y,y)}F (D_y j(h)) = D_{(y,y)}F(h,h) = D_1 F(y,y)h + D_2 F(y,y)h = \bb{\int^y_a D_2g(x,y)dx + g(y,y)}h.
\ee
Thus, $H'(y) = \int^y_a \fp{}{y}g(x,y)dx + g(y,y)$.
\end{solution}


\begin{problem}
\ben
\item [(i)] For each of the following metric spaces $Y$
\be
\text{(a)}\ Y = \R,\quad\quad \text{(b)}\ Y = [0, 2],\quad\quad \text{(c)}\ Y = (1, 3),\quad\quad \text{(d)}\ Y = (1, 2] \cup (3, 4],
\ee
with metric $d(x, y) = \abs{x - y}$, is the set $(1, 2]$ an open subset of $Y$? Is it closed?

\item [(ii)] Suppose that $X$ is a metric space and $A_1,A_2$ are two closed balls in $X$ with radius respectively $r_1, r_2$, such that $r_1 > r_2 > 0$. Can $A_1$ be a proper subset of $A_2$ (i.e. $A_1 \subset A_2$ and $A_1 \neq A_2$)?
\een
\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] \ben 
\item [(a)] Not open. It contains 2 but no open ball about 2.

Not close. The complement contains 1 but no open ball about 1.

\item [(b)] Open. Given $x\in(1,2]$, $B(x;x-1)$ lies in the set.

Not closed. As in (a).

\item [(c)] Not open. As in (a).

Closed. The complement is (2,3), and given $x\in (2,3)$, $B(x;x-2)$ lies in it.

\item [(d)] Open. Given $x\in (1,2]$, $B(x;x-1)$ lies in the set. 

Closed. The complement is $(3,4]$, and given $x\in(3,4]$, $B(x;x-3)$ lies in it.
\een
\item [(ii)] Take $X=[0,\infty)$ with the usual metric. Let $A_1$ be the closed ball of radius 1 centred at 0, and $A_2$ be the closed ball of radius $\tfrac 34$ centred at $\frac 12$, $A_1=[0,1]$ is proper subset of $A_2 = [0,\frac 54]$.
\een



\end{solution}

\begin{problem}\label{ques:open_ball} For each of the following sets $X$, determine whether or not the given function d defines a metric on $X$. In each case where the function does define a metric, describe the open ball $B(x;r)$ for each $x \in X$ and $r > 0$ small.
\ben
\item [(i)] $X = \R^n$; $d(x, y) = \min\{\abs{x_1 -y_1}, \abs{x_2 - y_2},\dots,\abs{x_n - y_n}\}$.
\item [(ii)] $X = \Z$; $d(x, x) = 0$ and for $x \neq y$, $d(x, y) = 2^n$, where $x - y = 2^n a$ with $n$ a non-negative integer and $a$ an odd integer.
\item [(iii)] $X = \Q$; $d(x, x) = 0$ and for $x \neq y$, $d(x, y) = e^{-n}$, where $x - y = 3^{-n}a/b$ for $n, a, b \in \Z$ with both $a$ and $b$ not divisible by 3.
\item [(iv)] $X$ is the set of functions from $\N$ to $\N$; $d(f, f) = 0$ and for $f \neq g$, $d(f, g) = 2^{-n}$ for the least $n$ such that $f(n) \neq g(n)$.
\item [(v)] $X = \C$; $d(z, z) = 0$ and for $z \neq w$, $d(z,w) = \abs{z} + \abs{w}$.
\item [(vi)] $X = \C$; $d(z,w) = \abs{z - w}$ if $z$ and $w$ lie on the same straight line through the origin, $d(z,w) = \abs{z} + \abs{w}$ otherwise.
\een



\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] No. If $x=\bb{1,0,\dots,0}$, $y=\bb{0,0,\dots,0}$ then $x\neq y$ but $d(x,y) = 0$.
\item [(ii)] No. Take $n=0,a=1$, $d(0,1) + d(1,4) = 1 + 1 = 2 < 4 = d(0,4)$.
\item [(iii)] No. Take $x-y = 3^{-2}$ and $x-z = 3^{-1}$, then $y-z = 2\cdot 3^{-2}$. Thus
\be
d(x,z) = e^{-1} > e^{-1} \frac 2{e} = 2e^{-2} = d(x,y) + d(y,z).
\ee
\item [(iv)] Yes. We check
\ben
\item [(a)] $d(f,g)\geq 0$ for all $f,g\in X$. 
\item [(b)] If $d(f,g)=0\ \lra f=g$.
\item [(c)] The symmetry is obvious.
\item [(d)] Given $f,g,h$ the triangle inequality is clear if two of them are equal, while if $d(f,g) = 2^{-n}$ and $d(g,h) = 2^{-m}$ then $f$ and $g$ agree at $1,2,\dots \min(n,m)-1$ so 
\be
d(f,h) \leq 2^{-\min(n,m)} < 2^{-n} + 2^{-m} = d(f,g) + d(g,h).
\ee
\een
We have 
\be
B(f;r) = \{g:g(n)=f(n), \forall n \leq \log_2 \tfrac 1r\}.
\ee
\item [(v)] Yes. We check \ben
\item [(a)] $d(z,w)\geq 0$ for all $z,w\in X$. 
\item [(b)] If $d(z,w)=0\ \lra z=w$.
\item [(c)] The symmetry is obvious.
\item [(d)] Given $z,w,v$ the triangle inequality is clear if two of them are equal, while if $d(z,w) = \abs{z}+\abs{w}$ and $d(w,v) = \abs{w} + \abs{v}$ then \be
d(z,v) = \abs{z} + \abs{v} \leq \abs{z}+\abs{w}+\abs{w} + \abs{v} = d(z,w) + d(w,v).
\ee
\een
We have 
\be
B(0;r) = \{w:\abs{w}<r\}
\ee
while if $z\neq 0$ and $r<\abs{z}$ then $B(z;r) = \{z\}$.
\item [(vi)] Yes. We check
\ben
\item [(a)] $d(z,w)\geq 0$ for all $z,w\in X$. 
\item [(b)] If $d(z,w)=0\ \lra z=w$.
\item [(c)] The symmetry is obvious.
\item [(d)] Given $z,w,v$ the triangle inequality is clear if two of them are equal, or if all three lies on the same straight line through the orgin (by the standard triangle inequality). If $z,w$ do but not $v$ then
\be
d(z,v) = \abs{z} + \abs{v} \leq \abs{z-w} +\abs{w} + \abs{v} = d(z,w) + d(w,v).
\ee
If $v,w$ do but not $z$ the case is similar. If $z,v$ do but not $w$ then
\be
d(z,v) = \abs{z-v} \leq \abs{z}+\abs{v} \leq \abs{z}+\abs{w}+\abs{w} + \abs{v} = d(z,w) + d(w,v).
\ee
\een
If no two do the argument is as in previous question. We have 
\be
B(0;r) = \{w:\abs{w}<r\}
\ee
while if $z\neq 0$ and $r<\abs{z}$ then $B(z;r) = \left\{\lm z: \abs{\lm -1} < \frac r{\abs{z}}\right\}$.

\een



\end{solution}

\begin{problem}Let $d$ and $d'$ denote the usual and discrete metrics respectively on $\R$. Show that all functions $f$ from $\R$ with metric $d'$ to $\R$ with metric $d$ are continuous. What are the continuous functions from $\R$ with metric $d$ to $\R$ with metric $d'$?



\end{problem}

\begin{solution}[\bf Solution.]Given any $f:(\R,d') \to (\R,d)$, $\forall x\in\R$, $\forall \ve >0$ choose $\delta=\frac 12$, then
\be
d'(y,x) < \delta \ \ra \ y = x \ \ra \ d(f(y),f(x)) = 0< \ve,
\ee
so $f$ is continuous. 

Given $f:(\R,d) \to (\R,d')$, continous, if $f$ is not constant, $\exists a,b\in\R$ with 
\be
a<b,\quad f(a) \neq f(b).
\ee
Set $S = \{x>a:f(x)\neq f(a)\}$, then $b\in S$ and $a$ is a lower bound for $S$, $exists s = \inf S$. By continuity, $\exists \delta >0$ such that
\be
d(x,s) < \delta \ \ra \ d'(f(x),f(s)) < \frac 12 \ \ra \ f(x) = f(s).
\ee

If $s\in S$ then
\be
f(s-\tfrac{\delta}2) = f(s) \neq f(a)
\ee
which is contrary to $s$ being a lower bound for $S$, while if $s\notin S$, then $f(s) = f(a)$, so $\forall t\in [0,\delta)$ we have
\be
f(s+t) = f(a) \ \ra \ s+t\notin S \ \ra \ s+\delta \text{ is a lower bound for }S,
\ee
contrary to $s$ being the greatest such. Thus, $f$ must be constant. As constant functions are clearly continuous, the continuous functions $(\R,d)\to (\R,d')$ are the constant functions.



\end{solution}

\begin{problem}\ben
\item [(a)] Show that the intersection of an arbitrary collection of closed subsets of a metric space must be closed.
\item [(b)] We define the \emph{closure} of subset $Y$ of a metric space $X$ to be the smallest closed set $cl(Y)$ containing $Y$. Why does the result of (a) tell us that this definition makes sense?
%if $Y$ is a subset of a metric space $X$, there is a unique closed subset $Z$ of $X$ such that $Z$ contains $Y$ and any closed subset of $X$ containing $Y$ also contains $Z$. The set $Z$ is called the closure of $Y$ in $X$, denoted $\bar{Y}$ or $cl(Y)$.
\item [(c)] Show that
\be
cl(Y) = \{x \in X : x_n \to x \text{ for some sequence $(x_n)$ in }Y \}.
\ee
\een



\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(a)] Let $C_i$ for $i\in I$ be closed sets, then their complements $C_i^c$ are open. Given $x\in \bigcup_{i\in I}C_i^c$, take $i\in I$ such that $x \in C_i^c$, then $\exists r>0$ such that 
\be
B(x;r)\subseteq C_i^c \ \ra \ B(x;r)\subseteq \cup_{i\in I}C_i^c \ \ra \ \bigcup_{i\in I}C_i^c \text{ is open} \ \ra \ \bigcap_{i\in I}C_i = \bb{\bigcup_{i\in I}C_i^c}^c \text{ is closed}. 
\ee
\item [(b)] Given $Y$, let $C_i$ for $i\in I$ be the closed sets containing $Y$, then $\bigcap_{i\in I}C_i$ is closed by (a), contains $Y$, and lies in any closed set containing $Y$, so it is the smallest such.
\item [(c)] Let $S =\{x \in X : x_n \to x \text{ for some sequence $(x_n)$ in }Y \}$. Take $x\in X$. If there exists a sequence $(x_n)$ in $Y$ with $x_n\to x$, given $ve>0$, $\exists N\in\N$ such that
\be
n>N \ \ra \ d(x_n, x) < \ve \ \ra \ x_n \in B(x;\ve),
\ee
so that meets $Y$ and hence $cl(Y)$. Thus the open set $cl(Y)^c$ contains no open ball about $x$, so we must have $x\notin cl(Y)^c$, whence $x\in cl(Y)$ and $S\subseteq cl(Y)$.

Conversely, if $x\in cl(Y)$, then $\forall n \in\N$ the closed set $B(x;\frac 1n)^c$ does not contain $cl(Y)$, so cannot contain $Y$. So $\exists x_n \in B(x;\frac 1n)\cap Y$, and then $(x_n)$ is a sequence in $Y$ with $x_n\to x$. This gives $cl(Y) \subseteq S$.

\een



\end{solution}

\begin{problem}Let $V$ be a normed space, $x \in V$ and $r > 0$. Prove that the closure of the open ball $B(x; r)$ is the closed ball $\{y \in V : \dabs{x - y} \leq r\}$. Give an example to show that, in a general metric space $(X, d)$, the closure of the open ball $B(x; r)$ need not be the closed ball $\{y \in X : d(x, y) \leq r\}$.



\end{problem}

\begin{solution}[\bf Solution.]Let $A_r(x) = \{y \in V : \dabs{x - y} \leq r\}$. To show $A_r(x) \subseteq cl(B(x;r))$. Take $y\in V$. If $\dabs{x-y}\leq r$, define a sequence $(y_n)$ by 
\be
y_n = \frac 1n x + \frac {n-1}n y \ \ra \ \dabs{x-y_n} = \dabs{\frac{n-1}n(x-y)} = \frac {n-1}n \dabs{x-y} \leq \frac {n-1}n r<r \ \ra \ y_n \in B(x;r).
\ee
and
\be
\dabs{y_n -y} = \dabs{\frac 1n(x-y)} = \frac 1n \dabs{x-y} \to 0 \quad \text{as }n\to \infty \ \ra \ y_n \to y.
\ee
Thus by previous question we have $y\in cl(B(x;r))$. 

Now we need to show $cl(B(x;r)) \subseteq A_r(x) $. If $\dabs{x-y}>r$, let $r' = \dabs{x-y}-r$, then
\be
B(y;r')\cap B(x;r) = \emptyset \quad (\text{because if $\dabs{z-y}\leq r'$, then }\dabs{z-x}\geq \dabs{x-y}-\dabs{z-y} \geq \dabs{x-y} - r' = r)
\ee
so $B(y;r')^c$ is a closed set containing $B(x;r)$, so it contains $cl(B(x;r))$, and hence $y \notin cl(B(x;r))$. Thus,
\be
cl(B(x;r)) = \{y \in V : \dabs{x - y} \leq r\} = A_r(x).
\ee

Let $V=\Z$ with the discrete metric. If $r=1$ then $B(x;r)=\{x\}$ which is closed, so $cl(B(x;r))=\{x\}$, while $A_r(x) = V$.



\end{solution}

\begin{problem}Show that the space of real sequences $a = (a_n)$, such that all but finitely many of the $a_n$ are zero, is not complete in the norm defined by $\dabs{a}_1 = \sum^\infty_{n=1} \abs{a_n}$. Is there an obvious 'completion'?



\end{problem}

\begin{solution}[\bf Solution.]Let $\ell_0$ be the space of real sequence with finitely many non-zero terms, with the norm 
\be
\dabs{a}_1 = \sum^\infty_{n=1}\abs{a_n}.
\ee

Define
\be
a_n^{(k)} = \left\{\ba{ll}
\frac 1{2^n} \quad\quad & n\leq k\\
0 & n > k
\ea\right.
\ee
then each sequence $a^{(k)}$ lies in $\ell_0$. Given $\ve>0$, take $M\in \N$ with $\frac 1{2^M} < \ve$, then if $k>l>M$ we have
\be
\dabs{a^{(k)}-a^{(l)}} = \sum^\infty_{n=1} \abs{a_n^{(k)}-a_n^{(l)}} = \sum^k_{n=l+1} \frac 1{2^n} < \sum^\infty_{n=M+1} \frac 1{2^n} = \ve.
\ee

Thus $a^{(k)}$ is a Cauchy sequence in $\ell_0$. Given $a\in X$, take $N\in\N$ with $a_n = 0$ for all $n>N$, and let $\ve = \frac 1{2^{n+2}}$. Then if $k>N$ we have
\be
\dabs{a^{(k)}-a}_1 = \sum^\infty_{n=1}\abs{a_n^{(k)}-a_n} \geq \abs{a_{N+1}^{(k)} - a_{N+1}} = \frac 1{2^{N+1}} > \ve.
\ee

Thus $a$ is not the limit of the sequence $a^{(k)}$, so this Cauchy sequence has no limit in $\ell_0$, whence $\ell_0$ is not complete.

Set 
\be
X = \left\{(a_n): \sum^\infty_{n=1} \abs{a_n} <  \infty \right\},
\ee
so that $X$ contains $\ell_0$. We claim that $X$ is complete. Take a Cauchy sequence $a^{(k)}$ in $X$. For all $n\in \N$ the sequence $a_n^{(k)}$ is then Cauchy, so converges to some $a_n\in \R$. Let $a$ be the sequence whose $n$th term is $a_n$. 

We claim that $a\in X$ and that $a$ is the limit of the sequence $a^{(k)}$.

Take $M\in\N$ such that $k>l\geq M$, then $\sum^\infty_{n=1}\abs{a_n^{(k)}-a_n^{(l)}} < 1$. Let $\sum^\infty_{n=1} \abs{a_n^{(M)}}=R<\infty$. Take $N\in\N$, then for all $n\leq N$ there exists $K_n\in\N$ such that 
\be
k>K_n \ \ra \ \abs{a_n-a_n^{(k)}} < \frac 1N.
\ee

Let $K = \max\{K_1,\dots,K_N,M\} + 1$, then 
\be
\sum^N_{n=1} \abs{a_n - a_n^{(K)}} < N \cdot \frac 1N = 1, \quad\quad \sum^\infty_{n=1}\abs{a_n^{(K)}-a_n^{(M)}} < 1,
\ee
so
\be
\sum^N_{n=1} \abs{a_n} \leq \sum^N_{n=1} \abs{a_n - a_n^{(K)}} + \sum^N_{n=1} \abs{a_n^{(K)}-a_n^{(M)}} + \sum^N_{n=1} \abs{a_n^{(M)}} < 1+ 1+R = R+2.
\ee

Thus, $\sum^\infty_{n=1}\abs{a_n}$ is bounded by $R+2$, and so $a\in X$.

Now choose $M\in\N$ such that 
\be
\sum^\infty_{n=M+1} \abs{a_n} < \frac {\ve}4,\quad\quad \sum^\infty_{n=M+1} \abs{a_n^{(k)}} < \frac {\ve}4. \quad\quad (\text{since }a,a^{(k)}\in X)
\ee

For all $n\leq M$ take $L_n\in \N$ such that $l\geq L_n$, we have
\be
\abs{a_n - a_n^{(l)}} <  \frac {\ve}{4M}.
\ee

Let $l = \max\{L_1,\dots,L_M,K\}+1$, then
\beast
\sum^\infty_{n=1} \abs{a_n - a_n^{(k)}} & = & \sum^M_{n=1} \abs{a_n - a_n^{(k)}} +\sum^\infty_{n=M+1} \abs{a_n - a_n^{(k)}} \\
& \leq & \sum^M_{n=1} \abs{a_n - a_n^{(l)}} + \sum^M_{n=1} \abs{a_n^{(l)} - a_n^{(k)}} + \sum^\infty_{n=M+1} \abs{a_n} + \sum^\infty_{n=M+1} \abs{a_n^{(k)}} \\
& \leq & \sum^M_{n=1} \abs{a_n - a_n^{(l)}} + \sum^\infty_{n=1} \underbrace{\abs{a_n^{(l)} - a_n^{(k)}}}_{\text{Cauchy sequence}} + \sum^\infty_{n=M+1} \abs{a_n} + \sum^\infty_{n=M+1} \abs{a_n^{(k)}} \\
& < & M\cdot \frac{\ve}{4M} + \frac {\ve}4 + \frac {\ve}4 + \frac {\ve}4 = \ve.
\eeast
Thus $a$ is the limit of the sequence $a^{(k)}$ as required.



\end{solution}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\begin{problem}
Use the Contraction Mapping Theorem to show that the equation $\cos x = x$ has a unique real 

Find this solution to some reasonable accuracy using a pocket calculator or the calculator on your computer (remember to work in radians!), and justify the claimed accuracy of your approximation.
\end{problem}

\begin{solution}[\bf Solution.]Since $\abs{\cos x}\leq 1$, $\forall x\in\R$, any solution to $\cos x = x$ must lies in $[-1,1]$. Let $f(x) = \cos x$, then by the mean value theorem, $\forall x,y \in [-1,1]$, $\exists z \in (x,y)$ such that
\be
\abs{f(y)-f(x)} = \abs{f'(z)}\cdot\abs{y-x} = \abs{\sin z}\cdot \abs{y-x} = (\sin 1) \abs{y-x}.
\ee
So $f$ is a contraction on $[-1,1]$ with constant $K =\sin 1 \simeq 0.841$, and hence $f$ has a unique fixed point. Take $x_0 =1$, 
\be
x_n = f(x_{n-1}) \quad \text{for }n\in\N \ \ra\ x_1 = 0.540 \dots \ \ra \ d(x_0,x_1) = 0.46.
\ee
Thus $m>n$, we have
\be
d(x_n,x_m) < d(x_0,x_1) \frac {K^n}{1-K} < 0.46 \frac{0.842^n}{0.158} < 3(0.842)^n,
\ee
so for 3 decimal place accuracy take $n > \frac{\log\bb{\frac{0.001}3}}{\log 0.842} = 46.55\dots$. Thus if we take $x_{47} = 0.739\dots$, this is correct to 3 decimal places. 



\end{solution}

\begin{problem}Let $I = [0,R]$ be an interval and let $C(I)$ be the space of continuous functions on $I$. Show that, for any $\alpha \in \R$, we may define a norm by $\dabs{f}_\alpha = \sup_{x\in I} \abs{f(x)e^{-\alpha x}}$, and that the norm $\dabs{\cdot}_\alpha$ is Lipschitz equivalent to the uniform norm $\dabs{f} = \sup_{x\in I}\abs{f(x)}$.

Now suppose that $\varphi: \R^2 \to \R$ is continuous, and Lipschitz in the second variable $\abs{\varphi(t,x) - \varphi(t, y)} \leq K\abs{x - y}$, for all $t,x,y \in \R$. Consider the map $T$ from $C(I)$ to itself sending $f$ to $y_0+ \int^x_0 \varphi(t,f(t))dt$. Give an example to show that $T$ need not be a contraction under the uniform norm. Show, however, that $T$ is a contraction under the norm $\dabs{\cdot}_\alpha$ for some $\alpha$, and deduce that the differential equation $f' = \varphi(x,f(x))$ has a unique solution on $I$ satisfying $f(0) = y_0$.



\end{problem}

\begin{solution}[\bf Solution.]First, we check
\ben
\item [(i)] Clearly, $\dabs{\cdot}_\alpha$ takes non-negative values.
\item [(ii)] If $\dabs{f}_\alpha = 0 \ \ra \ f=0$.
\item [(iii)] For $\lm \in\R$, 
\be
\dabs{\lm f}_\alpha = \sup_{x\in I} \abs{\lm f(x)e^{-\alpha x}} = \abs{\lm }\sup_{x\in I} \abs{f(x)e^{-\alpha x}} = \abs{\lm}\dabs{f}_\alpha.
\ee
\item [(iv)]
\be
\dabs{f+g}_\alpha = \sup_{x\in I} \abs{(f(x)+g(x))e^{-\alpha x}} \leq \sup_{x\in I} \abs{f(x)e^{-\alpha x}} + \sup_{x\in I} \abs{g(x)e^{-\alpha x}} = \dabs{f}_\alpha + \dabs{g}_\alpha.
\ee
\een
so $\dabs{\cdot}_\alpha$ is a norm.

$\forall x\in I$, we have
\be
\abs{f(x)e^{-\alpha x}} \leq \abs{f(x)}e^{\abs{\alpha} R} \leq e^{\abs{\alpha} R} \dabs{f} \ \ra \ \dabs{f}_\alpha = \sup_{x\in I} \abs{f(x)e^{-\alpha x}} \leq e^{\abs{\alpha} R}\dabs{f}.
\ee
Conversely, $\forall x\in I$, we have
\be
e^{-\abs{\alpha}R}\abs{f(x)} \leq \abs{f(x)e^{-\alpha x}} \leq \dabs{f}_\alpha \ \ra \ e^{-\abs{\alpha}R}\dabs{f} \leq \sup_{x\in I} e^{-\abs{\alpha}R} \abs{f(x)} \leq \dabs{f}_\alpha.
\ee
Thus 
\be
e^{-\abs{\alpha}R}\dabs{f} \leq \dabs{f}_\alpha \leq e^{\abs{\alpha}R}\dabs{f} \ \ra \ \dabs{\cdot}_\alpha \text{ is Lipschitz equivalent to }\dabs{\cdot}.
\ee

Take $R=1$, $\varphi(t,s) = 2s$ (which is continuous and Lipschitz in the second variable), $f(x)=1$, $g(x)=0$ for all $x$. Then $\dabs{f-g} =1$ while 
\be
\dabs{T(f)-T(g)} = \sup_{x\in[0,1]} \abs{\int^x_0 \bb{\varphi(t,f(t)) - \varphi(t,g(t))}dt} = \sup_{x\in[0,1]} 2 \abs{\int^x_0 \bb{f(t) - g(t)}dt} = 2 = 2\dabs{f-g},
\ee
so that $T$ is not a contraction under norm $\dabs{\cdot}$. Now take $\alpha >0$. Given $\varphi$ continuous and Lipschitz in the second variable with Lipschitz constant $K$, $\forall f,g\in C(I)$ we have
\beast
\dabs{T(f)-T(g)}_\alpha & = & \sup_{x\in I} \abs{e^{-\alpha x}\bb{\int^x_0(\varphi(t,f(t))- \varphi(t,g(t))dt}} \leq \sup_{x\in I} \bb{\int^x_0 e^{-\alpha x}\abs{\varphi(t,f(t))- \varphi(t,g(t)}dt}\\
& \leq & \sup_{x\in I} \bb{\int^x_0 e^{-\alpha x}K\abs{f(t)- g(t)}dt} \leq \sup_{x\in I} \bb{\int^x_0 e^{-\alpha x}K\dabs{f-g}_\alpha e^{\alpha t}dt}\\
& \leq & K\dabs{f-g}_\alpha \sup_{x\in I} e^{-\alpha x} \bb{\int^x_0  e^{\alpha t}dt} = \frac K{\alpha}\dabs{f-g}_\alpha \sup_{x\in I} \bb{1-e^{-\alpha x}} < \frac K{\alpha}\dabs{f-g}_\alpha.
\eeast
then $T$ is a contraction under the norm $\dabs{\cdot}_\alpha$. If $f$ is the fixed point of $T$ then
\be
f(x) = (T(f))(x) = y_0 + \int^x_0 \varphi(t,f(t))dt.
\ee

Differentiating gives 
\be
f'(x) = \varphi(x,f(x)), \quad\quad f(0)= y_0.
\ee

Conversely, if $f$ is a solution of $f'(x) = \varphi(x,f(x))$ and $f(0) = y_0$, then the fundamental theorem of calculus gives 
\be
f(x) = y_0 + \int^x_0 f'(t)dt = y_0 + \int^x_0 \varphi(t,f(t))dt.
\ee
So $f$ is a fixed point of $T$. Thus the differential equation $f'(x) = \varphi(x,f(x))$ has a unique solution on $I$ satisfying $f(0)=y_0$.



\end{solution}

\begin{problem}Let $(X,d)$ be a non-empty complete metric space. Suppose $f : X \to X$ is a contraction and $g : X \to X$ is a function which commutes with $f$, i.e. such that $f(g(x)) = g(f(x))$ for all $x \in X$.

Show that $g$ has a fixed point. Must this fixed point be unique?



\end{problem}

\begin{solution}[\bf Solution.]Let $x$ be the unique fixed point of $f$, then $f(g(x)) = g(f(x)) = g(x)$, so $g(x)$ is fixed point of $f$, by uniqueness we must have
\be
g(x) = x,
\ee
i.e., $x$ is a fixed point of $g$. It need not be unique, e.g. $g$ might be the identity map on $X$.



\end{solution}

\begin{problem}Give an example of a non-empty complete metric space $(X,d)$ and a function $f : X \to X$ satisfying $d(f(x), f(y)) < d(x, y)$ for all $x, y \in X$ with $x \neq y$, but such that $f$ has no fixed point.

Suppose now that $X$ is a non-empty closed bounded subset of $\R^n$ with the Euclidean metric. Show that in this case $f$ must have a fixed point. If $g : X \to X$ satisfies $d(g(x), g(y)) \leq d(x, y)$ for all $x, y \in X$, must $g$ have a fixed point?



\end{problem}

\begin{solution}[\bf Solution.]Take $X=[1,\infty)$ with usual metric $d$, and define $f:X\to X$ by 
\be
f(x) = x + \frac 1x.
\ee
Then
\be
d(f(x),f(y)) = \abs{x+\frac 1x - y - \frac 1y} = \abs{x-y - \frac{x-y}{xy}} = \abs{x-y}\bb{1-\frac 1{xy}} < \abs{x-y} = d(x,y),
\ee
but $f$ has no fixed point.

If $X$ is a closed bounded set in $\R^n$, define $h:X\to \R$ by $h(x) = d(x,f(x))$. Given $\ve>0$, if $d(y,x) < \frac {\ve}2$ then
\beast
d(f(y),f(x)) < \frac {\ve}2 \ \ra \ d(y,f(y)) & \leq & d(y,x) + d(x,f(x)) + d(f(x),f(y)) \\
& < & \frac {\ve}2 + d(x,f(x)) + \frac {\ve}2 = d(x,f(x)) + \ve.
\eeast

Similarly, $d(x,f(x)) < d(y,f(y)) + \ve$, so that
\be
\abs{h(y)-h(x)} < \ve.
\ee
Thus $h$ is continuous, so it is bounded and attains its bounds, and hence $\exists x\in X$ such that $\forall y\in X$ we have $h(y)\geq h(x)$. Now if $h(x)>0$ then
\be
h(f(x)) = d(f(x),f(f(x))) < d(x,f(x)) = h(x),
\ee
a contradiction (since $f(x)\in X$), so we must have $h(x) = 0 \ \ra \ d(x,f(x)) = 0 \ \ra\ x=f(x)$, and thus $x$ is a fixed point of $f$. 

Take $X= [-2,-1]\cup [1,2]$ and define $g:X\to X$ by $g(x)=-x$. Then $\forall x,y \in X$ we have
\be
d(g(x),g(y)) = d(x,y),
\ee
but $g$ has no fixed point.



\end{solution}

\begin{problem}\ben
\item [(i)] Suppose that $(X, d)$ is a non-empty complete metric space, and $f : X \to X$ a continuous map such that, for any $x,y \in X$, the sum $\sum^\infty_{n=1} d(f^n(x),f^n(y))$ converges. ($f^n$ denotes the function $f$ applied $n$ times.) Show that $f$ has a unique fixed point.
\item [(ii)] By considering the function $x \mapsto \max \{x - 1, 0\}$ on the interval $[0,1) \subset \R$, show that a function satisfying the hypotheses of (i) need not be a contraction mapping.
\item [(iii)] Give an example to show that the result of (i) need not be true if $f$ is not assumed to be continuous.
\een



\end{problem}

\begin{solution}[\bf Solution.]\ben
\item [(i)] Take $x_0 \in X$ and set $x_n = f^n(x_0)$ for $n\in\N$. Take $\ve>0$, since 
\be
\sum^\infty_{n=1}d(x_n,x_{n+1}) = \sum^\infty_{n=1}d(f^n(x_0),f^n(x_1)) 
\ee
converges, there exists $N\in\N$ such that 
\be
\sum_{n>N} d(x_n,x_{n+1}) < \ve.
\ee

Thus if $k>l>N$, we have 
\be
d(x_k,x_l) \leq \sum^{l-1}_{n=k}d(x_n,x_{n+1})\leq \sum_{n>N} d(x_n,x_{n+1}) < \ve \ \ra \ \text{the sequence $(x_n)$ is Cauchy.}
\ee

As $X$ is complete it therefore converges to some $x\in X$. Since $f$ is continuous we have 

\be 
f(x) = \lim_{n\to \infty} f(x_n) = \lim_{n\to\infty} x_{n+1} = x,
\ee
so that $x$ is a fixed point. If $y \neq x$ were any other fixed point then for all $n\in\N$, we would have
\be
d(f^n(x),f^n(y)) = d(x,y) \ \ra \ \sum^\infty_{n=1} d(f^n(x),f^n(y)) = \sum^\infty_{n=1} d(x,y) ,
\ee
which would not converge, so that $f$ has a unique fixed point.

\item [(ii)] Set $X=[0,\infty)$ with standard metric, and define $f:X\to X$ by $f(x)= \max\{x-1,0\}$. Given $x,y\in X$, we have
\be
d(f(x),f(y)) \leq d(x,y)
\ee
so $f$ is continuous, and if we choose $M\in\N$ with $M>\max{x,y}$, then
\be
f^M(x) = 0 = f^M(y) \ \ra\  \sum^\infty_{n=1}d(f^n(x),f^n(y)) \text{ is finite sum and hence converges.}
\ee
However, $f$ is not a contraction since 
\be
d(f(1),f(2)) = d(0,1) = 1 = d(1,2).
\ee

\item [(iii)] Let $X=[-1,1]$ with standard metric, and define $f: X\to X$ by 
\be
f(x) = \left\{\ba{ll}
\frac x2 \quad\quad x\neq 0\\
1 & x = 0
\ea\right.
\ee
then clearly, $f$ has no fixed point. However, given $x,y\in X$, for all $m\in \N$, we have $f^m(x), f^m(y)\neq 0$ whence ($f(x) \neq 0$)
\be
d\bb{f^{m+1}(x),f^{m+1}(y)} = \abs{f^{m+1}(x)-f^{m+1}(y)} = \abs{f^m(f(x))- f^m(f(y))} = \frac 12 \abs{f^m(x)- f^m(y)}.
\ee

Thus,
\be
\sum^\infty_{n=1} d\bb{f^n(x),f^n(y)} = d(f(x),f(y)) \sum^\infty_{n=0}\frac 1{2^n} = 2d(f(x),f(y)) \leq 3.
\ee
\een




\end{solution}

\begin{problem}\ben
\item [(i)] Let $(X, d)$ be a metric space. For a nonempty subset $Y \subset X$ and $x \in X$ define
\be
d(x, Y) = \inf_{y\in Y} d(x, y).
\ee
Show that for fixed $Y$, the function $x \mapsto d(x, Y)$ defines a continuous function on $X$, and determine the subset of $X$ on which it vanishes.

\item [(ii)] For $Y, Z \subset X$ nonempty, define
\be
d(Y,Z) = \inf_{y\in Y} d(y,Z).
\ee

Show that if $Y$ and $Z$ are closed subsets of $\R^n$, and at least one of $Y$, $Z$ is bounded, then $d(Y,Z) > 0$ iff $Y$ and $Z$ are disjoint. Show that this conclusion can fail if the boundedness condition is removed.
\een



\end{problem}

\begin{solution}[\bf Solution.]



\end{solution}

\begin{problem}A metric $d$ on a set $X$ is called an ultrametric if it satisfies the following stronger form of the triangle inequality:
\be
d(x, z) \leq \max \{d(x, y), d(y, z)\} \text{ for all }x, y, z \in X.
\ee

Which of the metrics in question \ref{ques:open_ball} are ultrametrics? Show that in an ultrametric space 'every triangle is isosceles' (that is, at least two of $d(x, z)$, $d(y, z)$ and $d(x, y)$ must be equal), and deduce that every open ball in an ultrametric space is a closed set. Does it follow that every open set must be closed?



\end{problem}

\begin{solution}[\bf Solution.]The metric in question \ref{ques:open_ball}.(iv) is an ultrametric: in the notation of the answer above we have
\be
d(f,h) \leq 2^{-\min(n,m)} = \max\{2^{-n}, 2^{-m}\} = \max\{d(f,g), d(g,h)\}.
\ee

Those in question \ref{ques:open_ball}.(v) and (vi) are not ultrametric: in both cases 
\be
d(-1,0) = d(0,1) = 1, \quad d(-1,1) = 2.
\ee

Suppose $X$ is an ultrametric space. Take $x,y,z\in X$ and assume wlog that 
\be
d(x,y) \leq d(y,z) \ \ra \ d(x,z) \leq \max\{d(x,y),d(y,z)\} = d(y,z),
\ee
and $d(y,z) \leq \max\{d(y,x),d(x,z)\}$, we must have $d(y,z) \leq d(x,z)$, so $d(y,z) = d(x,z)$. Thus 'every triangle is isosceles', and indeed in any triangle the two longest sides have the same length. 

Now take $x\in X$ and $r>0$. If $y \notin B(x;r)$ then $d(x,y)\geq r$, then given $z\in B(y;r)$ we have
\be
d(z,y)< r\leq d(x,y) \ \ra \ d(x,z) = d(x,y) \geq r  \ \ra \ z\notin B(x;r).
\ee

Hence every point in $B(x;r)^c$ lies in an open ball contained in $B(x;r)^c$, so $B(x;r)^c$ is open, and thus $B(x;r)$ is closed.

It does not follow that every open set must be closed: take $X$ as in question \ref{ques:open_ball}.(iv) and choose $x\in X$, then $\{x\}^c$ is open since if $y\in \{x\}^c$ then $B(y;d(x,y)/2) \subseteq \{x\}^c$. But $\{x\}$ is not open since it contains no open ball, i.e., $\{x\}^c$ is not closed.



\end{solution}

\begin{problem}There is (rumoured to be) a persistent 'urban myth' about the mathematics research student who spent three years writing a thesis about properties of 'antimetric spaces', where an antimetric on a set $X$ is a function $d : X \times X \to \R$ satisfying the same axioms as a metric except that the triangle inequality is reversed (i.e. $d(x, z) \geq d(x, y) + d(y, z)$ for all $x,y,z$). Why would such a thesis be unlikely to be considered worth a Ph.D.?



\end{problem}

\begin{solution}[\bf Solution.]If $a,b\in X$ with $a\neq b$, then
\be
0 = d(a,a) \geq d(a,b) + d(b,a) =  2d(a,b) > 0,
\ee
a contradiction. So $\abs{X} \leq 1$.



\end{solution}

\begin{problem}Let $X$ be the space of bounded real sequences. Is there a metric on $X$ such that a sequence $(x^{(k)})$ in $X$ converges to $x$ in this metric if and only if $(x^{(k)})$ converges to $x$ in every coordinate (i.e. $x^{(k)}_n \to x_n$ in $\R$ for every $n$)? Is there a norm with this property?



\end{problem}

\begin{solution}[\bf Solution.]First, note that if $0\leq p\leq q$, then
\be
p(1+q) = p + pq \leq q + pq = q(1+p) \ \ra \ \frac p{1+p} \leq \frac q{1+q}.
\ee

Then if $0\leq r \leq p+q$ we have
\be
\frac r{1+r} \leq \frac{p+q}{1+p+q} = \frac p{1+p+q} + \frac q{1+p+q} \leq \frac p{1+p} + \frac q{1+q}.
\ee

Thus if we define $\delta:\R^2\to\R$ by 
\be
\delta(a,b) = \frac{\abs{a-b}}{1+\abs{a-b}},
\ee
then $\delta$ satisfies the triangle inequality, and hence $\delta$ is a metric on $\R$ and clearly $\forall a,b\in \R$ we have $\delta(a,b)<1$.

Thus if we define $d:X^2\to\R$ by 
\be
d(x,y) = \sum^\infty_{n=1} \frac 1{2^n}\delta(x_n,y_n),
\ee
it immediately follows that $d$ is metric on $X$. Suppose $x^{(k)}\to x$ in this metric. $\forall n\in\N$, $\forall \ve>0$,  $\exists K\in\N$ such that $\forall k>K$
\be
d(x^{(k)},x) < \frac 1{2^n} \frac {\ve}{1+\ve} \ \ra \  \frac 1{2^n} \frac {\ve}{1+\ve} > \sum^\infty_{m=1}\frac 1{2^m}\delta(x_m^{(k)},x_m) \geq \frac 1{2^n}\delta(x_n^{(k)},x_n) = \frac 1{2^n} \frac{\abs{x_n^{(k)}-x_n}}{1+\abs{x_n^{(k)}-x_n}},
\ee
giving 

\be
\frac {\ve}{1+\ve} > \frac{\abs{x_n^{(k)}-x_n}}{1+\abs{x_n^{(k)}-x_n}} \ \ra \ \abs{x_n^{(k)}-x_n} < \ve.
\ee
Thus, for all $n\in\N$, we have $x_n^{(k)}\to x_n$.

Conversely, assume that for all $n\in\N$ we have $x_n^{(k)}\to x_n$, and $\forall \ve>0$, we can choose $N\in\N$ such that $\frac 1{2^N} < \frac{\ve}2$. For all $n\leq N$, choose $K_n$ such that 
\be
\forall k > K \ \ra \ \abs{x_n^{(k)}-x_n} < \frac {\ve}2,
\ee
and set $K=\max\{K_1,\dots,K_N\}$. Then if $k>N$ we have
\beast
d(x^{(k)},x) & = & \sum^\infty_{n=1} \frac 1{2^n}\delta(x_n^{(k)},x_n) = \sum^N_{n=1}\frac 1{2^n}\delta(x_n^{(k)},x_n) + \sum^\infty_{n=N+1} \frac 1{2^n}\delta(x_n^{(k)},x_n) \\
& < & \sum^N_{n=1}\frac 1{2^n}\abs{x_n^{(k)}-x_n)} + \sum^\infty_{n=N+1} \frac 1{2^n} < \frac {\ve}2 \frac {\frac 12-\frac 1{2^N}}{1-\frac 12} + \frac 1{2^N} < \frac {\ve}2 + \frac {\ve}2 = \ve.
\eeast
Thus, $x^{(k)}\to x$ in the metric $d$.

However, if $\dabs{\cdot}'$ is any norm on $X$, for each $k\in\N$ let $y^{(k)}$ be the sequence with $y_n^{(k)} = \delta_{nk}$ and set $x^{(k)} = \frac{y^{(k)}}{\dabs{y^{(k)}}'}$ so that $\dabs{x^{(k)}}' = 1$. Let $x$ be the zero sequence, then for all $n\in\N$, as $k\to \infty$, we have $x_n^{k} \to 0 = x_n$, but for all $k\in\N$ we have $\dabs{x^{(k)}-x}' = \dabs{x^{(k)}}' = 1$ so that $x^{(k)}\nrightarrow x$. Thus there is no norm on $X$ with the requied property. 



\end{solution}

\begin{problem}Metrics $d$, $d'$ on $X$ are said to be uniformly equivalent if the identity maps $(X, d) \to (X, d')$ and $(X, d') \to (X, d)$ are both uniformly continuous. Give an example of a pair of metrics on $\R$ which are uniformly equivalent but not Lipschitz equivalent. Show that for every metric space $d$
on a set $X$ there exists a metric $d'$ which is uniformly equivalent to $d$ and which is bounded.



\end{problem}

\begin{solution}[\bf Solution.]



\end{solution}

\begin{problem}Let $(X, d)$ be a non-empty complete metric space and let $f : X \to X$ be a function such that for each positive integer $n$ we have
\ben
\item [(i)] if $d(x, y) < n + 1$ then $d(f(x), f(y)) < n$; and
\item [(ii)] if $d(x, y) < 1/n$ then $d(f(x), f(y)) < 1/(n + 1)$.
\een
Must $f$ have a fixed point?



\end{problem}

\begin{solution}[\bf Solution.]Take $x\in X$ and define $x_0 = x$, $x_n = f(x_{n-1})$ for $n\in\N$. Take $n\in\N$ such that 
\be
d(x_0,x_1) < n+1 \ \ra \ d(f(x_0),f(x_1))=d(x_1, x_2) < n \ \ra \ \dots \ \ra \ d(x_n,x_{n+1}) < 1. \quad (\text{by (i)})
\ee
Write $y_i=x_{n+i}$ for $i\geq 0$, so $d(y_0,y_1)<1$. Given $k\in\N$, by (ii) we have
\be
d(y_k,y_{k+1})< \frac 1{k+1} < 1.
\ee
Assume $d(y_k,y_{k+n}) <1$ for some $n\in\N$, then
\be
d(y_k,y_{k+n+1}) \leq d(y_k,y_{k+1}) + d(y_{k+1},y_{k+n+1}) < \frac 1{k+1} + d(f(y_k),f(y_{k+n})) < \frac 1{k+1} + \frac 12 \leq 1.
\ee

Thus by induction $\forall n\in\N$, $d(y_k,y_{k+n}) < 1$, and this is true $\forall k\in\N$ (since $\frac 1{k+1} + \frac 12 \leq 1$).

Now $\forall \ve>0$, $\exists N > \frac 1{\ve} -1$, then $m,n>N$
\be
d(y_{m-N},y_{n-N}) < 1 \ \ra \ d(y_m,y_n) = d\bb{f^N(y_{m-N}),f^N(y_{n-N})} < \frac 1{N+1} < \ve \ \ra \ (y_n) \text{ is a Cauchy sequence}.
\ee

So as $X$ is complete we have $y_n \to y$ for some $y\in X$. As $f$ is Lipschitz with Lipschitz constant 1, it is continuous, so
\be
y_{n+1} = f(y_n) \to f(y) (\text{by continuity})
\ee
Thus by uniqueness of limits we must have $f(y)=y$, i.e., $y$ is fixed point of $f$.
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

