
\chapter{Stochastic Calculus}

\subsection{The bigger picture, in a nutshell}

We will now spend rather a lot of time to give a precise and rigorous construction of the stochastic integral, for as large a class of processes as possible, subject to continuity. This level of generality has a price, which is that the construction can appear quite technical without shedding any light on the sort of processes we are talking about. So before we embark on this journey, here are a few points which, in my opinion, guide the whole construction and should also guide your intuition throughout. What follows is only informal, and in particular, we do not describe issues related to measurability, and finiteness of the integral.

The real difficulty in the construction of the integral is in how to make sense of
\be\label{equ:ito_integral_1}
\int^t_0 H_sdM_s 
\ee
where $M$ is a martingale and $H$ is, say, left-continuous or continuous. Even though $dM$ does not make sense as a measure (the paths of martingales, just like Brownian paths, have too wild oscillations for that), it is easy to cook up a definition which makes intuitive sense when $H$ is a simple process, that is, $H$ takes only finitely many (bounded) values. Indeed, it suffices to require that the integral process in (\ref{equ:ito_integral_1}) on any interval over which $H$ is constant, then the increments of the integral are those of $M$ multiplied by the value of $H$ on this interval. A natural approach is then to try to extend this definition to more general classes of processes by "taking a limit" of integrals
\be\label{equ:ito_integral_2}
\int^t_0 H^n_s dM_s \to \int^t_0 H_sdM_s
\ee
where the integrands in the left-hand side are simple and approximate $H$.

In implementing this method, one faces several technical difficulties. The strategy is to construct a suitable function space where the sequence on the left-hand side of (\ref{equ:ito_integral_2}) forms a Cauchy sequence. If the function space is complete, the sequence of integrals has a limit, which we may call the integral of $H$ with respect to $M$. But we must also guarantee that this limit does not depend on the approximating sequence. It remains to find a space which has the desired properties. The key property which we will use (over and over again) is that martingales have a finite quadratic variation.
\be\label{equ:ito_integral_3}
[M]_t := \lim_{n\to \infty} \sum^{\floor{2^nt}-1}_{k=0} \bb{M_{(k+1)2^{-n}} -M_{k2^{-n}}}^2
\ee
exists and is finite, and is non-decreasing in $t$. Furthermore, one can show (Theorem \ref{thm:quadratic_variation_process_uniqueness_existence}) that $M^2_t - [M]_t$ is a martingale. Now, when $H$ is simple, it is not hard to convince yourself that the integral (\ref{equ:ito_integral_1}) must also be a martingale. So what should be the quadratic variation of $\int^t_0 H_sdM_s$? Based on the approximation (\ref{equ:ito_integral_3}), the amount of quadratic variation that we add to the integral between $t$ and $t+dt$ is approximately $H^2_t d[M]_t$. Hence any sensible definition of the stochastic integral must satisfy
\be\label{equ:ito_integral_4}
\bsb{\int^t_0 H_sdM_s}_t = \int^t_0 H^2_s d[M]_s 
\ee

The key insight of It\^o was the realization that this property was sufficient to define the integral. Indeed, using the optional stopping theorem, this is essentially the same as requiring. 
\be\label{equ:ito_integral_5}
\E\bb{\bb{\int^\infty_0 H_sdM_s}^2} = \E\bb{\int^\infty_0 H^2_s d[M]_s}.
\ee

Interpreting the right-hand side as an $\sL^2$ norm on the space of bounded integrands, this statement is saying that the stochastic integral must be a certain isometry between Hilbert spaces. The left-hand side shows that the correct space of martingales is the set of martingales with $\E\bb{[M]^2_\infty} < \infty$, or, equivalently (as it turns out), martingales which are bounded in $\sL^2$. This space, endowed with the norm on the left-hand side of (\ref{equ:ito_integral_5}) is indeed complete and simple processes are dense in it. Formula (\ref{equ:ito_integral_5}) is then relatively easy to prove for simple processes. This implies, at once, that the sequence in the left-hand side of (\ref{equ:ito_integral_2}) is Cauchy (and hence has a limit), and the isometry property shows that this limit cannot depend on the approximating sequence.

At this point we have finished the construction of the stochastic integral for martingales which are bounded in $\sL^2$. Stopping at suitable stopping times, it is then easy to extend this definition to general martingales, or indeed to processes known as local martingales. Adding a "finite variation" component for which the integral (\ref{equ:ito_integral_1}) is defined as a good old Stieltjes-Lebesgue integral finishes the construction for semi-martingales.

Having spoken about the bigger picture in a nutshell, it is now time to rewind the tape
and go back to the beginning.


%In the next section we will introduce a suitable class of integrands $H$ for a pathwise definition of the stochastic integral 
%\be
%(H \cdot A)(\omega, t) = \int_{(0,t]}H(\omega, s) dA(\omega, s).
%\ee

\begin{example}
\ben
\item [(i)] Brownian motion is previsible by Proposition \ref{pro:previsible}, since it is continuous.
\item [(ii)] A Poisson process $(N_t)_{t\geq 0}$ or, indeed, any other continuous-time Markov chain with discrete state space is not previsible, since $N_t$ is not $\sF_{t^-}$-measurable.
\een
\end{example}


%\subsection{Local martingales}
%\begin{remark}[remark for theorem \ref{thm:local_martingale_indistinguishable_0}]
%\ben
%\item [(i)] In particular Brownian motion is not of finite variation.
%\item [(ii)] This makes it clear that the theory of finite variation integrals we have developed is useless for integrating with respect to continuous local martingales.
%\een
%\end{remark}


Note that as a consequence of Theorem \ref{thm:local_martingale_indistinguishable_0}, the decomposition is unique up to indistinguishability. This is known as the Doob-Meyer decomposition.

%\begin{remark}
%The proof of the last theorem tells us something extremely useful for the following. If $t_k$ is the dyadic subdivision, then the calculation shows that
%\be\label{equ:martingale_quadratic}
%\E(M^2_t) = \E\bb{\sum_{k:t_k\leq t} (M_{t_{k+1}} -M_{t_k} )^2}
%\ee
%so there is good reason to believe that if $M$ is say, bounded in $\sL^2$, then it has finite quadratic variation $Q_t$ and moreover 
%\be
%M^2_t - Q_t
%\ee
%has constant expectation 0. In fact, we will see that this is indeed the case and $M^2_t - Q_t$ is also a martingale.
%\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Stochastic integral


\begin{remark}[quadratic variation]

so the idea of the definition of $H^n$ and then $X^n = H^n \cdot M$ is to take an approximation of $M$ and $M \cdot M$. So the martingale $Y$ in the proof really is $M \cdot M$, and this is why we define $[M] = M^2 - 2Y$.
\end{remark}

$[M^T] = [M]^T$ for any random variable $T$? ($M \in \sM_{c,loc}$)


%{\bf It\^o integrals}
%\begin{remark}
%Note that the Kunita-Watanabe identity $[H \cdot X, Y ] = H \cdot [X, Y ] = [X,H \cdot Y ]$ also holds for continuous semi-martingales.
%\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









We will also occasionally need the following generalization of It\^o's formula which allows us to localise the process in some open set.


\begin{proposition}\label{pro:ito_lemma_c2_function}
Let $D$ be a domain (open and connected subset of $\R^d$) which is a proper subset. Let $f : D \to \R$ be a $\sC^2$ function on $D$. Then if $X$ is a semimartingale such that $X_0 \in D$ almost surely, and if $T = \inf\bra{t \geq 0 : X_t \notin D}$ then we have.
\be
f(X_t) = \int^t_0 \sum^d_{i=1} \fp{f}{x_i}(X_s)dX^i_s + \frac 12 \sum^d_{i,j=1} \int^t_0 \frac{\partial^2f}{\partial x_i\partial x_j} (X_s)d[X^i,X^j]_s
\ee
almost surely for all $t < T$.
\end{proposition}

\begin{proof}[\bf Proof]
We may assume without loss of generality that $X^i \in \sM^2_c$ for each $1 \leq i \leq d$. Let $n \geq 1$ and define $\tau_n = \inf\bra{t \geq 0 : d(X_t,D^c) \leq 1/n}$. Then $\tau_n \leq T$ almost surely and $\tau_n$ is non-decreasing, hence $L = \lim_{n\to\infty} \tau_n$ exists. We have $L \leq T$ by passing to the limit in $\tau_n \leq T$, and we also claim that $L \geq T$. Indeed, since the distance is a continuous function, $d(X_L,D^c) = 0$. Note that $d(X_L,D_c) = \inf_{y\in D^c} d(X_L, y) = \inf_{y\in D^c\cap \overline{B}(X_L,1)} d(X_L, y)$. Since $D$ is open, $D^c\cap \overline{B} (X_L, 1)$ is compact and thus this distance is attained. This means that $X_L \in D^c$ which implies $L \geq T$. Thus $L = T$. Let $\tau'_n = \inf\bra{t \geq 0 : \dabs{X_t} + \sum_{1\leq i,j\leq d} V ([X^i,X^j]_t) \geq n}$, where $V (X)$ denotes the total variation of the process $X$. Let $T_n = \tau_n \land \tau'_n$. Then $T_n < T$ for all $n \geq 0$ and $T_n$ increases towards $T$ almost surely.

Now, let us introduce a sequence of functions $(\varphi_m)_{m\geq1}$ which are $\sC^\infty$-approximations of the identity (such as the Gaussian density with mean 0 and covariance matrix $(1/m)I$.) Consider the function
\be
f_{n,m} = (f\ind_{D_n}) \star \varphi_m
\ee
where $D_n$ is the subdomain $D_n = \bra{x \in D: d(x,D^c) > 1/n}$ and $\star$ denotes the convolution of two functions, i.e., $f \star g(x) = \int_{\R^d} f(y)g(x - y)dy$. Since $\varphi_m$ is $\sC^\infty$, and since convolution is a regularizing operation, the function $f_{n,m}$ is $\sC^\infty$ for all $n,m$. Thus we can apply It\^o's formula to $f_{n,m}$. Stopping at time $T_n$, we get.
\be\label{equ:ito_lemma_stopping_time}
f_{n,m}(X_{t\land T_n}) = \int^{t\land T_n}_0 \sum^d_{i=1} \fp{f_{n,m}}{x_i} (X_s)dX^i_s + \frac 12 \sum^d_{i,j=1} \int^{t\land T_n}_0 \frac{\partial^2f_{n,m}}{\partial x_i\partial x_j} (X_s)d[X^i,X^j]_s.
\ee

However, since $f$ is $\sC^2$ inside $D$, we have for all $x \in D_n$.
\be
\fp{f_{n,m}}{x_i}(x) = \varphi_m \star \fp{f}{x_i} ,\quad\quad \frac{\partial^2f_{n,m}}{\partial x_i\partial x_j} (x) = \varphi_m \star \frac{\partial^2f}{\partial x_i\partial x_j}(x).
\ee

Since $\varphi_m$ is an approximation of the identity, this means that as $m\to \infty$,
\be
\fp{f_{n,m}}{x_i} (x) \to \fp{f}{x_i}(x),\quad\quad \frac{\partial^2f_{n,m}}{\partial x_i\partial x_j} (x) \to \frac{\partial^2f}{\partial x_i\partial x_j(x)}
\ee
pointwise in $D$. This implies that one can take the limit $m \to \infty$ in (\ref{equ:ito_lemma_stopping_time}). Indeed, the second term
\be
\int^{t\land T_n}_0 \frac{\partial^2f_{n,m}}{\partial x_i\partial x_j} (X_s)d[X^i,X^j]_s \to \int^{t\land T_n}_0 \frac{\partial^2f}{\partial x_i\partial x_j}
(X_s)d[X^i,X^j]_s
\ee
converges because of the Lebesgue convergence theorem since each $(X^i)^{T_n} \in \sM^2_c$ ($1 \leq i \leq d$ and $n \geq 1$). To see that the first term also converges, applying the isometry property of the stochastic integral:
\be
\dabs{\bb{\fp{f_{n,m}}{x_i} - \fp{f}{x_i}}(X) \cdot (X^i)^{T_n}}^2_X = \E\bb{\int^\infty_0 \bb{\fp{f_{n,m}}{x_i} - \fp{f}{x_i}}^2(X_s)d[(X^i)^{T_n}]_s}.
\ee

Since for a fixed $n$, $[(X^i)^{T_n}]$ is bounded by $n$, and since $\fp{f_{n,m}}{x_i} (x) \to \fp{f}{x_i}(x)$ pointwise in $x \in D_n$, and these functions are uniformly bounded in $m$, we may apply the Lebesgue dominated convergence theorem, and get that the right-hand side converges to 0. Thus we
get the desired It\^o formula for all $t \leq T_n$ almost surely. Letting $n\to\infty$ finishes the proof. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Applications to Brownian motion and Martingales}

%\subsection{Brownian martingales}



\begin{theorem}[Dubins-Schwarz theorem]
Let $M \in \sM_{c,loc}$ with $M_0 = 0$ and $[M]_\infty = \infty$ a.s.. Set $\tau_s = \inf\bra{t \geq 0 . [M]_t > s}$, $B_s = M_{\tau_s}$. Then $\tau_s$ is an $(\sF_t)_{t\geq0}$-stopping time. If $\sG_s = \sF_{\tau_s}$ then $(\sG_s)_{s\geq0}$ is a filtration and $B$ is a $(\sG_t)_{t\geq0}$-Brownian motion. Moreover $M_t = B_{[M]_t}$.
\end{theorem}

\begin{remark}
So any continuous local martingale is a (stochastic) time-change of Brownian motion. In this sense, Brownian motion is the most general continuous local martingale. 
\end{remark}

\begin{proof}[\bf Proof]
Since $[M]$ is continuous and adapted, $\tau_s$ is a stopping time, and since $[M]_\infty = \infty$ it must be that $\tau_s < \infty$ a.s. for all $s \geq 0$. We start the proof by the following lemma.
\end{proof}

\begin{lemma} 
$B$ is a.s. continuous.
\end{lemma}

\begin{proof}[\bf Proof]
Note that $s \to \tau_s$ is \cadlag and non-decreasing and thus $B$ is \cadlag. So it remains to show $B_{s^-} = B_s$ for all $s > 0$, or equivalently $M_{\tau^{s^-}} = M_{\tau_s}$, where 
\be
\tau_{s^-} = \inf\bra{t \geq 0 : [M]_t = s}
\ee
and note that $\tau_{s^-}$ is also a stopping time. Let $s > 0$. We need to show that $M$ is constant between $\tau_{s^-}$ and $\tau_s$ whenever $\tau_{s^-} < \tau_s$, i.e. whenever $[M]$ is constant. Note that by Theorem \ref{thm:continuous_m2_martingale_implies_ui}, $(M^2 - [M])^{\tau_s}$ is uniformly integrable since $\E([M_{\tau_s} ]_\infty) < \infty$. Hence, by the optional stopping theorem (the uniformly integrable version, see (\ref{thm:optional_stopping_ui_continuous})), we get:
\be
\E\bb{M^2_{\tau_s} - [M]_{\tau_s} |\sF_{\tau_{s^-}}} = M^2_{\tau_{s^-}} - [M]_{\tau_{s^-}}.
\ee

But by assumption, $[M]_{\tau_s} = [M]_{\tau_{s^-}}$ and $M$ is a martingale, we obtain
\be
\E\bb{\left.M^2_{\tau_s} -M^2_{\tau_{s^-}} \right|\sF_{\tau_{s^-}}} = \E\bb{\left.\bb{M_{\tau_s} -M_{\tau_{s^-}}}^2\right|\sF_{\tau_{s^-}} } = 0
\ee
and so $M$ is a.s. constant between $\tau_{s^-}$ and $\tau_s$. This proves that $B$ is almost surely continuous at time $s$. To prove that $B$ is a.s. continuous simultaneously for all $s \geq 0$, note that if $T_r = \inf\bra{t > 0 : M_t \neq M_r}$ and $S_r = \inf\bra{t > 0: [M]_t \neq [M]_r}$ then the previous argument says that for all fixed $r > 0$ (and hence for all $r \in \Q^+$), $T_r = S_r$ a.s. But observe that $T_r$ and $S_r$ are both \cadlag. Thus equality holds almost surely for all $r \geq 0$ and hence almost surely, $M$ and $[M]$ are constant on the same intervals. This implies the result.
\end{proof}

We also need the following lemma.

\begin{lemma}
$B$ is adapted to $(\sG_t)_{t\geq0}$.
\end{lemma}

\begin{proof}[\bf Proof]
It is trivial to check that $(\sG_s)_{s\geq0}$ is a filtration. Indeed, if $S \leq T$ a.s. are two stopping times for the complete filtration ($\sF_t$), and if $A \in \sF_S$, then for all $t \geq 0$, 
\be
A \cap \bra{T \leq t} = (A \cap \bra{S \leq t}) \cap \bra{T \leq t}
\ee
up to zero-probability events. The first event in the right-hand side is in $\sF_t$ by assumption, and the second is also in $\sF_t$ since $T$ is a stopping time. Since ($\sF_t$) is complete, we conclude that $A \in \sF_T$ as well, and hence $\sF_S \subseteq \sF_T$. From this, since $\tau_r \leq \tau_s$ almost surely if $r \leq s$, ($\sG_s$) is a filtration. It thus suffices to show that if $X$ is a \cadlag adapted process and $T$ is a stopping time, then $X_T \ind_{T<\infty}$ is $\sF_T$ -measurable. Note that a random variable $Z$ is $\sF_T$ -measurable if $Z\ind_{T\leq t} \in \sF_t$ for every $t \geq 0$. If $T$ only takes countably many values $\bra{t_k}^\infty_{k=1}$, then
\be
X_T \ind_{T<\infty} = \sum^\infty_{k=1} X_{t_k}\ind_{T=t_k}
\ee
so it is trivial to check that $X_T \ind_{T<\infty}$ is $\sF_T$ -measurable, since every term in the above sum is. In the general case, let $T_n = 2^{-n}\ceil{2^nT}$ where $\ceil{x}$ denotes smallest $n \in \Z^+$ with $n \geq x$. Then $T_n$ is also a stopping time, finite whenever $T < \infty$, and such that $T_n \geq T$ while $T_n \to T$ almost surely. Thus for all $u \geq 0$, and for all $n \geq 1$, $X_{T_n}\ind_{T_n\leq u}$ is $\sF_u$-measurable. Furthermore, by right-continuity of $X$, $\lim_{n\to \infty} X_{T_n}\ind_{T_n\leq u} = X_T \ind_{T<u}$. Thus $X_T \ind_{T<u}$ is $\sF_u$-measurable. Naturally, $X_T \ind_{T=u} = X_u\ind_{T=u}$ is also $\sF_u$-measurable, so we deduce that $X_T \ind_{T\leq u}$ is $\sF_u$-measurable.

Having proved this lemma, we can now finish the proof of the Dubins-Schwarz theorem. Fix $s > 0$. Then $[M^{\tau_s} ]_\infty = [M]_{\tau_s} = s$, by continuity of $[M]$. Thus by Theorem \ref{thm:continuous_m2_martingale_implies_ui}, $M_{\tau_s} \in \sM^2_c$ since $\E\bb{[M^{\tau_s}]_\infty} < \infty$. In particular, $(M_{t\land \tau_s}, s \geq 0)$ is uniformly integrable by Doob's inequality. In particular, we get that $M_{\tau_r} \in \sL^2(\pro)$ for $r \leq s$ (and $s$ was arbitrary). Applying the uniformly integrable version of the optional stopping theorem (see (\ref{thm:optional_stopping_ui_continuous})) a first time, we obtain
\be
\E(M_{\tau_s}|\sF_{\tau_r}) = M_{\tau_r} 
\ee
a.s. and thus $B$ is a $\sG$-martingale. Furthermore, since $M^{\tau_s} \in \sM^2_c$, by Theorem \ref{thm:continuous_m2_martingale_implies_ui}, $\bb{M^2 - [M]}^{\tau_s}$ is also a uniformly integrable martingale. By (\ref{thm:optional_stopping_ui_continuous}) again, for $r \leq s$,
\be
\E(B^2_s - s|\sG_r) = \E\bb{(M^2 - [M])_{\tau_s} |\sF_{\tau_r}} = M^2_{\tau_r} - [M]_{\tau_r} = B^2_r - r .
\ee

Hence, $B \in \sM_c$ with $[B]_s = s$ and so, by L\'evy's characterization, $B$ is a $(\sG_t)_{t\geq0}$-Brownian motion.
\end{proof}

Before we head on to our next topic, here are a few complements to this theorem, given without proof. The first result is a strengthening of the Dubins-Schwarz theorem.

\begin{theorem}[Dubins-Schwarz theorem]
Let $M$ be a continuous local martingale with $M_0 = 0$ a.s. Then we may enlarge the probability space and define a Brownian motion $B$ on it in such a way that
\be
M = B_{[M]_t} \quad \text{ a.s. for all }t \geq 0.
\ee

More precisely, taking an independent Brownian motion $\beta$, if
\be
B_s =\left\{\ba{ll}
M_{\tau_s} & \text{for }s \leq [M]_\infty\\
M_\infty + \beta_{s-[M]_\infty}\quad\quad & \text{for all }s \geq [M]_\infty
\ea\right.
\ee
then $B$ is a Brownian motion and for all $t \geq 0$, $M_t = B_{[M]_t}$.
\end{theorem}

\begin{proof}[\bf Proof]
See Revuz-Yor (Chapter V, Theorem (1.10)) for a proof.
\end{proof}

\begin{remark}
One informal (but very informative!) conclusion that one can draw from this theorem is the fact that the quadratic variation should be regarded as a natural clock for the martingale. This is demonstrated for instance in the following theorem.
\end{remark}

\begin{example}
Let $B$ be a Brownian motion and let $h$ be a deterministic function in $\sL^2(\R^+, B, \lm)$ with Lebesgue measure $\lm$. Set $X = \int^\infty_0 h_s dB_s$. Then $X \sim \sN\bb{0, \dabs{h}^2_2}$.
\end{example}

\begin{proof}[\bf Proof]
Apply Dubins-Schwarz's theorem to the local martingale $M_t = \int^t_0 h_s dB_s$.
\end{proof}

\begin{theorem}
Let $M$ be a continuous local martingale. Then
\ben
\item [(i)] $\pro\bb{\lim_{t\to\infty} \abs{M}_t = \infty} = 0$
\item [(ii)] $\bra{\omega:\lim_{t\to\infty} M_t \text{ exists and is finite}} = \bra{\omega:[M]_\infty< \infty}$  up to null events.
\item [(iii)] $\bra{[M]_\infty = \infty} = \bra{\limsup_{t\to\infty} M_t = +\infty \text{ and }\liminf_{t\to\infty} M_t = -\infty}$ up to null events.
\een
\end{theorem}

\subsection{Planar Brownian motion}

As explained in the introduction to these notes, Brownian motion is the scaling limit of $d$-dimensional random walks (this theorem will actually be proved in its strong form in the next subsection). One of the most striking results about random walks is Polya's theorem which says that simple random walk is recurrent in dimension 1 and 2, while it is transient in dimension 3. What is the situation for Brownian motion? Being the scaling limit of simple random walk, one might expect the answer to be the same for Brownian motion. It turns out that this is almost the case. there is however something subtle happening in dimension 2. In the planar case, Brownian motion is neighbourhood-recurrent (it visits any neighbourhood of any point "infinitely often") but almost surely does not hit any point chosen in advance. 

We work with the Wiener measure $\W$ on the space of continuous functions, and recall that $\W_x$ denote the law of a Brownian motion started at $x$. Let $\E_x$ denote the expectation under this probability measure. In the sequel, $B(x, r)$ and $\overline{B}(x, r)$ denote the Euclidean ball of radius $r$ about $x \in\R^d$.

\begin{theorem}[Recurrence/Transience classification]
Let $d \geq 1$. We have the following dimension-dependent behaviour.
\ben
\item [(i)] If $d = 1$, Brownian motion is point-recurrent in the sense that. 
\be
\W_0 \text{ - a.s. for all $x \in\R$, $\bra{t \geq 0 : B_t = x}$ is unbounded}
\ee

\item [(ii)] If $d \geq 3$, Brownian motion is transient, in the sense that $\dabs{B_t} \to \infty$ almost surely as $t \to \infty$.
\item [(iii)] If $d = 2$, Brownian motion is neighbourhood-recurrent, in the sense that for every $x \in \R^d$, every open set is visited infinitely often $\W_x$-almost surely. Equivalently, for any $\ve > 0$,
\be
\bra{t \geq 0 : \dabs{B_t} < \ve} \text{ is unbounded}
\ee
$\W_x$-almost surely for every $x \in \R^2$. However, points are polar in the sense that for every $x \in \R^2$, 
\be
\W_0(B_t = x \text{ for some }t > 0) = 0.
\ee
\een
\end{theorem}


\begin{proof}[\bf Proof]
\ben
\item [(i)] is a consequence of (ii) in Proposition \ref{pro:brownian_motion_limit_value}. 
\item [(ii)] Let $B = (B^1,\dots,B^d)$ be a $d$-dimensional Brownian motion with $d \geq 3$. Clearly it suffices to prove the result for $d = 3$ since
\be
\dabs{B_t}^2 \geq R^2_t := \sum^3_{i=1} \dabs{B^i_t}^2
\ee
and we are precisely claiming that the right-hand side tends to infinity as $t \to \infty$. Now, for $d = 3$, a simple calculation shows that if $f(x) = 1/\dabs{x}$, then $\Delta f = 0$ in $\R^3\bs \{0\}$. Thus by the local It\^o's formula, $1/R_{t\land \tau}$ is a local martingale, where $\tau$ is the hitting time of 0. Since it is nonnegative, it follows from lemma \ref{lem:sl2_martingale_trick} that it is a supermartingale. Being nonnegative, the martingale convergence theorem tells us that it has an almost sure limit $M$ as $t \to \infty$, and it suffices to prove that $M = 0$ almost surely. Note that $\E(M) \leq \E(1/R_0) < \infty$, so that $M < \infty$ almost surely and thus $\tau = \infty$ almost surely. Now on the event $\bra{M > 0}$, $R_t$ must be bounded, and thus so is $\abs{B^1_t}$. This has probability 0 by (i) and hence $M = 0$ a.s.

\item [(iii)] Let $d = 2$ and let $B$ be a planar Brownian motion. Assume without loss of generality that $B_0 = 1$. We are going to establish that starting from there, $B$ will never hit 0 but will come close to it "infinitely often" (or rather, "unboundedly often"). For $k \in \Z$, let $R_k = e^k$ and let
\be
\tau_k = \inf\bra{t \geq 0 : \dabs{B_t} = R_k}
\ee
and let
\be
\tau = \tau_{-\infty} = \inf\bra{t \geq 0 : \dabs{B_t} = 0}.
\ee

Our first goal will be to show that $\tau = \infty$, almost surely. Define a sequence of stopping times $T_n$ as follows. $T_0 = 0$, and by induction if $Z_n = \dabs{B_{T_n}}$ then
\be
T_{n+1} = \inf\bra{t \geq T_n, \dabs{B_t} \in \{e^{-1}Z_n, eZ_n\}}.
\ee

Notice that if $k,m \geq 1$ are two large integers, the probability that $\tau_{-k} < \tau_m$ is the probability that $Z_n$ visits $e^{-k}$ before $e^m$. Put it another way, it is also the probability that $(\log Z_n, n \geq 0)$ visits $-k$ before $m$.

On the other hand, by It\^o's formula, $M_t = \log \dabs{B_{t\land \tau}}$ is a local martingale since
\be
(x, y) \mapsto \log(x^2 + y^2) \ \text{ is harmonic on }\R^2 \bs \{(0, 0)\}.
\ee

Since $M_t$ is bounded on $[T_n, T_{n+1}]$, it follows from the Optional Stopping Theorem that given $\log Z_n = k \in \Z$,
\be
\pro(\log Z_{n+1} = k + 1| \log Z_n = k) = \pro(\log Z_{n+1} = k - 1|\log Z_n = k) = 1/2.
\ee

Moreover, the strong Markov property of Brownian motion implies that $(\log Z_n, n \geq 0)$ is a Markov chain. In other words, $(\log Z_n, n \geq 0)$ is nothing but simple random walk on $\Z$. In particular, it is recurrent. Therefore, for any $m \geq 0$,
\be
\pro(\tau_{-k} < \tau_m) \to 0
\ee
as $k \to\infty$. Therefore,
\be
\pro(\tau < \tau_m) = 0
\ee
for all $m \geq 0$. This implies that $\tau = \infty$ almost surely since $\tau_m \to \infty$ as $m \to \infty$. On the other hand, this argument shows that $\tau_k < \infty$ for all $k \in \Z$, and there are infinitely many times that $B$ visits this ball after returning to a radius greater than 1. Thus the set of times such that $B_t \in B(0,R_k)$ is unbounded a.s. 
\een
\end{proof}

\begin{remark}
Notice that (iii) implies the fact that $\bra{t \geq 0 :B_t \in B(x, \ve)}$ is unbounded for every $x \in \R^2$ and every $\ve > 0$, almost surely. Indeed, one can cover $\R^2$ by a countable union of balls of a fixed radius. In particular, the trajectory of a 2-dimensional Brownian motion is everywhere dense. On the other hand, it will a.s. never hit a fixed countable family of points (except maybe at time 0), like the points with rational coordinates!
\end{remark}

\begin{problem}
Show that the Lebesgue measure of $\gamma = \bra{B_t, t \geq 0}$ is 0 almost surely, where $B$ is a planar Brownian motion.
\end{problem}

We start discussing finer properties of planar Brownian motion. The key is often conformal invariance, already observed by L\'evy in the 40's. This says that, given a conformal mapping $f$ between two simply-connected planar domains $D$ and $D'$, the trajectories of a Brownian motion in $D'$ (that is, stopped upon reaching the boundary $\partial D'$) are the image by $f$ of Brownian motion in $D$, up to reparametrization.

\begin{theorem}\label{thm:finer_planar_property_of_bm}
Let $d = 2$ and identify $\R^2$ with the complex field $\C$. Let $f: D \to D'$ be analytic (i.e., complex differentiable). Let $z \in D$ and consider a Brownian motion $(B_t, t \geq 0)$ started at $z$. Let $\tau_D = \inf\bra{t > 0 : B_t \notin D}$. Then there exists a Brownian motion $B'$ started at $f(z)$, and a non-decreasing random function $\sigma(t)$, such that
\be
f(B_{t\land \tau_D}) = B'_{\sigma(t)\land \tau'_{D'}},
\ee
where $\sigma(t) = \int^t_0 \abs{f'(B_s)}^2ds$. In other words, $f(B)$ is a time-changed Brownian motion stopped when it leaves $D'$.
\end{theorem}

Problem 2.8 asks for a proof. It will be useful to recall the Cauchy-Riemann equations for complex analytic functions. if $f = u + iv$ is a complex-differentiable function with real and imaginary parts $u$ and $v$, then 
\be
\fp{u}{x} = \fp{v}{y},\quad\quad \fp{u}{y} = -\fp{v}{x}
\ee
from which it follows by further differentiation that both $u$ and $v$ are harmonic functions (i.e., $\Delta u \equiv \Delta v \equiv 0$) all over $D$). Applying It\^o's formula and the Cauchy-Riemann equations shows that the real and imaginary parts of $f(B_t)$ have zero covariation and that they have identical quadratic variation. Applying the ideas of the Dubins-Schwartz theorem yields the result. 

In principle, Theorem \ref{thm:finer_planar_property_of_bm} (in combination with the famous Riemann mapping theorem) can be used to extract all the information we need about the behaviour of Brownian motion. For instance, the exit distribution from a domain $D$ is simply the conformal image of the
uniform measure of the circle by a map from the disc to this domain. 

\begin{remark}
The ramifications of this result are huge. On the one hand, it serves as a bridge between probability and complex analysis, and in the example sheet you will prove some results of complex analysis just using Brownian motion (and the proofs are really easier than the original proofs, once you know conformal invariance of Brownian motion). This is one aspect of the deep connection between random processes and harmonic analysis (which will be further developed later on). On the other hand, conformal invariance of Brownian motion, already observed by Paul L\'evy in the 1940's, can be seen as the starting point of
Schramm-Loewner Evolution (SLE), one of the most fascinating recent theories developed in probability, which may be seen as a study random processes in the complex plane that possess the conformal invariance property.

As an example of application of conformal invariance to planar Brownian motion, we will mention the following result, due to Spitzer. Let $(B_t, t \geq 0)$ denote a planar Brownian motion started from $(1, 0)$. Let $W_t$ denote the total number of windigs of the curve $B$ about 0 up to time $t$. This counts +1 for each clockwise winding around 0 and -1 for each counterclockwise winding.
\end{remark}

\begin{theorem}[Spitzer's winding number theorem]\label{thm:spitzer_winding_number}
We have the following convergence in distribution:
\be
\frac{4\pi W_t}{\log t} \stackrel{d}{\to} \sC,
\ee
a Cauchy distribution with parameter 1.
\end{theorem}

Recall that a Cauchy distribution (with parameter 1) has density $\frac 1{\pi (1+x^2)}$ and has the same distribution as $\sN/\sN'$, where these are two independent standard Gaussian random variables. Problem 2.10 asks for the main step in the proof. To deduce Theorem \ref{thm:spitzer_winding_number} from
Problem 2.10, observe that by scaling, Wt has the same distribution as $\overline{W}_\ve$, where $\overline{W}_\ve$ is the number of windings by time 1 of a Brownian motion started from $(\ve, 0)$ and $\ve = 1/\sqrt{t}$. 

It is a simple estimate that $\abs{\overline{W}_\ve - \theta_\ve}$ is bounded in probability, where $\theta_\ve$ is the number of windings up to time $T$, the hitting time of the unit sphere $\bra{z:\abs{z} = 1}$. Problem 2.10 uses conformal invariance under the exponential map to show that $2\pi \theta_\ve = \log \ve \to \sC$, a Cauchy random variable. The result follows since $\ve = 1/\sqrt{t}$ and $\sC$ is symmetric about 0. (This proof is inspired by a 1982 paper of Durrett.)


\subsection{Donsker's invariance principle}

The following theorem completes the description of Brownian motion as a 'limit' of centered random walks as depicted in the beginning of the chapter, and strengthens the convergence of finite-dimensional marginals to that convergence in distribution. 

We endow $\sC([0, 1],\R)$ with the supremum norm, and recall (see the exercises on continuous-time processes) that the product $\sigma$-algebra associated with it coincides with the Borel $\sigma$-algebra associated with this norm. We say that a function $F :\sC([0, 1]) \to \R$ is continuous if it is continuous with respect to this norm. Often, functions $F$ defined on $\sC$ will be called functionals. For instance, the supremum of a path on the interval $[0,1]$ is a (continuous) functional.

\begin{theorem}[Donsker's invariance principle]
Let $(X_n, n \geq 1)$ be a sequence of $\R$-valued integrable independent random variables with common law $\mu$, such that
\be
\int x\mu (dx) = 0,\quad\quad \int x^2\mu (dx) = \sigma^2 \in (0,\infty).
\ee

Let $S_0 = 0$ and $S_n = X_1 +\dots +X_n$, and define a continuous process that interpolates linearly between values of $S$, namely
\be
S_t = (1 - \{t\})S_{\floor{t}} + \{t\}S_{\floor{t}+1},\quad\quad t \geq 0,
\ee
where $\floor{t}$ denotes the integer part of $t$ and $\{t\} = t - \floor{t}$. Then
\be
S^{[N]} := \bb{\frac{S_{N_t}}{\sqrt{\sigma^2N}}, 0 \leq t \leq 1}
\ee
converges in distribution to a standard Brownian motion between times 0 and 1, i.e. for every bounded continuous function $F: \sC([0, 1]) \to \R$,
\be
\E\bsb{F(S^{[N]})} \stackrel{n\to\infty}{\to} \E_0[F(B)].
\ee
\end{theorem}

Notice that this is much stronger than what Proposition \ref{pro:marginal_distribution_brownian_motion} says. Despite the slight difference of framework between these two results (one uses \cadlag continuous-time version of the random walk, and the other uses an interpolated continuous version), Donsker's invariance principle is stronger. For instance, one can infer from this theorem that the random variable $N^{-1/2} \sup_{0\leq n\leq N} S_n$ converges to $\sup_{0\leq t\leq 1} B_t$ in distribution, because $f \mapsto \sup f$ is a continuous operation on $\sC([0, 1],\R)$. Proposition \ref{pro:marginal_distribution_brownian_motion} would be powerless to address this issue.

The proof we give here is an elegant demonstration that makes use of a coupling of the random walk with a Brownian motion, called the Skorokhod embedding theorem. It is however specific to dimension $d = 1$. Suppose we are given a Brownian motion $(B_t, t \geq 0)$ on some probability space $(\Omega,\sF, \pro)$.

Let $\mu^+(dx) = \pro(X_1 \in dx)\ind_{x\geq0 }$ and $\mu^-(dy) = \pro(-X_1 \in dy)\ind_{y>0}$ define two nonnegative measures. Assume that $(\Omega,\sF,\pro)$ is a rich enough probability space so that we can further define on it, independently of $(B_t, t \geq 0)$, a sequence of independent identically
distributed $\R^2$-valued random variables $((Y_n,Z_n), n \geq 1)$ with distribution
\be
\pro((Y_n,Z_n) \in dxdy) = \frac 1C (x + y)\mu^+(dx)\mu^-(dy),
\ee
where $C > 0$ is the appropriate normalizing constant that makes this expression a probability measure (this is possible because $X$ has a well-defined expectation).

We define a sequence of random times, by $T_0 = 0$, $T_1 = \inf\bra{t \geq 0: B_t \in \{Y_1,-Z_1\}}$, and recursively,
\be
T_n = \inf\bra{t \geq T_{n-1}: B_t - B_{T_{n-1}} \in \{Y_n,-Z_n\}}.
\ee
By (ii) in Proposition \ref{pro:brownian_motion_limit_value}, these times are a.s. finite. We claim that


\begin{lemma}[Skorokhod's embedding]\label{lem:skorokhod_embedding}
The sequence $(B_{T_n}, n \geq 0)$ has the same law as $(S_n, n \geq 0)$. Moreover, the intertimes $(T_n - T_{n-1}, n \geq 1)$ form a sequence of independent random variables with same distribution, and expectation $\E[T_1] = \sigma^2$.
\end{lemma}

\begin{proof}[\bf Proof]
Let $\sF^B$ be the filtration of the Brownian motion, and for each $n \geq 0$, introduce the filtration $\sG^n = (\sG^n_t , t \geq 0)$ defined by
\be
\sG^n_t = \sF^B_t \lor \sigma(Y_1,Z_1,\dots, Y_n,Z_n).
\ee

Since $(Y_i,Z_i)$ are independent from $sF^B$, $(B_t, t \geq 0)$ is a $\sG^n$-Brownian motion for every $n \geq 0$. Moreover, $T_n$ is a stopping time for $\sG^n$. It follows that if $\wt{B}_t = (B_{T_n+t} - B_{T_n}, t \geq 0)$ then $\wt{B}$ is independent from $\sG^n_{T_n}$. Moreover, $(Y_{n+1},Z_{n+1})$ is independent both from $\sG^n_{T_n}$ and from $\wt{B}$, therefore $(T_{n+1} - T_n)$, which depends only on $\wt{B}$ and $(Y_{n+1},Z_{n+1})$ is independent from $\sG^n_{T_n}$. In particular, $(T_{n+1} - T_n)$ is independent from $\sigma(T_0, T_1,\dots, T_n)$. More generally, we obtain that the processes $(B_{t+T_{n-1}} - B_{T_{n-1}} , 0 \leq t \leq T_n - T_{n-1})$ are independent with the same distribution.

It therefore remains to check that $B_{T_1}$ has the same law as $X_1$ and $\E[T_1] = \sigma^2$. Remember from Proposition \ref{thm:brownian_motion_double_bounded} that given $Y_1$,$Z_1$, the probability that $B_{T_1} = Y_1$ is $Z_1/(Y_1+Z_1)$, as follows from the optional stopping theorem. Therefore, for every non-negative measurable function $f$, by first conditioning on $(Y_1,Z_1)$, we get
\beast
\E[f(B_{T_1} )] & = & \E\bsb{f(Y_1)\frac{Z_1}{Y_1 + Z_1} + f(-Z_1) \frac{Y_1}{Y_1 + Z_1}}\\
& = & \int_{\R^+\times{\R^+}^*} \frac 1C (x + y)\mu^+(dx)\mu^-(dy)\bb{f(x)\frac y{x + y} + f(-y)\frac x{x + y}}\\
& = & \frac 1C \int_{\R^+\times{\R^+}^*}  \mu^+(dx)\mu^-(dy)(yf(x) + xf(-y))\\
& = & \frac 1C \int_{\R^+} \mu^+(dx)f(x)\int_{{\R^+}^*} y\mu^-(dy) + \frac 1C \int_{{\R^+}^*} \mu^-(dy)f(-y) \int_{\R^+} x\mu^+(dx)
\eeast

Now observe that since $\E(X_1) = 0$, it must be the case that
\be
\int_{\R^+} x\mu^+(dx) = \int_{{\R^+}^*} y\mu^-(dy) = C',
\ee
say, and thus, the left hand side is equal to
\be
\E[f(B_{T_1} )] = \frac{C'}C \int_{\R^+} (f(x)\mu^+(dx) + f(-x)\mu^-(dx)) = \frac{C'}C \int_\R f(x)\mu (dx) =\frac {C'}C \E[f(X_1)].
\ee

By taking $f \equiv 1$, it must be that $C' = C$, and hence $B_{T_1}$ has the same law as $X_1$. For $\E[T_1]$, recall from Proposition \ref{thm:brownian_motion_double_bounded} that $\E[\inf\bra{t \geq 0 : B_t \in \{x,-y\}}] = xy$, so by a similar conditioning argument as above,
\beast
\E[T_1] & = &  \int_{\R^+\times{\R^+}^*} \frac 1C (x + y)xy\mu^+(dx)\mu^-(dy)\\
& = & \frac 1C  \int_{\R^+} x^2\mu^+(dx)  \int_{{\R^+}^*} y\mu^-(dy) + \frac 1C  \int_{{\R^+}^*} y^2\mu^-(dy)  \int_{\R^+} x\mu^+(dx)\\
& = & \frac {C'}C  \int_{\R} x^2\mu (dx) =\frac{C'}C \sigma^2
\eeast
but since we already know that $C' = C$, this shows that $\E(T_1) = \sigma^2$, as claimed.
\end{proof}

We will need another lemma, which tells us that the times $T_m$ are in a fairly strong sense localized around there mean $m\sigma^2$.

\begin{lemma}\label{lem:sup_convergence_almost_surely}
We have the following convergence as $N\to\infty$.
\be
N^{-1} \sup_{0\leq n\leq N} \abs{T_n - \sigma^2n}\to 0 \quad \text{a.s. }
\ee
\end{lemma}

\begin{proof}[\bf Proof]
By the strong law of large numbers, note that $T_n/n \to \sigma^2$ almost surely. Thus, fix $\ve > 0$. Then there exists $N_0 = N_0(\ve, \omega)$ such that if $n \geq N_0$, $\abs{n^{-1}T_n - \sigma^2} \leq \ve$. Thus if $N_0 \leq n \leq N$, then
\be
N^{-1}\abs{T_n - n\sigma^2} \leq \frac nN \ve \leq \ve
\ee

Moreover, $N^{-1} \sup_{0\leq n\leq N_0} \abs{T_n - n\sigma^2}$ tends to 0 almost surely as $N \to\infty$, so this implies the required result.
\end{proof}

\begin{proof}[\bf Proof of Donsker's invariance principle]
We suppose given a Brownian motion $B$. For $N \geq 1$, define $B^{(N)}_t = N^{1/2}B_{N^{-1}t}, t \geq 0$, which is a Brownian motion by scaling invariance.
Perform the Skorokhod embedding construction on B(N) to obtain variables $(T^{(N)}_n , n \geq 0)$. Then, let $S^{(N)}_n = B^{(N)}_{T^{(N)}_n}$. Then by Lemma \ref{lem:skorokhod_embedding}, $(S^{(N)}_n , n \geq 0)$ is a random walk with same law as $(S_n, n \geq 0)$. We interpolate linearly between integers to obtain a continuous process $(S^{(N)}_t , 0 \leq t \leq 1)$ which thus has the distribution as $(S_t, 0 \leq t \leq 1)$. Finally, let
\be
\wt{S}^{(N)}_t = \frac{S^{(N)}_{N_t}}{\sqrt{\sigma^2N}},\quad t \geq 0
\ee
and $\wt{T}^{(N)}_n = N^{-1}T^{(N)}_n$. Finally, let $B'_t = B_{\sigma^2t}/\sqrt{\sigma^2}$, which is also a Brownian motion. We are going to show that the supremum norm
\be
\dabs{B'_t - \wt{S}^{(N)}_t }_\infty \stackrel{p}{\to} 0
\ee
as $N\to\infty$, where $\stackrel{p}{\to}$ denotes convergence in probability.

First recall what we have proved in Lemma \ref{lem:sup_convergence_almost_surely}, and note that this implies convergence in probability. Since $(T^{(N)}_n , n \geq 0)$ has the same distribution as $(T_n, n \geq 0)$ we infer from this that for every $\delta > 0$, letting $\delta' = \delta \sigma^2 > 0$, we have:
\be
\pro\bb{N^{-1} \sup_{0\leq n\leq N} \abs{T^{(N)}_n - n\sigma^2} \geq \delta'} \to 0\quad\text{ as }N\to \infty.
\ee

Therefore dividing by $\sigma^2$:
\be
\pro\bb{\sup_{0\leq n\leq N} \abs{\wt{T}^{(N)}_n /\sigma^2 - n/N} \geq \delta} \to 0\quad\text{ as }N\to \infty.
\ee

Now, note that if $t = n/N$, then
\be
\wt{S}^{(N)}_t = \frac{S^{(N)}_n}{\sqrt{N\sigma^2}} = \frac{B^{(N)}_{\wt{T}^{(N)}_n}}{\sqrt{\sigma^2}} = B'_{\wt{T}^{(N)}_n /\sigma^2}.
\ee

Thus, by continuity, if $t\in [n/N, (n + 1)/N]$, there exists $u \in [T^{(N)}_n/\sigma^2, T^{(N)}_{n+1}/\sigma^2]$ such that $S^{(N)}_t = B'_u$. Therefore, for all $\ve > 0$ and all $\delta > 0$, the event
\be
\bra{\sup_{0\leq t\leq1} \abs{\wt{S}^{(N)}_t - B'_t} > \ve} \subseteq  K^N_\delta \cup L^N_{\delta,\ve},
\ee
where
\be
K^N_\delta = \bra{\sup_{0\leq n\leq N} \abs{\wt{T}^{(N)}_n/\sigma^2 - n/N}  > \delta}
\ee
and
\be
L^N_{\delta,\ve} = \bra{\exists t \in [0, 1], \exists u \in [t - \delta, t + \delta + 1/N] : \abs{B'_t - B'_u} > \ve }.
\ee

We already know that $\pro(K^N_\delta) \to 0$ as $N\to\infty$. For $L^N_{\delta,\ve}$, since $B'$ is a.s. uniformly continuous on $[0, 1]$, by taking $\delta$ small enough and then $N$ large enough, we can make $\pro(L^N)$ as small as wanted. More precisely, if
\be
L_{2\delta,\ve} = \bra{\exists t \in [0, 1], \exists u \in [t - 2\delta, t + 2\delta]: \abs{B'_t - B'_u} > \ve}.
\ee
then for $N \geq 1/\delta$, $L^N_{\delta,\ve} \subseteq  L_{2\delta,\ve}$, and thus for all $\delta > 0$:
\be
\limsup_{N\to\infty} \pro\bb{\dabs{\wt{S}^{(N)} - B'}_\infty > \ve} \leq \pro(L_{2\delta,\ve})
\ee

However, as $\delta\to 0$, $\pro(L_{2\delta,\ve}) \to 0$ by almost sure continuity of $B'$ on $(0, 1)$ and the fact that these events are decreasing. Hence it must be that 
\be
\limsup_{N\to\infty} \pro\bb{\dabs{\wt{S}^{(N)} - B'}_\infty > \ve} = 0.
\ee

Therefore, $(\wt{S}^{(N)}, 0 \leq t \leq 1)$ converges in probability for the uniform norm to $(B_t, 0 \leq t \leq 1)$, which entails convergence in distribution. This concludes the proof.
\end{proof}


\subsection{Brownian motion and the Dirichlet problem}

Let $D$ be a connected open subset of $\R^d$ for some $d \geq 1$ (though the story is interesting only for $d \geq 2$). We will say that $D$ is a domain. Let $\partial D$ be the boundary of $D$. We denote by $\Delta$ the Laplacian on $\R^d$.

\begin{definition}\label{def:unique_bounded_solution_Dirichlet_problem}
Let $g : \partial D \to\R$ be a continuous function. A solution of the Dirichlet problem with boundary condition $g$ on $D$ is a function $u :\overline{D} \to\R$ of class $\sC^2(D) \cap \sC(\overline{D})$, such that
\be
\left\{\ba{ll}
\Delta u = 0 & \text{on }D\\
u|_{\partial D} = g.\quad\quad& 
\ea\right.
\ee
\end{definition}

A solution of the Dirichlet problem is the mathematical counterpart of the following physical problem. given an object made of homogeneous material, such that the temperature $g(y)$ is imposed at point $y$ of its boundary, the solution $u(x)$ of the Dirichlet problem gives the temperature at the point $x$ in the object when equilibrium is attained.

As we will see, it is possible to give a probabilistic resolution of the Dirichlet problem with the help of Brownian motion. This is essentially due to Kakutani. We let $\E_x$ be the law of the Brownian motion in $\R^d$ started at $x$. In the remaining of the section, let $T = \inf\bra{t \geq 0 : B_t \notin D}$ be the first exit time from $D$. It is a stopping time, as it is the first entrance time in the closed set Dc. We will assume that the domain $D$ is such that $\pro(T < \infty) = 1$ to avoid complications. Hence $B_T$ is a well-defined random variable.

In the sequel, $\abs{ \cdot}$ is the Euclidean norm on $\R^d$. The goal of this section is to prove the following result.

\begin{theorem}\label{thm:local_exterior_cone_condition}
Suppose that $g \in \sC(\partial D,\R)$ is bounded, and assume that $D$ satisfies a local exterior cone condition (l.e.c.c.), i.e. for every $y \in \partial D$, there exists a nonempty open convex cone with origin at $y$ such that $C \cap B(y, r) \subseteq D^c$ for some $r > 0$. Then the function
\be
u :x \mapsto \E_x [g(B_T )]
\ee
is the unique bounded solution of the Dirichlet problem (\ref{def:unique_bounded_solution_Dirichlet_problem}). In particular, if $D$ is bounded and satisfies the l.e.c.c., then $u$ is the unique solution of the Dirichlet problem.
\end{theorem}

We start with a uniqueness statement.

\begin{proposition}
Let $g$ be a bounded function in $\sC(\partial D,\R)$. Set
\be
u(x) = \E_x [g(B_T )] .
\ee
If $v$ is a bounded solution of the Dirichlet problem, then $v = u$.
\end{proposition}

In particular, we obtain uniqueness when $D$ is bounded. Notice that we do not make any assumption on the regularity of $D$ here besides the fact that $T < \infty$ a.s.

\begin{proof}[\bf Proof]
Let $v$ be a bounded solution of the Dirichlet problem. Let $T_n = \inf\bra{t \geq 0 : d(X_t,D^c) < 1/n}$. Since $\Delta v = 0$ inside $D$, we know by Proposition \ref{pro:ito_lemma_c2_function} that $M_t = v(B_{t\land T_n}) - v(B_0)$ is a local martingale started at 0 (here, $B_0 = x$ almost surely). Moreover, since $v$ is bounded, $M$ is a true martingale which is uniformly integrable. Applying the optional stopping theorem (\ref{thm:optional_stopping_ui_continuous}) at the stopping time $T_n$,
\be
\E(M_{T_n}) = \E_x(v(B_{T_n}) - v(x)) = 0.
\ee

Since $T_n \to T$ almost surely as $n\to\infty$, and since $v$ is continuous on $\sC(\overline{D})$, we get:
\be
v(x) = \E_x(g(B_T ))
\ee
as claimed.
\end{proof}

For every $x \in \R^d$ and $r > 0$, let $\sigma_{x,r}$ be the uniform probability measure on the sphere $S_{x,r} = \bra{y \in \R^d : \abs{y - x} = r}$. It is the unique probability measure on $S_{x,r}$ that is invariant under isometries of $S_{x,r}$. We say that a locally bounded measurable function $h: D \to \R$ is harmonic on $D$ if for every $x \in D$ and every $r > 0$ such that the closed ball $\overline{B}(x, r)$ with center $x$ and radius $r$ is contained in $D$,
\be
h(x) = \int_{S_{x,r}} h(y)\sigma_{x,r}(dy).
\ee

\begin{proposition}
Let $h$ be harmonic on a domain $D$. Then $h \in \sC^\infty(D,\R)$, and $\Delta h = 0$ on $D$.
\end{proposition}

\begin{proof}[\bf Proof]
Let $x \in D$ and $\ve > 0$ such that $\overline{B}(x, \ve) \subseteq D$. Then let $\varphi \in \sC^\infty(\R,\R)$ be non-negative with non-empty compact support in $[0, \ve]$. We have, for $0 < r < \ve$,
\be
h(x) = \int_{S(0,r)} h(x + y)\sigma_{0,r}(dy).
\ee

Multiplying by $\varphi(r)r^{d-1}$ and integrating over $r \in (0, \ve)$ gives
\be
ch(x) = \int_{B(0,\ve)} \varphi(\abs{z})h(x + z)dz,
\ee
where $c > 0$ is some constant, where we have used the fact that
\be
\int_{\R^d} f(x)dx = C \int_{\R^+} r^{d-1}dr \int_{S(0,r)} f(ry)\sigma_{0,r}(dy)
\ee
for some $C > 0$. Therefore, $ch(x) = \int_{B(x,\ve)} \varphi(\abs{z - x})h(z)dz = \int_{\R^d} \varphi(\abs{z - x})h(z)dz$ since $\varphi$ is supported on $B(0, \ve)$. By derivation under the $\int$ sign, we easily get that $h$ is $\sC^\infty$. (Indeed, we may assume that $r \mapsto \varphi(r^{1/2})$ is $\sC^\infty$). Another way to say this is to say that $ch(x) = \wt{\varphi}\star h$ where $\wt{\varphi}(z) = \varphi(\abs{z})$. If $r \to \varphi(r^{1/2})$ is $\sC^\infty$, then $\wt{\varphi} \in \sC^\infty(\R^d,\R)$ and thus, the convolution being a regularizing operation, this implies $\wt{\varphi} \star h \in \sC^\infty(D,\R)$. Thus $h \in \sC^\infty(D,\R)$.

Next, by translation we may suppose that $0 \in D$ and show only that $\Delta h(0) = 0$. we may apply Taylor's formula to $h$, obtaining, as $x \to 0$,
\be
h(x) = h(0) + \inner{\nabla h(0)}{x} + \sum^d_{i=1} x^2_i \frac{\partial^2h}{\partial x^2_i} (0) + \sum_{i\neq j} x_ix_j \frac{\partial^2h}{\partial x_i\partial x_j} (0) + o(\abs{x}^2).
\ee

Now, integration over $\S_{0,r}$ for $r$ small enough yields
\be
\int_{\BS_{x,r}} h(x)\sigma_{0,r}(dx) = h(0) + Cr\Delta h(0) + o(r^2),
\ee
where $C_r = \int_{\BS_{0,r}} x^2_1 \sigma_{0,r}(dx)$, as the reader may check that all the other integrals up to the second order are 0, by symmetry. Now, it is easy to see that there exists $c > 0$ such that $C_r \geq cr^2$ for all $0 \leq r \leq 1$. Since the left-hand side is $h(0)$ and the error term on the right-hand side is $o(r^2) = o(C_r)$, it follows that $\Delta h(0) = 0$.
\end{proof}

Therefore, harmonic functions are solutions of certain Dirichlet problems.

\begin{proposition}
Let $g$ be a bounded measurable function on $\partial D$, and let $T = \inf\bra{t \geq 0: B_t \notin D}$. Then the function $h: x \in D \mapsto \E_x[g(B_T )]$ is harmonic on $D$, and hence $\Delta h = 0$ on $D$.
\end{proposition}

\begin{proof}[\bf Proof]
For every Borel subsets $A_1,\dots,A_k$ of $\R^d$ and times $t_1 <\dots< t_k$, the map
\be
x \mapsto \pro_x(B_{t_1} \in A_1,\dots,B_{t_n} \in A_n)
\ee
is measurable by Fubini's theorem, once one has written the explicit formula for this probability. Therefore, by the monotone class theorem, $x \to \E_x[F]$ is measurable for every integrable random variable $F$, which is measurable with respect to the product $\sigma$-algebra on $\sC(\R,\R^d)$. Moreover, $h$ is bounded by assumption.

Now, let $S = \inf\bra{t \geq 0:\abs{B_t - x} \geq r}$ the first exit time of $B$ form the ball of center $x$ and radius $r$. Then by (ii), Proposition \ref{pro:brownian_motion_limit_value}, $S < \infty$ a.s. By the strong Markov property, $\wt{B} = (B_{S+t}, t \geq 0)$ is an $(\sF_{S+t})$ Brownian motion started at $B_S$. Moreover, the first hitting time of $\partial D$ for $\wt{B}$ is $\wt{T} = T - S$. Moreover, $\wt{B}_{\wt{T}} = B_T$, so that 
\be
E_x[g(B_T )] = \E_x[g( \wt{B}_{\wt{T}})] = \int_{\R^d} \pro_x(B_S \in dy)\E_y[g(B_T )\ind_{T<\infty}],
\ee
and we recognize $\int \pro_x(B_S \in dy)h(y)$ in the last expression.

Since $B$ starts from $x$ under $\pro_x$, the rotation invariance of Brownian motion shows that $B_S -x$ has a distribution on the sphere of center 0 and radius $r$ which is invariant under the orthogonal group, so we conclude that the distribution of $B_S$ is the uniform measure on the sphere of center $x$ and radius $r$, and therefore that $h$ is harmonic on $D$.
\end{proof}

It remains to understand whether the function u of Theorem \ref{thm:local_exterior_cone_condition} is actually a solution of the Dirichlet problem. Indeed, is not the case in general that $u(x)$ has limit $g(y)$ as $x \in D$, $x \to y$, and the reason is that some points of $\partial D$ may be 'invisible' to Brownian motion. The reader can convince himself, for example, that if $D = B(0, 1) \bs \{0\}$ is the open ball of $\R^2$ with center 0 and radius 1, whose origin has been removed, and if $g = \ind_{\{0\}}$, then no solution of the Dirichlet problem with boundary constraint $g$ exists. The probabilistic reason
for that is that Brownian motion does not see the boundary point 0. This is the reason why we have to make regularity assumptions on $D$ in the following theorem. 

\begin{proof}[\bf Proof of Theorem \ref{thm:local_exterior_cone_condition}]
It remains to prove that under the l.e.c.c., $u$ is continuous on $\overline{D}$, i.e. $u(x)$ converges to $g(y)$ as $x \in D$ converges to $y \in \partial D$. In order to do that, we need a preliminary lemma. Recall that $T$ is the first exit time of $D$ for the Brownian path.
\end{proof}

\begin{lemma}
Let $D$ be a domain satisfying the l.e.c.c., and let $y \in \partial D$. Then for every $\eta > 0$, $\pro_x(T < \eta) \to 1$ as $x \in D \to y$.
\end{lemma}
\begin{proof}[\bf Proof]
Let $C_y = y +C$ be a nonempty open convex cone with origin at $y$ such that $C_y \subseteq  D^c$ (we leave as an exercise the case when only a neighborhood of this cone around $y$ is contained in $D^c$). Then it is an elementary geometrical fact that there exists a nonempty open convex cone $C'$ with origin at 0 such that for every $\delta > 0$ small enough, we can find an $\ve = \ve(\delta) > 0$ such that if $C'_x = x + C'$, then $(C'_x \bs \overline{B}(x, \delta)) \subseteq  C_y$ for all $x \in B(y, \ve)$.

[Here is a justification. Assume without loss of generality that $y = 0$ to simplify, and fix $\delta > 0$. Let O be an open set in the unit sphere such that $C = \bra{\lm z, z \in O, \lm > 0}$. There exists $\alpha > 0$ and another open set $O'$ in the unit sphere such that $O' \subseteq O$ and if $z \in \BS_{0,1}$ with $d(z,O') \leq \alpha$ then $z \in O$. (For instance consider the intersection of the sphere with two concentric open balls centered at some $z_0 \in O$, and take $O'$ the smaller of the two balls intersected with $\BS$). Now, choose $\ve = \delta \alpha /4$. Let $x \in B(0,\ve)$, and let us show that $(x+C') \bs \overline{B}(x, \delta ) \subseteq C$ where $C'$ is the cone generated by $O'$ originating at 0 (which does not depend on $\delta$). For $y \in O'$, let $z = (x + \lm y)/r$ where $r = \dabs{x + \lm y}$, then $z \in \BS_{0,1}$. Moreover, note that by the triangular inequality $\abs{r - \lm} \leq \ve$. Thus if $r \geq \delta/2$,
\be
\dabs{y - z} = \dabs{y - \frac 1r(\lm y + x)} = \frac 1r\dabs{(r - \lm)y - x} \leq \frac 2{\delta}((r - \geq) + \ve) \leq \frac {4\ve}{\delta} \leq \alpha
\ee
by definition of $\ve$. Hence $z \in O$ and hence $x + \lm y = rz \in C$. Now, if $\ve$ is further chosen such that $\ve \leq \delta/2$, then for all $x \in B(0,\ve)$ and for all $u \in (x + C') \bs \overline{B}(x, \delta)$, $r = \dabs{u} \leq \delta/2$ automatically by the triangular inequality, and thus the previous conclusion $u \in C$ holds. We have shown that $(x + C') \bs \overline{B}(x, \delta) \subseteq C$ as desired.]

Now by (iii) in Proposition \ref{pro:brownian_motion_limit_value}, if
\be
H^\delta_{C'} = \inf\bra{t > 0 : B_t \in C' \bs \overline{B}(0, \delta)},
\ee
then
\be
\pro_0(H^\delta_{C'} < \eta) \to \pro_0(H_{C'} < \eta) = 1 \quad \text{as }\delta \da 0.
\ee

Therefore, for all $\alpha > 0$ there exists $\delta > 0$ such that $\pro(H^\delta_{C'} \leq \eta) \geq 1 - \alpha$. Choosing $\ve = \ve(\delta)$ associated with this $\delta$, we find that for every $x \in B(y, \ve)$, we have by translation invariance, and letting $T_K$ be the hitting time of a set $K$,
\be
\pro_x(T > \eta) \leq \pro_x\bb{T_{C'_x\bs \overline{B}(x,\delta)} > \eta} = \pro_0(H^\delta_{C'}> \eta) \leq \alpha \quad \text{(by our choice of $\alpha$)}.
\ee

This means that $\pro_x(T > \eta) \to 0$ as $x \to y$, which proves the lemma.
\end{proof}

\begin{proof}[\bf The rest of the proof of Theorem \ref{thm:local_exterior_cone_condition}]
Let $y \in \partial D$. We want to estimate the quantity $\E_x[g(B_T)] - g(y)$ for some $x \in D$. For $\eta, \delta > 0$, let 
\be
A_{\eta,\delta} = \bra{\sup_{0\leq t\leq \eta} \abs{B_t - x} \geq \delta/2}.
\ee

This event decreases to $\emptyset$ as $\eta \da 0$ because $B$ has continuous paths. Now, for any $\delta, \eta > 0$,
\beast
\E_x[\abs{g(B_T ) - g(y)}] = \E_x[\abs{g(B_T ) - g(y)} , \bra{T \leq \eta}\cap A^c_{\delta,\eta}] + \E_x[\abs{g(B_T ) - g(y)} , \bra{T \leq \eta} \cap A_{\delta,\eta}] + \E_x[\abs{g(B_T ) - g(y)} , \bra{T \geq \eta}]
\eeast

Fix $\ve > 0$. We are going to show that each of these three quantities can be made $< \ve/3$ for $x$ close enough to $y$. Since $g$ is continuous at $y$, for some $\delta > 0$, $\abs{y-z} < \delta$ with $y, z \in \partial D$ implies $\abs{g(y) - g(z)} < \ve/3$. Moreover, on the event $\bra{T \leq \eta }\cap A^c_{\delta,\eta}$, we know that $\abs{B_T - x} < \delta/2$, and thus $\abs{B_T - y} \leq \delta$ as soon as $\abs{x - y} \leq \delta/2$. Therefore, for every $\eta > 0$, the first quantity is less than $\ve/3$ for $x \in \overline{B}(y, \delta/2)$.

Next, if $M$ is an upper bound for $\abs{g}$, the second quantity is bounded by $2M\pro(A_{\delta,\eta})$. Hence, by now choosing $\eta$ small enough, this is $< \ve/3$.

Finally, with $\delta, \eta$ fixed as above, the third quantity is bounded by $2M\pro_x(T \geq \eta)$. By the previous lemma, this is $< \ve/3$ as soon as $x \in B(y, \alpha) \cap D$ for some $\alpha > 0$. Therefore, for any $x \in B(y, \alpha \land \delta/2) \cap D$, $\abs{u(x) - g(y)} <\ve$. This entails the result.
\end{proof}

\begin{corollary}
A function $u: D \to \R$ is harmonic in $D$ if and only if it is in $\sC^2(D,\R)$, and satisfies $\Delta u = 0$.
\end{corollary}

\begin{proof}[\bf Proof]
Let $u$ be of class $\sC^2(D,\R)$ and be of zero Laplacian, and let $x \in D$. Let $\ve$ be such that $B(x,\ve) \subseteq  D$, and notice that $u|_{\overline{B}(x,\ve)}$ is a solution of the Dirichlet problem on $B(x, \ve)$ with boundary values $u|_{\partial B(x,\ve)}$. Then $B(x, \ve)$ satisfies the l.e.c.c., so that $u|_{B(x,\ve)}$ is the unique such solution, which is also given by the harmonic function of Theorem \ref{thm:local_exterior_cone_condition}. Therefore, $u$ is harmonic on $D$.
\end{proof}

\subsection{Girsanov's theorem}

Given a local martingale $M$, recall the definition of its exponential martingale (Theorem \ref{thm:exponential_martingale}), which will play a crucial role in what follows. Recall that if $M \in \sM_{c,loc}$ with $M_0 = 0$, then $Z_t = \exp\bb{M_t - \frac 12 [M]_t}$ defines a continuous local martingale by It\^o's formula. $Z$ is the
exponential (local) martingale of $M$ (sometimes also called the stochastic exponential of $M$) and we write $Z = \sE(M)$.

We start by the following inequality which will be useful in the proof of Girsanov's theorem, but is also interesting in its own right.


\begin{proposition}[Exponential martingale inequality]
Let $M \in \sM_{c,loc}$ with $M_0 = 0$. Then for all $x > 0$, $u > 0$,
\be
\pro\bb{\sup_{t\geq0} M_t > x , [M]_\infty \leq u} \leq e^{-x^2/(2u)}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Fix $x > 0$ and set $T = \inf\bra{t \geq 0 : M_t \geq x}$. Fix $\theta \in\R$ and set 
\be
Z_t = \exp\bb{\theta M^T_t - \frac 12\theta^2[M]^T_t}.
\ee

Then $Z \in\sM_{c,loc}$ and $\abs{Z} \leq e^{\theta x}$. Hence, $Z \in \sM^2_c$ and, by OST, $\E(Z_\infty) = \E(Z_0) = 1$. For $u > 0$ we get by Markov's inequality
\be
\pro\bb{\sup_{t\geq0} M_t > x , [M]_\infty \leq u} \leq \pro\bb{Z_\infty \geq e^{\theta x-\frac 12\theta^2u}} \leq e^{-\theta x+\frac 12 \theta^2u}.
\ee

Optimizing in $\theta$  gives $\theta = x/u$ and the result follows. (It is also possible to use the Dubins-Schwarz theorem, as a calculus argument shows that $\pro(\abs{Z} > \lm) \leq e^{-\lm^2/2}$ for all $\lm \geq 0$, when $Z$ is a standard Gaussian random variable).
\end{proof}


\begin{proposition}\label{pro:exponential_ui_martingale}
Let $M \in \sM_{c,loc}$ with $M_0 = 0$ and suppose that $[M]$ is a.s. uniformly bounded. Then $\E(M)$ is a UI martingale. 
\end{proposition}

\begin{proof}[\bf Proof]
Let $C$ be such that $[M]_\infty \leq C$ a.s. By the exponential martingale inequality, for all $x > 0$
\be
\pro\bb{\sup_{t\geq0} M_t \geq x} = \pro\bb{\sup_{t\geq0} M_t \geq x, [M]_\infty \leq C} \leq e^{-x^2/(2C)}.
\ee

Now, $\sup_{t\geq0} \sE(M)_t \leq \exp\bb{\sup_{t\geq0} M_t}$ and
\be
\E\bb{\exp\bb{\sup_{t\geq0}M_t}} = \int^\infty_0 \pro\bb{\sup_{t\geq0} M_t\geq \log \lm}d\lm \leq 1+ \int^\infty_1 e^{-(\log \lm)^2/(2C)} d\lm < \infty.
\ee

Hence, $\sE(M)$ is UI and, by Proposition \ref{pro:martingale_local_martingale_equivalent}, $\sE(M)$ is a martingale.
\end{proof}

Girsanov's theorem is a result which relates absolute continuous changes of the underlying probability measure $\pro$ to changes in the drift of the process. The starting point of the question could be formulated as follows. Suppose we are given realisations of two processes $X$ and $Y$, where $X$ is a Brownian motion and $Y$ is a Brownian motion with drift $b$. However, we do not know which is which. Can we tell them apart with probability 1 just by looking at the sample paths? If we get to observe them up to time 1 then we can, since $\lim_{t_\infty} Y_t/t = b$ almost surely. However, if we get to observe them only on a finite window, it will not be possible to distinguish them with probability 1. we ay that their law (restricted to $[0, T]$ for
any $T > 0$) is absolutely continuous with respect to one another. When such is the case, there is a "density" of the law of one process wit respective to the other. This density is a random variable which depends on $T$, and which will turn out to be a certain exponential martingale. 

Recall that for two probability measures $\pro_1$, $\pro_2$ on a measurable space $(\Omega,\sF), \pro_1$ is absolutely continuous with respect to $\pro_2$, $\pro_1 \ll \pro_2$, if
\be
\pro_2(A) = 0 \ \ra\ \pro_1(A) = 0 \quad\text{for all }A \in \sF.
\ee

In this case, by the Radon-Nikodym theorem, there exists a density $f: \Omega \to [0,\infty)$ which is $\sF$-measurable and $\pro_2$ unique almost surely (and hence $\pro_1$ unique almost surely as well), such that $\pro_1 = f \pro_2$. That is, for all $A \in \sF$, 
\be
\pro_1(A) = \int_\Omega f(\omega)\ind_A(\omega)d\pro_2(\omega).
\ee
$f$ is also called the Radon-Nikodym derivative, and we denote:
\be
\left.\frac{d\pro_1}{d\pro_2}\right|_{\sF} = f
\ee
(Note that in general, the density $f$ depends on the $\sigma$-field $\sF$).

In order to see where Girsanov's theorem comes from on a simple example where one can compute everything by hand, consider the following. Let $\sigma > 0$ and $b \neq 0$, and let $X_t = \sigma B_t + bt$. Then we claim that the law of $X$ is absolutely continuous with respect to the law of Brownian motion $Y_t = \sigma B_t$ with speed $\sigma$ (but without drift), so long as we restrict ourselves to events of $\sF_t$ for some fixed $t > 0$. Indeed, if $n \geq 1$ and $0 = t_0 < t_1 <\dots t_n = t$ and $x_0 = 0$, $x_1,\dots, x_n \in\R$, then we have:
\be
\pro(X_{t_1} = x_1,\dots,X_{t_n} = x_n) = C \exp\bb{- \sum^{n-1}_{i=0} \frac{(x_{i+1} - x_i - b(t_{i+1} - t_i))^2}{2\sigma^2(t_{i+1} - t_i)} }\prod^n_{i=1} dx_i
\ee

where $C$ is a factor depending on $t_1,\dots, t_n$ and $\sigma$, whose value is of no interest to us. It follows that
\be
\frac{\pro(X_{t_1} = x_1,\dots,X_{t_n} = x_n)}{\pro(Y_{t_1} = x_1,\dots, Y_{t_n} = x_n)} = e^{-Z}
\ee
where
\beast
Z & = & \sum^{n-1}_{i=0} \frac{(x_{i+1} - x_i - b(t_{i+1} - t_i))^2}{2\sigma^2(t_{i+1} - t_i)} - \frac{(x_{i+1} - x_i)^2}{2\sigma^2(t_{i+1} - t_i)} = \sum^{n-1}_{i=0} -\frac b{\sigma^2} (x_{i+1} - x_i) + \frac 1{2\sigma^2}  b^2(t_{i+1} - t_i)\\
& \to & - \int^t_0 \sigma^{-2} bdY_s + \frac 12 \int^t_0 b^2\sigma^{-2}ds\quad\text{ as }n\to\infty.
\eeast

(We have written the last bit as a convergence although there is an exact equality. This makes it clear that when $\sigma$ and $b$ depend on the position $x$ - which is precisely what defines the SDE's developed in the next chapter - then a similar calculation holds and Girsanov's theorem will hold.) Thus if $\Q$ is the law of $X$, and $\pro$ the law of $Y$,
\be
\left.\frac{d\Q}{d\pro}\right|_{\sF_t} = \exp\bb{\int^t_0 \frac b{\sigma} dB_s - \int^t_0 \frac{b^2}{2\sigma^2} ds} = \sE(b\sigma^{-2}Y)_t
\ee

So we have written the density of $X$ with respect to $Y$ as an exponential martingale. The point of view of Girsanov's theorem is a slightly different perspective, essentially the other side of the same coin. We will consider changes of measures given by a suitable exponential martingale, and observe the effect on the drift. It is of fundamental importance in mathematical finance (in the context of "risk neutral measures").

\begin{theorem}[Girsanov's theorem]\label{thm:girsanov}
Let $M \in \sM_{c,loc}$ with $M_0 = 0$. Suppose that $Z = \sE(M)$ is a UI martingale. We can define a new probability measure $\wt{\pro} \ll \pro$ on $(\Omega,\sF)$ by 
\be
\wt{\pro}(A) = \E(Z_\infty \ind_A), \quad A \in \sF.
\ee

Then for every $X \in \sM_{c,loc}(\pro)$, $X - [X,M] \in \sM_{c,loc}(\wt{\pro})$. Moreover the quadratic variation of $X$ under $\pro$ and of $X - [X,M]$ under $\wt{\pro}$ are identical $\pro$ and $\wt{\pro}$ almost surely.
\end{theorem}

\begin{proof}[\bf Proof]
Since $Z$ is UI, the limit $Z_\infty = \lim_{t\to\infty} Z_t$ exists $\pro$-almost surely, $Z_\infty \geq 0$ and $\E(Z_\infty) = \E(Z_0) = 1$. Thus $\wt{\pro}(\Omega) = 1$, $\wt{\pro}(\emptyset) = 0$ and countable additivity follows by linearity of expectation and the monotone convergence theorem. $\pro(A) = 0$ then $\wt{\pro}(A) = \int_A Z_\infty d\pro = 0$, so $\wt{\pro} \ll \pro$. Let $X \in \sM_{c,loc}$ and set 
\be
T_n = \inf\bra{t \geq 0 :\abs{X_t - [X,M]_t}\geq n}.
\ee

Since $X - [X,M]$ is continuous, $\pro(T_n \ua\infty) = 1$ with implies $\wt{\pro}(T_n \ua\infty) = 1$. So to show that $Y = X - [X,M] \in \sM_{c,loc}(\wt{\pro})$, it suffices to show that
\be
Y^{T_n} = X^{T_n} - [X^{T_n},M] \in \sM_c(\wt{\pro}) \quad \text{ for all }n \in \N.
\ee

Replacing $X$ by $X^{T_n}$, we reduce to the case where $Y$ is uniformly bounded. By the integration by parts formula and the Kunita-Watanabe identity,
\be
d(Z_t Y_t) = Y_tdZ_t + Z_t dY_t + d[Z_t, Y_t] =  Y_tZ_tdM_t + Z_t(dX_t - d[X_t,M_t]) + Z_t d[M_t,X_t] = Y_tZ_tdM_t + Z_tdX_t 
\ee
and so $Z Y \in \sM_{c,loc}(\pro)$. Also $\bra{Z_T: T \text{ is a stopping time}}$ is UI.

So since $Y$ is bounded, $\bra{Z_T Y_T : T \text{ is a stopping time}}$ is UI. Hence, $Z Y \in \sM_c(\pro)$. But then for $s \leq t$, if $A \in \sF_s$,
\be
\wt{\E}((Y_t - Y_s)\ind_A) = \E(Z_\infty(Y_t - Y_s)\ind_A) = \E[\ind_A(\E(Z_\infty Y_t |\sF_t) - \E(Z_\infty Y_s|\sF_s))] = \E[\ind_A(Z_tY_t - Z_sY_s)] = 0
\ee
since $Z Y \in \sM_c(\pro)$. Therefore, $Y \in \sM_c(\wt{pro})$ as required. The fact that the quadratic variation $[Y]$ is the same under $\wt{\pro}$ as it comes from the discrete approximation under $\pro$:
\be
[Y]_t = [X]_t = \lim_{n\to\infty} \sum^{\floor{2^n t}-1}_{k=0} (X_{(k+1)2^{-n}} - X_{k2^{-n}})^2
\ee
$\pro$-u.c.p. Thus there exists a subsequence $n_k$ for which the convergence holds $\pro$-almost surely uniformly on compacts. Since $\wt{\pro}$ is absolutely continuous with respect to $\pro$ this limit also holds $\wt{\pro}$ almost surely for this particular subsequence. Since the whole sequence converges in probability to $[Y]$ in the $\wt{\pro}$-u.c.p. sense (by general theory, since $Y \in \sM_{c,loc}(\wt{\pro}))$, this uniquely identifies the limit, and hence the quadratic variation $[Y]$ under $\wt{\pro}$ has the same value as under $\pro$.
\end{proof}


\begin{corollary}\label{cor:brownian_motion_change_measure}
Let $B$ be a standard Brownian motion under $\pro$ and $M \in \sM_{c,loc}$ such that $M_0 = 0$. Suppose $Z = \sE(M)$ is a UI martingale and $\wt{\pro}(A) = \E(Z_\infty \ind_A)$ for all $A \in \sF$. Then $\wt{B} := B - [B,M]$ is a $\wt{\pro}$-Brownian motion.
\end{corollary}

\begin{proof}[\bf Proof]
Since $\wt{B} \in \sM_{c,loc}(\wt{\pro})$ by Theorem \ref{thm:girsanov} has $[\wt{B}]_t = [B]_t = t$, by L\'evy's characterization, it is a Brownian motion.
\end{proof}


\begin{remark}
This corollary should be read backward. if $X$ is a Brownian motion, then changing the measure by the exponential martingale $\sE(M)$, $X = \wt{X} + [X,M]$ where $\wt{X}$ is a Brownian motion under the new measure. So the old process (which was just Brownian motion) becomes under the new measure a Brownian motion plus a "drift" term given by the covariation $[X,M]$.
\end{remark}

Let $(W,\sW,\W)$ be the Wiener space. (Recall that $W = C\bb{[0,\infty),\R}, \sW = \sigma(X_t : t \geq 0)$ where $X_t: W\to\R$ with $X_t(w) = w(t)$. The Wiener measure $\W$ is the unique probability measure on $(W,\sW)$ such that $(X_t)_{t\geq0}$ is a Brownian motion started from 0.)

\begin{definition}
Define the Cameron-Martin space
\be
H = \bra{ h \in W : h(t) = \int^t_0 \phi(s) ds \ \text{ for some }\phi^2 \in \sL^2\bb{[0,\infty)}}.
\ee

For $h \in H$, write $\dot{h} = \phi$ the derivative of $h$.
\end{definition}

\begin{theorem}[Girsanov, Cameron-Martin theorem]\label{thm:girsanov_cameron_martin}
Fix $h \in H$ and set $\W^h$ to be the law on $(W,\sW)$ of $(B_t + h(t), t \geq 0)$ where $B_t$ is a Brownian motion. that is, for all $A \in \sW$,
\be
\W^h(A) = \W\bb{\bra{w \in W : w + h \in A}}.
\ee

Then $\W^h$ is a probability measure on $(W,\sW)$ and $\W^h \ll \W$ with Radon-Nikodym density 
\be
\left.\frac{d\W^h}{d\W}\right|_{\sW} = \exp\bb{\int^\infty_0 \dot{h}(s)dX_s - \frac 12 \int^\infty_0 \dot{h}(s)^2 ds} .
\ee
\end{theorem}

\begin{remark}
So if we take a Brownian motion and shift it by a deterministic function $h \in H$ then the resulting process has a law which is absolutely continuous with respect to that of the original Brownian motion.
\end{remark}

\begin{proof}[\bf Proof]
Set $\sW_t = \sigma(X_s, s \leq t)$ and $M_t = \int^t_0 \phi(s)dX_s$. Then $M \in \sM^2_c\bb{W,\sW,(\sW_t)_{t\geq0},\W}$ and
\be
[M]_\infty = \int^\infty_0 \phi^2(s) ds = :C < \infty .
\ee

By Proposition \ref{pro:exponential_ui_martingale}, $\sE(M)$ is a UI martingale, so we can define a new probability measure $\pro \ll \W$ on $(W,\sW)$ by
\be
\frac{d\pro}{d\W}(\omega)= \exp\bb{M_\infty(\omega)- \frac 12 [M]_\infty(\omega)} =\exp\bb{\int^\infty_0 \phi(s)dX_s(\omega)- \frac 12\int^\infty_0 \phi^2(s)ds }
\ee
and $\wt{X} = X - [X,M] \in \sM_{c,loc}(\pro)$ by Girsanov's theorem. Since $X$ is a $\W$-Brownian motion, by Corollary \ref{cor:brownian_motion_change_measure}, $\wt{X}$ is a $\pro$-Brownian motion. But by the Kunita-Watanabe identity, 
\be
[X,M]_t = [X, \phi \cdot X]_t = \phi \cdot [X,X]_t = \int^t_0 \phi(s) ds = h(t)
\ee
hence we get that $\wt{X} (\omega) = X(\omega) - h = \omega - h$. Hence, under $\pro$, $X = \wt{X} + h$, where $\wt{X}$ is a $\pro$-Brownian motion. Therefore, $\W^h = \pro$ on $\sF_\infty = \sW$:
\be
\pro(A) = \sP\bb{\omega:\omega \in A} = \pro\bb{\bra{\omega: \wt{X} (\omega) + h \in A}} = \W\bb{\bra{\omega:\omega + h \in A}} = \W^h(A)
\ee
as required.
\end{proof}

One of the most important applications of Girsanov's theorem is to the study of Brownian motion with constant drift. Indeed, applying the previous result with $\phi(s) = \ind_{s\leq t}$ gives us the following corollary (check it!)

\begin{corollary}
Let $\gamma \neq 0$ and let $\W^\gamma$ be the law of $(X_t + \gamma t, t \geq 0)$ under $\W$. Then for all $t > 0$, and for any $A \in \sF_t$,
\be
\W^\gamma (A) = \E_\W \bb{\ind_A \exp(\gamma X_t - \frac 12 \gamma^2 t)} .
\ee
\end{corollary}

This allows us to compute functionals of Brownian motion with drift in terms of Brownian motion without drift - a very powerful technique. You will see some applications of this result in Example Sheet 3.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stochastic Differential Equations}

Suppose we have a differential equation, say $\frac{dx(t)}{dt} = b(x(t))$, or, in integral form,
\be
x(t) = x(0) + \int^t_0 b(x(s)) ds ,
\ee

which describes a system evolving in time, be it the growth of a population, the trajectory of a moving object or the price of an asset. Taking into account random perturbations, it may be more realistic to add a noise term.
\be
X_t = X_0 + \int^t_0 b(X_s) ds + \sigma B_t,
\ee
where $B$ is a Brownian motion and $\sigma$ is a constant controlling the intensity of the noise. It may be that this intensity depends on the state of the system, in which case we have to consider an equation of the form
\be\label{equ:stochastic_differential_equation_integral}
X_t = X_0 + \int^t_0 b(X_s) ds + \int^t_0 \sigma(X_s) dB_s,
\ee
where the last term is, of course, an It\^o integral. (\ref{equ:stochastic_differential_equation_integral}) is a stochastic differential equation and may also be written 
\be
dX_t = b(X_t) dt + \sigma(X_t) dB_t .
\ee

\subsection{General definitions}

Let $B$ be a Brownian motion in $\R^m$ with $m \geq 1$. Let $d \geq 1$ and suppose
\be
\sigma(x) = \bb{\sigma_{ij}(x)}_{1\leq i\leq d, 1\leq j\leq m} : \R^d \to \R^{d\times m} 
\ee
and
\be
b(x) = \bb{b_i(x)}_{1\leq i\leq d}:\R^d \to \R^d
\ee
are given Borel functions, bounded on compact sets. Consider the equation in $\R^d$:
\be\label{equ:stochastic_differential_equation}
dX_t = \sigma(X_t) dB_t + b(X_t) dt,
\ee
which may be written componentwise as
\be
dX^i_t = \sum^m_{j=1} \sigma_{ij}(X_t) dB^j_t + b_i(X_t) dt , \quad 1 \leq i \leq d .
\ee

This general SDE will be called $E(\sigma, b)$. A solution to $E(\sigma, b)$ in (\ref{equ:stochastic_differential_equation}) consists of 
\ben
\item a filtered probability space $\bb{\Omega,\sF, (\sF_t)_{t\geq0}, \pro}$ satisfying the usual conditions,
\item an $(\sF_t)_{t\geq0}$-Brownian motion $B = (B^1,\dots,B^m)$ taking values in $\R^m$,
\item an $(\sF_t)_{t\geq0}$-adapted continuous process $X = (X^1,\dots,X^d) \in \R^d$ such that
\be
X_t = X_0 + \int^t_0 \sigma(X_s) dB_s + \int^t_0 b(X_s) ds.
\ee
\een

When, in addition, $X_0 = x \in \R^d$, we say that $X$ is a solution started from $x$. 

There are several different notions of existence and uniqueness for SDE's, and we need to carefully distinguish between the various modes in which solutions can exist and be unique.

\begin{definition}
Let $E(\sigma, b)$ be the SDE in (\ref{equ:stochastic_differential_equation}).
\ben
\item We say that $E(\sigma, b)$ has a solution if for all $x \in\R^d$, there exists a solution to the SDE started from $x$.
\item There is uniqueness in law if all solutions to $E(\sigma, b)$ started from $x$ have the same distribution.
\item There is pathwise uniqueness if, when we fix $\bb{\Omega,\sF, (\sF_t)_{t\geq0},\pro}$ and $B$ then any two solutions $X$ and $X'$ satisfying $X_0 = X'_0$ a.s. are indistinguishable from one another.
\item We say that a solution $X$ of $E(\sigma, b)$ started from $x$ is a strong solution if $X$ is adapted to the natural filtration of $B$.
\een
\end{definition}

\begin{remark}
In general, $\sigma(B_s, s \leq t) \subseteq \sF_t$ and a solution might not be measurable with respect to the Brownian motion $B$. A strong solution depends only on $x \in \R^d$ and the Brownian motion $B$, and is moreover non-anticipative. if the path of $B$ is known up to time $t$, then so is the path of $X$ up to time $t$. We will term weak any solution that is not strong.
\end{remark}

\begin{remark}
If every solution is strong, then pathwise uniqueness holds. Indeed, any solution must then be a certain measurable functional of the path $B$. If two functionals $F_1$ and $F_2$ of $B$ gave two solutions to the SDE, then we would construct a third one by tossing a coin and choosing $X_1$ or $X_2$. This third solution would then not be adapted to $\sF^B$. 
\end{remark}

\begin{example}
It is possible to have existence of a weak solution and uniqueness in law without pathwise uniqueness. Suppose $\beta$ is a Brownian motion in $\R$ with $\beta_0 = x$. Set
\be
B_t = \int^t_0 \sgn(\beta_s) d\beta_s \quad \text{ where }\quad \sgn(x) = \left\{\ba{ll}
-1\quad\quad & \text{if }x \leq 0\\
1 & \text{if }x > 0
\ea\right.
\ee

Since sgn is left-continuous, $\bb{\sgn(\beta_t)}_{t\geq0}$ is previsible, so that the It\^o integral is well defined and $B \in \sM_{c,loc}$. By L\'evy's characterization, $B$ is a Brownian motion started from 0, since $[B]_t = [\beta]_t = t$. It is also true that
\be
\beta_t = x + \int^t_0 \sgn(\beta_s) dB_s.
\ee

(Indeed, by definition $dB_s = \sgn(\beta_s)d\beta_s$ so multiplying by $\sgn(\beta_s)$ yields, by the stochastic chain rule, $\sgn(\beta_s)dB_s = d\beta_s$.) Hence, $\beta$ is a solution to the SDE
\be\label{equ:sde_sgn}
dX_t = \sgn(X_t) dB_t ,\quad X_0 = x .
\ee

Thus (\ref{equ:sde_sgn}) has a weak solution. Applying L\'evy's characterization again, it is clear that any solution must be a Brownian motion and so there is uniqueness in law. On the other hand, pathwise uniqueness does not hold. Suppose that $\beta$ is a solution to (\ref{equ:sde_sgn}) with $\beta_0 = 0$. Then both $\beta$ and $-\beta$ are solutions to (\ref{equ:sde_sgn}) started from 0. Indeed, we may write
\be
-\beta_t = - \int^t_0 \sgn(\beta_s)dBs_ = \int^t_0 \sgn(-\beta_s)dB_s + 2 \int^t_0 \ind_{\beta_s=0}dB_s.
\ee

The second term is a local martingale since it is an integral with respect to $B$. The quadratic variation of this local martingale is $4\int^t_0 \ind_{\beta_s=0}ds$ which is 0 almost surely by Fubini's theorem (since $\beta$ must be a Brownian motion by L\'evy's characterization). Hence this local martingale is indistinguishable from 0, and $-\beta$ is a solution to (\ref{equ:sde_sgn}). 

It also turns out that $\beta$ is not a strong solution to (\ref{equ:sde_sgn}).
\end{example}

The following theorem (whose proof is omitted) shows however that there is no counterexample in the opposite direction.

\begin{theorem}[Yamada-Watanabe]
Let $\sigma$, $b$ be measurable functions. If pathwise uniqueness holds for $E(\sigma, b)$ and there exist solutions, then there is also uniqueness in law. In this case, for every filtered probability space $(\Omega,\sF, (\sF_t)_{t\geq0},\pro)$ and every $\sF_t$-Brownian motion $B = (B - t, t \geq 0)$, and for every $x \in \R^d$, the unique solution $X$ to $\E_x(\sigma, b)$ is strong.
\end{theorem}

In particular pathwise uniqueness is stronger than weak uniqueness, provided that there exist solutions.

\subsection{Lipschitz coefficients}

For $U \subseteq \R^d$ and $f: U \to \R^d$, say $f$ is Lipschitz with Lipschitz constant $K < \infty$ if 
\be
\abs{f(x) - f(y)} \leq K\abs{x - y} \quad \text{ for all }x, y \in U,
\ee
where $\abs{\cdot}$ denotes the Euclidean norm on $\R^d$. (If $f : U \to \R^{d\times m}$ then the left-hand side is the Euclidean norm in $\R^{d\times m}$). The key result of this section will be that SDE with Lipschitz coefficients have pathwise unique solutions which are furthermore always strong.

We start preparing for this result by recalling two important results which will be used in the proof.

\begin{theorem}[Contraction Mapping Theorem]
Let $(X, d)$ be a complete metric space and $F: X \to X$. Suppose that the iterated function $F^n$ is a contraction for some $n \in \N$, i.e.
\be
\exists r < \infty, \forall x, y \in X : d\bb{F^n(x), F^n(y)} \leq r d(x, y).
\ee
Then $F$ has a unique fixed point.
\end{theorem}

\begin{remark}
This theorem is most well-known when $F$ itself is contractive, rather than $F^n$. However, the theorem for $n \geq 1$ easily follows from the $n = 1$ result. Indeed, if $n \geq 1$ and $F^n$ is contractive, then (by the theorem for $n = 1$) $F^n$ must have a unique fixed point $x$. We claim that $x$ is also a fixed point of $F$. Indeed, let $x_1 = F(x)$, $x_2 = F^2(x),\dots, x_{n-1} = F^{n-1}(x)$. Then since $F^n(x) = x$, we have
\be
F^n(x_1) = F(F^{n-1}(x_1)) = F(F^n(x)) = F(x) = x_1 
\ee
so $x_1$ is a fixed point of $F^n$ as well. But $F^n$ has a unique fixed point, so $x = x_1$. Therefore, $F(x) = x_1 = x$ and $x_1$ is a fixed point of $F$.
\end{remark}

\begin{lemma}[Gronwall's Lemma]
Let $T > 0$ and let $f$ be a non-negative bounded measurable function on $[0, T]$. Suppose that for some $a, b \geq 0$:
\be
f(t) \leq a + b \int^t_0 f(s) ds 0,\quad  \leq t \leq T.
\ee
Then $f(t) \leq a \exp(b t)$ for all $t \in [0, T]$. In particular if $a = 0$ then $f = 0$.
\end{lemma}

\begin{proof}[\bf Proof]
The proof uses a trick which is close to what we will do in the proof of the next theorem. The idea is to iterate the required inequality. We get:
\beast
f(t) & \leq & a + b \int^t_0 a + b \int^s_0 f(u) du ds = a + abt + b^2 \int \int_{0\leq t_1\leq t_2} f(t_1)dt_1dt_2 \\
& \leq & \dots\leq a + abt + a \frac{b^2t^2}{2!} +\dots+ a\frac{b^nt^n}{n!} + b^{n+1} \int_{0\leq t_1\leq\dots\leq t_{n+1}\leq t} f(t_1)dt_1\dots dt_{n+1}
\eeast
where the term $b^nt^n/n!$ comes from the fact that $\int_{0\leq t_1\dots t_n\leq t} dt_1\dots dt_n = t_n/n!$, since the volume of the cube is $t^n$ and the ordering $t_1 \leq \dots t_n$ is one of $n!$ possible ordering of the variables, with each ordering contributing the same fraction to the total volume. This argument shows that the last term in the right-hand side of the inequality tends to 0 (since f is bounded). We recognize the Taylor expansion of the exponential function in all the other terms when $n\to\infty$. Thus $f(t) \leq ae^{bt}$.
\end{proof}


\begin{theorem}\label{thm:strong_solution}
Suppose that $\sigma:\R^d \to \R^{d\times m}$ and $b :\R^d \to \R^d$ are Lipschitz. Then there is pathwise uniqueness for the SDE
\be
dX_t = \sigma(X_t) dB_t + b(X_t) dt .
\ee

Moreover, for each filtered probability space $\bb{\Omega,\sF, (\sF_t)_{t\geq0}, \pro}$ and each $(\sF_t)_{t\geq0}$-Brownian motion $B$, there exists a strong solution to the SDE started from $x$, for any $x \in \R^d$.
\end{theorem}

\begin{proof}[\bf Proof]
(for $d = m = 1$). Fix $\bb{\Omega,\sF, (\sF_t)_{t\geq0},\pro}$ and $B$. Let $(\sF^B_t)_{t\geq0}$ be the natural filtration generated by $B$ so that $\sF^B_t \subseteq \sF_t$ for all $t \geq 0$. Suppose that $K$ is the Lipschitz constant for $\sigma$ and $b$.

Pathwise uniqueness. Suppose $X$ and $X'$ are two solutions on $\bb{\Omega,\sF, (\sF_t)_{t\geq0}, \pro}$ with $X_0 = X'_0$ a.s.. Fix $M$ and let
\be
\tau = \inf\bra{t \geq 0 : \abs{X_t} \vee \abs{X'_t} \geq M}.
\ee

Then $X_{t\land \tau} = X_0 + \int^{t\land \tau}_0 \sigma(X_s) dB_s + \int^{t\land \tau}_0 b(X_s) ds$, and similarly for $X'$.

Let $T > 0$. If $0 \leq t \leq T$ then, since
\be
(x + y)^2 \leq 2(x^2 + y^2)\quad\quad (*)
\ee
for all $x, y \in \R$, we have:
\beast
\E\bb{(X_{t\land \tau} - X'_{t\land \tau})^2} & \leq & 2\E\bb{\bb{\int^{t\land \tau}_0 \bb{\sigma(X_s) - \sigma(X'_s)}dB_s}^2} + 2\E\bb{\bb{\int^{t\land \tau}_0 (b(X_s) - b(X'_s))ds}^2} \\
& \leq &\underbrace{2\E\bb{\int^{t\land\tau}_0 \bb{\sigma(X_s) - \sigma(X'_s)}^2 ds} + 2T\E\bb{\int^{t\land\tau}_0 \bb{b(X_s) - b(X'_s)}^2 ds}}_{\text{by the It\^o isometry and the Cauchy-Schwarz inequality}}\\
& \leq & 2K^2(1 + T)\E\bb{\int^{t\land \tau}_0 (X_s - X'_s )^2 ds} \quad\text{(by the Lipschitz property)}\\
& \leq & 2K^2(1 + T) \int^t_0 \E\bb{(X_{s\land\tau} - X'_{s\land\tau})^2} ds. \quad\quad(**)
\eeast

Let $f(t) = \E\bb{(X_{t\land\tau} - X'_{t\land\tau} )^2}$. Then $f(t)$ is bounded by $4M^2$ and
\be
f(t) \leq 2K^2(1 + T) \int^t_0 f(s) ds.
\ee

Hence, by Gronwall's lemma, $f(t) = 0$ for all $t \in [0, T]$. So $X_{t\land\tau} = X'_{t\land \tau}$ a.s. and, letting $M, T \to\infty$, we obtain that $X$ and $X'$ are indistinguishable.

Existence of a strong solution. We start by constructing a weak solution as a fixed point of a certain mapping. Let $(\Omega, \sF, (\sF_t), \pro)$ be a filtered probability space and let $B$ be a Brownian motion. Write $\sC_T$ for the set of continuous processes $X :[0, T] \to \R$ adapted to $(\sF_t)$ such that
\be
\tabs{X}_T := \dabs{\sup_{t\leq T} \abs{X_t}}_2 < \infty
\ee
and $\sC$ for the set of continuous adapted processes $X : [0,\infty) \to \R$ such that 
\be
\tabs{X}_T < \infty \quad \text{ for all }T > 0 .
\ee

Recall from Proposition \ref{pro:cadlag_triple_norm_complete} that $\bb{\sC_T , \tabs{\cdot}_T}$ is complete. Let $\sC'_T = \sC_T \cap \{X' = x\}$, and let $\Phi$ be a mapping defined on $\sC'_T$ by
\be
\Phi(X)_t = x + \int^t_0 \sigma(X_s) dB_s + \int^t_0 b(X_s) ds\quad\text{ for all }t \leq T.
\ee

Note that a solution to $E(\sigma, b)$ is a fixed point of $\Phi$. We start by showing that if $X \in \sC'_T$, then so is $\Phi(X)$. For all $y \in \R$, 
\be
\abs{\sigma(y)} \leq \abs{\sigma(0)} + K\abs{y},\quad\quad \abs{b(y)} \leq \abs{b(0)} + K\abs{y}.\quad\quad(\dag)
\ee

Suppose $X \in \sC_T$ for some $T$. Let $M_t = \int^t_0 \sigma(X_s) dB_s$, $0 \leq t \leq T$. Then $[M]_T = \int^T_0 \sigma^2(X_s) ds$ and so by ($*$)
\be
\E\bb{[M]_T} \leq 2T\bb{\abs{\sigma(0)}^2 + K^2\tabs{X}^2_T} < \infty .\quad\quad (\dag\dag)
\ee

Hence, by Theorem \ref{thm:continuous_m2_martingale_implies_ui}, $(M_t)_{0\leq t\leq T}$ is a martingale bounded in $\sL^2$. So by Doob's $\sL^2$ inequality and ($\dag\dag$)
\be
\E\bb{\sup_{t\leq T} \abs{\int^t_0 \sigma(X_s) dB_s}^2} \leq 8T\bb{\abs{\sigma(0)}^2 + K^2\tabs{X}^2_T} < \infty .
\ee

Therefore $(M_t, t \geq 0)$ belongs to $\sC_T$. Similarly, by ($*$), ($\dag$) and the Cauchy-Schwarz inequality:
\be
\E\bb{\sup_{t\leq T} \abs{\int^t_0 b(X_s) ds}^2} \leq T\E\bb{\int^T_0 \abs{b(X_s)}^2 ds} \leq 2T^2\bb{\abs{b(0)}^2 + K^2\tabs{X}^2_T} < \infty.
\ee

Therefore, $(\int^t_0 b(X_s)ds, t \geq 0)$ belongs to $\sC_T$ as well. By the triangular inequality, it follows that $\Phi(X) \in \sC_T$ and thus $\Phi(X) \in \sC'_T$ since by definition $\Phi(X)_0 = x$. Now, let $X, Y \in \sC'_T$. By Doob's inequality again and ($**$),
\beast
\tabs{\Phi(X) - \Phi(Y )}^2_T & = & \E\bb{\sup_{0\leq t\leq T} \abs{\Phi(X)_t - \Phi(Y )_t}^2} \\
& \leq & 2\E\bb{\sup_{0\leq t\leq T} \abs{\int^t_0 \sigma(X_s)dB_s - \int^t_0 \sigma(Y_s)dB_s}^2} + 2\E\bb{\sup_{0\leq t\leq T} \abs{\int^t_0 b(X_s)ds -
\int^t_0 b(Y_s)ds}^2}\\
& \leq & 2K^2(4 + T) \int^T_0 \E(\abs{X_t - Y_t}^2)dt \leq \underbrace{2K^2(4 + T)}{C_T} \int^T_0 \tabs{X - Y}^2_t dt
\eeast

By induction using above equation, we have for all $n \geq 0$ that 
\be
\tabs{\Phi^n(X) - \Phi^n(Y)}^2_T \leq C^n_T \int\dots \int \tabs{X - Y}^2_{t_n}\ind_{0\leq t_n\leq\dots\leq t_1\leq T} dt_1\dots dt_n = \frac{C^n_T T^n}{n!} \tabs{X - Y}^2_T \quad (\dag\dag\dag)
\ee
by symmetry (see Gronwall's lemma). For $n$ sufficiently large, $\Phi^n$ is a contraction on the complete metric space $\bb{\sC'_T , \tabs{\cdot}_T}$. Hence, by the Contraction Mapping Theorem, $\Phi$ has a unique fixed point which we may call $X^{(T)} \in \sC'_T$.

By uniqueness of this fixed point, $X^{(T)}_t = X^{(T')}_t$ for all $t \leq T \land T'$ a.s. and so we may consistently define $X \in \sC$ by
\be
X_t = X^{(N)}_t \quad\text{for }t \leq N, N \in \N.
\ee

This is the pathwise unique solution to the SDE started from $x$. It remains to prove that it is $(\sF^B_t)_{t\geq0}$-adapted. Define a sequence $(Y^n)_{n\geq0}$ in $\sC_T$ by
\be
Y^0 \equiv x ,\quad \quad Y^n = \Phi(Y^{n-1}) \quad \text{for }n \geq 1.
\ee

Then $Y^n$ is $(\sF^B_t)_{t\geq0}$-adapted for each $n \geq 0$. Since $X = \Phi^n(X)$ for all $n \geq 0$ by ($\dag\dag\dag$) we have
\be
\tabs{X - Y^n}^2_T \leq \frac{C^n_T T_n}{n!} \tabs{X - x}^2_T.
\ee

Hence, $Y^n \to X$ in $\sC_T$ and thus $Y^n_t \to X_t$ in probability for a fixed $t > 0$. Thus there exists a subsequence $n_k$ such that $Y^{n_k}_t \to X_t$ almost surely. Since $Y^{n_k}_t$ is $\sF^B_t$-measurable, then so is $X_t$. Therefore $X$ is $(\sF^B_t)_{t\geq0}$-adapted and the proof of this theorem is finished.
\end{proof}


\begin{remark}
The uniqueness of the fixed point in the contraction mapping theorem can not be invoked directly to prove pathwise uniqueness of the solutions. What this result give is pathwise uniqueness of solutions in $\sC_T$ for any $T > 0$. So if we knew a priori that any solution belongs to $\sC_T$, we could invoke this result. (Note that our proof that $\Phi(X) \in \sC_T$ relies on the fact that $X$ is already assumed to be in $\sC_T$). Thus a byproduct of our proof is that indeed any solution belongs to $\sC_T$ for any $T > 0$.
\end{remark}

\begin{corollary}\label{cor:lipschitz_strong_solution}
Let $\sigma_{ij}$, $b_i$ be Lipschitz functions on $\R^d$ for $1 \leq i, j \leq d$. Then every solution to $\E_x(\sigma, b)$ is strong, and there is uniqueness in distribution for the solutions to $\E(\sigma, b)$.
\end{corollary}

\begin{proof}[\bf Proof]
The proof of the theorem constructs a strong solution for every filtered probability space and Brownian motion defined on it. On the other hand there is pathwise uniqueness of solutions so any solution must be strong. By the Yamada-Watanabe theorem, it also follow from existence of solutions and pathwise uniqueness that uniqueness in distribution holds.
\end{proof}

\begin{example}[Ornstein-Uhlenbeck process]
Fix $\lm \in\R$ and consider the SDE in $\R^2$
\be
dV_t = dB_t - \lm V_t dt ,\quad V_0 = v_0 ,\quad dX_t = V_t dt,\quad X_0 = x_0.
\ee

When $\lm > 0$ this models the motion of a pollen grain on the surface of a liquid, and $\lm$ then represents the viscosity of that liquid. $X$ represents the $x$-coordinate of the grain's position and $V$ represents its velocity in the $x$-direction. $-\lm V$ is the friction force due to viscosity. Whenever $\abs{V}$ becomes large, the system acts to reduce it. (This is a much more realistic model of random motion from a physical point of view than Brownian motion which oscillates too widly!) $V$ is called the Ornstein-Uhlenbeck (velocity) process. Then there is pathwise uniqueness for this SDE. In fact, this is a rare example of a SDE we can solve explicitly, see Problem 4.6.
\end{example}

\begin{remark}
If $\sigma$ and $b$ are only defined on a closed set $K$, then there is strong existence and pathwise uniqueness at least up until the time $\tau = \inf\bra{t \geq 0 :X_t \in K^c}$.
\end{remark}


\subsection{Strong Markov property and diffusion processes}

In an ordinary differential equation, the future of the trajectory of a particle is entirely determined by its present position. The stochastic analogue for stochastic differential equations is true as well. solutions to SDE's have the strong Markov property, i.e., the distribution of their future depends only on their present position. (In fact, SDE solutions should be viewed as the prototypical example of a strong Markov process.)

\begin{theorem}[Strong markov property]
Assume that $\sigma$ and $b$ are two Lipschitz functions. Then for all $x \in \R^d$, if $X^x$ denotes a weak solution started from $x$ to $\E(\sigma, b)$, if $F$ is any measurable nonnegative functional on $C([0,\infty),\R^d)$ then almost surely, for any stopping time $T$:
\be\label{equ:strong_markov_property_lipschitz_function_expectation}
\E\bsb{F(X^x_{T+t}, t \geq 0) |\sF_T} = \E[F(X^y_t , t \geq 0)]_{y=X_T},
\ee
on the event $\{T < \infty\}$.
\end{theorem}

\begin{proof}[\bf Proof]
By considering $T \land n$, it suffices to consider the case where $T < \infty$ a.s. As we will see, the strong Markov property is a relatively straightforward consequence of Corollary \ref{cor:lipschitz_strong_solution}. Let $Y_t = X^x_{T+t}$. Since $X$ is a solution to $E_x(\sigma, b)$, we have
\be\label{equ:strong_markov_property_lipschitz_function}
X^x_{T+t} - X^x_T = \int^{T+t}_T \sigma(X^x_s )dB_s + \int^{T+t}_T b(X^x_s )ds 
\ee
\end{proof}

To make the change of variable $u = t + T$, we use the following Lemma:

\begin{lemma}
Let $H$ be a previsible locally bounded process, and let $X$ be a continuous local martingale. If $T$ is a stopping time and $X^{(T)} = (X_{t+T} - X_T , t \geq 0)$ then
\be
\int^{t+T}_T H_sdX_s = \int^t_0 H_{T+u}dX^{(T)}_u
\ee
\end{lemma}

\begin{proof}[\bf Proof]
Only the case where $X$ is a local martingale needs to be discussed. The statement is trivial for processes of the form $H = \ind_{A\times(s,t]}$ where $A \in \sF_s$ and the general result follows by linearity and the It\^o isometry when $M \sM^2_c$ and $H \in \sL^2(M)$. Finally the general result follows by localization.

Thus, if $y = X_T$, then making the change of variable in (\ref{equ:strong_markov_property_lipschitz_function}) we get:
\be
Y_t = y + \int^t_0 \sigma(Y_u)dB^{(T)}_u + \int^t_0 b(Y_u)du
\ee
where $B^{(T)}_t = B_{T+t} - B_T$ is a Brownian motion independent from $\sF_T$. $Y$ is adapted to the filtration $(\sF_{T+u}, u \geq 0)$ which satisfies the usual conditions. Therefore, the previous theorem applies and $Y$ is adapted to $(\sG_t)_{t\geq0}$, where for all $t > 0$, $\sG_t$ is the $\sigma$-field generated by $X_T$ and $B^{(T)}_s$, $s \leq t$. Thus, we can write $(Y_t)_{t\geq 0}$ as a certain deterministic and measurable functional $\Phi$ of its starting point $X_T$ and the driving Brownian motion, $\Phi\bb{X_T ,B^{(T)}}$. 

Furthermore, note that by definition $\Phi(y,B)$ is the unique solution to $\E_y(\sigma, b)$ corresponding to the driving Brownian motion $B$. Hence (by weak uniqueness) $\Phi(y,B)$ has the same law as $X^y$. Since $B^{(T)}$ is independent from $\sF_T$, it is independent from $X_T$ (because $X$ is adapted to $\sF$). It follows that the left-hand side of (\ref{equ:strong_markov_property_lipschitz_function_expectation}) may be computed as:
\beast
\E[F(Y_t, t \geq 0)|\sF_T ] & = & \E\bb{F\bb{\Phi\bb{X_T ,B^{(T)}}}|\sF_T} \\
& = & \left.\E\bb{F\bb{\Phi\bb{y,B^{(T)}}}}\right|_{y=X_T} \quad\text{a.s.}\\
& = & \left.\E[F(X^y_t , t \geq 0)]\right|_{y=X_T}\quad \text{a.s.}
\eeast
which is exactly the content of the strong Markov property.
\end{proof}

In the remainder of this course we now provide a brief introduction to the theory of diffusion processes, which are Markov processes characterized by martingale properties. We first construct these processes with SDE's and then move on to describe some fundamental connection with PDE's. In the next section we show how diffusions arise as scaling limits of Markov chains.

Define, for $f \in C^2(\R^d)$,
\be\label{equ:l_form}
\sL f(x) = \frac 12 \sum^d_{i,j=1} a_{i,j}(x) \frac{\partial^2f}{\partial x_i\partial x_j} + \sum^d_{i=1} b_i(x) \fp{f}{x_i} 
\ee
where $a_{i,j}(x)$ is a measurable function called the diffusivity and $b(x)$, another measurable function, is called the drift. We assume that $(a_{i,j}(x))_{i,j}$ is a symmetric nonnegative matrix for all $x \in \R^d$.

\begin{definition}
Let $(\Omega,\sF, (\sF_t), \pro)$ be a filtered probability space satisfying the usual conditions. Say that a process $X = (X_t, t \geq 0)$ is an $\sL$-diffusion (or diffusion generated by $\sL$) if for all $f \in C^2_b (\R^d)$, the process $M^f$ is a local martingale, where for all $t \geq 0$:
\be
M^f_t = f(X_t) - f(X_0) - \int^t_0 \sL f(X_s)ds.
\ee
\end{definition}

For the moment, we don't know whether such processes exist, and we haven't shown any sort of uniqueness. The following result takes care of the existence part. 

\begin{theorem}\label{thm:sde_diffusion}
Let $X$ be a solution (in $\R^d$) to the SDE 
\be
dX_t = \sigma(X_t)dB_t + b(X_t)dt
\ee
where $B$ is a $(\sF_t)$-Brownian motion in $\R^d$, and where $\sigma = (\sigma_{i,j}(x))_{1\leq i,j\leq d}$ and $b = (b_i(x))_{1\leq i\leq d}$ are measurable. Then for all $f \in C^{1,2}(\R^+ \times \R^d)$,
\be\label{equ:sde_diffusion}
M^f_t = f(t,X_t) - f(0,X_0) - \int^t_0 \bb{\fp{f}{t} + \sL f} (s,X_s)ds
\ee
is a local martingale, where $\sL$ has the form (\ref{equ:l_form}) and $a = \sigma\sigma^T$. In particular, if the coefficients $\sigma$, $b$ are bounded, then $X$ is an $\sL$-diffusion. 
\end{theorem}

This results follows simply from an application of It\^o's formula.

\begin{remark}
\ben
\item If $a_{i,j}$ is uniformly positive definite (that is, there exists $\ve > 0$ such that
\be
\inner{A\xi}{\xi} = \sum^d_{i,j=1} \xi_ia_{ij}(x)\xi_j \geq \ve \abs{\xi}^2
\ee
for all $\xi\in \R^d$ and all $x \in \R^d$), then a has a positive-definite square root matrix $\sigma$. If $a$ is furthermore Lipschitz, then it can be shown that $\sigma(x)$ is also Lipschitz. It follows that if $a$, $b$ are bounded Lipschitz functions and $a$ is uniformly positive definite, then $\sL$-diffusions exist, by Theorem \ref{thm:strong_solution}, for any given starting point $X_0$.

\item Brownian motion in $\R^d$ is an $\sL$-diffusion for $\sL = \frac 12\Delta$.

\item In the language of Markov processes, we say that $\sL$ is the infinitesimal generator of $X$. Intuitively, $\sL f(x)$ describes the infinitesimal expected change in $f(X)$ given that $X_t = x$. That is, 
\be
\lim_{\ve\to 0}\E\bb{\left.\frac{f(X_{t+\ve}) - f(X_t)}{\ve}\right|\sF_t,X_t = x} = \sL f(x)
\ee
for every $f \in C^2_b (\R^d)$.
\een
\end{remark}


\subsection{Some links with PDEs}

In this section we state a theorem which we do not prove due to time constraints. (Parts of this result are easy and other less trivial...)

\begin{theorem}\label{thm:l_diffusion_solution}
Let $D$ be an open set in $\R^d$. Let L be defined by (\ref{equ:l_form}) for uniformly positive definite Lipschitz bounded coefficients $a$, $b$. Let $g \in C(\partial D)$ and let $\phi \in C(\overline{D})$ such that both $\phi$ and $g$ are bounded. Define:
\be
u(x) = \E_x\bb{\int^T_0 \phi(X_s)ds + g(X_T )}, \quad x \in D
\ee
where $X$ is an $\sL$-diffusion and $T = \inf\bra{t > 0 : X_t \notin D}$. Then $u$ is the unique continuous function on $\overline{D}$ which is solution to the Dirichlet problem:
\be
\left\{\ba{ll}
\sL u + \phi = 0 \quad\quad & \text{in }D\\
u = g & \text{on }\partial D.
\ea\right.
\ee
\end{theorem}

Another link is provided by the following Cauchy problem - that is, an evolution problem for which the initial condition is prescribed.

\begin{theorem}\label{thm:bounded_diffusion}
Let $g :\R^d \to \R$ be a given continuous bounded function, and let $X$ be an $\sL$ diffusion where $\sL$ satisfies the same assumptions as in Theorem \ref{thm:l_diffusion_solution}. Then if we define:
\be
u(t, x) = \E_x(g(X_t))\quad\text{ for all }t \geq 0, x \in \R^d
\ee
then $u$ is the unique solution in $C^{1,2}(\R^+ \times \R^d)$ to the problem:
\be
\left\{\ba{ll}
\fp{u}{t} = \sL u \quad\quad & \text{on }\R^+ \times \R^d\\
u(0, \cdot) = g & \text{on }\R^d.
\ea\right.
\ee
\end{theorem}

One word about the proof of the uniqueness part. let $v$ be a solution to this problem, and let $u$ be our candidate. Let us show that $v = u$. Fix $T > 0$ and let $f(t, x) = v(T - t, x)$. Applying (\ref{thm:sde_diffusion}) to the function $f$, we see that 
\be
M_t = v(T - t,X_t), \quad 0 \leq t \leq T
\ee
is a martingale. Thus
\be
\E(M_0) = \E(M_T)
\ee
and it follows that $v(T, x) = \E_x(g(X_T))$. The uniqueness part of the Theorem is proved. 

\begin{remark}
The application of (\ref{thm:sde_diffusion}), which defines diffusions) is a bit tricky to justify at this point. We will soon see that diffusions solve suitable SDE's (see Theorem \ref{thm:m_solution}) from which Theorem \ref{thm:sde_diffusion} follows. Alternatively, if $X$ is a diffusion then by the integration by parts formula, the process $M^f$ of (\ref{equ:sde_diffusion}) is a local martingale as soon as $f(t, x) = f_1(t)f_2(x)$ for some $\sC^2$ functions $f_1$ and $f_2$. Thus the class of $f$ for which $M^f$ is a local martingale contains all linear combinations of product functions $f_1(t)f_2(x)$. That (\ref{equ:sde_diffusion}) holds for general functions $f$ now follows from an approximation argument.
\end{remark}

\begin{remark}
Note that the Cauchy problem may reformulated as a Dirichlet problem in $\R^{d+1}$ by changing $L$ into
\be
\wt{L} = L - \fp{}{t}
\ee

Fix a point $(t, x)\in \R^{d+1}$. By Theorem \ref{thm:l_diffusion_solution}, the solution $u(t, x)$ is given by $\E( \wt{X}_T )$ where $X$ is the diffusion with generator $\wt{L}$. This corresponds to adding a coordinate $X^{d+1}$ to the diffusion $X$, such that $X^{d+1}_s = X^{d+1}_0 -s$, that is, time is decreasing at speed 1. The time $T$ corresponds to the first time that the "time" coordinate hits 0, i.e., time $t$ if we start from $(x, t)$. The other $d$ coordinates are then distributed according to $\pro_x(X_t \in \cdot)$. This proves Theorem \ref{thm:bounded_diffusion}, given Theorem \ref{thm:l_diffusion_solution}.
\end{remark}

\begin{example}
Let $(B_t, t \geq 0)$ be a 3-dimensional Brownian motion with $B_0 = 0$. Let $\tau = \inf\bra{t > 0:\dabs{B_t} = 1}$. Compute $\E(\tau)$. Answer. Let $R_t = \dabs{B_t}$. Then an application of It\^o's formula shows that
\be
dR_t = dB_t + \frac 1{R_t} dt.
\ee

It follows that $(R_t, t \geq 0)$ is a diffusion process on $(0,\infty)$, with generator:
\be
\sL = \frac 12 \frac{d^2}{dx^2} + \frac 1x \frac d{dx}.
\ee
Thus if $\phi \equiv 1$ and $g \equiv 0$ in the previous theorem, $\E_0(\tau) = u(0)$ where $u(x) = \E_x(\tau)$ is a function solving:
\be
\sL u = -1, \quad \text{ for all }x \in (0, 1).
\ee

Solving this ODE yields that if $f = u'$ then
\be
f(x) = \bb{-\frac 23 x^3 - c}x^{-2} 
\ee
for some constant $c \in\R$, so integrating:
\be
u(x) = -\frac 13x^2 + \frac cx + c'
\ee
for a constant $c' \in \R$. But we note that $c$ must be equal to 0. Indeed, otherwise $\E_0(\tau ) = \infty$ by the monotone convergence theorem, which is impossible by comparison with a one-dimensional Brownian motion and Theorem \ref{thm:brownian_motion_double_bounded}.

Thus
\be
u(x) = -\frac 13x^2 + c'
\ee
and since $u(1) = 0$ we have $u(x) = \frac 13(1 - x^2)$. Hence $\E_0(\tau ) = 1/3$.
\end{example}


\begin{problem}
The above calculation is not rigorous because of the singularity of $1/x$ at 0 (hence the SDE is not even properly defined at 0, let alone Lipschitzian...) Make it rigorous by considering the diffusion $R_t$ started at level $\ve$ and taking $D' = (\ve', 1)$ with $\ve' \ll \ve$. Letting $\ve'$ and $\ve \to 0$, recover the fact that $\E_0(\tau) = 1/3$.
\end{problem}

We now present a result called the Feynman-Kac formula, which is a systematic way of solving a class of PDE's by using Brownian motion. 

\begin{theorem}
Let $f\in C^2_b (\R^d)$ and let $V \in \sL^\infty(\R^d)$, that is, $V$ is uniformly bounded. For all $t, x \geq 0$, let
\be
u(t, x) = \E_x\bb{f(B_t) \exp\bb{\int^t_0 V (B_s)ds}}.
\ee

Then $u$ is the unique solution $w \in C^{1,2}_b (\R^+ \times \R^d)$ .
\be
\left\{\ba{ll}
\fp{u}{t} = \frac 12 \Delta u + V u \quad\quad & \text{on }\R^+ \times \R^d\\
u(0, \cdot) = f & \text{on }\R^d.
\ea\right.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Here again, the uniqueness part is an easy application of It\^o's formula. Let $u$ be a solution and let $M_t = u(T - t,B_t)E_t$ where $E_t = \exp\bb{\int^t
_0 V (B_s)ds}$ is of finite variation. By It\^o's formula:
\be
dM_t = \nabla u(T - t,B_t)E_tdB_t + \bb{-\dot{u} + \frac 12 \Delta u + V u} (T - t,B_t)E_tdt = \nabla u(T - t,B_t)E_tdB_t
\ee
since the second term is equal to 0 (because u is a solution to the PDE problem). Thus $M$ is a local martingale, and it is uniformly bounded on $[0, T]$, hence a true martingale. By the Optional Stopping Theorem:
\be
u(T, x) = \E_x(M_0) = \E_x(M_T ) = \E_x(f(B_T )E_T )
\ee
which is precisely the claim.
\end{proof}

\begin{remark}
This formula turns out to be very useful when applied the other way round. in fact, it was originally introduced to compute expectations involving exponential functionals of Brownian motion, which tend to occur frequently in mathematical finance and in statistical mechanics, where $V$ is a potential. (This is presumably why Feynman got interested in this problem). Then we can write:
\be
\E_x \bb{\exp\bb{-\beta \int^T_0 V (X_s)ds} f(X_T )} = u(x, T)
\ee
where
\be
\fp{u}{t} = \frac 12 \Delta u - \beta uV \quad \text{ on }\R^+ \times \R^d
\ee
and $u(x, 0) = f(x)$ for all $x \in \R^d$. This often makes these computations easier, by bringing in techniques that were developed in analysis (e.g., Fourier analysis). In mathematical finance, the Feynman-Kac formula allows to compute the Black-Scholes formula for the price of a call in terms of a certain PDE. This point of view is in some sense dual to ours, and it is a great advantage to have these two approaches for what is, fundamentally, the same object.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Stroock-Varadhan theory of diffusion approximation}

\subsection{Martingale problems}

Let $\sigma_{i,j}(x)_{1\leq i,j\leq d}$ and $(b_i(x))_{1\leq i\leq d}$ be a family of measurable functions with values in $\R$. Let $a(x) = \sigma(x)\sigma^T(x)$. (Here we assume for simplicity that $m = d$).

\begin{definition}
We say that a process $X = (X_t, t \geq 0)$ with values in $\R^d$, together with a filtered probability space $(\Omega,\sF, (\sF_t),\pro)$, solves the martingale problem $M(a, b)$ if for all $1 \leq i, j \leq d$,
\be
Y^i = \bb{X^i_t - \int^t_0 b_i(X_s)ds,\quad t \geq 0},\quad\quad \bb{Y^i_t Y^j_t - \int^t_0 a_{i,j}(X_s)ds,\quad t \geq 0}
\ee
are local martingales.
\end{definition}

Of course, the second condition implies that $[Y^i, Y^j ]_t = [X^i,X^j ]_t = \int^t_0 a_{i,j}(X_s)ds$. For instance, if $\sigma$, $b$ are in addition Lipschitz, then there exists $(\Omega,X, (\sF)_{t\geq0})$ and an $(\sF_t)$-Brownian motion $(B_t, t \geq 0)$ solution to the stochastic differential equation:
\be
dX_t = \sigma(X_t) \cdot dB_t + b(X_t)dt.
\ee
$X$ then solves the martingale problem $M(a, b)$. In fact, note that any (weak) solution to $E(\sigma, b)$ gives a solution to the martingale problem $M(a, b)$. More generally even, any $\sL$-diffusion will solve the martingale problem.

\begin{problem}
Let $X$ be an adapted continuous process. Show that the following are equivalent:
\ben
\item [(i)] $M^f_t = f(X_t) - f(X_0) - \int^t_0 \sL f(X_s)ds$ is a local martingale for every coordinate function $f$ (i.e., $f(x_1,\dots, x_n) = x_i)$ and every pairwise coordinate product function (i.e., $f(x_1,\dots, x_n) = x_ix_j$.)
\item [(ii)] $X$ solves the martingale problem $M(a, b)$. [Hint. In (i) ) (ii), to show that $Y^iY^j - \int a_{i,j}(X_s)ds$ is a local martingale, apply the
integration by parts formula to relate this $X^iX^j - \int^t_0 \sL f(X_s)ds$, showing that the difference between these two processes is a local martingale].
\een
\end{problem}

Rather remarkably, as the next theorem shows, these are the only solutions.

\begin{theorem}\label{thm:m_solution}
Let $a = \sigma\sigma^T$ and let $X$ and $(\Omega, (\sF_t), \pro)$ be a solution to $M(a, b)$. Then there exists an $(\sF_t)$-Brownian motion $(B_t, t \geq 0)$ in $\R^d$ defined on an enlarged probability space, such that $(X,B)$ solves $E(\sigma, b)$.
\end{theorem}

\begin{proof}[\bf Proof]
Assume first that $\sigma$ is invertible for every $x \in \R^d$. Then define $Y^i_t = X^i_t- \int^t_0 b^i_s(X_s)ds$, so that $Y^i \in \sM_{c,loc}$, and by definition we have $d[Y^i, Y^j ]_t = a_{i,j}(X_t)dt$. Define:
\be
B^i_t = \int^t_0 \sum^d_{k=1} (\sigma^{-1})_{i,k}(X_s)dY^k_s
\ee

Thus $B^i \in \sM_{c,loc}$. Since $a = \sigma\sigma^T$ and thus $\rho a\rho^T = I$ where $\rho  = \sigma^{-1}$, or, in coefficients, $\sum_{k,l} \rho_{i,k}a_{k,l}\rho_{j,l} = \delta_{i,j}$, we have:
\be
[B^i,B^j ]_t = \sum^d_{k,l=1} \int^t_0 \rho_{i,k}(X_s)\rho_{j,l}(X_s)a_{k,l}(X_s)ds = \delta_{i,j}t
\ee
so by L\'evy's characterization, $B$ is an Brownian motion in $\R^d$. 

Moreover, by the stochastic chain rule (Theorem \ref{thm:stochastic_chain_rule}),
\be\label{equ:stochastic_chain_rule}
\int^t_0 \sigma(X_s)dB_s = Y_t - Y_0 = X_t - \int^t_0 b(X_t)dt.
\ee

Indeed the $i$th component of the left-hand side may be written as
\be
\sum^d_{j=1} \int^t_0 \sigma_{i,j}(X_s)dB^j_s = \int^t_0 \sum^d_{j,k=1} \sigma_{i,j}\sigma^{-1}_{j,k} dY^k_s = \int^t_0 dY_s.
\ee
But (\ref{equ:stochastic_chain_rule}) is simply the statement that $(X,B)$ solves $\E(\sigma, b)$.

When $\sigma$ is not everywhere invertible, we proceed like in the generalized version of Dubins-Schwartz's (when $[M]_\infty < \infty$) and let the Brownian motion evolve independently when $s$ is such that $\sigma(X_s)$ is not invertible. See pp.190-191 of Revuz-Yor for the details and Durrett,
p.200 for the case $d = 1$.
\end{proof}

Theorem \ref{thm:m_solution} shows that there is a one-to-one correspondence between solutions to the stochastic differential equation $E(\sigma, b)$ and the martingale problem $M(a, b)$. In particular, there is uniqueness in distribution to the solutions of $E(\sigma, b)$, if and only if the solutions to
the martingale problem $M(a, b)$ are unique, where uniqueness means that all solutions to $M(a, b)$ with identical starting points have the same law.

\subsection{Notions of weak convergence of processes}

In this section we describe some basic results in the theory of weak convergence of processes, which we do not prove due to the time constraints. We will however use these results in the next section about the convergence of Markov chains towards solutions of certain stochastic differential equations.

The point of view here is similar to the one in Donsker's theorem. We view a process as a random variable with values in the space $\Omega$ of trajectories. We thus need to recall a few notions about weak convergence in general metric space. Let $(S, d)$ be a metric space. The distance function $d(x, y)$ satisfies $d(x, y) = 0$ if and only if $x = y$, $d(x, y) \geq 0$, $d(x, z) \leq d(x, y)+d(y, z)$. The open ball $B(x, r)$ is the set $\bra{y \in S: d(x, y) < r}$. The Borel $\sigma$-field is the field generated by all open sets.

The notion of convergence in distribution is defined in terms of test functions, which are only required to be bounded and continuous (for the topology of S).

\begin{definition}
Let $(\mu_n)_{n\geq1}$ be a sequence of probability distributions on $S$. We say that $\mu_n \to \mu$  weakly as $n\to\infty$, if $\int_S f d\mu_n \to \int_S fd\mu$ as $n\to\infty$ for all bounded continuous functions $f$. If $\mu_n$ is the law of a random variable $X_n$ and $\mu$ that of a random variable $X$, we say that $X_n \to X$ in distribution (or in law).
\end{definition}

There are a number of ways one can reformulate the notion of weak convergence in terms of the mass assigned to events that are either closed or open. If $A \subseteq S$, we recall the definition of the frontier of $A$, which is the set $\partial A := \ol{A}\bs \inter{A}$.

\begin{theorem}\label{thm:convergence_distribution}
Let $(X_n)_{n\geq1}$ be a sequence of random variables with values in $S$. The following are equivalent:
\ben
\item [(i)] $X_n \to X$ in distribution.
\item [(ii)] For all closed set $K$, $\limsup_{n\to\infty} \pro(X_n \in K) \leq \pro(X \in K)$.
\item [(iii)] For all open set $O$, $\liminf_{n\to\infty} \pro(X_n \in O) \geq \pro(X \in O)$.
\item [(iv)] For all sets $A$ such that $\pro(X \in \partial A) = 0$, $\limsup_{n\to\infty} \pro(X \in A) = \pro(X \in A)$.
\item [(v)] For all sets $A$ such that $\pro(X \in \partial A) = 0$, $\lim_{n\to\infty} \pro(X \in A) = \pro(X \in A)$.
\item [(vi)] For any bounded function $f$, denote by $D_f$ the set of discontinuities of $f$. Then for any $f$ such that $\pro(X \in D_f ) = 0$, $\E(f(X_n)) \to \E(f(X))$ as $n\to\infty$.
\een
\end{theorem}

It is important to note that the random variables $X_n$ need not be related in any particular way. In fact they may even be defined on different probability spaces. However, it turns out that (provided the metric space is sufficiently nice), one can always choose a common probability space for the random variables and define a sequence of random variables $Y_n$ with law identical to $X_n$, in such a way that convergence occurs almost surely. This is the content of the "Skorokhod representation theorem", which we may occasionally need.

\begin{lemma}
Suppose $S$ is complete and separable. If $\mu_n \to \mu$ weakly then there exists random variables $Y_n$ defined on $\Omega = [0, 1]$ equipped with the Lebesgue measure $\pro$, such that $Y_n \stackrel{d}{=} \mu_n$ for all $n \geq 1$, and $\lim_{n\to\infty} Y_n = Y$, $\pro$-almost surely, where $Y \stackrel{d}{=} \mu$.
\end{lemma}

We now specialize to the case where the random variables $X_n$ take values in the space $C$ of continuous trajectories over the compact interval $[0, 1]$. This is precisely the point of view in Donsker's theorem. We equip $C$ with the distance of the sup-norm:
\be
d(f, g) = \dabs{f - g}_\infty = \sup_{t\in [0,1]} \abs{f(t) - g(t)}.
\ee
This turns $C$ into a complete, separable metric space, on which it makes sense to talk about weak convergence.

\begin{example}
If $(S_n, n \geq 0)$ is a simple random walk on $\Z$, then by Donsker's theorem: $(S^{[N]}_t , 0 \leq t \leq 1)$, converges weakly towards a Brownian motion on $[0, 1]$, where $S^{[N]}_t = N^{-1/2}S_{N_t}$. 
\end{example}

\begin{problem}
Use Donsker's theorem and (vi) in Theorem \ref{thm:convergence_distribution} to show that if $L_N$ is the last time before time $N$ that $S_n = 0$:
\be
L_N = \max\bra{n \leq N : S_n = 0},
\ee
then $L_N/N \to L$ in distribution, where $L$ is the last zero of a Brownian motion before time 1. (By an exercise in the first example sheet, this has the arcsine law). The issue here is that the functional which associates to a path $\omega \in C$ its last zero $L(\omega)$ before time 1, is not continuous with respect to the topology of $C$. for instance, consider the sequence of function $f_\ve(t) = \ve t$, $0 \leq t \leq 1$, as $\ve \to 0$. However, it turns out that the set of functions for which $L$ is discontinuous has zero Wiener measure, and so the functions $L_N/N$ also converge in distribution to $L$.
\end{problem}


A classical trick in analysis for proving convergence of a sequence xn towards a limit $x$ is to prove that (a) the sequence takes its values in a compact set, and (b) there can only be one subsequential limit. It is usually part (a) which demands slightly harder work to establish, as part (b) follows from usually softer arguments (you typically have identified the limit at this stage). Fortunately there is a general criterion and fairly easy to use in practice, which tells us when the set $K = \bra{x_n}^\infty_{n=1}$ is compact (or, actually, precompact, meaning that $\ol{K}$ is compact). When this happens, we say that the sequence of processes $(X_n)_{n\geq1}$ is tight. This criterion consists in, roughly speaking, showing that the process doesn't oscillate too wildly.

This is the content of the following theorem. For a continuous path $\omega(t)$, $t \in [0, 1]$, let
\be
\osc_\delta(\omega) = \sup\bra{\abs{\omega(s) - \omega(t)} : \abs{s - t} \leq \delta}.
\ee

$\osc_\delta$ is simply the modulus of continuity of the path $\omega$, at precision $\delta$.

\begin{theorem}\label{thm:osc_pro}
Suppose that $(X_n)_{n\geq1}$ is a sequence of processes with values in $C$. Then $X^n$ is tight, if and only if for each $\ve > 0$, there exists $n_0,M \geq 1$ and $\delta > 0$ such that:
\ben
\item [(i)] $\pro(\abs{X^n(0)} > M) \leq \ve$ for all $n \geq n_0$.
\item [(ii)] $\pro(\osc_\delta > \ve) \leq \ve$.
\een
\end{theorem}

To summarize, to show that a sequence converges weakly in $C$, it suffices to prove that (i) and (ii) hold above and that there is a unique weak subsequential limit. This is for instance the case if one has already established convergence of the finite-dimensional distributions, i.e., convergence of the k-dimensional vector $(X^n_{t_1} ,\dots,X^n_{t_k})$ towards $(X_{t_1} ,\dots,X_{t_k})$, for any $k \geq 1$ and any choice of "test times" $t_1,\dots, t_k$. This could have been a possible route for proving Donsker's theorem, as convergence of finite-dimensional distributions is easy to establish.

Note that condition (i) in the above theorem says that the starting point of the process $X^n(0)$ takes values in a compact set with arbitrarily high probability. This is usually trivial since very often, the starting point of a process is a deterministic point such as 0.

In the next section, we will prove weak convergence of certain rescaled Markov chains towards diffusion processes. For this, we will usually use the fact that any weak subsequential limit must satisfy the associated martingale problem $M(a, b)$ for which sufficient smoothness of the coefficients proves uniqueness in distribution. However there is one (small) additional difficulty in this case. it will be more natural to work with right-continuous processes $X^n$ rather than with the linear-interpolation of $X^n$, which typically loses some of the Markov property. Let $D$ be the space of right-continuous paths on $[0, 1]$. Without entering into the details, $D$ can also be equipped with a complete separable metric $d$, which is called the Skorokhod topology. It can also be proved that if a sequence of right-continuous processes $X^n$ satisfy (i) and (ii) in Theorem \ref{thm:osc_pro}, then $X^n$ is also tight and any subsequential limit $X$ must be continuous, in the sense that $\pro(X \in C) = 1$. Furthermore, weak convergence with respect to the Skorkhod topology towards a continuous process $X$, implies weak convergence in $C$ of the linear interpolations. Another fact which will be needed is that if $x_n \to x$ in the Skorokhod topology, then $x_n(t) \to x(t)$ for all $t \geq 0$.

\subsection{Markov chains and diffusions}

The result which we now discuss is due to Stroock and Varadhan (Chapter 11 in [4]), and shows a link between rescaled Markov chains and certain diffusion processes. It is applicable in a remarkably wide variety of contexts, of which we will only have the time to give one example. Our treatment follows rather closely the book of Durrett [1] which can be used to look up additional details.

While the idea for the statement of the result is in fact fairly simple, there is quite a bit of notation to introduce. We assume that a certain Markov chain is given to us. A certain scaling parameter $h > 0$ is going to 0, and we assume that the chain has already been rescaled, so it takes it values in a certain set $S_h \subseteq\R^d$. We will denote this Markov chain by $(Y^h_n , n \geq 1)$. The transition probabilities of $Y$ are given by a transition kernel $\Pi_h$ which may depend on $h > 0$:
\be
\pro(Y^h_{n+1} \in A|Y^h_n = x) = \Pi_h(x,A).
\ee

We define the random process on $[0, 1]$ by
\be
X^h_t = Y^h_{h\floor{t/h}},\quad  t \in [0, 1]
\ee
so that $X^h$ is almost surely right-continuous and is constant between two successive jumps of the chain, which may occur every $h$ units of time for the process $X^h$. We let $K_h$ denote the rescaled transition kernel.
\be
K_h(x, dy) = \frac 1h \Pi_h(x, dy).
\ee

Roughly, the conditions of the theorem states that "the infinitesimal mean variance of the jumps of $X$ when $X = x$ are approximately given by $b(x)$ and $\sigma(x)$, respectively". The conclusion states that $X^h$ converges weakly towards the solution of $M(a, b)$.

For $1 \leq i, j \leq d$, define:
\be
a^h_{i,j} = \int_{\abs{y-x}\leq1} (y_i - x_i)(y_j - x_j)K_h(x, dy),\quad b^h_i (x) = \int_{\abs{y-x}\leq1} (y_i - x_i)K_h(x, dy),\quad \Delta^h_\ve(x) = K_h(x,B(x, \ve)^c).
\ee

Suppose that $a_{ij}$ and $b_i$ are continuous coefficients on $\R^d$ for which the martingale problem $M(a, b)$ is well posed, i.e., for each $x \in \R^d$ there is a unique in distribution process $(X_t, 0 \leq t \leq 1)$ such that $X_0 = x$ almost surely, and
\be
Y^i_t = X^i_t - \int^t_0 b_i(X_s)ds,\quad Y^i_t Y^j_t - \int^t_0 a_{ij}(X_s)ds
\ee
are both local martingales.

\begin{theorem}\label{thm:linear_interpolation_converge_weakly}
Suppose that the above holds, and that for every $1 \leq i, j \leq d$, and every $R > 0$, every $\ve > 0$,
\ben
\item [(i)] $\lim_{h\to 0} \sup_{\abs{x}\leq R}\abs{a^h_{ij}(x) - a_{ij}(x)} = 0$.
\item [(ii)] $\lim_{h\to 0} \sup_{\abs{x}\leq R}\abs{b^h_i (x) - b_i(x)} = 0$.
\item [(iii)] $\lim_{h\to 0} \sup_{\abs{x}\leq R} \Delta^h_\ve (x) = 0$.
\een

Then if $X^h_0 = x_h \to x_0$, we have $(X^h_t , 0 \leq t \leq 1) \to (X_t, 0 \leq t \leq 1)$ weakly in $D$, and in particular, the linear interpolations of $Y^h$ converge weakly in $C$.
\end{theorem}

The rest of this section is devoted to a proof of this result. By localization, one may replace (i), (ii) and (iii) by the following stronger conditions:
\ben
\item [(i)] $\lim_{h\to 0} \sup_{x\in\R^d} \abs{a^h_{ij}(x) - a_{ij}(x)} = 0$.
\item [(ii)] $\lim_{h\to 0}\sup_{x\in\R^d} \abs{b^h_i (x) - b_i(x)} = 0$.
\item [(iii)] $\lim){h\to 0} \sup_{x\in \R^d} \Delta^h_\ve (x) = 0$.
\item [(iv)] Moreover, $a^h_{i,j}$, $b^h_i$, $\Delta^h$ are uniformly bounded in $h$ and $x$.
\een

Step 1. Tightness

Let $f$ be a bounded and measurable function. Define the operator $\sL^h$ by 
\be
\sL^hf(x) = \int K_h(x, dy)(f(y) - f(x)).
\ee

This is the "generator" of the process. this represents the infinitesimal change in the function $f$ when the process is at $x$. In particular, note that the process 
\be
f(Y^h_k) - \sum^{k-1}_{j=0} h\sL^h f(Y^h_j), \quad k = 0, 1,\dots
\ee
is a (discrete-time) martingale. For our proof of tightness we are going to need an estimate on the time needed by the chain to make a deviation of size roughly $\ve > 0$, when it starts at position $y \in \R^d$. To do this, we introduce a function $g :\R \to\R$ such that $g \in \sC^2$, $0 \leq g \leq 1$
and $g(x) = 0$ if $x \geq 1$, while $g(0) = 1$. We also define for $x \in\R^d$, $f_\ve(x) = g(\abs{x}^2/\ve^2)$ which is also $\sC^2$, and becomes 0 when $\abs{x} \geq \ve$, and for $a \in \R^d$, let $f_{a,\ve}(x) = f_\ve(x - a)$.


\begin{lemma}\label{lem:bounded_diffusion}
There exists $C_\ve < \infty$, independent of $h$, such that $\abs{\sL^h f_{a,\ve}(x)} \leq C_\ve$ for all $a, x \in \R^d$.
\end{lemma}

\begin{proof}[\bf Proof]
This is simply an application of Taylor expansion. For $t \in [0, 1]$ and $x, y \in\R^d$, let $\phi(t) = f_{a,\ve}(x + t(y - x))$. Then by Taylor's theorem, there exists $c_{xy} \in [0, 1]$ such that
\be
f_{a,\ve}(y) - f_{a,\ve}(x) = \phi(1) - \phi(0) = \phi'(0) + \frac 1{2!} \phi''(c_{xy}) = \sum^d_{i=1} (y_i - x_i)D_if(x) + \sum^d_{1\leq i,j} (y_i - x_i)(y_j - x_j)D_{ij}f(z_{xy})
\ee
where $f \equiv f_{a,\ve}$ and $D_i$ and $D_{ij}$ stand for $\fp{f }{x_i}$ and $\frac{\partial^2f}{\partial x_i\partial x_j}$ respectively, while $z_{xy} = x+c_{xy}(y - x) \in [x, y]$.

To obtain $\sL^hf(x)$, we integrate the above with respect to $K_h(x, dy)$, and get.
\beast
\sL^hf_{a,\ve}(x)  & = & \int K_h(x, dy)(f_{a,\ve}(y) - f_{a,\ve}(x)) \leq \abs{\nabla f_{a,\ve}(x) \cdot \int_{\abs{y-x}\leq1} (y - x)K_h(x, dy)} \\
& & \quad +\abs{\int_{\abs{y-x}\leq1} \sum_{i,j} (y_i - x_i)(y_j - x_j)D_{ij}f_{a,\ve}(z_{xy})K_h(x, dy)} + 2\dabs{f_{a,\ve}}_\infty K_h(x,B(x, 1)^c).
\eeast

Let $A_\ve = \sup_x \abs{\nabla f_{a,\ve}(x)}$, let $B_\ve = \sup_z \dabs{D f(z)}$, where $D f = (D_{ij}f)_{1\leq i,j\leq d}$ is the Hessian matrix of $f$ and for a matrix $M = (m_{ij})$ we note 
\be
\dabs{m_{ij}} := \sup_{u\in \R^d:\abs{u}=1} \abs{\inner{u}{Mu}}.
\ee

Thus 
\be
\abs{\sum_{i,j} (y_i - x_i)(y_j - x_j)D_{ij}f_{a,\ve}(z_{xy})} \leq \abs{y - x}^2B_\ve 
\ee
hence by Cauchy-Schwarz
\be
\sL^h f_{a,\ve}(x) \leq A_\ve \abs{b^h(x)} + B_\ve \int_{\abs{y-x}\leq 1} \abs{y - x}^2 K_h(x, dy) + 2K_h(x,B(x, 1)^c).
\ee

Since $\int_{\abs{y-x}\leq 1} \abs{y - x}^2 K_h(x, dy) = \sum_i a^h_{ii}(x)$ and since we have assumed in (iv) that all those quantities were uniformly bounded, we have proved the lemma. 
\end{proof}


To estimate $\osc_\delta(X^h)$, we introduce the following random variables. $\tau_0 = 0$,
\be
\tau_n = \inf\bra{t \geq \tau_{n-1}: \abs{X^h_t - X^h_{\tau_{n-1}} } \geq \ve}, \quad N = \min \bra{n: \tau_n > 1},\quad \sigma =\min \bra{\tau_n - \tau_{n-1} : 1 \leq n \leq N}
\ee
and, finally
\be
\theta = \max\bra{\abs{X^h(t) - X^h(t^-)} : 0 < t \leq 1}.
\ee

The relation between these random variables and tightness is provided by the following lemma.

\begin{lemma}
Assume that $\sigma >\delta$ and that $\theta < \ve$. Then $\osc_\delta(X^h) \leq 4\ve$.
\end{lemma}

\begin{proof}[\bf Proof]
The proof is straightforward. We want to show that for all $s, t \in [0, 1]$ with $\abs{s-t} \leq \delta$, $\abs{X^h(s) - X^h(t)} \leq 4\ve$. The point is that since $\abs{s - t} \leq\delta < \sigma$, $s$ and $t$ can only span at most one of the interval $[\tau_{n-1}, \tau_n]$, and by definition of these stopping times, everything behaves well on those intervals. Thus if $\tau_{n-1} \leq s < t < \tau_n$, then $\abs{f(s) - f(t)} \leq 2\ve$. If on the other hand, $\tau_{n-1} \leq s < \tau_n \leq t$, then
\be
\abs{f(s) - f(t)} \leq \abs{f(s) - f(\tau_{n-1})}  + \abs{f(t) - f(\tau_n)} + \abs{f(\tau_n) - f(\tau^-_n)} + \abs{f(\tau^-_n) - f(\tau_{n-1})} \leq 4\ve.
\ee
\end{proof}

We now use this to prove the tightness estimate. Since it is assumed that the starting point $X^h_0 = x^h_0$ is nonrandom and converges towards a fixed $x_0$, it suffices to prove the statement about oscillations. for all $\ve$, there exists $\delta > 0$ and h0 such that for all $h \leq h_0$,
\be
\pro(\osc_\delta(X^h) \geq \ve) \leq \ve.
\ee

Thus it follows to prove that for all $h$ sufficiently small and for $\delta$ small enough, $\pro_x(\theta > \ve/4) \to 0$ as $h \to 0$, and $\pro_x(\sigma > \delta) \to 0$ for $h \to 0$ for all $x \in \R^d$. The first one is very simple. since there are at most $1/h$ time steps in the unit interval $[0, 1]$, a simple union bound yields 
\be
\pro_x(\theta > \ve) \leq \frac 1h \sup_y \Pi_h(y,B(y, \ve)^c) \leq \sup_y \Delta^h_\ve (y) \to 0
\ee
by (iii). The second one requires more arguments. We follow the elegant argument introduced by Stroock and Varadhan (Theorem 1.4.6). The first step is to estimate $pro_x(\tau_1 \leq u)$ for small $u$. Note that by Lemma \ref{lem:bounded_diffusion}, the process
\be
f_{x,\ve}(Y^h_k ) + C_\ve hk,\quad k = 0, 1, \dots 
\ee
is a submartingale. Thus letting $\tau = \inf\bra{k \geq 1 : \abs{Y^h_k - x} > \ve}$, so that $\tau_1 = h\tau$. Using the Optional stopping theorem at $\tau \land u'$ with $u' = u/h$,
\be
\E_x\bb{f_{x,\ve}(Y^h_{\tau\land u'}) + C_\ve h(\tau \land u')} \geq 1.
\ee

Since $\tau \land u \leq u$ and since on the event that $\tau \leq u'$, we have that $\abs{Y^h_{\tau\land u'} - x} \geq \ve$, so $f_{x,\ve}(Y^h_{
\tau\land u'}) = 0$, we have.
\be
\pro_x(\tau_1 \leq u) = \pro(\tau \leq u') \leq \E_x\bb{1 - f_{x,\ve}(Y^h_{\tau\land u'})} \leq hC_\ve u' = C_\ve u.
\ee

This has the following consequence. for all $u > 0$, letting $p = \pro_x(\tau \leq u)$:
\be
\E_x(e^{-\tau}) \leq \pro_x(\tau \leq u) + e^{-u}\pro_x(\tau \geq u) \leq p + e^{-u}(1 - p) \leq e^{-u} + p(1 - e^{-u}) \leq e^{-u} + pu \leq 1 - u + C_\ve u^2
\ee

Thus by choosing $u$ small enough, we can find $\lm < \infty$, independent of $x$ or $\delta$ (depending solely on $\ve$ through $C_\ve$), such that $\E_x(e^{-\tau}) \leq \lm$. Now, iterating and using the strong Markov property at times $\tau+1,\dots, \tau_n$, which are stopping times,
\be
\E_x(e^{-\tau_n}) \leq \lm^n
\ee
since $\lm$ does not depend on $x$, and thus by Markov's inequality:
\be
\pro_x(N > n) = \pro_x(\tau_n < 1) \leq \pro_x(e^{-\tau_n} \geq e^{-1}) \leq e\E_x(e^{-\tau_n}) \leq e\lm^n.
\ee

We finish by saying that
\be
\pro_x(\sigma \leq \delta) \leq k \sup_y \pro_y(\tau \leq \delta) + \pro_x(N > k) \leq C_\ve k\delta + e\lm^k.
\ee
Thus we take $k$ large enough that $e \lm^k < \ve/2$ and then pick $\delta$ small enough that $C_\ve k\delta < \ve/2$. We are then done for the proof of tightness.

Step 2. Uniqueness of the weak subsequential limits.

Since we have assumed that the martingale problem $M(a, b)$ was well posed, it suffices to show that the limit of any weakly convergent subsequence solves the martingale problem $M(a, b)$. Our first step for doing so is to show that the generator of the Markov chain $\sL^h$ converges in a suitable sense to the generator $\sL$ of the diffusion:
\be
\sL f(x) = \frac 12 \sum^d_{i,j=1} a_{ij}(x) \frac{\partial^2f}{\partial x_i\partial x_j} (x) + \sum^d_{i=1} b_i(x) \fp{f}{x_i}(x)
\ee

\begin{lemma}
Let $f \in \sC^2_K$ be twice differentiable and with compact support. Then $\sL^h f(x) \to \sL f(x)$ uniformly over $x \in \R^d$ as $h \to 0$.
\end{lemma}

\begin{proof}[\bf Proof]
Going back to our Taylor expansion of $\sL^h f(x)$, and recalling the definition of $b^h_i (x)$ and $a_{ij}(x)$, we may write:
\be
\sL^hf(x) = \sum^d_{i=1} b^h_i (x)D_if(x) + \int_{\abs{y-x}\leq1} \sum^d_{i,j=1} (y_i - x_i)(y_j - x_j)D_{ij}f(z_{xy})Kh(x, dy) + \int_{\abs{y-x}>1} [f(y) - f(x)]K_h(x, dy)
\ee

The final term in the right-hand side converges to 0 uniformly in $x$ by assumption (iii) with $\ve = 1$. To deal with the first term, note that
\be
\abs{\sum^d_{i=1} b^h_i (x)D_if(x) - \sum^d_{i=1} b_i(x)D_if(x) } \leq \sup_{1\leq i\leq d} \abs{b^h_i (x) - b_i(x)} \sum^d_{i=1} \dabs{D_if}_\infty
\ee
which converges to 0 uniformly in $x$ by assumption (ii) (since $f\in \sC^2_K$). It remains to deal with the central term. Recalling the definition of $a^h_{ij}(x)$, we get:
\beast
& & \abs{\int_{\abs{y-x}\leq1} \sum^d_{i,j=1} (y_i - x_i)(y_j - x_j)D_{ij}f(z_{xy})K_h(x, dy) - \sum^d_{i,j=1} a_{ij}(x)D_{ij}f(x) }\\
& \leq & \abs{\sum^d_{i,j=1} a^h_{ij}(x)D_{ij}f(x) - \sum^d_{i,j=1} a_{ij}(x)D_{ij}f(x)} + \abs{\int_{\abs{y-x}\leq1} \sum^d_{i,j=1} (y_i - x_i)(y_j - x_j )[D_{ij}f(z_{xy}) - D_{ij}f(x)]K_h(x, dy)}.
\eeast

The first term converges to 0 uniformly in $x$ by (i) and the fact that the derivatives of $f$ are uniformly bounded. The second term can be split in an integral over $\abs{y - x} > \ve$ and $\abs{y -x} \leq \ve$. The first one converges to 0 uniformly in $x \in \R^d$ thanks to (iii) and the fact that
the integrand is bounded. For the other term, let 
\be
\Gamma(\ve) = \sup_{1\leq i,j\leq d} \sup_{\abs{y-x}\leq\ve} \abs{D_{ij}f(z_{xy}) - D_{ij}f(x)}.
\ee

Then since $z_{xy}$ lies on the segment between $x$ and $y$, and since $D_{ij}f$ is continuous on the compact set $K$ (and hence uniformly continuous), $\Gamma(\ve) \to 0$ as $\ve \to 0$. On the other hand,
\be
\abs{\int_{\abs{y-x}\leq1} \sum^d_{i,j=1} (y_i - x_i)(y_j - x_j )[D_{ij}f(z_{xy}) - D_{ij}f(x)]K_h(x, dy)} \leq \Gamma(\ve) \int_{\abs{y-x}\leq\ve} \abs{y - x}^2K_h(x, dy),
\ee
by Cauchy-Schwarz's inequality, so the proof of the lemma is complete. 
\end{proof}

\begin{proof}[\bf Proof]
We now use this lemma to conclude the proof of Theorem \ref{thm:linear_interpolation_converge_weakly}. Fix $h_n \to 0$ such that $X^{h_n} \to X$ weakly (in $D$) as $n\to\infty$. (Recall that $X^h$ is defined as $X^h_t = Y^h_{\floor{t/h}}$.) Fix $s < t$. Then for any continuous functional $F : D \to\R$ which is measurable with respect to $\sF_s$, we have, since $\sL^h$ is the discrete generator of $Y$,
\be
f(X^{h_n}_{kh_n}) - \sum^{k-1}_{j=0} h_n\sL^{h_n} f(X^{h_n}_{jh_n}), \quad k = 0, 1, \dots 
\ee
is a martingale. In particular, taking $k = k_n$ such that $kh_n > s$, i.e., $k_n = \ceil{s/h_n}$, and taking $\ell_n$ similarly so that $\ell_nh_n > t$, i.e., $\ell_n = \ceil{t/h_n}$, we get
\be
\E_x\bb{F(X^{h_n}) \bb{f(X^{h_n}_{\ell_nh_n} ) - f(X^{h_n}_{k_nh_n} ) - \sum^{\ell_n-1}_{j=k_n} h_n\sL^{h_n} f(X^{h_n}_{jh_n})}} = 0 .
\ee

By using the Skorokhod representation theorem, one may find $Y^n$ such that $Y^n \stackrel{d}{=} X^{h_n}$ and $Y^n \to Y$ almost surely, where $Y \stackrel{d}{=} X$. We recognize a Riemann sum in this expectation. Since almost sure convergence in $D$ implies almost sure convergence of the marginals, we
conclude by the Lebesgue convergence theorem that
\be
\E_x\bb{F(X)\bb{f(X_t) - f(X_s) - \int^t_s \sL f(X_u)du}} = 0.
\ee

Since $F$ is an arbitrary continuous function on $D$, it follows that
\be
f(X_t) - \int^t_0 \sL f(X_u)du,\quad t \geq 0
\ee
is a martingale for all $f \in\sC^2_K$. Since the martingale problem has a unique solution, the desired conclusion follows. This ends the proof of Theorem \ref{thm:linear_interpolation_converge_weakly}.
\end{proof}

\subsection{Example}

This result has literally thousands of practical applications, and we show one particularly simple such application.

The Ehrenfest chain. This is a Markov chain which models a box filled with gas molecules which is divided in two equal pieces, and where gas molecules can be exchanged between the two pieces through a small hole. Mathematically, we have two urns with a total of $2n$ balls (molecules). At each time step we pick one ball uniformly at random among the $2n$ balls of the urn, and move it to the other urn (we think of this event as a certain gas molecule going
through that hole). Let $Y^n_t$ denote the number of molecules in the left urn.

Define a normalized process $X^n_t = (Y^n_{\floor{tn}} - n)/\sqrt{n}$, and assume for instance that $Y^n_0 = n$, i.e., equal number of molecules in each urn.

\begin{theorem}
The process $(X^n_t , 0 \leq t \leq 1)$ converges weakly to an Orstein-Uhlenbeck diffusion $(X_t, 0 \leq t \leq 1)$ with unit viscosity, i.e., the pathwise unique solution to
\be
dX_t = -X_tdt + dB_t,\quad X_0 = 0.
\ee
\end{theorem}

Thus the number of molecules in each urn never deviates too much from $n$. Writing $K^n(x, dy) = n\Pi^n(x, dy)$,

\begin{proof}[\bf Proof]
The state space for $Y^n$ is $S_n = \bra{k/\sqrt{n}: -n \leq k \leq n}$. The transition probability $\Pi^n$ of $Y^n$ is given
\be
\Pi^n(x, x + n^{-1/2}) = \frac{n - x\sqrt{n}}{2n},\quad\quad \Pi^n(x, x - n^{-1/2}) = \frac{n + x\sqrt{n}}{2n}.
\ee

Here $d = 1$, and the expected infinitesimal drift
\be
\wh{b}^n(x) = \int (y - x)K^n(x, dy) = n\bb{n^{-1/2} \frac{n - x\sqrt{n}}{2n} - n^{-1/2} \frac{n + x\sqrt{n}}{2n}} = -x,
\ee
while the infinitesimal variance
\be
\wh{a}^n(x) = \int (y - x)^2K^n(x, dy) = n\bb{n^{-1}\frac{ n - x\sqrt{n}}{2n} + n^{-1}\frac{ n + x\sqrt{n}}{2n}} = 1.
\ee

It follows without difficulty that the truncated expected drift and variance, respectively $b^n(x) = \int_{\abs{y-x}\leq 1} (y - x)^2K^n(x, dy)$ and $a^n(x) = \int_{\abs{y-x}\leq 1} (y - x)^2K^n(x, dy)$, satisfy:
\be
a^n(x) \to 1,\quad b^n(x) \to -x
\ee
uniformly on every compact set. Since the coefficients of the Ornstein-Uhlenbeck diffusion are Lipschitz, there is pathwise uniqueness for the associated SDE and thus uniqueness in distribution. Therefore, $(X^n_t , 0 \leq t \leq 1)$ converges to $(X_t, 0 \leq t \leq 1)$ weakly, by Theorem \ref{thm:linear_interpolation_converge_weakly}.
\end{proof}



\section{Problems}

\ben

%\item [1.1] Let $(B_t, t \geq 0)$ be a one-dimensional Brownian motion, and for $x \in \R$ let $T_x = \inf\bra{t > 0:B_t = x}$.
%\ben
%\item [(a)] Show that
%\be
%T_x = \frac{x^2}{\sN^2}
%\ee
%where $\sN$ is a standard Gaussian random variable.

%\item [(b)] Show that $(T_x, x \geq 0)$ has independent and stationary increments. [We say that a process $(X_t, t \geq 0)$ has independent and stationary increments if for all $t_1,\dots, t_n$ the random variables $X_{t_{i+1}} -X_{t_i}$ are mutually independent and their law depends only on $t_{i+1} - t_i$.]
%\een

%\item [1.2] Define $R = \inf\bra{t \geq 1 : B_t = 0}$ and let $L = \sup\bra{t \leq 1 : B_t = 0}$.
%\ben
%\item [(a)] Show that L is not a stopping time. Is $R$ a stopping time?
%\item [(b)] Show that 
%\be
%\pro(R > 1 + u) = 1 - \frac 2{\pi}\arctan(\sqrt{u}).
%\ee
%(it is recalled that the quotient of two independent standard Gaussian random variables has the Cauchy distribution).
%\item [(c)] Deduce that $L$ has the arcsince law:
%\be
%\pro(L < t) = \int^t_0 \frac{dx}{\pi \sqrt{x(1 - x)}}.
%\ee
%\een

\item [1.3] Brownian bridge. Let $B$ be a standard Brownian motion. Fix $y \in \R$, and define a process $(b^y_t , 0 \leq t \leq 1)$ by putting 
\be
b^y_t = yt + (B_t - tB_1)
\ee

Let $\W^y_0$ denote the law of this process on the space $\sC(0, 1)$ of continuous functions on $[0, 1]$. We wish to show that by can be interpreted as Brownian motion conditioned upon $B_1 = y$, even though the latter is a zero probability event. To do so, fix a functional $F :\sC(0, 1) \to\R$ such that $F$ is bounded and continuous for the topology of uniform convergence on $[0, 1]$. For $y \in\R$, define
\be
f(y) = \W^y_0(F) = \E_{\W} [F(b^y_t , 0 \leq t \leq 1)].
\ee
where $\W$ is the standard Wiener measure. Show that:
\be
\E_{\W}(F|B_1) = f(B_1),\quad\text{ a.s.}
\ee
(Hint. find a simple argument to show that $(B_t - tB_1, 0 \leq t \leq 1)$, is independent from $B_1$.) Conclude that $\W^y_0$ is the weak limit as $\ve\to 0$ of Brownian motion conditioned upon the event $\bra{\abs{B_1 - y} \leq \ve}$.

\begin{solution}[\bf Solution.]
$B_1$ and $(B_t- tB_1)_{0\leq t\leq 1}$ are independent, since they are both Gaussian and 
\be
\cov\bb{B_1,B_t - tB_1} = \cov\bb{B_1,B_t} - t\cov (B_1,B_1) = 1\land t - t = t-t = 0.
\ee

Next, let $x\in \R$ and $w \in C[0,1]$ and let $f_{B_1,\bb{B_t}_{t\in [0,1]}} =f(x,w)$ denote the joint density.

Considering $B_t$ under the standard Wiener measure we get:
\beast
f(x,\omega) dx dw & = & \pro\bb{B_1\in dx, \bb{B_t}_{t\in [0,1]}\in dw} = \pro\bb{\left.\bb{B_t}_{t\in [0,1]}\in dw\right|B_1\in dx}  \pro\bb{B_1\in dx}\\
& = &  \pro\bb{\left.\bb{tB_1 + B_t - tB_1}_{t\in [0,1]}\in dw\right|B_1\in dx}  \pro\bb{B_1\in dx}\\
& = &  \pro\bb{\left.\bb{tx + B_t - tB_1}_{t\in [0,1]}\in dw\right|B_1\in dx}  \pro\bb{B_1\in dx}
\eeast

Then using the above mentioned independence.
\be
f(x,\omega) dx dw = \pro\bb{(B_t^x) \in dw} \pro(B_1\in dx) = W_0^x(dw)\pro(B_1\in dx).
\ee

Let now $A\in \sF_{B_1}$. Then
\beast
\E\bb{\ind_A F(B)} & = & \int_{\R}\int_{[0,1]} \ind_{x\in A} F(w) W_0^x(dw) \pro(B_1\in dx)\\
& = & \int_\R \ind_{x\in A} f(x) \pro(B_1 \in dx) = \E\bb{\ind_A f(B_1)}
\eeast
as required.

We want to prove that $\forall F:\sC[0,1] \to \R$ continuous and bounded 
\be
W_0^y (F) = \lim_{\ve \to 0} \E_W\bb{F|\abs{B_1-y}\leq \ve}.
\ee

But $\E_W\bb{F|\abs{B_1-y} \leq \ve} = \E\bb{f(B)|\abs{B_1-y}\leq \ve}$. Since $F$ is continuous, $f$ is continuos too. Hence $\forall \ve' >0$ $\exists \ve >0$ s.t. if $\abs{B_1-y} \leq \ve$, then $\abs{f(B_1) - f(y)} \leq \ve'$.

So as $\ve \to 0$, $f(B_1)$ tends to $f(y)$ on the event $\bra{\abs{B_1-y}\leq \ve}$. $f(y) = W_0^y(F)$, hence result.
\end{solution}



\item [1.5] Let $a \in \sL^1[0,\infty)$ and set
\be
x(t) = \int^t_0 a(s)ds.
\ee

Show that $x$ is continuous and of finite variation, with 
\be
v(t) = \int^t_0 \abs{a(s)}ds,
\ee
and that if $h$ is non-negative and Borel measurable, then
\be
\int^t_0 h(s) dx(s) = \int^t_0 h(s) a(s) ds.
\ee

\begin{solution}[\bf Solution.]
To show that $x$ is continuous let $\ve >0$, then there exists $M>0$ s.t.
\be
\int^\infty_0 \abs{a(s)}\ind_{\bra{\abs{a(s)}>M}}ds < \frac {\ve}2,
\ee
since $a\in \sL^2[0,\infty)$.

If $\abs{t-s} < \frac{\ve}{2M}$, then 
\be
\abs{x(t)-x(s)} \leq \int^t_s \abs{a(u)}du \leq \int^t_s \abs{a(u)}\ind_{\bra{\abs{a(u)}>M}}du + \int^t_s \abs{a(u)}\ind_{\bra{\abs{a(u)}< M}}du \leq \frac{\ve}2 + M(t-s) \leq \frac {\ve}2 + \frac{\ve}2 = \ve.
\ee

\be
x(t) = \int^t_0 a(s)ds = \int^t_0 \abs{a(s)}\cdot \ind_{\bra{a(s) > 0}}ds - \int^t_0 \abs{a(s)}\cdot \ind_{\bra{a(s)< 0}}ds 
\ee

so $x$ is the difference of 2 increasing process, hence of finite variation.

Form lectures we have that if $v$ is the variation of $x$, then
\be
\left.\ba{l}
\frac 12 \bb{v(t) + \int^t_0 a(s)ds} \leq \int^t_0 \abs{a(s)}\ind_{\bra{a(s)>0}}\\
\frac 12 \bb{v(t) - \int^t_0 a(s)ds} \leq \int^t_0 \abs{a(s)}\ind_{\bra{a(s)<0}}
\ea\right\} \ \ra \ v(t) \leq \int^t_0 \abs{a(s)}ds
\ee

Since $v(t)+ \int^t_0 a(s)ds \ua$ and $v(t) - \int^t_0 a(s)ds \ua$,
\be
\left.\ba{l}
\int^t_0 \ind_{\bra{a(s)<0}}(dv(s) + a(s)ds) \geq 0\\
\int^t_0 \ind_{\bra{a(s)\geq 0}}(dv(s) - a(s)ds) \geq 0
\ea\right\} \ \ra \ v(t) \geq \int^t_0 \abs{a(s)}ds.
\ee

Hence $v(t) = \int^t_0 \abs{a(s)}ds$.

The measures $dx(s)$ and $a(s)ds$ obviously agree on the $\pi$-system consisting of the intervals. This $\pi$-system generates teh Borel $\sigma$-algebra of $\R$, so by uniqueness of the extension $dx(s)$ and $a(s)ds$ are equal. Hence the integral of any $h$ measurable and non-negative wrt $dx(s)$ ($\int^t_0 h(s)dx(s)$) will be equal to the one wrt $a(s)ds$ ($\int^t_0 h(s)a(s)ds$).
\end{solution}


\item [1.6] Let $a : [0,\infty) \to \R$ and $x: [0,\infty) \to \R$ be continuous and suppose $a$ is of finite variation. Show that, for all $t \geq 0$,
\be
\lim_{n\to\infty} \sum^{\floor{2^n t}-1}_{k=0} (a((k + 1)2^{-n}) - a(k2^{-n}))(x((k + 1)2^{-n}) - x(k2^{-n})) = 0.
\ee

Deduce that if $B$ is a Brownian motion, then $B$ is almost surely not of finite variation.

\begin{solution}[\bf Solution.]
\beast
& & \abs{\sum^{\floor{2^n t}-1}_{k=0} (a((k + 1)2^{-n}) - a(k2^{-n}))(x((k + 1)2^{-n}) - x(k2^{-n}))}\\
& \leq & \sum^{\floor{2^n t}-1}_{k=0} \abs{a((k + 1)2^{-n}) - a(k2^{-n})} \sup_{0\leq k\leq \floor{2^n t}-1} \abs{x((k + 1)2^{-n}) - x(k2^{-n})}\\
& \leq & v(t) \sup_{0\leq r,s\leq t,\abs{r-s} \leq 2^{-n}}\abs{x(r) - x(s)} \to 0
\eeast
(where $v$ is the variation of $a$) as $n\to \infty$ since $x$ is continuous and hence uniformly continuous on $[0,t]$.
\end{solution}


\item [1.7] Suppose $B$ is a standard Brownian motion.
\ben
\item Let $T = \inf\bra{t \geq 0 : B_t = 1}$. Show that $H$ defined by $H_t = \ind_{T\geq t}$ is a previsible process.
\item Let 
\be
\sgn(x) =\left\{\ba{ll}
-1 \quad\quad & \text{if }x \leq 0\\
1 & \text{if }x > 0
\ea\right.
\ee
\een

Show that $(\sgn(B_t))_{t>0}$ is a previsible process (which is neither left- nor right-continuous).

\begin{solution}[\bf Solution.]
\ben
\item [(a)] As $T$ is a stopping time, $\bra{T\geq t}\in \sF_t$ $\forall t\geq 0$. So $(\wt{H}_t)_{t\geq 0}$ defined by $\wt{H}_t = \ind_{\bra{T>t}}$ is adapted. It is clearly \cadlag\ and $H_t = \wt{H}_{t^-}$, so we get that $H$ is previsible.

\item [(b)] Let $f(x) = \sgn(x)$ and 
\be
f_n(x) = \left\{\ba{ll}
-1 & x\leq 0\\
2nx-1 \quad\quad & 0< x < \frac 1n\\
1 & x\geq \frac 1n
\ea\right.
\ee

Then $f_n \ua f$ as $n\to \infty$. $f_n$ is continuous and so $(f_n(B_t))_{t\geq 0}$ is a continuous adapted process $\forall n$. Hence $f_n(B)$ is previsible $\forall n$. Thus the limit $f(B)$ must also be previsible.
\een
\end{solution}

%\item [1.8] Let $H$ be a previsible process. Show that $H_t$ is $\sF_t$--measurable for all $t > 0$, where $\sF_{t^-} = \sigma(\sF_s : s < t)$.

%\item [1.9] Let $H$ be a left-continuous adapted process which is bounded on compact time intervals and let $A$ be a \cadlag adapted finite variation process. Show that
%\be
%(H \cdot A)_t = \lim_{n\to\infty} \sum^\infty_{k=0} H_{k2^{-n}} (A_{(k+1)2^{-n}\land t} - A_{k2^{-n}\land t})
%\ee
%with convergence uniform on compact time intervals. (Consider the limit $\omega$ by $\omega$.)

\item [1.10]
\ben
\item Let $(\sF_t)_{t\geq0}$ satisfy the usual conditions (i.e. completeness and right continuity). Show that the map $M \to M_\infty: \sM^2 \to \sL^2(\sF_\infty)$ is onto.
\item Let $Y$ be a random variable taking values $\pm 1$, each with probability $\tfrac 12$. Set $\sF_t = \{\emptyset,\Omega\}$ for $t \leq 1$ and $\sF_t = \sigma(Y)$ for $t > 1$. Show that all \cadlag $(\sF_t)_{t\geq0}$-martingales are constant. (So, in this case, the map $M \to M_\infty: \sM^2 \to \sL^2(\sF_\infty)$ is not onto.)
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] When $(\sF_t)_{t\geq 0}$ satisfies the usual conditions, we know that for all $X\in \sL^2(\sF_\infty)$ the process $X_t = \E\bb{X|\sF_{t}}$ has a \cadlag version, which is an $\sL^2$-bounded martingale with $X_\infty =X$ a.s., so the map is onto.
\item [(b)] For $t>s>1$ we have $\E(M_t|\sF_s) = M_s$. But $\sF_s = \sF_t = \sigma(Y)$, so $M_t = M_s$, for $t>s>1$. Similarly, $M_t = M_s$ for $s,t\leq 1$. So $M\in \sM$ is constant on $[0,1]$ and $[1,\infty)$ by right-continuity.

By the martingale property we also have
\be
M_1 = \E\bb{M_2|\sF_1} = \E\bb{M_2|\sF_0} = M_0.
\ee

So $M\in \sM$ is constant.

In this case the map $M\mapsto M_\infty$ is obviously not onto ($Y\in \sL^2(\sF_\infty)$, but $X_t = \E\bb{Y|\sF_t}$ is not constant).
\een
\end{solution}

%\item [1.11] An essential fact. Let $M$ be a continuous local martingale starting from 0. Show that $M$ is an $\sL^2$-bounded martingale if and only if $\E([M]_\infty) < \infty$. (This is Theorem \ref{thm:continuous_m2_martingale_implies_ui})

%\item [1.12] Let $X^n$ be a sequence of \cadlag processes and let $T_k$ a sequence of random times, with $T_k \ua \infty$ a.s. as $k \to\infty$. Suppose $\dabs{(X^n)^{T_k}} \to 0$ as $n\to\infty$ for all $k$. Show that $X^n \to 0$ u.c.p.

%\item [1.13]
%\ben
%\item Suppose that $M$ and $N$ are two independent continuous local martingales. Show that $[M,N]_t= 0$ for all $t \geq 0$. In particular, if $B^{(1)}$ and $B^{(2)}$ are the coordinates of a standard Brownian motion in $\R^2$, this shows that $[B^{(1)},B^{(2)}]_t = 0$ for all $t \geq 0$.
%\item Now let $B$ be a standard Brownian motion in $\R$ and let $T$ be a stopping time which is not constant a.s. By considering $B^T$ and $B - B^T$, show that the converse to (a) is false. [Hint. show that $T$ is measurable with respect to the $\sigma$-algebras generated by both $B^T$ and $B - B^T$.]
%\een

\item [1.14] Let $a < b < c < d$. Show that, a.s.:
\be
\sup_{t\in [a,b]} B_t \neq \sup_{t\in [c,d]} B_t.
\ee

Deduce that every local maximum of $B$ is a strict local maximum.

\begin{solution}[\bf Solution.]
By he Markov property of BM we have that $\sup_{t\in [a,b]}(B_t - B_b)$ is independent of 
\be
\sup_{t\in [c,d]}(B_t - B_c) + (B_c - B_b) \ \ra \ \sup_{t\in [a,b]}(B_t - B_c) \neq \sup_{t\in [c,d]}B_t\text{ a.s.}
\ee
since they are indepedent and absolutely continuous random variables (i.e., they have a density).
\end{solution}

\item [1.15] Let $B$ be a one-dimensional Brownian motion. Show that
\be
\bb{\int^t_0 e^{B_s}ds}^{1/\sqrt{t}} 
\ee
converges in distribution towards a certain limit. (Hint. what is the limit as $p \to \infty$ of $\bb{\int^1_0 \abs{f(x)}^pdx}^{1/p}$?)

\begin{solution}[\bf Solution.]
We have that $\dabs{f}_p \to \dabs{f}_\infty$ as $p\to\infty$. So
\be
\bb{\int^1_0 \abs{f(x)}^p dx }^{1/p} \to \sup_{x\in [0,1]}\abs{f(x)} \quad\text{as }p\to \infty.
\ee

By the scaling invariance of the BM we have 
\be
\bb{\int^t_0 e^{B_u}du}^{1/\sqrt{t}} = \bb{\int^1_0 te^{\sqrt{t}B_s}ds}^{1/\sqrt{t}} = t^{1/\sqrt{t}} \bb{\int^1_0 e^{\sqrt{t}B_s}ds}^{1/\sqrt{t}}.
\ee

$t^{1/\sqrt{t}} \to 1$ as $t\to\infty$ and $\bb{\int^1_0 e^{\sqrt{t}B_s}ds}^{1/\sqrt{t}} \to \max_{0\leq s\leq 1}e^{B_s}$ as $t\to\infty$.

So the limit process is $\max_{0\leq s\leq 1}e^{B_s}$.
\end{solution}

\item [1.16] Iterated law of the logarithm, upper-bound. Let $B$ be a standard Brownian motion, and let $S_t = \sup_{s\in [0,t]} B_sds$. The goal of this problem is to show that
\be\label{equ:brownian_motion_log_log_limsup_leq}
\limsup_{t\to \infty} \frac{S_t}{\sqrt{2t \log \log t}} \leq 1, \quad\text{ a.s.}
\ee

(In the next problem you will be asked to prove a matching lower-bound).
\ben
\item [(a)] Fix $\ve > 0$. Let $c = 1 + \ve$ and set $t_n = c^n$. Show that:
\be
S_{t_n} \leq (1 + \ve)\sqrt{2t_n \log \log t_n}
\ee
for all large enough $n$, almost surely.
\item [(b)] Deduce (\ref{equ:brownian_motion_log_log_limsup_leq}).
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] Let $A_n = \bra{S_{t_n}\geq (1+\ve)\sqrt{2t_n \log\log t_n}}$. Then since $S_{t_n} \sim \abs{B_{t_n}} \sim \sqrt{t_n}\abs{B_1}$.
\beast
\pro(A_n) & = & \pro\bb{S_{t_n} \geq (1+\ve)\sqrt{2t_n \log\log t_n}} = \pro\bb{\sqrt{t_n}\abs{B_1} \geq (1+\ve)\sqrt{2t_n \log\log t_n}} \\
& = &  \pro\bb{\abs{B_1} \geq (1+\ve)\sqrt{2 \log\log t_n}} \leq \frac 1{\sqrt{2\pi}}\frac 1{\bb{1+\ve}\sqrt{2\log\log t_n}}
\eeast
by the inequality $\pro(X\leq x) \leq \frac 1{x\sqrt{2\pi}} e^{-x^2/2}$ when $X\sim \sN(0,1)$. Then we have
\be
\pro(A_n) \leq \frac 1{\sqrt{2\pi}}\frac 1{\bb{1+\ve}\sqrt{2\log\log t_n}} \frac 1{\bb{\log t_n}^{\bb{1+\ve}^2}} \leq \frac 1{\sqrt{2\pi}}\frac 1{n^{\bb{1+\ve}^2}} \frac 1{\bb{\log c}^{\bb{1+\ve}^2}},
\ee
which is summable in $n$. So by the Borel-Cantelli lemma, we get that 
\be
\pro(A_n \text{ i.o.}) = 0 \ \ra \ \pro\bb{A_n^c \text{ e.v.}} = 1.
\ee

Hence $\frac{S_{t_n}}{\sqrt{2t_n\log\log t_n}} \leq 1 + \ve$ for all $n$ sufficiently large a.s. and for all $\ve >0$, hence
\be
S_{t_n} \leq (1+\ve)\sqrt{2t_n\log\log t_n} \text{ e.v. a.s.}
\ee

\item [(b)] Let $t>0$. Then $\exists n$ s.t. $t_n = c^n \leq t\leq c^{n+1} = t_{n+1}$. We have $S_{t_n} \leq S_t \leq S_{t_{n+1}}$ and 
\be
\frac{S_t}{\sqrt{2t\log\log t}} \leq \frac{S_{t_{n+1}}}{\sqrt{2t_n\log\log t_n}}.
\ee

But $\frac{\sqrt{2t_{n+1} \log\log t_{n+1}}}{\sqrt{2t_n \log\log t_n}} \to \sqrt{c}$ as $n\to \infty$. So 
\be
\limsup_{t\to\infty} \frac{S_t}{\sqrt{2t\log\log t}} \leq (1+\ve)c, \ \forall c>1,\ve >0 \ \ra \ \limsup_{t\to \infty} \frac{S_t}{\sqrt{2t\log\log t}} \leq 1.
\ee
\een
\end{solution}


\item [1.17] Iterated law of the logarithm, lower-bound. The setup is the same as above. Fix $\theta > 1$ and let $t_n = \theta^n$. Let $\alpha$ be such that $0 <\alpha < \sqrt{1 - 1/\theta}$.
\ben
\item [(a)] Let $A_n$ be the event that $\bra{B_{t_n} - B_{t_{n-1}} \geq \alpha \sqrt{2t_n \log \log t_n}}$. Show that $A_n$ occurs infinitely often.
\item [(b)] Show that
\be\label{equ:brownian_motion_log_log_limsup_geq}
\limsup_{t\to\infty} \frac{S_t}{\sqrt{2t \log \log t}} \geq 1, \quad\text{a.s. }
\ee
\item [(c)] What do (\ref{equ:brownian_motion_log_log_limsup_leq}) and (\ref{equ:brownian_motion_log_log_limsup_geq}), put together, say about the behaviour of $B_t$ near $t = 0$? What you get is called the iterated law of logarithm.
\een

\begin{solution}[\bf Solution.]

\end{solution}

\item [1.18] Let $B$ be a one-dimensional Brownian motion and let $Z$ be its zero set:
\be
Z = \bra{t \geq 0 : B_t = 0}.
\ee
Show that $Z$ is almost surely closed, unbounded and has no isolated point. (In particular, it can be shown that this implies that $Z$ is a.s. uncountable.) Show however that $Z$ has zero Lebesgue measure. (It can be shown that Z has Hausdorff dimension equal to 1/2).

\begin{solution}[\bf Solution.]
The set $Z$ is closed as the zero set of a continuous function (inverse image of the closed set $\bra{0}$). Unboundedness comes from the fact the Brownian motion oscillates ($\liminf_{t\to \infty} B_t = -\infty$ and $\limsup_{t\to \infty}B_t = +\infty$). Then $\text{Leb}(Z) = \int^\infty_0 \ind_{t\in Z}dt$, by Fubini theorem
\be
\E\bb{\text{Leb}(Z)} = \E\bb{\int^\infty_0 \ind_{t\in Z}dt}  = \int^\infty_0 \pro(t\in Z)dt = \int^\infty_0 \pro(B_t = 0) dt  = 0.
\ee

So $\text{Leb}(Z) =0$ a.s.

For each $q\in \Q^+$ we define $\sD_q = \inf\bra{t>q:B_t = 0}$. These are stopping times. Now let's observe that 
\be
\bra{\omega\in \Omega: Z(\omega) \text{ has an isolated point in }(0,\infty)} \subseteq \bigcup_{\substack{a,b\in \Q \\ 0\leq a<b<\infty}}\bra{\omega\in \Omega:\text{ there is exactly one $s$ in $(a,b)$ s.t. }B_s(\omega) = 0}
\ee

We also have that $\sD_0 = 0$ a.s.. Furthermore, 
\be
\sD_{\sD_q} = \inf\bra{t> D_q:B_t = 0} = \sD_q + \inf\bra{t>0: B_{t+\sD_q} - B_{\sD_q} = 0} = \sD_q,
\ee
by the strong Markov property applied to $\sD_q$. So the set $\bra{\omega\in\Omega: \exists! s\in (a,b) \text{ s.t. }B_s(\omega) = 0}$ is contained in the event $\bra{\omega\in \Omega:\sD_a< b\text{ and }\sD_{\sD_a} > b}$, which has probability 0.
\end{solution}


%\item [1.19] Show that the covariation $[M,N]$ of continuous local martingales $M$ and $N$ is a symmetric bilinear form.

%\item [1.20] Suppose that $H$ and $K$ are previsible processes which are bounded on compact time intervals (i.e. $\sup_{s\leq t} H_s < \infty$ and $\sup_{s\leq t} K_s < \infty$ for all $t$). Show that if $A$ is a \cadlag adapted finite variation process then
%\be
%H \cdot (K \cdot A) = (HK) \cdot A.
%\ee
%[Hint. use a monotone class argument.]

\item [2.1] Let $B$ be a standard Brownian motion. Use It\^o's formula to prove that the following processes are martingales with respect to the natural filtration of $B$.
\ben
\item $X_t = \exp(\lm^2t/2) \sin(\lm B_t)$
\item $X_t = (B_t + t) \exp(-B_t - t/2)$.
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] By It\^o formula applied to $f(t,x) = \exp\bb{\lm^2t /2} \sin(\lm x)$ we get
\beast
f(t,B_t) & = & f(0,B_0) + \int^t_0 \frac{\lm^2}2 \exp\bb{\frac{\lm^2 s}2} \sin (\lm B_s)ds \\
& & \quad\quad + \int^t_0 \exp\bb{\frac{\lm^2 s}2} \lm \cos(\lm B_s)dB_s + \frac 12 \int^t_0 -\lm^2 \exp\bb{\frac{\lm^2 s}2}\sin(\lm B_s)ds.
\eeast

So $X_t = f(t,B_t) = \int^t_0 \lm \exp\bb{\frac{\lm^2 s}2} \cos(\lm B_s)dB_s$. Hence $X$ is an $\sF_t$-local martingale.

If $T$ is a stopping time with $T\leq t$, then by It\^o isometry,
\beast
\dabs{X_T}^2_2 & = & \E\bb{\int^t_0 \lm \exp\bb{\frac{\lm^2 s}2}\cos(\lm B_s)\ind_{\bra{T\leq t}}dB_s}^2 = \E\bb{\int^t_0 \lm^2 \exp\bb{\lm^2 s}\cos^2(\lm B_s)\ind_{\bra{T\leq t}}ds} \\
& \leq & \int^t_0 \lm^2 \exp(\lm^2 s)ds = \exp(\lm^2 t) -1.
\eeast

So the set $\bra{X_T:T\text{ is a stopping time s.t. }T\leq t}$ is UI, since it is bounded in $\sL^2$. So $(X_t)_{t\geq 0}$ is a true martingale.

\item [(b)] Again we apply It\^o formula to $f(t,x) = (x+t)\exp(-x-t/2)$.
\beast
X_t = f(t,B_t) & = & f(0,B_0) + \int^t_0 \bb{\exp\bb{-B_s -\frac s2} - \frac 12 \exp\bb{-B_s - \frac s2}(B_s + s)}ds \\
& & \quad\quad + \int^t_0 \bb{\exp\bb{-B_s - \frac s2} - (B_s + s)\exp\bb{-B_s - \frac s2}}dB_s \\
& & \quad\quad + \frac 12 \int^t_0 - \exp\bb{B_s - \frac s2} + \exp\bb{-B_s - \frac s2} + (B_s + s)\exp\bb{-B_s - \frac s2} ds\\
& = & \int^t_0 \bb{\exp\bb{-B_s - \frac s2} - (B_s +s)\exp\bb{-B_s -\frac s2}}dB_s
\eeast

Hence $X$ is an $\sF_t$-local martingale. Furthermore, by first derivative analysis
\be
f(t,x) = (x+t)\exp(-(x+t))e^{t/2} \leq e^{-1}e^{t/2}.
\ee

Note that this is bounded above not below since $x\to -\infty$, the whole thing $\to -\infty$. Thus, we can not have $\dabs{X_T}^2_2 \leq e^{t-2}$.

Thus, by It\^o isometry, we have $\bb{1 - B_s -s}\exp\bb{-B_s -\frac s2} \in \sL^2$ and
\be
\dabs{X_T}^2_2  = \E\bb{\int^t_0 \bb{1 - B_s - s}\exp\bb{-B_s -\frac s2}\ind_{\bra{T\leq t}}dB_s}^2 = \E\bb{\int^t_0 \bb{1 - B_s - s}^2\exp\bb{-2B_s - s}\ind_{\bra{T\leq t}}ds}
\ee

Then by Fubini theorem,
\beast
\dabs{X_T}^2_2 & \leq & \int^t_0 \E\bb{\bb{1 - B_s - s}^2\exp\bb{-2B_s - s}}ds = e^{-s} \int^t_0 \E\bb{\bb{1 - B_s - s}^2\exp\bb{-2B_s}}ds\\
& = & \int^t_0 e^{-s}  \E\bb{\bb{1 + B_s^2 + s^2 - 2B_s - 2s + 2sB_s}\exp\bb{-2B_s}}ds\\
& = & \int^t_0 e^{-s}  \bb{(s-1)^2 e^{2s} + 2(s-1) 2s e^{2s} + (1-4s + 8s^2)e^{2s} }ds = \int^t_0 \bb{13s^2 - 10s + 2}e^{s}ds < \infty.
\eeast

So the set $\bra{X_T:T\text{ is a stopping time s.t. }T\leq t}$ is UI, since it is bounded in $\sL^2$. So $(X_t)_{t\geq 0}$ is a true martingale.
\een
\end{solution}

\item [2.2] Let $H$ be a real-valued adapted continuous process, which is uniformly bounded, and let $(B_t, t \geq 0)$ be a one-dimensional Brownian motion.
\ben
\item [(a)] Using Cauchy-Schwarz's inequality and Jensen's inequality, show that
\be
\E\bb{\abs{\frac 1{B_\ve} \int^\ve_0 (H_u - H_0)dB_u}^{1/4} } \to 0
\ee
as $\ve \to 0$.

\item [(b)] Use (a) to prove that for all $t > 0$,
\be
\lim_{\ve \to 0} \frac 1{B_{t+\ve} - B_t} \int^{t+\ve}_t H_sdB_s = H_t
\ee
in probability.
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] We start by noticing that the integral is well-defined, because continuous processes are previsible locally bounded. By Cauchy-Schwartz inequality and Jensen's inequality ($x\mapsto x^{1/4}$ is concave in $\R^+$) we have
\beast
\E\abs{\frac 1{B_\ve}\int^\ve_0 (H_u - H_0)d B_u}^{1/4} & \leq & \sqrt{\E\bb{\frac 1{\sqrt{\abs{B_\ve}}}} \E \abs{\int^\ve_0 (H_u - H_0)d B_u}^{1/2}}\\
& \leq & \sqrt{\E\bb{\frac 1{\sqrt{\abs{B_\ve}}}}} \sqrt{ \bb{\E \abs{\int^\ve_0 (H_u - H_0)d B_u}^2}^{1/4}}.
\eeast

But by Brownian scaling we have
\be
\E\bb{\frac 1{\sqrt{\abs{B_\ve}}}} = \frac 1{\ve^{1/4}} \E\bb{\frac 1{\sqrt{\abs{B_1}}}} < \frac c{\ve^{1/4}}.
\ee

By It\^o isometry $\E\bb{\int^\ve_0 (H_u - H_0)dB_u}^2 = \E\bb{\int^\ve_0 (H_u - H_0)^2 du}$. Now
\be
\frac{\E\bb{\int^\ve_0 (H_u - H_0)^2 du}}{\ve} \leq \E\bb{\sup_{0\leq u\leq \ve}(H_u - H_0)^2} \to 0 \text{ as }\ve \to 0
\ee
by bounded convergence and the continuity of $H$. So
\be
\E\bb{\int^\ve_0 (H_u - H_0)^2 du} \E\bb{\frac 1{\sqrt{\abs{B_\ve}}}} \leq c \bb{\frac{\E\bb{\int^\ve_0 (H_u - H_0)^2du}}{\ve}}^{1/4} \to 0 \text{ as }\ve \to 0.
\ee

\item [(b)] Note that as a result of (a) $\abs{\frac 1{B_\ve} \int^\ve_0 (H_u - H_0)dB_u} \to 0$ in probability. Hence,
\be
\frac 1{B_\ve}\int^\ve_0 H_u dB_u \to H_0 \text{ as }\ve \to 0 \text{ in prob.}
\ee

Next, by the simple Markov property at time $t$ we have that $\wt{B}_u = B_{u+t} - B_t$ is a std BM and we have
\be
\frac 1{\wt{B}_\ve} \int^\ve_0 H_{u+t} d\wt{B}_u \stackrel{p}{\to} H_t \text{ as }\ve \to 0 \ \ra \ \ \frac 1{B_{t+\ve} - B_\ve} \int^{t+\ve}_t H_{u} d B_u \stackrel{p}{\to} H_t \text{ as }\ve \to 0.
\ee
\een
\end{solution}

\item [2.3] Let $h: [0,1) \to \R$ be a measurable, square integrable function on $[0, t]$, for all $t \geq 0$. Show that the process $H_t = \int^t_0 h(s) dB_s$ is Gaussian, and compute its covariances. (A real-valued process $(X_t, t \geq 0)$ is Gaussian if for any finite family $0 \leq t_1 < t_2 < \dots < t_n < \infty$, the random vector $(X_{t_1} ,\dots,X_{t_n})$ is Gaussian.)

\begin{solution}[\bf Solution.]
We have $[H]_t = [H,H]_t = [h\cdot B, h\cdot B]_t = h^2 [B]_t = \int^t_0 h^2(s)ds < \infty$. Set
\be
Z_t = \exp\bb{iuH_t + \frac 12 u^2 \int^t_0 h^2(s)ds}.
\ee

Then $Z$ is a local martingale (proved in lectures). Hence $\exists T_n \ua \infty$ as $n\to \infty$ of stopping times s.t. $Z_{t\land T_n}$ is a martingale. So
\be
\E\bb{\exp\bb{iuH_{t\land T_n} + \frac 12 u^2 \int^{t\land T_n}_0 h^2(s)ds}} = 1.
\ee

Now using dominated convergence theorem, we obtain that
\be
\E\bb{\exp(iu H_t)} = \exp\bb{-\frac 12 u^2 \int^t_0 h^2(s)ds},
\ee
which means that $H_t \sim \sN\bb{0,\int^t_0 h^2(s)ds}$. Next, for $0\leq t_1 < t_2 < \dots < t_n < \infty$ the random vector $\bb{H_{t_1},\dots, H_{t_n}}$ is Gaussian, since $\forall a_1,\dots,a_n \in \R$, $a_1H_{t_1} + \dots + a_nH_{t_n}$ has normal distribution.

Namely, $a_1H_{t_1} + \dots + a_nH_{t_n} = \int^{t_n}_0 \bb{a_1h(s)\ind_{\bra{s\leq t_1}} + \dots + a_nh(s)}dB_s$ which is normally distributed because of previous calculations and the fact that the function under the integral sign is square integrable, by Minkowski's inequality. For $t_1< t_2$, by independence of the increments of $B$,
\beast
\cov(H_{t_1},H_{t_2}) & = & \E\bb{H_{t_1}H_{t_2}} = \E\bb{\int^{t_1}_0 h(s)dB_s\int^{t_2}_0 h(s)dB_s} \\
& = & \E\bb{\int^{t_1}_0 h(s)dB_s\int^{t_1}_0 h(s)dB_s} + \E\bb{\int^{t_1}_0 h(s)dB_s\int^{t_2}_{t_1} h(s)dB_s} \\
& = & \E\bb{\int^{t_1}_0 h(s)dB_s}^2 + 0 = \int^{t_1}_0 h^2(s)ds.
\eeast
\end{solution}



\item [2.4] Let $B$ be a standard Brownian motion and let $(\sF_t)_{t\geq0}$ be its natural filtration. Let $\sG_t$ be the $\sigma$-algebra generated by $\sF_t$ and $B_1$. Prove that for $0 \leq s \leq t \leq 1$,
\be
\E(B_t - B_s|\sG_s) = \frac{t - s}{1 - s} (B_1 - B_s).
\ee

In particular, observe that $B$ is not a $(\sG_t)_{t\geq0}$-martingale. Prove, however, that
\be
\beta_t = B_t - \int^{t\land 1}_0 \frac{B_1 - B_s}{1 - s} ds
\ee
is a continuous $(\sG_t)_{t\geq0}$-martingale and so $B$ is a continuous $(\sG_t)_{t\geq0}$-semimartingale. What is the quadratic variation of $\beta$? Conclude that $\beta$ is a $(\sG_t)_{t\geq0}$-Brownian motion. [Hint. Show that
\be
\E(B_t-B_s)\ind_{A\cap \bra{B_1 \in D}} = \frac{t - s}{1 - s} \E(B_1-B_s) \ind_{A\cap \bra{B_1\in D}},
\ee
for all $A \in \sF_s$, $D \in \sB(\R)$.]

(This demonstrates that if a process is a continuous semimartingale in two different filtrations $(\sF_t)_{t\geq0}$ and $(\sG_t)_{t\geq0}$, its Doob-Meyer decompositions may be different, even when $\sF_t \subseteq \sG_t$ for all $t$.)

\begin{solution}[\bf Solution.]
Observe that $\sG_s = \sF_s \vee \sigma(B_1) = \sF_s \vee \sigma(B_1 - B_s)$. Also $\sigma(B_t - B_s), \sigma(B_1 - B_s) \perp \sF_s$. So 
\be
\E\bb{B_t - B_s |\sF_s\vee \sigma(B_1 - B_s)} = \E\bb{B_t - B_s|B_1 - B_s} = a(B_1 - B_s),
\ee
since $\bb{B_t-B_s, B_1-b_s}$ are jointly normal. Also $a$ must satisfy 
\be
\cov\bb{B_t - B_s,B_1-B_s} = a \var(B_1 - B_s) \ \ra \ a= \frac{t-s}{1-s}.
\ee

Hence we get $\E\bb{B_t - B_s|\sG_s} = \frac{t-s}{1-s}(B_1 - B_s)$. So $(B_t)_{t\geq 0}$ is not a $\sG_t$-measurable. It is straightforward to check that $\beta_t$ is adapted and integrable. Now if $0\leq s\leq t\leq 1$, we have
\beast
\E\bb{\beta_t - \beta_s|\sG_s} & = & \E\bb{B_t - B_s|\sG_s} - \int^t_s \bb{\frac{B_1 - B_u}{1-u} - \E\bb{\left.\frac{B_u-B_s}{1-u}\right|\sG_s}}du\\
& = & \frac{t-s}{1-s}(B_1 - B_s) - \int^t_s (B_1 - B_s)\bb{\frac 1{1-u} - \frac{u-s}{(1-u)(1-s)}}du = 0.
\eeast

Hence $\beta$ is a continuous $\sG_t$-measurable. Since $\beta_t$ is the martingale part of the semimartingale $B_t$, they have the same quadratic variation, i.e. $t$. Thus, by \levy's characteristic $\beta_t$ is a $\sG_t$-adapted Brownian motion.

Remark. Note that we have written the difference of 2 Brownian motions as a finite variation function (the integral). This however does not contradict the theorem that a continuous local martingale is of finite variation iff it is constant, since $\beta_t - B_t$ is not a martingale under any filtration.
\end{solution}


\item [2.5] Let $M$ be a continuous local martingale started from 0. Use It\^o's formula to find the Doob-Meyer decomposition of $\abs{M}^p$ when $p \geq 2$. Deduce, via H\"older's inequality, that there exists a constant $C_p \in (0,\infty)$ such that
\be
\E(M^*)^p \leq C_p\E\bb{[M]^{p/2}_\infty},
\ee
where $M^* = \sup_{t\geq0} \abs{M_t}$. (This is a special case of the Burkholder-Davis-Gundy inequality. for every $p \in (0,\infty)$, there exist constants $c_p$ and $C_p$ such that for all local martingales $M$ with $M_0 = 0$,
\be
c_p\E([M]^{p/2}_\infty ) \leq \E(M^*)^p \leq C_p\E([M]^{p/2}_\infty).
\ee
See Revuz and Yor for a proof.)

\begin{solution}[\bf Solution.]
We have $f(x) = \abs{x}^p$, $p\geq 2$. So $f'(x) = p\abs{x}^{p-1}\sgn(x)$ and $f''(x) = p(p-1)\abs{x}^{p-2}$. 

Applying It\^o formula to $f$ we obtain
\be
f(M_t) = \abs{M_t}^p = \int^t_0 p \abs{M_s}^{p-1} \sgn(M_s)dM_s + \frac 12 \int^t_0 p(p-1)\abs{M_s}^{p-2}d[M]_s.
\ee

Let $T_n = \inf\bra{t\geq 0:\abs{M_t} > n}$. Then, replacing $M$ by $M^{T_n}$ and then letting $n\to \infty$, we see that it is sufficient to prove the inequality for $M$ bounded.

Since $M$ is bounded in $\sL^2$, the process 
\be
\int^t_0 p\abs{M_s}^{p-1}\sgn(M_s)dM_s \text{ is an $\sL^2$-bounded martingale}.
\ee

But then 
\be
\E\abs{M_t}^p = \frac{p(p-1)}2 \E\bb{\int^t_0 \abs{M_s}^{p-2}d[M]_s} \leq \frac{p(p-1)}2 \E\bb{\bb{M_t^*}^{p-2}[M]_t} \leq \frac{p(p-1)}2 \bb{\E\bb{\bb{M_t^*}^p}}^{(p-2)/p} \E\bb{[M]_t^{p/2}}^{2/p}
\ee
by H\"older's inequality. By Doob's $\sL^2$-inequality,
\be
\E\bb{\bb{M_t^*}^p} \leq \bb{\frac{p}{p-1}}^p \E\abs{M_t}^p
\ee
and so we obtain
\be
\E\bb{\bb{M_t^*}^p} \leq \bb{\bb{\frac{p}{p-1}}^{p-2} \frac{p(p-1)}2}^{p/2} \E\bb{[M]_t^{p/2}}.
\ee

The result follows on letting $t\to \infty$.
\end{solution}


\item [2.6]
\ben
\item Let $X$ be a nonnegative continuous martingale such that $X_0 = x_0$ and $X_t \to 0$ a.s. as $t \to\infty$. Define $X^* = \sup_t X_t$. Let $x > 0$. Prove that
\be
\pro(X^* \geq x) = \min\bra{1, x_0/x}
\ee
\item Let $B$ be a standard Brownian motion. Show that $B_t - bt \to -\infty$ a.s. as $t \to\infty$, for all $b > 0$. Using part (a) and the fact that $\exp(aB_t - a^2t/2)$ is a martingale, show that the distribution of $\sup_{t\geq0}(B_t - bt)$ is exponential with parameter $2b$.
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] The case $x\leq x_0$ is immediate. Suppose $x>x_0$ and let $T=\inf\bra{t\geq 0:X_t \geq x}$. Since $X$ is continuous, this is a stopping time and furthermore $X_{T} = x$.

Hence, by the OST we have
\be
x_0 = \E(X_0) = \E(X_{T\land t}) = x\pro\bb{T\leq t} + \E\bb{X_t \ind_{\bra{T> t}}}.
\ee

But $\pro(T\leq t) \to \pro(T<\infty) = \pro(X^* \geq x)$ as $t\to \infty$.

Also $\E\bb{X_t\ind_{\bra{T>t}}} \to 0$ as $t\to\infty$, since $X_t \to 0$ a.s. and is bounded $(X_t\cdot \ind_{T>t})$ by $x$ (so dominated). Hence $\pro(X^* \geq x) = \frac{x_0}x$.

\item [(b)] We have that $\lim_{t\to\infty} \frac{B_t}t = 0$ a.s., because 
\be
\lim_{t\to\infty} \frac{B_t}t = \lim_{t\to 0} tB_{\frac 1t} = 0
\ee
since $tB_{\frac 1t}$ is a standard Brownian motion. So
\be
\lim_{t\to\infty}(B_t - tb) = \lim_{t\to\infty} t\bb{\frac{B_t}t -b}= -\infty \quad (b>0)\quad\text{a.s.}.
\ee

For $x>0$ we have
\be
\pro\bb{\sup_{t\geq 0}(B_t - bt)\geq x} = \pro\bb{\sup_{t\geq 0}\bb{2bB_t - 4b^2 \frac t2} \geq 2bx} = \pro\bb{\sup_{t\geq 0}\exp\bb{2bB_t - 4b^2 \frac t2} \geq \exp(2bx)} .
\ee

$X_t = \exp\bb{2bB_t - 4b^2 \frac t2}$ is a positive martingale and $\lim_{t\to\infty} X_t = 0$ a.s. So by (a) we have
\be
\pro\bb{\sup_{t\geq 0}(B_t - bt)\geq x}= \frac 1{e^{2xb}} = e^{-2bx}.
\ee

For $x<0$ it is obvious that $\pro\bb{\sup_{t\geq 0}(B_t - bt)\geq x} =1$. Hence $\sup_{t\geq 0}(B_t - bt)$ is exponentially distributed with parameter $2b$.
\een
\end{solution}



\item [2.7] Let $d \geq 3$ and let $B$ be a Brownian motion in $\R^d$, started at $B_0 = \ol{x}$ where $\ol{x} = (x, 0,\dots, 0) \in \R^d$ for some $x > 0$. Let $\dabs{\cdot}$ be the Euclidean norm on $\R^d$. Let $\tau_a = \inf\bra{t > 0:\dabs{B_t} = a}$, and for a stopping time $T$, define $B^T$ to be the process $(B_{t\land T} , t \geq 0)$.
\ben
\item [(a)] Let $D = \R^d \bs\{0\}$ and let $h: D \to \R$ be defined by $h(x) = \dabs{x}^{2-d}$. Show that $h$ is harmonic on $D$ and that $M_t = \dabs{B^{\tau_a}_t}^{2-d}$ is a local martingale for all $a \geq 0$. Is $M$ a true martingale? (Consider the case $a > 0$ and $a = 0$ separately).

\item [(b)] Use (a) to show that for any $a < b$ such that $0 < a < x < b$,
\be
\pro_{\ol{x}}(\tau_a < \tau_b) = \frac{\phi(b) - \phi(x)}{\phi(b) - \phi(a)}
\ee
where $\phi$ is the function defined on $\R^+$ by $\phi(a) = a^{2-d}$. Conclude that if $x > a > 0$
\be
\pro_x(\tau_a < \infty) = (a/x)^{d-2}
\ee
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] To show that $h$ is harmonic on $\sD$ we must show that $\Delta h = 0$ on $\sD$. 
\be
\fp{h}{x_i} = \frac{x_i(2-d)}{\bb{x_1^2 + \dots + x_d^2}^{d/2}},\quad \frac{\partial^2 h}{\partial x_i^2} = \frac{(2-d)\bb{1-dx_1^2(x_1^2 + \dots + x_d^2)^{-1}}}{\bb{x_1^2 + \dots + x_d^2}^{d/2}}
\ee

So $\sum^d_{i=1} \frac{\partial^2 h}{\partial x_i^2} = 0 \ \ra \ \Delta h = 0$. $M_t$ is well-defined, because if $\tau = \inf\bra{t>0: B_t\notin \sD}$, then $\tau = \infty$ a.s. (not point-recurrent).

By It\^o formula, it follows that
\be
h(B_t) = \int^t_0 \nabla h(B_s)\cdot dB_s + \frac 12 \int^t_0 \Delta h(B_s)ds.
\ee

Since $\Delta h = 0$ on $\sD \ \ra \ M_t = h(B_t)$ is a local martingale.

If $a=0$, then $M_{t\land \tau_a}$ is not a true martingale, because if it were, its expectation would stay constant over time. But, since also $\tau_\alpha = \infty$ a.s., $\E\bb{h(B_t)} \sim C t^{\frac {2-d}2}$ as $t\to \infty$ by Brownian scaling, where $C = \E\dabs{x+B_1}^{2-d} > 0$.

For $x>a$, then $M_t$ is clearly a martingale, since it is bounded.

\item [(b)] Let $T=T_a \land T_b$. Then $M^T$ is bounded and hence uniformly integrable, and moreover $T<\infty$ a.s. (because the BM is unbounded a.s.) By the OST, $\E_{\bar{x}}\bb{h(B_T)} = \phi(x)$. Let $p = \pro_{\bar{x}}\bb{\tau_a < \tau_b}$ so
\be
\phi(x) = p\phi(a) + (1-p)\phi(b) \ \ra \ p = \frac{\phi(b) - \phi(x)}{\phi(b) - \phi(a)}.
\ee

As $b\to \infty$, $\tau_b \to \infty$ a.s. by continuity of $B$. Taking the limit in (b) by the monotone convergence theorem,
\be
\pro_{\bar{x}}\bb{\tau_\alpha < \infty} = \lim_{b\to \infty} \frac{\phi(b)-\phi(x)}{\phi(b)-\phi(a)} = \frac{\phi(x)}{\phi(a)} = \bb{\frac ax}^{d-2}.
\ee
\een
\end{solution}

\item [2.8] Let $f: \C \to \C$ be analytic and let $Z_t = X_t + iY_t$ where $(X, Y)$ is a Brownian motion in $\R^2$. Use It\^o's formula to show that $M = f(Z)$ is a local martingale (in $\R^2$). Show further that $M$ is a time-change of Brownian motion in $\R^2$.

\begin{solution}[\bf Solution.]
$f:\C\to \C$ analytic. Let $f(x+iy) = u(x,y) + iv(x,y)$. Then $u,v$ satisfy the Cauchy-Riemann equations, i.e.
\be
\fp{u}{x} = \fp{v}{y},\quad \fp{u}{y} = - \fp{v}{x}.
\ee

From these equations, we have that $\Delta u = \Delta v = 0$ ($u,v$ are harmonic). Applying It\^o formula to $u$ we get
\be
u(Z_t) = u(X_t,Y_t) = u(X_0,Y_0) + \int^t_0 \fp{u}{x}(X_s,Y_s)dX_s + \int^t_0 \fp{u}{y}(X_s,Y_s)dY_s + \frac 12 \int^t_0 \bb{\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2}}(X_s,Y_s)ds.
\ee

So
\be
u(Z_t) = u(X_0,Y_0) + \int^t_0 \fp{u}{x}(X_s,Y_s)dX_s + \int^t_0 \fp{u}{y}(X_s,Y_s)dY_s. 
\ee

Similarly, 
\be
v(Z_t) = v(X_0,Y_0) + \int^t_0 \fp{v}{x}(X_s,Y_s)dX_s + \int^t_0 \fp{v}{y}(X_s,Y_s)dY_s.
\ee

Hence both $u(Z_t)$ and $v(Z_t)$ are local martingales. We also have that 
\be
[u(Z)]_t = \int^t_0 \abs{f'(Z_s)}^2 ds,\quad [v(Z)]_t = \int^t_0 \abs{f'(Z_s)}^2 ds,\quad \sigma_t = \inf\bra{s: \int^s_0 \abs{f'(Z_s)}^2 du \geq t}.
\ee

Define $A_t = \int^t_0 \abs{f'(Z_s)}^2 ds$. Then $f(Z_{\sigma_t})$ is a continuous 2-d process each of its components is a local martingale with $[u(Z_\sigma)]_t = t$ and $[v(Z_\sigma)]_t = t$ and also
\be
[u(Z_\sigma),v(Z_\sigma)]_t = 0
\ee
by independence of $X$, $Y$. Also $A_t$ is strictly increasing, since $f'$ has at most countable number of zeros. So up to time $A_\infty = \int^\infty_0 \abs{f'(Z_s)}^2 ds$, $f(Z_{\sigma_t})$ behaves as a 2-d BM. The only thin we need to prove last is that $\sigma_\infty = \infty$ a.s. 

(Assuming that $f$ is not constant.)

Suppose not, i.e., that $A_\infty < \infty$. Then $f(Z_t)$ would have a limit as $t\to \infty$. But if we make the assumption that $f$ is not constant, then we can find two open sets $U_1$ and $U_2$, $U_1\cap U_2 =\emptyset$ and s.t. $f(U_1)\cap f(U_2) = \emptyset$. But $Z_t$ is a BM in 2 dimensions, hence it will visit both sets infinitely often, contradiction. Hence $A_\infty = \infty$ a.s..
\end{solution}



\item [2.9] Let $D = \bra{\abs{z} \leq 1}$ and take $z \in D$. What is the hitting distribution for $Z$ on $\partial D$ in the case $Z_0 = 0$? By considering an analytic map $f : D \to D$ with $f(0) = z$, determine the hitting distribution for $Z$ on $\partial D$ when $Z_0 = z$. (You don't need to give the details of the computations here. just explain how you would do start the computations).

\begin{solution}[\bf Solution.]
The hitting distribution for $Z$ on $\partial \sD$ when it starts at 0 is clearly the Lebesgue uniform measure. Because BM is rotationally invariant and the only such measure is the uniform one.

Let $f:\sD \to \sD$ be given by $f(w) = \frac{w-z}{1-\bar{z}w}$ (M\"obius),
\be
e^{i\theta} = \frac{e^{it}-z}{1-\ol{z}e^{it}}.
\ee
by conformal invariance.
\be
\pro_z\bb{B_{T_\sD} \text{ hits }[t_1,t_2]} = \pro_0 \bb{B_{T_\sD}\text{ hits }[\theta_1,\theta_2]} = \frac{\theta_2 - \theta_1}{2\pi} = \frac{d\theta}{2\pi}.
\ee

So $h_\sD (z,t) = \frac 1{2\pi} \frac{d\theta}{dt}$. But from $e^{i\theta} = \frac{e^{it}-z}{1-\ol{z}e^{it}}$ we get
\be
\frac{d\theta}{dt} = \frac{1-\abs{z}^2}{\abs{e^{it}-z}^2} \ \ra \ h_\sD(z,t) = \frac 1{2\pi} \frac{1-\abs{z}^2}{\abs{e^{it}-z}^2}.
\ee
\end{solution}

\item [2.10] Winding numbers of planar Brownian motion. Let $(X_t, t \geq 0)$ and $(Y_t, t \geq 0)$ be two independent one-dimensional Brownian motions.
\ben
\item [(a)] Assume that $X_0 = Y_0 = 0$ almost surely. Show that if $\tau_x = \inf\bra{t > 0 :X_t = x}$ for some $x > 0$ then $\tau_x$ has the same distribution as $x^2/\sN^2$ where $\sN$ is a standard normal random variable. Use this result and a scaling argument to conclude that 
\be
Y_{\tau_x} \stackrel{d}{=} x\frac{\sN}{\sN'}
\ee
where $\stackrel{d}{=}$ means that the distributions of the left and right hand side are equal, and where $\sN$, $\sN'$ are independent standard normal random variables. The distribution of $\sN=\sN'$ is known as the Cauchy distribution.

\item [(b)] Regard $Z_t = (X_t, Y_t)$ as a two-dimensional Brownian motion, and assume that $X_0 = -x$ and $Y_0 = 0$, with $x > 0$. Let $\tau = \inf\bra{t > 0 : \Re Z_t \geq 0}$, where $\Re z$ denote the real part of $z \in \C$. Deduce from the previous question that
\be
Y_\tau \stackrel{d}{=} xC
\ee
where $C$ has the Cauchy distribution.
\item [(c)] Let $x > 0$ and let $\ve = e^{-x}$. Let $Z'_t$ be a two dimensional Brownian motion with $Z'_0 = (\ve, 0)$. Let $\theta_\ve$ denote the number of windings of $Z'$ prior to time $T = \inf\bra{t >0 : \dabs{Z'_t} = 1}$. That is, each time $Z'$ completes a winding around zero in the counterclockwise direction, this adds +1 to "subseteq", and each clockwise winding about 0 adds -1 to "subseteq". Use (b) and conformal invariance of Brownian motion to show
that as $\ve \to 0$,
\be
\frac{\theta_\ve}{\log \ve} \stackrel{d}{\to} \frac 1{2\pi }C
\ee
where $C$ is a Cauchy random variable, and where $\stackrel{d}{\to}$ stands for convergence in distribution. [Hint. consider $\exp(X_t + iY_t)$ where $Z_t = (X_t, Y_t)$ is the Brownian motion from part (b)]. This result is Spitzer's law on windings of Brownian motion.
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] The first part was prove in the previous example sheet. Then since $\tau_x$ is independent from $Y$ and $Y$ has the scaling property we have in distribution
\be
Y_{\tau_x} \sim \sqrt{\tau_x}Y_1 \sim x\frac{\sN}{\abs{\sN'}},
\ee
where $\sN,\sN'$ are 2 independent random variables. We now claim that $\frac{\sN}{\abs{\sN'}}$ has the same distribution as $\frac{\sN}{\sN'}$. Indeed they can both be written as $\ve\abs{\frac{\sN}{\sN'}}$ where $\ve = \pm 1$ with probability $\frac 12$ independent from $\sN$ and $\sN'$.

\item [(b)] $\tau$ is measurable wrt $X$ and by translation invariance, $\tau$ has the same law as $\tau_x$ in (a). Thus, $Y_\tau = xC$, where $C$ is a Cauchy random variable.

\item [(c)] Consider the exponential map $\exp:z \mapsto e^z$, which is analytic in the complex plane. The exponential map sends the half plane $\Re z<0$ onto the unit disk, and thus by conformal invariance of Brownian motion, the image $\exp(Z_{t\land })$ is a time-change of $Z'_{t\land T}$ stopped when $Z'$ reaches the boundary of the unit disk. Furthermore, note that since $\exp(Z_t) = \exp\bb{X_t}\exp\bb{iY_t}$ it follows that 
\be
\theta(t) = \floor{\frac{Y_t}{2\pi}}\ind_{Y_t >0} + \ceil{\frac{Y_t}{2\pi}}\ind_{Y_t\leq 0}
\ee
is precisely equal to the number of windings of $exp(Z_t)$. Since $\theta_\ve$ doesn't depend on the time-parametrization of $Z'_t$, we obtain:
\be
\theta_\ve = \theta(\tau_x) = \floor{\frac{xC}{2\pi}}\ind_{C >0} + \ceil{\frac{xC}{2\pi}}\ind_{C\leq 0}.
\ee

Therefore if $\ve = e^{-x}$ or $x=-log \ve \to \infty$ as $\ve \to 0$,
\be
\frac{\theta_\ve}{x} = \frac 1x\bb{\floor{\frac{xC}{2\pi}}\ind_{C >0} + \ceil{\frac{xC}{2\pi}}\ind_{C\leq 0}} \stackrel{d}{\to} \floor{\frac{C}{2\pi}}\ind_{C >0} + \ceil{\frac{C}{2\pi}}\ind_{C\leq 0} = \frac {C}{2\pi}
\ee
and thus $-\frac{\theta_\ve}{\log\ve} \stackrel{d}{\to} \frac 1{2\pi}C$ and since $C \stackrel{d}{=} -C$ the result is proved.
\een
\end{solution}

\item [2.11] Let $N$ be a Poisson process of rate 1. Set $M_t = N_t -t$. Show that both $M_t$ and $M^2_t -t$ are martingales. Why does this not contradict L\'evy's characterization of Brownian motion?

\begin{solution}[\bf Solution.]
Let $\sF_t =\sigma(N_s:s\leq t)$. Then $M_t$ is adapted and $\E\abs{M_t} \leq \E N_t + t = 2t < \infty$. So $M_t$ is integrable. If $0<s<t$, then 
\be
\E\bb{M_t|\sF_t} = \E\bb{N_t - t|\sF_s} = \E\bb{N_t - N_s|\sF_s} + N_s -t = N_s -s = M_s.
\ee
So $M_t$ is a martingale. $(M_t^2 - t)$ is adapted and $\E\abs{M_t^2 -t} \leq 2t+t^2 < \infty$. If $0<s<t$, then 
\beast
\E\bb{M_t^2 - t|\sF_s} & = & \E\bb{(M_t - M_s)^2 + 2M_s(M_t - M_s) + M_s^2|\sF_s}- t \\
& = & \E\bb{\bb{(N_t - N_s) - (t-s)}^2} + 2M_s\E\bb{(N_t - N_s) - (t-s)} + M_s^2 -t \\
& = & t-s + 2M_s((t-s) - (t-s)) + M_s^2- t = M_s^2 -s.
\eeast

So $M_t^2 -t$ is a martingale. This doesn't contradict \levy's characterization of Brownian motion since it only holds for continuous martingales, whereas $N_t$ and hence $M_t$ are discontinuous.
\end{solution}

\item [2.12] Use conformal invariance of Brownian motion to prove Liouville's theorem. if $f:\C \to \C$ is a bounded analytic function in the complex plane then it must be constant. 

\begin{solution}[\bf Solution.]
Suppose that $f$ is not constant. Then from previous question (a) we have that $f(Z_t)$ is a time-change of a 2-$d$ Brownian motion. We also proved that $[f(Z)]_\infty = \infty$ a.s. $f(Z_t) = B_{[f(Z)]_t}$ $\forall t$ and $[f(Z)]_t$ is strictly increasing and continuous. So if $f$ is bounded, this implies that $B_{f[Z]_t}$ will be bounded, which is a contradiction, since Brownian motion is a.s. unbounded.
\end{solution}

\item [3.1] Let $(S_n, n \geq 0)$ be a simple random on $Z$, with $S_0 = 0$. For every $n \geq 1$, define a measure $\mu_n$ on $\R$, the average occupation measure of $S$, by:
\be
\mu_n(A) = \E\bb{\frac 1n \sum^n_{i=1} \ind_{\bra{\frac{S_i}{\sqrt{n}} \in A}}},
\ee
for all Borel sets $A$. Show that $\mu_n$ converges weakly towards a measure $\mu$ on $\R$ which you should identify. (It is recalled that a measure $\mu_n$ converges weakly if $\int \varphi d\mu_n \to \int \varphi d\mu$ for all continuous bounded functions $\varphi$).

\begin{solution}[\bf Solution.]
Let $f$ be a bounded and uniformly continuous. We want to show that $\mu_n(f) \to \mu(f)$ as $n\to\infty$. Let $(\wt{S}_t)_{t\geq 0}$ be the process given by 
\be
\wt{S}_n = S_n,\ \forall n\in \N,\quad \wt{S}_t = S_n,\ \forall t\in [n,n+1).
\ee

Thus, $\abs{S_t - \wt{S}_t} \leq 1$. Now we have
\beast
\mu_n (f) & = & \E\bb{\frac 1n \sum^n_{i=1} f\bb{\frac{S_i}{\sqrt{n}}}} = \E\bb{\frac 1n \int^n_0 f\bb{\frac{\wt{S}_t}{\sqrt{n}}}dt} = \E\bb{\int^1_0 f\bb{\frac{\wt{S}_{t_n}}{\sqrt{n}}}dt}\\
& = & \E\bb{\int^1_0 \bb{f\bb{\frac{\wt{S}_{t_n}}{\sqrt{n}}} - f\bb{\frac{S_{t_n}}{\sqrt{n}} }}dt} + \E\bb{\int^1_0 f\bb{\frac{S_{t_n}}{\sqrt{n}}}dt}
\eeast

By Donsker's invariance principle we have that
\be
\E\bb{\int^1_0 f\bb{\frac{S_{t_n}}{\sqrt{n}}}dt } \to \E\bb{\int^1_0 f(B_t)dt}\text{ as $n\to \infty$ }
\ee
where $(B_t)_{t\geq 0}$ is a standard Brownian motion in $\R$. Also we have that $\abs{\frac{S_{t_n}- \wt{S}_{t_n}}{\sqrt{n}}} \leq \frac 1{\sqrt{n}}$, so since $f$ is uniformly continuous $f\bb{\frac{\wt{S}_{t_n}}{\sqrt{n}}} - f\bb{\frac{S_{t_n}}{\sqrt{n}} } \to 0$ as $n\to \infty$.

So by bounded convergence ($f$ is bounded) we get that
\be
\E\bb{\int^1_0 \bb{f\bb{\frac{\wt{S}_{t_n}}{\sqrt{n}}} - f\bb{\frac{S_{t_n}}{\sqrt{n}} }}dt}  \to 0 \text{ as }n\to\infty.
\ee

So $\mu_n(f) \to \E\bb{\int^1_0 f(B_t)dt}$ as $n\to \infty$.
\end{solution}

\item [3.2] Characterize the solutions to the Dirichlet problem in 1 dimension (distinguish between the case $D$ bounded or not).

\begin{solution}[\bf Solution.]
Let $D$ be a domain (i.e., connected \& open) in $\R$. 

\ben
\item [(a)] $D$ is bounded.

$D$ must be an interval of the form $(a,b)$, $a<b$. In this case the Dirichlet problem is the following: Find $u: \ol{D} = [a,b]\to \R$ s.t. $\Delta u = 0$ on $D$ and $u|_{\partial D} =g$. $g$ is a function defined on the boundary, i.e., on $\bra{a,b}$. So $u(x) = \E_x[g(B_T)]$, $x\in (a,b)$, $T=\inf\bra{t\geq 0:B_t \notin D}$. $T = T_a \land T_b$ and 
\be
\pro\bb{T_a < T_b} = \frac{b-x}{b-a},\quad \pro\bb{T_a > T_b} = \frac{x-a}{b-a}.
\ee

So 
\be
\E_x(g(B_T)) = g(a)\bb{\frac{b-x}{b-a}} + g(b)\bb{\frac{x-a}{b-a}}.
\ee

Hence, $x\in [a,b]$,
\be
u(x) = g(a)\bb{\frac{b-x}{b-a}} + g(b)\bb{\frac{x-a}{b-a}}.
\ee

\item [(b)] $D$ is unbounded.

In this case $D$ will have the form $(a,\infty)$, $a\in \R$ or $(-\infty,a)$. $\partial D= \bra{a}$ in both cases.

Again, $u(x) = \E_x\bb{g(B_T)}$, $T = \inf\bra{t\geq 0:B_t = a}$. But since $\pro(T<\infty) =1$, we have that 
\be
u(x) = g(a),\quad\forall x\in \ol{D}.
\ee

\item [(c)] $D= \R$.

In this case any harmonic function solves the Dirichlet problem. 
\een
\end{solution}

\item [3.3] Let $D \subseteq \R^d$ be a domain satisfying the local exterior cone condition, it is desired to prove the boundary continuity of the harmonic function $u(x) = \E_x(g(B_T ))$ where $B$ is a $d$-dimensional Brownian motion, $g :\partial D \to \R$ is a bounded continuous function and $T$ is the hitting time of $D^c$. To this end, we prove. as $x \to y$, $y \in \partial D$ and $x \in D$, then
\be\label{equ:lecc_pro_go_to_zero}
\pro_x(T > \ve) \to 0
\ee
for all $\ve > 0$.
\ben
\item [(a)] Fix $\ve > 0$ and for all $\delta > 0$, let $g_\delta(x) = \pro_x(X_t \in D, \forall t \in (\delta, \ve + \delta))$. Show that $g_\delta$ is continuous on $\ol{D}$. (Hint. apply the Markov property at time $\delta$).

\item [(b)] A function $f:U \subseteq \R^d \to \R$ is called upper semi-continuous if for all $x_0 \in U$, $\limsup_{x\to x_0} f(x) \leq f(x_0)$. An important property of upper-continuous functions is that if $f_n$ are continuous functions on $U$ and $f_n(x) \da f(x)$ for all $x \in U$ as $n\to\infty$, then $f$ is upper-semi-continuous. Deduce from this property that $g(x) = \pro_x(X_t \in D, \forall t\in (0, \ve])$ is upper-semi continuous. Hence show that (\ref{equ:lecc_pro_go_to_zero}) holds.

\item [(c)] Conclude that $u$ is continuous on $\ol{D}$.
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] Since $d=1$,
\beast
g_\delta(x) & = & \pro_x\bb{B_t\in D,\forall t\in (\delta,\ve]} = \int_{\R^d}\pro_x\bb{B_t\in D, \forall t\in (\delta,\ve],B_\delta \in dy}\\
& = & \int_{\R^d}\pro_y\bb{B_t\in D, \forall t\in (\delta,\ve]} \pro_x\bb{B_\delta \in dy}\\
& = & \int_{\R^d}\pro_y\bb{B_t\in D, \forall t\in (\delta,\ve]} \pro_0\bb{B_{\delta+x} \in dy} \\
& = & \int_{\R^d}\pro_y\bb{B_t\in D, \forall t\in (\delta,\ve]} \frac 1{\sqrt{2\pi \delta^2}} e^{-\frac{(y-x)^2}{2\delta^2}} dy
\eeast

So by dominated convergence we see that $g_\delta$ is continuous on $\ol{D}$.

\item [(b)] We have that $g_\delta (x) \da g(x)$ as $\delta \da 0$. So $g$ is upper-semi continuous. Also,
\be
\pro_x(T>\ve) = \pro_x\bb{X_t\in D,\forall t\in (0,\ve]} \to 0
\ee
as $x\to y$, $x\in D$, $y\in \partial D$.

\item [(c)] Look at the proof of Thm 4.18 in the lecture notes.
\een
\end{solution}


\item [3.4] Let $\W^\mu$ be the law of Brownian motion with drift $\mu$ and started at zero, and let $T = \inf\bra{t > 0:X_t = \pm 1}$. Using Girsnaov's theorem, show that $X_T$ and $T$ are independent under $\W^\mu$. (It's worth thinking about this counterintuitive statement !)

\begin{solution}[\bf Solution.]
Under $W$, $T$ and $X_T$ are independent, because
\be
\pro\bb{X_T = 1,T\leq t} = \pro\bb{X_T = -1,T\leq t}
\ee
by the reflection of BM. So $2\pro(X_T = 1,T\leq t) = \pro(T\leq t)$. Also $\pro(X_T = 1) = \frac 12$. Hence,
\be
\pro(X_T = 1,T\leq t) = \pro(T\leq t)\pro(X_T = 1).
\ee

By Girsanov's theorem we have that
\be
\W^\mu \bb{f(X_T)g(T)} =\W\bb{f(X_T)g(T)\exp\bb{\mu X_T - \frac 12 \mu^2 T}} = \W\bb{f(X_T)e^{\mu X_T}}\W\bb{g(T)e^{-\frac 12\mu^2 T}}
\ee
using the independence of $X_T$, $T$ under $\W$. Thus,
\beast
\W^\mu \bb{f(X_T)g(T)} & = & \W\bb{f(X_T)e^{\mu X_T}}\W\bb{g(T)e^{-\frac 12\mu^2 T}}\W\bb{e^{\mu X_T - \frac 12 \mu^2 T}}\\
& = & \W\bb{f(X_T)e^{\mu X_T - \frac 12\mu^2 T}} \W\bb{g(T)e^{\mu X_T - \frac 12\mu^2T}} = \W^\mu \bb{f(X_T)}\W^\mu \bb{g(T)}
\eeast
\end{solution}

\item [3.5] 
\ben
\item [(a)] Let $(Z_t, t \geq 0)$ be a continuous local martingale which is strictly positive almost surely. Show that there is a unique continuous local martingale such that $Z = \sE(M)$, where $\sE(M)$ denotes the exponential local martingale associated with $M$:
\be
\E(M)_t = \exp\bb{M_t - \frac 12[M]_t}
\ee
and $[M]$ denotes as usual the quadratic variation process of $M$. [Hint. for the existence part, define $M_t = \ln Z_0 + \int^t_0 \frac{dZ_s}{Z_s}$, and then apply It\^o's formula to $\ln Z_t$.]

\item [(b)] Let $(\Omega,\sF, (\sF_t), \pro)$ be a filtered probability space satisfying the usual conditions. Let $\Q$ be another probability measure on $(\Omega,\sF, (\sF_t))$ such that $\Q$ is absolutely continuous with respect to $\pro$ on $\sF$. Show that if $Z_t = \left.\frac{d\Q}{d\pro}\right|_{\sF_t}$ for all $0 \leq t \leq T$, then $Z$ is nonnegative $\pro$-martingale. Assuming that it is strictly positive almost surely and continuous, what can we say about the relations between semi-martingales with respect to $\pro$ and $\Q$?
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] Define $M_t = \ln Z_0 + \int^t_0 \frac{dZ_s}{Z_s} \in \sM_{c,loc}$. Then $[M]_t = \int^t_0 \frac 1{Z_s^2}d[Z]_s$.

By applying It\^o formula to $\ln Z_t$ we get
\be
\ln Z_t = \ln Z_0 + \int^t_0 \frac 1{Z_s}dZ_s + \frac 12 \int^t_0 -\frac 1{Z_s^2}d[Z]_s = \ln Z_0 + \int^t_0 \frac 1{Z_s}dZ_s - \frac 12 [M]_t = M_t - \frac 12[M]_t.
\ee

So $Z_t = \exp\bb{M_t - \frac 12 [M]_t}$. Now , to show uniqueness, suppose there is another continuous local martingale $N$, s.t. $Z_t = \sE(N)_t$. Then we will have $M_t - \frac [M]_t = N_t - \frac 12 [N]_t$ or equivalently
\be
M_t - N_t = \frac 12[M]_t - \frac 12 [N]_t.
\ee

But $M-N \in \sM_{c,loc}$ (as the difference of 2 local martingales) and $\frac 12\bb{[M]_t - [N]_t}$ is a finite variation process. So $M_t = N_t$, $\forall t$ a.s..

\item [(b)] If $A\in \sF_t$, $\Q(A) = \E^\pro\bb{Z_t \ind_A}$. Let $s<t$ and $A\in \sF_s$. Then, since $\sF_s \subset \sF_t$, $A\in \sF_t$. Hence $\Q(A) = \E^\pro\bb{Z_s \ind_A}$. So $\E^\pro\bb{Z_t|\sF_s} =Z_s$ $\pro$-a.s. and therefore $Z$ is a $\pro$-martingale.

Let $A = \bra{Z_t < 0}$. Then $A\in \sF_t$. So 
\be
\Q(A) = \E^\pro\bb{Z_t\ind_A} = \E^\pro\bb{Z_t \ind_{\bra{Z_t< 0}}} \leq 0.
\ee

Thus, $\Q(A) = 0$ implies that $\pro(Z_t \geq 0) = 1$.

On the event $\bra{Z_t\geq 0}$, $Z_s = \E\bb{Z_t|\sF_s}$ a.s. ($s<t$). So $Z_s \geq 0$ for any $s<t$. Hence,
\be
\pro\bb{Z_s \geq 0,\ s\in [0,t]} = 1 \ \ra \ \pro\bb{Z_t \geq 0 \text{ for all }t\geq 0} = 1.
\ee

If we now assume that it is strictly positive and continuous, then by (a) $\exists M\in \sM_{c,loc}$ s.t. $Z_t = \sE(M)_t$.

Also, since $\Q \ll \pro$ we have that $Z$ is a UI martingale and we have $\frac{d\Q}{d\pro} = Z_\infty$ a.s.

So, by Girsanov's theorem, if $X\in \sM_{c,loc}(\pro)$, then $X- [X,M]\in \sM_{c,loc}(\Q)$. 

Since $Z_t >0$, we have also $\pro(A) = \Q\bb{Z^{-1}_t \ind_A}$, $A\in \sF_t$. Also, 
\be
Z^{-1}_t = \exp\bb{-M_t + \frac 12[M]_t} = \exp\bb{\bb{-M_t + [M]_t} + \frac 12 [-M + [M]]_t}.
\ee
($M-[M]\in \sM_{c,loc}(\pro)$ by Girsanov's theorem applied to $M$ itself.)

Now, mimicking the proof of Girsanov's theorem, we get that if $X\in \sM_{c,loc}(\Q)$, then $X-[X,-M+[M]]\in \sM_{c,loc}(\pro)$. So if $X+A$ is a $\pro$-semimartingale, (with $X\in \sM_{c,loc}$, $A$ finite variation). Then 
\be
X+A = X-[X,M] + [X,M]+ A,
\ee
which is a $\Q$-semimartingale. Likewise in the other direction.
\een
\end{solution}

\item [3.6] Let $\gamma \in \R$, and let $\W$ be the Wiener measure, i.e., the law of Brownian motion on $\R$ and let $\sF = (\sF_t, t \geq 0)$ be the filtration generated by the Brownian motion. Define a probability measure $\Q$ on $\Omega$ by
\be
\Q|_{\sF_t} = \exp\bb{\gamma X_t - \frac{\gamma^2}2 t} \cdot \W
\ee

Let $T$ be a stopping time. Show that
\be
\Q(T < \infty) = \E_{\W} \bb{\exp\bb{\gamma X_T - \frac{\gamma^2}2T}}.
\ee

In particular, $T < \infty$ $\Q$-almost surely if and only if $\E_{\W}\bb{\exp\bb{\gamma X_T - \frac{\gamma^2}2 T}} = 1$.

\begin{solution}[\bf Solution.]
We know that $Y_t = \exp\bb{\gamma X_t - \frac 12 \gamma^2 t}$ is a martingale under the Wiener measure $\W$. On each interval $[0,t]$ $Y$ is UI. So $\frac{d\Q}{d\W}|_{\sF_{T\land t}} = Y_{T\land t}$, because since on $[0,t]$ $Y$ is UI, by applying the OST we have that $\E\bb{Y_t|\sF_{T\land t}} = Y_{T\land t}$.

So if $A\in \sF_{T\land t} \subseteq \sF_t$, then $\Q(A) = \E_w\bb{Y_t \ind_A} = \E_w\bb{Y_{T\land t}\ind_A}$.
\be
\Q(T\leq t) = \E_w\bb{\ind_{\bra{T\leq t}}Y_{T\land t}} = \E_w \bb{\ind_{\bra{T\leq t}}\exp\bb{\gamma X_T - \frac 12 \gamma^2 T}}.
\ee

So $\Q(T<\infty) = \E_w\bb{\ind_{T<\infty} \exp\bb{\gamma X_T - \frac 12 \gamma^2 T}}$ by monotone convergence.
\end{solution}

\item [3.7] Let $B$ be a standard Brownian motion and for $a, b > 0$ let $\tau_{a,b} = \inf\bra{t \geq 0 :B_t+bt = a}$. Use Girsanov's theorem to prove that the density of $\tau_{a,b}$ is 
\be
f(t) = a(2\pi t^3)^{-1/2} \exp(-(a - bt)^2/2t).
\ee

\begin{solution}[\bf Solution.]
Suppose $X$ is a standard Brownian motion under $\pro$. Let $T=\inf\bra{t\geq 0:X_t = a}$. $\bb{\exp\bb{bX_t - \frac 12 b^2 t}}_{t\geq 0}$ is a martingale. So $\bb{\exp\bb{bX_{t\land T}- \frac 12 b^2 (T\land t)}}_{t\geq 0}$ is a UI martingale.

Hence, by Girsanov's theorem, we can define a new probability measure by 
\be
\wt{\pro}(A) = \E\bb{\exp\bb{bX_T - \frac 12 b^2 T}\ind_A},\quad A\in \sF.
\ee
and $(X_t - bt)_{0\leq t\leq T}$ is a Brownian motion under $\wt{\pro}$. Write $B_t = X_t - bt$. Then $T= \inf\bra{t\geq 0: B_t + bt = a}$.

Hence, 
\be
\wt{\pro}(T\leq t) = \E\bb{\exp\bb{bX_T - \frac 12 b^2 T}\ind_{T\leq t}} = \E\bb{\exp\bb{ba - \frac 12 b^2 T}\ind_{T\leq t}} = \int^t_0 e^{ab- \frac 12 b^2 s}f(s)ds,
\ee
where $f(t)$ is the density of $T$ under $\pro$. From advanced probability, 
\be
f(t) = \frac{a}{\sqrt{2\pi t^3}}\exp\bb{-\frac{a^2}{2t}}
\ee
and so
\be
\wt{\pro}(T\leq t) = \int^t_0 \frac{a}{\sqrt{2\pi s^3}}\exp\bb{-\frac{(a-bs)^2}{2s}}ds
\ee
as required.
\end{solution}

\item [4.1] Consider the stochastic differential equation with Lipschitz coefficients 
\be
dX_t = \sigma(X_t)dB_t + b(X_t)dt,\quad  X_0 = x_0.
\ee

Show without appealing to the Yamada-Watanabe theorem that there is uniqueness in law (i.e. that the pathwise unique solutions on different spaces given by Theorem 5.3.1 must have the same distribution).

\begin{solution}[\bf Solution.]
Let $X$ be the pathwise unique solution on $\bb{\Omega,\sF,(\sF_t)_{t\geq 0},\pro}$ with $(\sF_t)_{t\geq 0}$-Brownian motion $B$ and starting point $x$. Let $\wt{X}$ be the pathwise unique solution on $\bb{\wt{\Omega},\wt{\sF},(\wt{\sF}_t)_{t\geq 0},\wt{\pro}}$ with $(\wt{\sF}_t)_{t\geq 0}$-Brownian motion $\wt{B}$ and starting point $x$. Then we know that $X$ and $\wt{X}$ are both strong solutions.

Let 
\be
F(X)_t = x+ \int^t_0 \sigma(X_s)dB_s + \int^t_0b(X_s)ds,\quad \wt{F}(X)_t = x+ \int^t_0 \sigma(X_s)d\wt{B}_s + \int^t_0b(X_s)ds.
\ee

Then if we set $Y^\circ = \wt{Y}^\circ = x$ and 
\be
Y^n = F(Y^{n-1}),\quad \wt{Y}^n = F(\wt{Y}^{n-1}),\quad n\geq 1,
\ee
we have $Y^n \stackrel{d}{=} \wt{Y}^n$ $\forall n\geq 1$. But $Y^n \to X$ a.s. uniformly on compact time intervals and, likewise, $\wt{Y}^n \to \wt{X}$ a.s.. Hence, $X\stackrel{d}{=} \wt{X}$.
\end{solution}

\item [4.2] Suppose that $\sigma$, $b$ and $\sigma_n$, $b_n$, $n = 1, 2,\dots$ are Lipschitz, with constant $K$ independent of $n$. Suppose also that $\sigma_n \to \sigma$ and $b_n \to b$ uniformly. Define $X$ and $X^n$ by
\be
dX_t = \sigma(X_t)dB_t + b(X_t)dt, \quad X_0 = x,\quad\quad dX^n_t = \sigma_n(X^n_t )dB_t + b_n(X^n_t )dt,\quad X^n_0 = x.
\ee
Show that, as $n\to\infty$,
\be
\E_x \bb{\sup_{s\leq t} \abs{X^n_s - X_s}^2} \to  0.
\ee

\begin{solution}[\bf Solution.]
Since $\sigma_n,\sigma,b_n,b$ are Lipschitz there exist unique continuous adapted process on $\R$ solving
\be
X_t = x+\int^t_0 \sigma(X_s)dB_s + \int^t_0 b(X_s)ds,\quad X_t^n = x+\int^t_0 \sigma_n(X_s^n)dB_s + \int^t_0 b_n(X^n_s)ds.
\ee

Hence,
\beast
\abs{X_t^n - X_t}^2 & = & \abs{\int^t_0 \bb{\sigma_n(X^n_s) -\sigma(X_s)} dB_s + \int^t_0 \bb{b_n(X^n_s) - b(X_s)}ds}^2 \\
& \leq & 2\abs{\int^t_0 \bb{\sigma_n(X^n_s) -\sigma(X_s)} dB_s }^2 + 2\abs{\int^t_0 \bb{b_n(X^n_s) - b(X_s)}ds}^2
\eeast

So 
\be
\sup_{s\leq t}\abs{X_s^n - X_s}^2 \leq  2\sup_{s\leq t}\bra{\abs{\int^s_0 \bb{\sigma_n(X^n_u) -\sigma(X_u)} dB_u }^2 + 2\abs{\int^s_0 \bb{b_n(X^n_u) - b(X_u)}du}^2}\quad (*).
\ee

Since $\sigma_n \to \sigma$ uniformly and $b_n \to b$ uniformly we have that
\be
\forall \ve >0,\exists N\text{ s.t. }\forall n\geq N,\forall x,\ \abs{\sigma_n(x) -\sigma(x)} < \ve.
\ee
\be
\forall \ve >0,\exists N'\text{ s.t. }\forall n\geq N',\forall x,\ \abs{b_n(x) -b(x)} < \ve.
\ee

By Doob's inequality on the second term of ($*$) and C-S on the first we get
\be
\E\bb{\sup_{s\leq t}\abs{X_s^n - X_s}^2} \leq 2\bb{4\E\bb{\int^t_0 \abs{\sigma_n(X_u^n) - \sigma(X_u)}^2du} + t\E\bb{\int^t_0 \abs{b_n(X_u^n) - b(X_u)}^2 du}}.
\ee

We now use Lipschitz properties of $\sigma_n,\sigma,b_n,b$ together with uniform convergence.
\be
\sigma_n(X_u^n) - \sigma(X_u) = \sigma_n(X^n_u) - \sigma_n(X_u) + \sigma_n(X_u) - \sigma(X_u).
\ee

\be
\abs{\sigma_n(X_u^n) - \sigma(X_u)} \leq ) K\abs{X^n_u - X_u} + \abs{\sigma_n(X_u) - \sigma(X_u)}.
\ee

So provided $n\geq \max(N,N')$ we have $\abs{\sigma_n(X_u) - \sigma(X_u)}<\ve$. Using again $(a+b)^2 \leq 2a^2 + 2b^2$ we get
\be
\E\bb{\int^t_0 \abs{\sigma_n(X^n_u) - \sigma(X_u)}du} \leq 2\E\bb{\int^t_0 \abs{\sigma_n(X^n_u) - \sigma_n(X_u)}^2 du} + 2\E\bb{\int^t_0 \abs{\sigma_n(X_u) - \sigma(X_u)}^2 du}.
\ee

Similarly for $\abs{b_n(X^n_u) - b(X_u)}$. Setting $g_n(t) = \E\bb{\sup_{s\leq t}\abs{X^n_s - X_s}^2}$ we obtain $\forall t\in [0,T]$,
\be
g_n(t) \leq 2\sO \ve^2 T + 16K^2\int^t_0 g_n(s)ds + 4TK^2 \int^t_0 g_n(s)ds.
\ee

So by Granwall's inequality we get
\be
g_n(t)\leq 2\sO \ve^2 T e^{(16K^2 + 4TK^2)t}, \quad \forall t\in [0,T].
\ee

We can choose $\ve$ as small as we like, so
\be
\E\bb{\sup_{s\leq t}\abs{X_s^n - X_s}^2} \to 0 \quad\text{as }n\to \infty.
\ee
\end{solution}

\item [4.3] Let $B$ be a standard Brownian motion on $\R$. Use the integration by parts formula to give a simple proof that $\sE(\lm B)_t = x_0 \exp(\lm B_t - \frac{\lm^2}2 t )$ is the pathwise unique solution to the SDE
\be
dX_t = \lm X_tdB_t, \quad X_0 = x_0.
\ee
[Hint. if $Y$ is another solution, prove that $d(Y X^{-1}) = 0$.]

\begin{solution}[\bf Solution.]
First note that if $X_t = x_0 \exp\bb{\lm B_t - \frac{\lm^2}2 t}$, then
\be
dX_t = \lm x_0 \exp\bb{\lm B_t - \frac {\lm^2}2 t}dB_t + \frac 12 \lm^2 x_0 \exp\bb{\lm B_t - \frac {\lm^2}2t}dt - \frac{\lm^2}2 x_0 \exp\bb{\lm B_t - \frac {\lm^2}2t}dt = \lm X_t dB_t
\ee
and $X_0 = x_0$. So $X$ is a solution to the SDE. Now suppose that $Y$ is another solution, i.e., $dY_t = \lm Y_t dB_t$, $Y_0 = x_0$. Also note that 
\be
X_t^{-1} = x_0^{-1} \exp\bb{-\lm B_t + \frac {\lm^2}2t}
\ee
satisfies
\beast
dX_t^{-1} & = & -\lm x_0^{-1}\exp\bb{-\lm B_t + \frac {\lm^2}2t} dB_t + \frac{\lm^2}2 x_0^{-1}\exp\bb{-\lm B_t + \frac {\lm^2}2t}dt + \frac{\lm^2}2 x_0^{-1}\exp\bb{-\lm B_t + \frac {\lm^2}2t}dt\\
& = & -\lm X_t^{-1}dB_t + \lm^2 X_t^{-1}dt.
\eeast

Hence by the integration by parts formula we have
\be
d(Y_tX_t^{-1}) = Y_t dX_t^{-1} + X_t^{-1} dY_t + \bsb{Y_t,X_t^{-1}} = -\lm Y_tX_t^{-1}dB_t + \lm^2 Y_tX_t^{-1}dt + \lm Y_tX_t^{-1}dB_t -\lm^2 Y_tX_t^{-1}dt  = 0
\ee
and $Y_0 X_0^{-1} = 1$. So $Y_t = X_t$.
\end{solution}

\item [4.4] Suppose that $X$ satisfies the stochastic differential equation 
\be
dX_t = \sigma(X_t)dB_t, \quad \quad X_0 = x_0,
\ee
where $\sigma$ is Lipschitz and $B$ is a Brownian motion. Show that, for some constant $C < \infty$ depending only on the Lipschitz constant of $\sigma$, $X$ satisfies the estimate
\be
\E\sup_{s\leq t} \abs{X_s - x_0}^2 \leq Ct e^{Ct}\abs{\sigma(x_0)}^2.
\ee

Discuss the tightness of this estimate with reference to the special cases $\sigma(x) = 1$ and $\sigma(x) = x$.

\begin{solution}[\bf Solution.]
We have that $X$ satisfies the SDE, so
\be
X_t = x_0 + \int^t_0 \sigma(X_s) dB_s = x_0 + \int^t_0 (\sigma(X_s) -\sigma(x_0))dB_s + \sigma(x_0)B_t.
\ee

So using again $(a+b)^2 \leq 2a^2 + 2b^2$ we obtain
\be
\sup_{0\leq s\leq t}\abs{X_s - x_0}^2 \leq 2\sup_{0\leq s\leq t}\abs{\int^s_0 (\sigma(X_u) - \sigma(x_0))dB_u}^2 + 2\abs{\sigma(x_0)}^2 \sup_{s\leq t}B_s^2.
\ee

Hence taking expectations
\be
\E\bb{\sup_{s\leq t}\abs{X_s - x_0}^2 } \leq 2\E\bb{\sup_{s\leq t}\abs{\int^s_0 (\sigma(X_u) - \sigma(x_0))dB_u}^2} + 2\E\bb{\abs{\sigma(x_0)}^2 \sup_{s\leq t}B_s^2}.\quad (*)
\ee

Let $M_t = \int^t_0 (\sigma(X_s) - \sigma(x_0))dB_s$. Then $M\in \sM_{c,loc}$, so $\exists T_n \ua \infty$ reducing sequence s.t. $(M_{t\land T_n})_t$ is a martingale $\forall n$. So applying Doob's ineqaulity to the stopped martingale we get
\be
\E\bb{\sup_{s\leq t\land T_n}\abs{M}_s^2} \leq 4\E\bb{\abs{M_{t\land T_n}}^2} = 4 \E\bb{\int^{t\land T_n}_0 \abs{\sigma(X_s)- \sigma(x_0)}^2 ds} \leq 4\E\bb{\int^t_0 (\sigma(X_s)- \sigma(x_0))^2 ds}.
\ee

But, by monotone convergence we have that
\be
\E\bb{\sup_{s\leq t\land T_n}\abs{M_s}^2} \ua \E\bb{\sup_{s\leq t}\abs{M_s}^2} \quad \text{as }n\to\infty.
\ee

Also by Doob's inequality we have 
\be
\E\bb{\sup_{s\leq t}B_s^2} \leq 4\E\bb{B_t^2} = 4t.
\ee

So $(*)$ can be further bounded
\be
\E\bb{\sup_{s\leq t}\abs{X_s - x_0}^2} \leq 8\E\bb{\int^t_0(\sigma(X_s) - \sigma(x_0))^2ds} + 8 \abs{\sigma(x_0)}^2 t \leq 8K^2 \E\bb{\int^t_0\abs{X_s - x_0}^2ds} + 8t \abs{\sigma(x_0)}^2.
\ee

Setting $g(t) = \E\bb{\sup_{s\leq t}\abs{X_s - x_0}^2}$ we get
\be
g(t) \leq 8K^2 \int^t_0 g(s)ds + 8t\abs{\sigma(x_0)}^2,\quad \forall t\in [0,T] \ \ra \ g(t) \leq 8K^2 \int^t_0 g(s)ds + 8T\abs{\sigma(x_0)}^2,\quad \forall t\in [0,T].
\ee

Hence by Gronwall's lemma
\be
g(t)\leq 8T\abs{\sigma(x_0)}^2 e^{8K^2t}\ \forall t\in [0,T] \ \ra \ g(T)\leq 8T\abs{\sigma(x_0)}^2 e^{8K^2T}.
\ee

Setting $C=\max(8,8K^2)$ we obtain $g(t)\leq Ct\abs{\sigma(x_0)}^2 e^{Ct}$.
\ben
\item When $\sigma(x) =1$, then $K=0$, so $C=8$, so the bound is equal to $8te^{8t}$.

But in this case $\E\bb{\sup_{s\leq t}\abs{X_s - x_0}^2} = \E\bb{\sup_{s\leq t}\abs{B_s}^2} \leq 4t$. So the above bound is much weaker than Doob's bound.

\item When $\sigma(x) = x$, then $K=1$, so $C=8$, so the bound is $8te^{8t}\abs{x_0}^2$.

Let's try to find a solution to the SDE
\be
d(e^{B_t}) = e^{B_t} dB_t + \frac 12 e^{B_t}dt.
\ee

So if we try $X_t = e^{B_t}u_t$, $u_t$ is deterministic, we will get 
\be
dX_t = u_t e^{B_t} dB_t + \frac 12 u_t e^{B_t} dt + du_t e^{B_t} \ \ra \ du_t = -\frac 12 u_t \ \ra \ u_t = e^{-\frac 12t}.
\ee

Hence $X_t = x_0 \exp\bb{B_t -\frac 12t}$ solves the SDE and since $\sigma$ is Lipschitz there is pathwise uniqueness.

By Doob's $\sL^2$-inequality we have (since $e^{B_t - \frac12 t}$ is a martingale),
\beast
\E\bb{\sup_{s\leq t}\abs{X_s - x_0}^2} & = & \E\bb{\sup_{s\leq t}\abs{x_0}^2 \abs{e^{B_s -\frac 12s} -1}^2} \leq 4\abs{x_0}^2\E\bb{\bb{e^{B_t-\frac 12 t}-1}^2}\\
& = & 4\abs{x_0}^2 \E\bb{e^{-t}e^{2B_t} + 1- 2e^{B_t -\frac 12t}} = 4\abs{x_0}^2 \bb{e^{-t}\E\bb{e^{2B_t}} + 1- 2e^{-\frac t2}\E\bb{e^{B_t}}}.
\eeast

By scaling invariance of the BM we have that $2B_t \sim B_{4t}$. So $\E\bb{e^{2B_t}} = \E\bb{e^{B_{4t}}}$. Also $e^{B_t - \frac 12 t}$ is a martingale, hence $\E\bb{e^{B_{4t}}} = e^{2t}$ and $\E\bb{e^{B_t}} = e^{t/2}$.

So 
\be
\E\bb{\sup_{s\leq t}\abs{X_s - x_0}^2} \leq 4x_0^2 \bb{e^{-t}e^{2t} - 2e^{-\frac t2}e^{\frac t2} + 1} = 4x_0^2(e^t -1).
\ee

So again in this case Doob's bound is stronger.
\een
\end{solution}



\item [4.5] Consider the stochastic differential equation in $\R$
\be
dX_t = dB_t + b(X_t)dt,\quad X_0 = x,
\ee
where $b$ is bounded and measurable. Suppose that, under $\pro$, $X$ is a Brownian motion started from $x$. Use Girsanov's theorem to find a new probability measure $\wt{\pro}$, absolutely continuous with respect to $\pro$, such that if
\be
B_t = X_t - \int^t_0 b(X_s)ds,
\ee
then $(B_t)_{0\leq t\leq 1}$ is a Brownian motion under $\wt{\pro}$. This is called solution by transformation of drift.

\begin{solution}[\bf Solution.]
$X$ is a Brownian motion under $\pro$. Let's define $M_t = \int^{t\land 1}_0 b(X_s)dX_s$. $(M_t)_{0\leq t\leq 1}\in \sM_{c,loc}(\pro)$ since it is the integral wrt BM ($X$).

Also, $[M]_t = \int^{t\land 1}_0 b^2(X_s)ds < C$, $\forall t$, since $b$ is bounded. So if we define $Z_t = \exp\bb{M_t - \frac 12 [M]_t}$, then $Z$ is a UI martingale and $Z_t \to Z_\infty = \exp\bb{M_1 -\frac 12 [M]_1}$.

Hence we can define a new measure $\wt{\pro}\ll \pro$ by $\wt{\pro}(A) = \E^\pro\bb{Z_\infty \ind_A}$, $A\in \sF$.

So by Girsanov's theorem we have that since $X$ is a $\pro$-BM, then $X-[X,M]$ is a $\wt{\pro}$-BM. But
\be
[X,M]_t = [X,b(X)\cdot X]_t = \int^{t\land 1}_0 b(X_s)ds.
\ee

Hence under $\wt{\pro}$, $X_t - [X,M]_t = X_t - \int^t_0 b(X_s)ds$ is a BM for $0\leq t\leq 1$.

So $B_t = X_t - \int^t_0 b(X_s)ds$, $0\leq t\leq 1$ is a $\wt{\pro}$-BM and also satisfies the SDE $dX_t = dB_t + b(X_t)dt$.
\end{solution}


\item [4.6] Consider the Ornstein-Uhlenbeck SDE:
\be
\left\{\ba{l}
dV_t = dB_t - \lm V_t dt\\
V_0 = v_0
\ea\right.
\ee
\ben
\item [(a)] Show that any solution satisfies $V_t = v_0 e^{-\lm t} + \int^t_0 e^{-\lm (t-s)} dB_s$. What is the distribution of $V_t$?
\item [(b)] Consider now the two dimensional system:
\be
dV_t = dB_t - \lm V_tdt,\quad V_0 = 0,\quad\quad dX_t = V_tdt, \quad X_0 = 0.
\ee

Find the joint distribution of $(X_t, V_t)$. [Physical interpretation. when $\lm > 0$ this models the motion of a pollen grain on the surface of a liquid. $X$ represents the $x$-coordinate of the grain's position and $V$ represents its velocity in the $x$-direction. $-\lm V$ is the friction force due to viscosity. Whenever $\abs{V}$ becomes large, the system acts to reduce it. $V$ is called the Ornstein-Uhlenbeck (velocity) process.]
\een

\item [4.7] Let $a, b, c :\R \to \R$ be bounded with bounded derivatives and with $a > 0$. Suppose that $u \in C^{1,2}_b (\R^+ \times \R,\R)$ solves the Cauchy problem
\be
\fp{u}{t} = \sL u\quad \text{ in } \R^+ \times \R,\quad \quad u(0, \cdot) = f \quad\text{ on }\R
\ee
where
\be
\sL u = \frac 12 a(x)u'' + b(x)u' + c(x)u.
\ee

Set $\sigma = \sqrt{a}$ and define $X$ by the SDE
\be
dX_t = \sigma(X_t)dB_t + b(X_t)dt, \quad X_0 = x.
\ee

Define also $E_t = \exp\bra{\int^t_0 c(X_s)ds}$. Show that
\be
u(t, x) = \E_x\bb{E_tf(X_t)}.
\ee

\begin{solution}[\bf Solution.]
Let $M_t = u(T-t,X_t)E_t$. Applying It\^o formula to $M$ we get
\be
dM_t = - \fp{u}{t}(T-t,X_t)E_t dt + u(T-t,X_t)dE_t + u'(T-t,X_t)E_tdX_t + \frac 12 u''(T-t,X_t)E_t d[X]_t.
\ee

But we know that $X$ solves the SDE $dX_t = \sigma(X_t)dB_t + b(X_t)dt$, hence $[X]_t = \int^t_0 \sigma^2(X_s)ds$. So
\beast
dM_t & = & -\fp{u}{t}(T-t,X_t)E_t dt + c(X_t)u(T-t,X_t)E_t dt + u'(T-t,X_t)E_t\sigma(X_t) dB_t \\
& & \quad\quad + u'(T-t,X_t)E_t b(X_t)dt + \frac 12 u''(T-t,X_t)E_t \sigma^2(X_t) dt\\
& = & \bb{-\fp{u}{t}(T-t,X_t) + c(X_t)u(T-t,X_t) +  u'(T-t,X_t)b(X_t) + \frac 12 u''(T-t,X_t)\sigma^2(X_t)}E_t dt \\
& & \quad\quad + u'(T-t,X_t)E_t\sigma(X_t) dB_t 
\eeast
($u$ solves the Cauchy problem). So 
\be
dM_t = \bb{-\sL u(T-t,X_t) + \sL u(T-t,X_t)}E_t dt + u'(T-t,X_t) E_t \sigma(X_t) dB_t.
\ee

Hence, $dM_t = u'(T-t,X_t)E_t \sigma(X_t)dB_t$. So $M$ is a local martingale. SInce $u',c,\sigma$ are all bounded, $M$ is uniformly bounded on $[0,T]$, hence a true martingale. By the OST we have
\be
u(T,x) = \E_x\bb{M_0} = \E_x\bb{M_T} = \E_x\bb{f(X_T)E_T}.
\ee
\end{solution}

\item [4.8] Let $b : \R^d \to \R^d$ be Lipschitz and let $x_0 \in \R^d$. Consider for each $\ve > 0$ the diffusion process $X^\ve$ in $\R^d$, starting from $x_0$ and having generator
\be
\sL^\ve = \frac 12\ve^2 \Delta + b(x)\cdot \nabla.
\ee

Show that, for all $t \geq 0$,
\be
\sup_{s\leq t} \abs{X^\ve_s - x_s} \to 0
\ee
in probability as $\ve \to 0$, where $(x_t)_{t\geq0}$ is given by the differential equation $\dot{x}_t = b(x_t)$, starting from $x_0$.

\begin{solution}[\bf Solution.]
Since $X$ is an $\sL$-diffusion, we have that $\forall f\in C_b^2(\R^d)$ the following  process is a martingale.
\be
M^f_t = f(X_t) - f(X_0) - \int^t_0\sL f(X_s)ds,\quad t\geq 0.
\ee

By taking limits of bounded functions, it follows that $\forall f\in C^2_b(\R^d)$
\be
(M^f_t)' = f(X_t) - f(X_0) - \int^t_0 \sL f(X_s)ds, \quad t\geq 0
\ee
is a local martingale. Choose $f(x) = x$. Then we get that $X_t - X_0 - \int^t_0 b(X_s)ds$ is a local martingale. Also for $f(x)= x^2$ we have that (take $d = 1$) $X_t^2 - X_0^2 - \int^t_0 a(X_s)ds - 2\int^t_0 X_s b(X_s)ds$ is a local martingale.

From that it follows that if we define $M_t = X-t - \int^t_0 b(X_s)ds$, then $M^2_t - \int^t_0 a(X_s)ds$ is also a local martingale (integration by parts $X_t = X_0 - \int^t_0 b(X_s)ds$, $Y_t = \int^t_0 b(X_s)ds$ and use a Gronwall factor trick.)

So if we define $B_t = \int^t_0 \sigma^{-1}(X_s)dM_s$ (when $\sigma$ is invertible), then following the same steps as in the proof of Theorem 6.1 we get that the $\sL$-diffusion $X$ satisfies the following SDE
\be
dX_t = \sigma(X_t)dB_t + b(X_t)dt.
\ee

Hence, we have established a correspondence between $\sL$-diffusion and solutions to SDE's.

So $X^\ve$ satisfies the following equation
\be
dX_t^\ve = \ve dB_t + b(X_s^\ve)ds,\quad t\geq 0,
\ee
where $(B_t)_{t\geq 0}$ is a standard BM. So
\be
X_t^\ve = x_0 + \ve B_t + \int^t_0 b(X^\ve_s)ds,\quad x_t = x_0 + \int^t_0 b(x_s)ds.
\ee

Hence,
\be
\sup_{s\leq t}\abs{X_s^\ve - x_s} \leq \ve \sup_{s\leq t}\abs{B_s} + K\int^t_0 \sup_{u\leq s}\abs{X_u^\ve - X_u}ds,
\ee
where $K$ is the Lipschitz constant of $b$. By Gronwall's lemma,
\be
\sup_{s\leq t}\abs{X_s^\ve - x_s} \leq \ve \sup_{s\leq t}\abs{B_s} e^{Kt}
\ee
and hence
\be
\pro\bb{\sup_{s\leq t}\abs{X_s^\ve- x_s} > \delta} \leq \pro\bb{\sup_{s\leq t}\abs{B_s} > \frac{\delta}{\ve} e^{-Kt}} \leq 2d \exp\bb{-\frac{\delta^2 e^{-2Kt}}{2\ve^2 td}}
\ee
by the exponential martingale inequality. Hence, $\lim_{\ve \to 0} \pro\bb{\sup_{s\leq t}\abs{X_s^\ve - x_s} > \delta} = 0$.
\end{solution}

\item [4.9] Consider a Galton-Watson process, where every individual lives one unit of time, and when they die, they leave a random number of offsprings distributed according to some fixed distribution $(p_k)_{k\geq 0}$: thus the probability that this individual has $k$ offsprings is $p_k$. The number of offsprings for different individuals is assumed to be independent. We suppose that the offspring distribution is critical (i.e., $\sum_{k\geq 0} kp_k = 1$), and has finite variance $\sigma^2$. We also assume that for some $\lm > 0$, $\sum^\infty_{k=1} e^{\lm k}p_k < \infty$.
\ben
\item Let $\delta > 0$. For $N \geq 1$, let $Z^{(N)}_t$ denote the size of the population at time $t$, when the initial population size is $Z^{(N)}_0 = \floor{xN}$, where $x > \delta$. Let $\ve > 0$. Show that
\be
\pro\bb{\abs{Z^{(N)}_1 - xN} \geq \ve N} \leq 2e^{-\alpha xN}
\ee
for some $\alpha > 0$ which depends only on $\ve$ and $\delta$ (and not on $x$ or $N$).

\item Let $0 <\delta < x$, and let $T^{(N)}_\delta = \inf\bra{t \geq 0 : Z^{(N)}_t \leq \delta N}$. Show that
\be
\bb{\frac 1N Z^{(N)}_{\floor{tN}\land T^{(N)}_\delta}, t \geq 0} \to \bb{Z_{t\land T_\delta}, t \geq 0}
\ee
weakly, where $Z$ is the solution to the stochastic differential equation:
\be
dZ_t = \sqrt{\sigma^2Z_t} dB_t,\quad  Z_0 = x,
\ee
where $B$ is a one-dimensional Brownian motion, and $T_\delta = \inf\bra{t \geq 0 : Z_t \leq \delta}$. Why is the law of $(Z_{t\land T_\delta}, t \geq 0)$ unique?
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] Write $Z_1^{(N)} = \sum^{\floor{xN}}_{i=1}\xi_i$, $(\xi)_i$ i.i.d. $\sim (p_k)$ and use Chernoff bound, i.e. $\pro\bb{X>x}\leq \E\bb{e^{\theta X}}e^{-\theta x}$, $\theta >0$.

\item [(b)] We are going to apply theorem 6.5 by first checking that all the conditions for this Markov process hold.
\beast
b(x) & = & \E(Y-x) = \E\bb{\sum^x_{i=1}\xi_i - x} = 0, \quad \text{since }\E\xi_i = 1.\\
a(x) & = & \E\bb{(Y-x)^2} = \var\bb{\sum^x_{i=1}\xi_i} = x\sigma^2,\quad \text{since $\xi_i$ are independent.}
\eeast

So it is easy to check that conditions i, ii, iii of the theorem hold, hence
\be
\bb{\frac 1N Z^{(N)}_{\floor{nt}\land T_\delta},t\geq 0} \stackrel{w}{N\to \infty} \bb{Z_{t\land T_\delta},t\geq 0},
\ee
where $Z$ is the solution to the SDE
\be
dZ_t = \sqrt{a(Z_t)} dB_t  \ \ra \ dZ_t = \sqrt{\sigma^2 Z_t}dB_t
\ee
and $Z_0 = x$. Since the coefficients of the SDE are Lipschitz we obtain that there is pathwise uniqueness, and thus uniqueness in distribution.
\een
\end{solution}

\item Show that pathwise unique does not hold for the SDE
\be
dX_t = 3X^{1/3}_t dt + 3X^{2/3}_t dW_t,\quad X_0 = 0.
\ee

\begin{solution}[\bf Solution.]
Clearly, $X_t =0$, $\forall t\geq 0$ is a solution. To find a second solution, let $f(x) = x^{1/3}$. Then 
\be
df(X_t) = \frac 13 X_t^{-2/3} \bb{3X_t^{1/3} dt + 3X_t^{2/3} dW_t} + \frac 12 \bb{-\frac 29 X_t^{-5/3}}\bb{9X_t^{4/3}dt} = dW_t.
\ee

Thus,
\be
dX_t^{1/3} = dW_t  \ \ra \ X_t = W_t^3 \quad \text{ as }X_0 = 0.
\ee

So $X_t = 0$ $\forall t\geq 0$ and $X_t = W_t^3$ are both solutions. So pathwise uniqueness does not hold.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

FROM Arun

\item Let $X$ be a weak solution of the scalar SDE $dX_t = b(X_t)dt + \sigma(X_t)dW_t$. Let $V : [0, T] \times \R \to \R$ satisfy the PDE
\be
\fp{V}{t} + b(x)\fp{V}{x} + \frac{\sigma(x)^2}2 \frac{\partial^2 V}{\partial x^2} + g(x) = 0
\ee
with boundary condition $V(T, x) = f(x)$ for all $x$. Show that
\be
M_t = V (t,X_t) + \int^t_0 g(X_s)ds
\ee
is a local martingale. If $M$ is a true martingale, conclude that
\be
V (t, x) = \E\bb{\left.\int^T_t g(X_s)ds + f(X_T )\right|X_t = x}.
\ee

\begin{solution}[\bf Solution.]
By It\^o formula,
\beast
dV(t,X_t) & = & \fp{V}{t} dt + \fp{V}{x}dX_t + \frac 12\frac{\partial^2 V}{\partial x^2} \sigma(X_t)^2 dt \\
& = & \bb{\fp{V}{t} + \fp{V}{x}b(X_t) + \frac 12 \frac{\partial^2 V}{\partial x^2}\sigma(X_t)^2}dt + \fp{V}{x} \sigma(X_t)dW_t\\
& = & -g(X_t)dt + \fp{V}{x}\sigma(X_t)dW_t.
\eeast

Thus, $M_t \equiv V(t,X_t) + \int^t_0 g(X_s)ds$ is a local martingale. If $M$ is a true martingale then $M_t = \E\bb{M_T|\sF_t}$. Since $X$ is Markov,
\beast
V(t,X_t) + \int^t_0 g(X_s)ds & = & \E\bb{V(T,X_T)+\int^T_0 g(X_s)ds|\sF_t}\\
V(t,X_t) & = & \E\bb{f(X_T) + \int^T_t g(X_s)ds |\sF_t} = \E\bb{f(X_T) + \int^T_t g(X_s)ds |X_t}
\eeast

Thus,
\be
V(t,x) = \E\bb{f(X_T) + \int^T_t g(X_s)ds |X_t = x}.
\ee
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Find the unique strong solution of the SDE
\be
dX_t = \frac 12 X_t dt + \sqrt{1 + X^2_t} dW_t,\quad  X_0 = x:
\ee
[Hint: consider $Y_t = \sinh^{-1} X_t$.]

\begin{solution}[\bf Solution.]
Let $f(x) = \sinh^{-1}x$. Then
\be
f'(x) = \frac 1{\sqrt{1+ \sinh^2 f(x)}},\quad f''(x) = -\frac {\sinh f(x)}{\bb{1+ \sinh^2 f(x)}^{3/2}}.
\ee

By It\^o formula, 
\be
df(X_t) = \frac 1{\sqrt{1+X_t^2}} \bb{\frac 12 X_t dt + \sqrt{1+X_t^2}dW_t} - \frac 12 \bb{\frac{X_t}{(1+X_t^2)^{3/2}}}(1+X_t^2)dt = dW_t.
\ee

Thus, $\sinh^{-1}X_t = W_t + \sinh^{-1}x$ (as $X_0 = x$). Thus, $X_t = \sinh \bb{W_t+ \sinh^{-1}x}$ is a solution, which is clearly strong. We have strong uniqueness because the coefficients of the SDE are locally Lipschitz.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Let $W$ and $W^\perp$ be independent Brownian motions, and let 
\be
X_t = e^{-W_t} \bb{x + \int^t_0 e^{W_s} dW^\perp_s}
\ee

Show that there exists a Brownian motion $Z$ such that
\be
dX_t = \frac 12 X_tdt + \sqrt{1 + X^2_t} dZ_t,\quad  X_0 = x.
\ee

Use the previous problem to find the density function of the random variable $\int^t_0 e^{W_s}dW^perp_s$.

\begin{solution}[\bf Solution.]
By It\^o formula,
\beast
dX_t & = & e^{-W_t} e^{W_t}dW_t^\perp + \bb{x+ \int^t_0 e^{W_s}dW_s}\bb{-e^{-W_t}dW_t + \frac 12 e^{-W_t}dt} + d\bsb{e^{-W_t},x+\int^t_0 e^{W_s}dW_s^\perp}\\
& = & dW_t^\perp + X_te^{W_t}\bb{-e^{-W_t}dW_t + \frac 12 e^{-W_t}dt} = dW_t^\perp - X_tdW_t + \frac 12 X_t dt.
\eeast

Let
\be
dZ_t = \frac 1{\sqrt{1+X_t^2}} \bb{dW_t^\perp - X_t dW_t}
\ee
then by \levy's characterisation of BM, $Z$ is a BM, and we have 
\be
dX_t = \sqrt{1+X_t^2}dZ_t + \frac 12 X_t dt
\ee
as required.

Finally, to find the density of $\int^t_0 e^{W_s}dW_s^\perp$, observe that
\be
\pro(X_t<0) = \pro\bb{\int^t_0 e^{W_s}dW_s^\perp < -x} = \pro\bb{Z_t < \sinh^{-1}x}
\ee
because we know that
\be
X_t = e^{-W_t}\bb{x+ \int^t_0 e^{W_s}dW_s^\perp} = \sinh\bb{Z_t + \sinh^{-1}x}
\ee
from Problem 3. Now, replacing $x$ by $-x$ gives
\be
\pro\bb{\int^t_0 e^{W_s}dW_s^\perp < x} = \pro\bb{Z_t < \sinh^{-1}x}.
\ee

The density of $\int^t_0 e^{W_s}dW_s^\perp$,
\be
f(x) = \frac{d}{dx}\pro\bb{Z_t < \sinh^{-1}x} = \frac 1{\sqrt{2\pi t}}\exp\bb{- \frac{(\sinh^{-1}x)^2}{2t}}\frac 1{\sqrt{1+x^2}}.
\ee
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Let $X$ be a weak solution of the SDE
\be
dX_t = \sin(\pi X_t)dW_t,\quad  X_0 = x \text{ where }0 < x < 1.
\ee
\ben
\item [(a)] Show that $0 < X_t < 1$ a.s for all $t \geq 0$. [Hint: Show that $\log X$ and $\log(1 - X)$ are well-defined.]
\item [(b)] Why must $(X_t)_{t\geq 0}$ converge a.s. to a random variable $X_\infty$?
\item [(c)] Show $\pro(X_\infty = 1) = x = 1 - \pro(X_\infty = 0)$.
\een

\begin{solution}[\bf Solution.]
\ben
\item [(a)] Consider $dZ_t = \frac 1{X_t}\sin(\pi X_t) dW_t$, $Z_0 = \log x$. This is well-defined as 
\be
\int^t_0 \frac{\sin^2(\pi X_t)}{X_t^2}dt < \pi^2 t < \infty
\ee
for all $t\geq 0$. So we can define $\sE(Z_t) = \exp\bb{Z_t - \frac 12 [Z]_t}$. But 
\be
dX_t = \sin (\pi X_t)dW_t = X_t \bb{\frac{\sin \pi X_t}{X_t}}dW_t = X_t dZ_t \ \ra \ X_t = \sE(Z_t) >0\text{ a.s.}
\ee

To show $X_t< 1$ a.s., observe that
\be
d(1-X_t) = -\sin \pi X_t dW_t = - \sin (\pi(1-X_t))dW_t .
\ee

Now let 
\be
dY_t = - \frac{\sin(\pi(1-X_t))}{1-X_t}dW_t,\quad Y_0 = \log(1-x).
\ee

Using the same argument as before, $Y$ is well-defined, and we have
\be
d(1-X_t) = (1-X_t)dY_t \ \ra \ (1-X_t) =\sE(Y_t) >0 \text{ a.s.}
\ee

So $0< X_t <1$ a.s. as required.

\item [(b)] $X$ is a bounded local martingale and thus a bouned true martingale. Then by maringale convergence theorem,
\be
X_t \to X_\infty \text{ in $\sL^1$ and a.s..}
\ee

\item [(c)] Claim: $X_\infty \in \bra{0,1}$ a.s.. Proof of claim: Suppose not, then on the set $X_\infty \notin \bra{0,1}$ (which has positive probability), $X_t \stackrel{\text{a.s.}}{\to} X_\infty \notin \bra{0,1}$. Thus, for $t$ large enough, $\sin(\pi X_t)$ is bounded away from 0
\be
[X]_t = \int^t_0 \sin^2(\pi X_t) dt \to \infty \text{ as }t\to \infty.
\ee

But by Dubin-Schwarz $X_t = B_{[X]_t}$ some BM, $B$ and $X_t$ converges as $t\to\infty$. Thus, $[X]_t < \infty$ a.s.. Contradiction. So we have our claim.

Lastly, by the optional sampling theorem, with $T=\infty$ ($X$ is UI)
\be
x = \E(X_0) = \E\bb{X_\infty} = \pro\bb{X_\infty = 1} = 1 - \pro\bb{X_\infty = 0}.
\ee

\een
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item (square-root diffusion) Let $W$ be an $n$-dimensional Brownian motion, and define an $n$-dimensional process $X$ to be the solution to the SDE
\be
dX_t = -X_tdt + dW_t
\ee
with $X_0 = x \in \R^n$. If $\R_t = \dabs{X_t}^2$, show that there exists a scalar Brownian motion $Z$ such that
\be
dR_t = (n - 2R_t)dt + 2\sqrt{R_t}dZ_t.
\ee

\begin{solution}[\bf Solution.]
By It\^o formula,
\beast
dR_t & = & \sum^n_{i=1} 2X_t^i dX_t^i + \sum^n_{i=1}dt = 2\bb{\sum^n_{i=1} X_t^i dW_t^i - (X_t^i)^2 dt} + ndt\\
& = & (n - 2R_t)dt + 2\sqrt{R_t}\bb{\frac 1{\sqrt{R_t}}\sum^n_{i=1} X_t^i dW_t^i}.
\eeast

Letting $dZ_t = \frac 1{\sqrt{R_t}} \sum^n_{i=1} X_t^i dW_t^i$, we see that $Z$ is a BM by Levy,
\be
dR_t = (n- 2R_t)dt + 2\sqrt{R_t}dZ_t
\ee
as required.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Compute formally the invariant density of the scalar SDE
\be
dX_t = b(X_t)dt + \sigma(X_t)dW_t.
\ee
Apply your formula to the process $R$ in the previous question.

\begin{solution}[\bf Solution.]
To find the invariant density we need to solve
\be
\fp{}{y}(bp) = \frac 12\frac{\partial^2}{\partial y^2}(\sigma^2 p).
\ee

First note that to derive the Fokker-Plank equation we need to assume that
\be
\left.\ba{l}
\lim_{\abs{x}\to \infty} b(x,t)p\bb{x,t|x_0,t_0} = 0\\
\lim_{\abs{x}\to \infty} \sigma^2(x,t)p\bb{x,t|x_0,t_0} = 0\\
\lim_{\abs{x}\to \infty} \fp{}{x}\bb{\sigma^2(x,t)p\bb{x,t|x_0,t_0}} = 0
\ea\right\} \quad (*)
\ee
so that the boundary terms vanish when we do the integration by parts. Thus,
\be
\fp{}{y}(bp) = \frac 12 \frac{\partial^2}{\partial y^2}(\sigma^2 p) \ \ra \ 2bp = \fp{}{y}(\sigma^2 p)
\ee
constant of integration is 0 by ($*$). Thus,
\beast
\sigma^2 \fp{p}{y} = (2b- 2\sigma \fp{\sigma}{y})p \ \ra \ \int \frac{dp}{p} = \int^x_{-\infty} \frac 1{\sigma^2}\bb{2b - 2\sigma \fp{\sigma}{y}}dy \ \ra \ \log p(x) = \int^x_{-\infty} \frac 1{\sigma^2}\bb{2b - 2\sigma \fp{\sigma}{y}}dy.
\eeast

Thus,
\be
p(x) \propto \exp \bb{ \int^x_{-\infty} \frac 1{\sigma^2}\bb{2b - 2\sigma \fp{\sigma}{y}}dy}.
\ee

So now for $dR_t = (n-2R_t)dt + 2\sqrt{R_t}dZ_t$ (as in Q6) we have
\be
b(x) = n - 2R_t,\ \sigma(x) = 2\sqrt{x} \ \ra \ \fp{\sigma}{x} = \frac 1{\sqrt{x}}.
\ee

\beast
p(x) & \propto & \exp\bb{\int^x \frac 1{4y}(2n- 4y -4)dy} \propto \exp\bb{\int^x \bb{\frac n{2y} - \frac 1y -1}dy} \\
& \propto & \exp\bb{\frac n2 \log x - \log x - x} \propto x^{n/2-1}e^{-x}.
\eeast
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Let $W$ be a three-dimensional Brownian motion, and $e = (1, 0, 0)$. Let
\be
X_t = \frac 1{\dabs{W_t + e}}.
\ee

Show that there exists a scalar Brownian motion $Z$ such that
\be
dX_t = X^2_t dZ_t,\quad X_0 = 1.
\ee

Verify that both $V^1(t, x) = x \bb{\Phi\bb{\frac 1{x\sqrt{T-t}}} - 1}$ and $V^2(t, x) = x$ solve the PDE
\be
\fp{V}{t} + \frac{x^4}2 \frac{\partial^2 V}{\partial x^2} = 0,\quad  V (T, x) = x
\ee

Which solution corresponds to $V (t, x) = \E\bb{X_T |X_t = x}$?

\begin{solution}[\bf Solution.]
$X_t = \frac 1{\dabs{W_t + e}}$. By It\^o formula,
\be
dX_t = - \frac 1{\dabs{W_t + e}^3}(W_t+e)dW_t = X_t^2 dZ_t
\ee
where
\be
dZ_t = - \frac{W_t + e}{\dabs{W_t +e}} dW_t
\ee
and by \levy's characterisation theorem, $Z$ is a BM. It is easy (if tedious) to check that both 
\be
V^1(t,x) = x\bb{2\Phi\bb{\frac 1{x\sqrt{T-t}}}-1},\quad V^2(t,x) = x.
\ee
solve
\be
\fp{V}{t} + \frac{x^4}2 \frac{\partial^2 V}{\partial x^2} = 0,\quad V(T,x) = x.
\ee

From Problem 2, example sheet 3, we know that
\be
\E(X_t) = 2\Phi\bb{\frac 1{\sqrt{t}}} -1. 
\ee

Thus, as $X$ is Markov,
\beast
\E\bb{X_T|X_t = x} & = & \E\bb{X_{T-t}|X_0 =x} = \E\bb{\frac 1{\dabs{W_{T-t}+\frac 1x e}}} = x\E\bb{\frac 1{\dabs{xW_{T-t} + e }}}\\
& = & x\E\bb{\frac 1{\dabs{W_{x^2(T-t)} + e }}} = x\bb{2\Phi\bb{\frac 1{x\sqrt{T-t}}}-1}.
\eeast

So $V^1$ corresponds to $V(t,x) = \E\bb{X_T|X_t = x}$. We can see that it can't have been $V^2$ because $V^2(t,X_t) = X_t$ is a strict local martingale. But 
\be
V(t,X_t ) = \E\bb{X_T|X_t} = \E\bb{X_T|\sF_t}
\ee
(as $X$ is Markov) is a true martingale.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Let $X$ be a scalar, continuous, adapted process with $X_0 = 0$ and such that
\be
f(X_t) - \frac 12 \int^t_0 f''(X_s)ds
\ee
is a local martingale for each twice continuously differentiable $f$. Show that $X$ is a Brownian motion.

\begin{solution}[\bf Solution.]
Take $f(x) = x$. $M_t = X_t$ is a local maringale, $X$ is a continuous local martingale. Now take $f(x) =x^2$,
\be
N_t = X_t^2 - \frac 12 \int^t_0 2ds = X_t^2 -t 
\ee
is a local martingale. Thus, $[X]_t = t$ a.s.. So by \levy, $X$ is a BM. Alternatively, 
\be
dX_t^2 = 2X_t dX_t + d[X]_t \ \ra \ X_t^2 = \int^t_0 2X_s dX_s + [X]_t \ \ra \ N_t = \int^t_0 2X_s dX_s + [X]_t - t.
\ee
(the first term is local martingale). By uniqueness of the Doob-Meyer decomposition, $[X]_t = t$ a.s.. Then conclude by \levy.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item (Brownian bridge) Let $W$ be a standard Brownian motion.
\ben
\item [(a)] Let $B_t = W_t - tW_1$. This is called a Brownian bridge. Can you see why? Show that $(B_t)_{t\in[0,1]}$ is a continuous, mean-zero Gaussian process. What is the covariance $c(s, t) = \E(B_sB_t)$?
\item [(b)] Is $B$ adapted to the filtration generated by $W$?
\item [(c)] Let 
\be
dX_t = - \frac{X_t}{1 - t}dt + dW_t,\quad X_0 = 0.
\ee

Verify that $X_t = (1 - t) \int^t_0 \frac{dW_s}{1-s}$ for $0 \leq t < 1$. Show $X_t \to 0$ as $t \ua 1$. [Hint: show that there
exists a Brownian motion $Z$ such that $\int^t_0 \frac{dW_s}{1-s} = Z_{t/(1-t)}$ and apply the Brownian strong law of large numbers.]

\item [(d)] Show that $X$ is a continuous, mean-zero, Gaussian process with the same covariance as $B$, i.e. $X$ is a Brownian bridge.
\een

\begin{solution}[\bf Solution.]\ben
\item [(a)] $B_t = W_t - tW_1$ is a composition of continuous functions so is continuous $\E\bb{W_t - tW_1} = \E W_t - t\E W_1 = 0$. So $B_t$ has mean zero. $B$ is a Gaussian process because $W$ is a Gaussian process.
\beast
c(s,t) & = & \E\bb{B_sB_t} = \E\bb{(W_s- sW_1)(W_t - tW_1)} = \E\bb{W_sW_t + stW_1^2 - sW_1W_t - tW_1W_s}\\
& = & s\land t + st - st - ts = (s\land t)(1- s\vee t).
\eeast

\item [(b)] $B$ is not adapted to the filtration generated by $W$ because $B_t = f(W_t,t,W_1)$ and $W_1$ is not $\sigma(W_t)$ measurable for $t<1$.

\item [(c)] $dX_t = - \frac{X_t}{1-t} + dW_t$, $X_0 = 0$. Let $Y_t = \frac{X_t}{1-t}$. By It\^o formula (applied to the function $f(x,t) = \frac x{1-t}$),
\be
dY_t = \frac{X_t}{(1-t)^2}dt + \frac 1{1-t}\bb{-\frac{X_t}{1-t} dt + dW_t} = \frac{dW_t}{1-t} .
\ee

Thus,
\be
\frac{X_t}{1-t} = \int^t_0 \frac{W_s}{1-s}  \ \ra \ X_t = (1-t)\int^t_0 \frac{W_s}{1-s} 
\ee
as required. Let 
\be
M_t = \int^t_0 \frac{W_s}{1-s}  \ \ra \ [M]_t = \int^t_0 \frac{1}{(1-t)^2} ds = \frac t{1-t}.
\ee

Also, $M$ is a continuous local martingale, so by Dubin-Schwarz, 
\be
M_t = Z_{[M]_t} = Z_{\frac t{1-t}} 
\ee
for some BM, $Z$. Thus, 
\be
X_t = (1-t)Z_{\frac t{1-t}} \stackrel{d}{=} Z_{\frac t{1-t} (1-t)^2} = Z_{t(1-t)}.
\ee

Thus,
\be
\pro\bb{X_t \to 0\text{ as }t\ua 1} = \pro\bb{Z_{t(1-t)}\to 0 \text{ as }t\ua 1} = 1
\ee
as $Z$ is continuous and $Z_0 = 0$ a.s. so $X_t \to 0$ a.s. as $t\ua 1$.

Alternatively, we can argue that
\be
X_t = (1-t) Z_{\frac t{1-t}} = \frac{Z_s}{1+s} = \bb{\frac s{1+s}}\frac {Z_s}s
\ee
where $s= \frac t{1-t}$, $t= \frac s{1+s}$. $t\ua 1$, $s\ua \infty$ implies that $\bb{\frac s{1+s}}\frac{Z_s}s \to 0$ because $\frac{s}{1+s} \to 1$ and $\frac{Z_s}{s} \to 0$ by the Brownian Strong law of Large numbers. So $X_t \to 0$ as $t\ua 1$ as required.

\item [(d)] $X$ is clearly continuous,
\be
\E\bb{X_t} = (1-t)\E\bb{Z_{\frac t{1-t}}} = 0.
\ee

It is a Gaussian process because $Z$ is a Gaussian process
\beast
\E\bb{X_s X_t} & = & (1-t)(1-s)\E\bb{Z_{\frac s{1-s}}Z_{\frac t{1-t}}}  = (1-t)(1-s)\bb{\bb{\frac s{1-s}}\land \bb{\frac t{1-t}}}\\
& = & s\land t (1-s\vee t) = \E\bb{B_s B_t}
\eeast
as required.
\een

\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Let $\int X \circ dY$ denote the Stratonovich integral, as defined by problem 5, example sheet 3. Show that the scalar SDE in It\^o form 
\be
dX_t = b(X_t)dt + \sigma(X_t)dW_t
\ee
when written in Stratonovich form becomes
\be
dX_t = \mu(X_t)dt + \sigma(X_t) \circ dW_t
\ee
where $\mu(x) = b(x) - \frac 12\sigma(x)\sigma'(x)$.

\begin{solution}[\bf Solution.]
By definition, $X_t \circ dY_t = X_t dY_t + \frac 12 d[X,Y]_t$. So,
\be
dX_t = b(X_t)dt + \sigma(X_t)dW_t =  b(X_t)dt + \sigma(X_t)\circ dW_t - \frac 12 d\bsb{\sigma(X_t),W_t}.
\ee

\be
d\bsb{\sigma(X),W}_t = d\bsb{\int^t_0 \sigma'(X_s) + \frac 12 \int^t_0 \sigma''(X_s)d[X]_s, W_t} = d\bsb{\int^t_0 \sigma'(X_s)\sigma(X_s)dW_s, W_t} = \sigma'(X_t)\sigma(X_t)dt.
\ee

\be
dX_t = b(X_t)dt + \sigma(X_t)\circ dW_t - \frac 12 \sigma'(X_t)\sigma(X_t)dt = \mu(X_t )dt + \sigma(X_t)\circ dW_t
\ee
where $\mu(x) = b(x) - \frac 12 \sigma(x)\sigma'(x)$.
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Let $\sigma$ be smooth and bounded away from 0, and let $\Phi : \R\times \R \to \R$ solve the family of ODEs
\be
\fp{\Phi}{s} (s, y) = \sigma(\Phi(s, y)),\quad \Phi(0, y) = y
\ee

\ben
\item [(a)] Show that $X_t = \Phi(W_t, x)$ is a strong solution of the Stratonovich SDE
\be
dX_t = \sigma(X_t) \circ dW_t,\quad  X_0 = x.
\ee

\item [(b)] Verify that
\be
\fp{\Phi}{y} (t, y) = \frac{\sigma(\Phi(t, y))}{\sigma(y)}.
\ee

\item [(c)] Let $\mu$ be smooth, and for each $\omega$, let $Y (\omega)$ solve the random ODE
\be
\frac{dY_t}{dt} = \frac{\mu (\Phi(W_t, Y_t))\sigma(Y_t)}{\sigma(\Phi(W_t, Y_t))},\quad  Y_0 = x.
\ee

Show that $X_t = \Phi(W_t, Y_t)$ is a strong solution of the Stratonovich SDE
\be
dX_t = \mu(X_t)dt + \sigma(X_t) \circ dW_t,\quad X_0 = x.
\ee
\een

\begin{solution}[\bf Solution.]\ben
\item [(a)]
\be
d\Phi(W_t,x) = \fp{\Phi}{s}(W_t ,x) \circ dW_t = \sigma(\Phi(W_t ,x))\circ dW_t.
\ee

Thus, $X_t = \Phi(W_t,x)$ is a strong solution of 
\be
dX_t = \sigma(X_t) \circ dW_t, \quad X_0 = x.
\ee
\item [(b)] 
\be
\frac{\partial \Phi(s,y)}{\partial y\partial s} = \sigma'(\Phi(s,y)) \fp{}{y}\Phi(s,y).
\ee

Let $\Gamma_s = \fp{\Phi(s,y)}{y}$, then $\fp{\Gamma_s}{s} = \sigma'(\Phi(s,y))\Gamma_s$. Thus,
\be
\int^t_0 \frac{d\Gamma_s}{\Gamma_s} = \int^t_0 \sigma'(\Phi(s,y))ds = \int^{\Phi(t,y)}_y \sigma'(u) \frac{du}{\sigma(u)}
\ee
where $u = \Phi(s,y)$, $du = \fp{\Phi}{s} = \sigma(u)ds$. Then
\be
\log \Gamma_t = \log \bb{\frac{\sigma(\Phi(t,y))}{\sigma(\Phi(0,y))}} \ \ra \ \fp{\Phi(t,y)}{y} = \frac{\sigma(\Phi(t,y))}{\sigma(y)}
\ee
as required.


\item [(c)] 
\be
\frac{dY_t}{dt} = \frac{\mu\bb{\Phi(W_t,Y_t)}\sigma(Y_t)}{\sigma(\Phi(W_t,Y_t))},\quad Y_0 = x.
\ee

Then
\be
\fp{\Phi}{y}(W_t,Y_t)dY_t = \mu(\Phi(W_t,Y_t))dt
\ee
using part (b). Let $X_t = \Phi(W_t,Y_t)$,
\beast
dX_t & = & \fp{\Phi(W_t,Y_t)}{t} dW_t + \fp{\Phi(W_t,Y_t)}{y} dY_t + \frac 12 \frac{\partial^2 \Phi}{\partial t^2} dt \\
& = & \sigma(X_t)dW_t + \mu\bb{\Phi(W_t,Y_t)}dt + \frac 12\sigma'\bb{\Phi(W_t,Y_t)} \fp{\Phi}{t}dt\\
& = & \sigma(X_t)dW_t + \mu(X_t)dt + \frac 12 \sigma'(X_t)\sigma(X_t)dt \\
& = & \mu(X_t)dt + \sigma(X_t)\circ dW_t.
\eeast

Thus, $X_t$ is a strong solution of the Stratonovich SDE.
\een
\end{solution}
\een


%[1] R. Durrett. Stochastic calculus. a practical introduction. CRC Press, Probability and Stochastics Series, 1996.
%[2] D. Revuz and M. Yor (1999). Continuous martingales and Brownian motion. 3rd edition. Springer, Grundlehren der mathematischen Wissenschaften, Vol. 293.
%[3] L.C.G. Rogers and D. Williams (1987). Diffusions, Markov Processes and Martingales, Volume 2. 2nd edition. Cambridge University Press.
%[4] D.W. Stroock and S.R.S. Varadhan (1997). Multidimensional Diffusion Processes. Springer, Classics in Mathematics. Reprint of Grundlehren der mathematischen Wissenschaften, Vol. 233.



