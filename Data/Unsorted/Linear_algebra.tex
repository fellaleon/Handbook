
\chapter{Linear Algebra}

\section{Matrix}

%\subsection{Adjugate Matrix}
%\begin{proposition}
%Let $A,B,I\in M_n(\F)$. if $\det A$ is a unit,
%\be
%\det\bb{\adj(A)} = \bb{\det A}^{n-1},\qquad \adj\bb{\adj A} = (\det A)^{n-2} A.
%\ee
%\end{proposition}

%\begin{proof}[\bf Proof]%By Theorem \ref{thm:adjugate_inverse_matrix}, we have $A\in M_n(\F)$ and $A$ is invertible,
%%\be(\adj A)A = (\det A)I,\qquad A^{-1} = \frac{1}{\det A} \adj A.\ee
%Since $I$ is invertible, $\adj I =  I^{-1}\det I = I \cdot 1 = I$ by Theorem \ref{thm:adjugate_inverse_matrix}.(ii).

%%By Theorem \ref{thm:adjugate_inverse_matrix}.(i), %be\det(AB) I = \bb{\adj (AB)} (AB)\ee\beast\adj\bb{AB}(AB) & = & \det(AB)I = \det A \det B I^2 = \det B I \det A = \bb{\adj B} B \det A \\& = & \bb{\adj B} \det A B =  \bb{\adj B} \bb{\det A} I B = \bb{\adj B}\bb{\adj A}(A B)\eeast

%For $i,j$, let $\adj A = (a'_{ij})$ and $\adj B = (b'_{ij})$. Thus, by Definition \ref{def:adjugate_matrix}, Theorem \ref{thm:determinant_product} (the product of determinants),
%\beast
%\bb{\adj B \adj A}_{ij} = \sum^n_{k=1} b'_{ik}a'_{kj} = \sum^n_{k=1} (-1)^{i+k} \det\bb{\wh{B}_{ki}} (-1)^{k+j} \det\bb{\wh{A}_{jk}} = (-1)^{i+j} \sum^n_{k=1} \det\bb{\wh{B}_{ki}} \det\bb{\wh{A}_{jk}}.%= (-1)^{i+j} \sum^n_{k=1} \det\bb{\wh{A}_{jk}\wh{B}_{ki}}.
%\eeast

%Thus, $\det\bb{\wh{A}_{jk}\wh{B}_{ki}} = (-1)^{j+k}\det \bb{A'_{jk}} (-1)^{i+k} \det\bb{B'_{ki}}$ where matrices $A'_{jk}$ and $B'_{ki}$ are
%\be
%\bb{A'_{jk}}_{pq} = \left\{\ba{ll}a_{pq}\quad\quad & p\neq j \\ 1 & p = j,\ q = k \\ 0 & \text{otherwise} \ea\right.,\qquad \bb{B'_{ki}}_{pq} = \left\{\ba{ll}b_{pq}\quad\quad & p\neq k,q\neq i \\ 1 & p = k,q = i\\ 0 & \text{otherwise}\ea\right.
%\ee

%by Theorem \ref{thm:determinant_product}
%Proposition \ref{pro:matrix_multiple_transpose}
%Definition \ref{def:volumn_form}
%Proposition \ref{pro:inverse_matrix_property}
%\end{proof}


\begin{theorem}[Jacobi's formula]
Let $A(t) \in M_n(\F,t)$. Then %The adjugate also appears in Jacobi's formula for the derivative of the determinant:
\be
\frac{d}{d t} \det (A(t)) = \tr \bb{\adj (A(t)) \ \frac{d A(t)}{d t}}. 
\ee
\end{theorem}

\begin{proof}[\bf Proof]
\footnote{need proof}
\end{proof}


\section{Endomorphisms, matrices, eigenvectors}

In this section, unless stated otherwise we assume $V$ is a vector space over $\F$, where $\F$ is $\R$ or $\C$, and $\alpha : V \to V$ is a linear map.

\begin{definition}
Let $\alpha \in \End(V)$. $\alpha$ is diagonalisable if there is a basis $B$ of $V$ such that $[\alpha]_B$ is diagonal, i.e. whenever $i \neq j$ then $a_{ij} = 0$. $\alpha$ is triangulisable if there is a basis $B$ of $V$ such that $[\alpha]_B$ is upper triangular, i.e. whenever $i > j$ then $a_{ij} = 0$.

A square matrix is diagonalisable (resp. triangulisable) if it is conjugate to a diagonal (resp. upper triangular) matrix.
\end{definition}

\begin{definition}
Let $\alpha \in \End(V )$. Then $\lm \in \F$ is an eigenvalue of $\alpha$ if there exists a vector $v \in V$ with $v \neq 0$ and $\alpha(v) = \lm v$. $v$ is then called an eigenvector corresponding to $\lm$.
\end{definition}

\begin{remark}
$\lm$ is an eigenvalue of $\alpha$ if and only if $\alpha - \lm\iota$ is singular, or $\det(\alpha - \lm\iota) = 0$, equivalently. In particular, $\lm = 0$ is an eigenvalue if and only if $\alpha$ is singular.
\end{remark}

\begin{definition}. The polynomial $\chi_{\alpha}(t) = \det(\alpha - t\iota)$ is the characteristic polynomial of $\alpha$. It is a polynomial of degree $n = \dim V$. If $A \in M_n(\F)$, $\chi_\alpha(t) = \det(A - tI)$.
\end{definition}

\begin{remark}
Eigenvalues of $\alpha$ are precisely the roots of the characteristic polynomial. For a matrix $A$, $\lm$ is an eigenvalue of $A$ if $\chi_A(\lm) = 0$ and $v \in \F^n$ is a corresponding eigenvector if $v \neq 0$ and $Av = \lm v$.
\end{remark}

\begin{lemma}
Conjugate matrices have the same characteristic polynomial and hence the same eigenvalues.
\end{lemma}

\begin{proof}[\bf Proof]
$\chi_{P^{-1}AP} (t) = \det(P^{-1}AP - tI) = \det P^{-1} \det(A - tI) \det P = \chi_A(t)$.
\end{proof}

\begin{remark}
The matrix $\bepm 1 & 1\\ 0 & 1\eepm$ is not diagonalisable. The only diagonal $2 \times 2$ matrix with eigenvalues 1, 1 is $I$, but this is self-conjugate.
\end{remark}

Recall that every polynomial of degree at least 1 over $\C$ has a root (and hence in fact $n$ roots, counting with multiplicities).

\begin{lemma}
If $V$ is a finite dimensional vector space over $\C$ and $\alpha \in End(V)$ then $\alpha$ has an eigenvector in $V$.
\end{lemma}

\begin{theorem}
Let $V$ be a finite dimensional vector space over $\C$ and let $\alpha \in End(V )$. There exists a basis $B$ of $V$ such that $[\alpha]_B$ is upper triangular. In other words, there exists a basis $B = \bra{v_1, \dots, v_n}$ with $\alpha(v_j) \in \bsa{v_1, \dots, v_j}$ for each $j = 1, \dots, n$.
\end{theorem}

\begin{proof}[\bf Proof]
We prove this by induction on $n$. It is clear if $n = 1$, so assume $n > 1$. Since $V$ is a vector space over $\C$, there exists $\lm \in \C$ such that $\alpha - \lm\iota$ is singular. Consider $U = \im(\alpha-\lm\iota) \lneq V$, where we know $U$ is a proper subspace by the rank-nullity theorem. Then $\alpha(U) \subseteq U$.
\be
\alpha(U) = \alpha((\alpha - \lm\iota)(V )) = (\alpha - \lm\iota)(\alpha V ) \leq  (\alpha - \lm\iota)(V ) = U.
\ee

Consider $\alpha' = \alpha|_U : U \to U$. This is a linear map and $\dim U < \dim V$. By induction, there exists a basis $B' = \bra{v_1, \dots, v_k}$ of $U$ with $A' = [\alpha']_{B'}$ upper triangular. Extend this to a basis $B = \bra{v_1, \dots, v_k, \dots, v_n}$ of $V$. We claim $[\alpha]_B$ is upper triangular. In fact,
\be
[\alpha]_B = \bepm A' & * \\ 0 & \lm I\eepm.
\ee

If $1 \leq  j \leq  k$, $\alpha(v_j) = \alpha'(v_j) \in U$, so the entries in the first $k$ columns are as claimed. If $j > k$, $(\alpha - \lm\iota)(v_j)\in U$ by definition of $U$, so $\alpha(v_j) = \lm v_j + u$ for some $u \in U$, so the first $k$ entries of column $j$ are the coordinates of $u$ with respect to $B'$ and the only other non-zero entry is $\lm$ in the position $(j, j)$.
\end{proof}

\begin{proof}[\bf Proof]
Let $v_1$ be an eigenvector of $\alpha$, say $\alpha(v_1) = \lm v_1$. Let $U$ be any complementary subspace to $\bsa{v_1}$ in $V$. For $v \in V$ we have $v = \lm_v v_1+u$ with $u \in U$ unique, $\lm_v \in \F$. Write $\pi(v) = u$, a projection from $V$ to $U$. For $u \in U$ define $\wt{\alpha} : U \to U$ by $\wt{\alpha}(u) = \pi(\alpha(u))$. Then $\wt{\alpha} \in End(U)$. By induction, there is a basis $v_2, \dots, v_n$ of $U$ with $\wt{\alpha}(v_j) \in \bsa{v_2, \dots, v_j}$ for $2 \leq  j \leq  n$. Then $\alpha(v_j) = \lm_{\alpha(v_j)} v_j + \wt{\alpha}(v_j) \in \bsa{v_1, \dots, v_j}$ and $\alpha(v_1) \in \bsa{v_1}$.
\end{proof}

\begin{theorem}
Every square matrix over $\C$ is conjugate to an upper triangular matrix.
\end{theorem}

\begin{remark}
This is not true over $\R$, e.g. rotations other than $\pm I$ on $\R^2$.
\end{remark}


\begin{theorem}
Let $V$ be a finite dimensional vector space over a field $\F$ and let $\alpha \in End(V)$. There exists a basis $B$ of $V$ such that $[\alpha]_B$ is upper triangular if and only if $\chi_\alpha$ factorises into linear factors, i.e. if and only all roots of $\chi_\alpha$ are in $\F$.
\end{theorem}

\begin{proof}[\bf Proof]
If
\be
[\alpha]_B = \bepm a_{11} & & * \\ & \ddots & \\ 0 & & a_{nn}\eepm 
\ee
then $\chi_\alpha(t) = (a_{11} - t) \dots (a_{nn} - t)$ with $a_{ij} \in\F$.

We prove the converse by induction on $\dim_\F V$. Let $\lm$ be an eigenvalue in $\F$, let $U = (\alpha - \lm\iota)(V)$. Then $\alpha(U) \leq U \lneq V$. Let $\alpha' = \alpha|_U \in End(U)$. Let $B'$ be any basis for $U$ and extend this to a basis $B$ of $V$. Then
\be
[\alpha]_B = \bepm [\alpha']_{B'} & * \\ 0 & \lm I\eepm.
\ee

Now $\chi_\alpha(t) = \chi_{\alpha'} (t)\chi_{\lm I} (t)$. It follows that all roots of $\chi_{\alpha'}$ lie in $\F$, so we can use induction. Replace $B'$ is necessary so that $[\alpha']_{B'}$ is upper triangular.
\end{proof}

\begin{remark}[See Examples Sheet 3]
Let $\alpha \in End(V)$ and asume $U \lneq V$ with $\alpha(U) \leq  U$. Let $B' = \bra{v_1, \dots, v_k}$ be a basis of $U$ and extend this to a basis $B = \bra{v_1, \dots, v_k, \dots, v_n}$ of $V$. Let $\ol{V} = V/U$, $\ol{v} = v +U$ for $v \in V$. Then $\ol{B} = \bra{\ol{v}_{k+1}, \dots, \ol{v}_n}$ is a basis of $\ol{V}$. We can define $\alpha' = \alpha|_U \in End(U)$ and $\ol{\alpha} : \ol{V} \to \ol{V}$, $\ol{v} \mapsto \alpha(v)$, a well-defined endomorphism of $\ol{V}$. Then
\be
[\alpha]_B = \bepm [\alpha']_{B'} & * \\ 0 & [\ol{\alpha}]_{\ol{B}}\eepm.
\ee

Note that also $\chi_\alpha = \chi_{\alpha'} \cdot \chi_{\ol{\alpha}}$.
\end{remark}

\begin{remark}
Let $V$ be a finite dimensional vector space over $\F$, $\alpha \in End(V)$ and $B$ a basis for $V$. Then $[\alpha]_B$ is diagonal if and only if $B$ consists of eigenvectors of $\alpha$.
\end{remark}

\begin{lemma}
If $\alpha \in End(V)$ and $\lm_1, \dots, \lm_k$ are distinct eigenvalues of $\alpha$, put $V_j = N(\alpha-\lm_j\iota)$, called the eigenspace of $\lm_j$. Then the sum $V_1+\dots +V_k$ is direct, so if $B_j$ is a basis for $V_j$ then $\bigcup^k_{j=1} B_j$ is a basis of $V_1+\dots +V_k$. In particular, if
$\sum^k_{j=1} \dim V_j = \dim V$ then $[\alpha]_B$ is diagonal and $V = V_1 \oplus \dots \oplus V_k$.
\end{lemma}

\begin{proof}[\bf Proof]
We need to show that if $v_1+\dots+v_k = 0$ with $v_j \in V_j$ then $v_j = 0$ for $j = 1, \dots, k$. Suppose not and let
\be
v+1 + \dots + v_j = 0
\ee
be the shortest non-trivial expression. Apply $\alpha$ and subtract $\lm_1$ times the above expression,
\be
\alpha(v_1) + \dots + \alpha(v_j) - \lm_1v_1 - \dots - \lm_1v_j = 0 \ \lra \ (\lm_2 - \lm_1)v_2 \dots + (\lm_j - \lm_1)v_j = 0,
\ee
which is a shorter non-trivial expression, contradiction. So $\sum V_j = \bigoplus V_j$. The rest follows by Lemma 1.15.
\end{proof}


\begin{theorem}
Let $V$ be a vector space over $\F$ of finite dimension. Then $\alpha \in End(V )$ is diagonalisable over $\F$ if and only if its minimal polynomial has distinct linear factors. 
\end{theorem}

\begin{proof}[\bf Proof]
If
\be
[\alpha]_B = \bepm \lm_1 & & 0\\ & \ddots & \\ 0 & & \lm_k \eepm
\ee
with $\lm_1, \dots, \lm_k$ distinct, put $p(t) = \prod^k_{j=1}(\lm_j - t)$. If $v \in B$ then $\alpha(v) = \lm_l v$ for some $l \leq  k$. So $(\lm_l\iota - \alpha)v = 0$, so $p(\alpha)(v) = 0$. But then $p(\alpha)$ is the 0 endomorphism since it is zero on $B$.

We need to show that each $v \in V$ is a sum of eigenvectors, the rest then follows from Lemma 4.6. Write $p(t) = \prod^k_{j=1}(\lm_j - t)$ with $\lm_1, \dots, \lm_k$ distinct. Put
\beast
p_j(t) & = & (\lm_1 - t) \dots (\lm_{j-1} - t)(\lm_{j+1} - t) \dots (\lm_k - t)\\
h_j(t) & = & \frac{p_j(t)}{p_j(\lm_j)}
\eeast
then $h_j(\lm_i) = \delta_{ij}$ for all $1 \leq  i$, $j \leq  k$. Hence $h(t) = \sum^k_{j=1} h_j(t) = 1$ as $h(t) - 1$ has degree less than $k$ and $\lm_1, \dots, \lm_k$ are $k$ distinct roots. Let $v \in V$. Then
\be
v = \iota(v) = h(\alpha)(v) = \sum^k_{j=1} h_j(\alpha)(v) = \sum^k_{j=1} v_j
\ee
where $v_j = h_j(\alpha)(v)$. Note that $(\alpha - \lm_j\iota)v_j = 0$ since $p(\alpha) = 0$, so $v_j$ is an eigenvector of $\alpha$ corresponding to $\lm_j$. Hence every $v \in V$ is a sum of eigenvectors, and the assertion follows from Lemma 4.6. (The $h_j(\alpha)$ are orthogonal projections, see Examples Sheet 2 Question 8. For another proof, see Examples Sheet 2 Question 13.)
\end{proof}

\begin{remark}
Let $A \in M_n(\F)$. Then $P^{-1}AP$ is diagonal for some $P$ if and only if $p(A) = 0$ for some polynomial $p \in \F[t]$ with distinct linear factors. To find $P$, consider
\be
P^{-1}AP = D = \bepm d_1 & & 0\\ & \ddots & \\ 0 & & d_n\eepm ,\quad\quad AP = PD,\quad\quad AP^{(j)} = d_jP^{(j)}
\ee
so the $j$th column of $P$ is an eigenvector of A corresponding to the eigenvalue $d_j$.
\end{remark}


\begin{theorem}[Simultaneous diagonalisation]
Let $\alpha_1, \alpha_2 \in End(V)$. If they are both diagonalisable and if $\alpha_1\alpha_2 = \alpha_2\alpha_1$ then they are simultaneously diagonalisable, i.e. there exists a basis $B$ of $V$ such that $[\alpha_1]_B$, $[\alpha_2]_B$ are diagonal.
\end{theorem}

\begin{proof}[\bf Proof]
We have $V = V_1\oplus\dots\oplus V_k$, where each $V_j$ is an eigenspace of $\alpha_1$, say, $\alpha_1(v_j) = \lm_jv_j$ for $v_j \in V_j$. Then $\alpha_2(V_j) \subseteq V_j$. If $v \in V_j$, $\alpha_1(\alpha_2(v)) = \alpha_2(\alpha_1(v)) = \alpha_2(\lm_jv) = \lm_j\alpha_2(v)$ so $\alpha_2(v) \in V_j$. Now $\alpha_2|_{V_j}$ is diagonalisable by Lemma 4.6, so there exists a basis $B_j$ of $V_j$ consisting of eigenvectors of $\alpha_2$, which are also eigenvectors of $\alpha_1$, of course. Putting these bases together gives a basis B consisting of eigenvectors of both $\alpha_1$ and $\alpha_2$.
\end{proof}

\begin{remark}
\ben
\item [(i)] The condition is necessary.
\item [(ii)] In fact, the statement is true for any number of commuting endomorphisms.
\een
\end{remark}

For a polynomial $p(t) \in\F[t]$,
\be
p(t) = a_nt^n + \dots + a_1t + a_0
\ee
where $a_i \in \F$ for $0 \leq  i \leq  n$. For $p(t), q(t) \in\F[t]$, addition and multiplication in $\F[t]$ are defined as follows.
\be
p(t) = a_nt^n + \dots + a_1t + a_0,\quad\quad q(t) = b_mt^m + \dots + b_1t + b_0.
\ee

Assuming $m \leq n$,
\beast
(p + q)(t) & = & a_nt^n + \dots + (a_m + b_m)t^m + \dots + (a_1 + b_1)t + (a_0 + b_0)\\
(pq)(t) & = & a_nb_mt^{n+m} + \dots + (a_1b_0 + a_0b_1)t + a_0b_0
\eeast

The degree $\deg p$ of a polynomial $p$ is the greatest $l$ with $a_l \neq 0$ (and it is -1 if $p$ is the 0 polynomial). Note that $\deg pq = \deg p + \deg q$.

There is a Euclidean algorithm in $\F[t]$. Given $a, b \in \F[t]$ with $b \neq 0$, there exists $q, r \in \F[t]$ such that $a = bq + r$ with $\deg r < \deg b$ or $r = 0$. For, if $a = a_nt^n + \dots + a_0$, $b = b_mt^m +\dots +b_0$ with $b_m \neq 0$, we may assume $n \geq m$ as otherwise we can take $q = 0$,
$r = a$. Replace $a$ by $a' = a- \frac{a_n}{b_m} t^{n-m} b$, then $\deg a' < \deg a$, so now we have $a' = bq' +r$ for some $q'$, $r$ with $\deg r < \deg b$. Now take $q = \frac{a_n}{b_m} t^{n-m} q'$ and keep repeating.

This has nice consequences, for example, $\F[t]$ has the unique factorisation property.

If $p \in \F[t]$ and $\lm \in \F$ is a root, so $p(\lm) = 0$, there exists $q \in \F[t]$ with $p(t) = (\lm - t)q(t)$. $\lm$ is a root of $p$ with multiplicity $e$ if $(\lm - t)^e$ divides $p$ but $(\lm - t)^{e+1}$ does not.

A polynomial of degree $n$ has at most $n$ roots counted with multiplicities.

If polynomials $p_1, p_2$ of degree less than $n$ have $n$ points in common then they are equal.

\begin{lemma}
\ben
\item [(i)] If $p, q \in \F[t]$, $\alpha \in End(V )$ then $p(\alpha)q(\alpha) = q(\alpha)p(\alpha)$.
\item [(ii)] If $\alpha(v) = \lm v$, $p \in \F[t]$ then $p(\alpha)(v) = p(\lm)v$.
\een
\end{lemma}


\begin{lemma}
Let $V$ be a finite dimensional vector space over $\F$ with $\dim V = n$ and let $\alpha \in End(V )$. There exists a non-zero polynomial $p$ of degree at most $n^2$ with $p(\alpha) = 0$.
\end{lemma}

\begin{proof}[\bf Proof]
We have $\dim End(V ) = n^2$, so there exist $a_{n^2}, \dots, a_1, a_0 \in \F$ not all zero so that
\be
a_{n^2} \alpha^{n^2} + a_{n^2-1}\alpha^{n^2-1} + \dots + a_1\alpha + a_0\iota = 0
\ee
as any $n^2+1$ endomorphisms are linearly dependent. Put $p(t) = a_{n^2} t^{n^2} +\dots +a_1t+a_0$.
\end{proof}

\begin{definition}
Let $\alpha \in End(V )$. The minimal polynomial $m_\alpha$ of $\alpha$ is the monic polynomial of minimal degree such that $m_\alpha(\alpha) = 0$.
\end{definition}

\begin{lemma}
If $\alpha \in End(V )$ and $p \in \F[t]$ with $p(\alpha) = 0$, then $m_\alpha$ divides $p$.
\end{lemma}

\begin{proof}[\bf Proof]
$\F[t]$ is a Euclidean domain so we can write $p = m_\alpha q + r$ with $q, r \in \F[t]$ and $\deg r < \deg m_\alpha$ or $r = 0$. But $p(\alpha) = 0 = m_\alpha(\alpha)$ so $r(\alpha) = 0$, so $r = 0$ by the minimality of $\deg m_\alpha$.
\end{proof}

\begin{corollary}
The minimal polynomial is unique.
\end{corollary}

\begin{theorem}[Caley-Hamilton]
Let $V$ be a vector space of finite dimension, let $\alpha \in End(V )$. Then $\chi_\alpha(\alpha) = 0$.
\end{theorem}

\begin{theorem}
Let $A \in M_n(\F)$. Then $\chi_A(A) = 0$.
\end{theorem}

\begin{proof}[\bf Proof]
Let $A \in M_n(\F)$. Then
\be
(-1)^n\chi_A(t) = t^n + a_{n-1} t^{n-1} + \dots + a_1t + a_0 = \det(tI - A)
\ee

Now for any matrix $B$ and its adjugate, $B \cdot \adj B = (\det B)I$. Hence as $\adj(tI -A)$ is a matrix of polynomials with matrix coefficients of degree less than $n$,
\beast
(tI - A)(B_{n-1}t^{n-1} + \dots + B_1t + B_0) & = & (tI - A) \adj(tI - A) = (t^n + a_{n-1}t^{n-1} + \dots + a_1t + a_0)I\\
& = & It^n + a_{n-1} It^{n-1} + \dots + a_1It + a_0I
\eeast

Comparing coefficients,
\beast
I & = & B_{n-1}\\
a_{n-1}I & = & B_{n-2} - AB_{n-1}\\
& \vdots & \\
a_1I & = & B_0 - AB_1\\
a_0I & = & -AB_0
\eeast

Pre-multiply the $j$th row by $A^{n+1-j}$ and add all rows,
\be
A^n + a_{n-1}A^{n-1} + \dots a_1A + a_0I = 0,
\ee
the zero matrix.
\end{proof}

\begin{proof}[\bf Proof (over $\C$)]
Let $\alpha \in End(V )$, let $B = \bra{v_1, \dots, v_n}$ be a basis of $V$ with $\alpha(v_j) \in \bsa{v_1, \dots, v_j} = U_j$. Hence
\be
[\alpha]_B = \bepm \lm_1 & & * \\ & \ddots & \\ 0 & & \lm_n\eepm,\quad\quad \chi_\alpha(t) = (\lm_1 - t) \dots (\lm_n - t)
\ee

Then $(\alpha - \lm_j\iota)U_j \subseteq U_{j-i}$. Hence
\be
(\alpha - \lm_1\iota) \dots (\alpha - \lm_{n-1}\iota)(\alpha - \lm_n\iota)V \subseteq (\alpha - \lm_1\iota) \dots (\alpha - \lm_{n-1}\iota)U_{n-1} \subseteq\dots \subseteq(\alpha - \lm_1\iota)U_1 = 0.
\ee
\end{proof}

\begin{corollary}
$m_\alpha$ divides $\chi_\alpha$.
\end{corollary}

\begin{lemma}
Let $V$ be a vector space over $\C$ with $\dim V = n$. We have
\be
\chi_\alpha(t) = \prod^k_{j=1} (t - \lm_j)^{a_j}
\ee
with $\lm_1, \dots, \lm_k$ all distinct eigenvalues of $\alpha$. Here $a_j$ is the algebraic multiplicity of $\lm_j$. Then $\sum^k_{j=1} a_j = n$.
\end{lemma}

\begin{lemma}
$m_\alpha(t) = \prod^k_{j=1}(t - \lm_j)^{e_j}$ for some $e_j$ with $1 \leq  e_j \leq  a_j$ for $1 \leq  j \leq  k$.
\end{lemma}

\begin{proof}[\bf Proof]
$m_\alpha$ divides $\chi_\alpha$, so $e_j \leq a_j$ for $1 \leq  j \leq  k$. If $\lm$ is an eigenvalue, $\alpha(v) = \lm v$ with $v \neq 0$, then $0 = m_\alpha(\alpha)v = m_\alpha(\lm)v$, and as $v \neq 0$ we have $m_\alpha(\lm) = 0$, so $(t - \lm)$ divides $m_\alpha(t)$.
\end{proof}

\begin{theorem}
Let $V$ be a vector space over $\F$ of finite dimension, let $\alpha \in End(V )$. Suppose $\alpha$ has distinct eigenvalues $\lm_1, \dots, \lm_k$. Then $\alpha$ is diagonalisable if and only if $m_\alpha(t) = \sum^k_{j=1} (t - \lm_j)$.
\end{theorem}

\begin{proof}[\bf Proof]
$\alpha$ is diagonalisable if and only if $p(\alpha) = 0$ for some polynomial with distinct linear factors if and only if $m_\alpha$ is the product of distinct linear factors if and only if $m_\alpha(t) = \sum^k_{j=1}(t - \lm_j)$.
\end{proof}

\begin{lemma}
Let $V$ be a finite dimensional vector space, $a \in End(V)$ and suppose $\lm_1, \dots, \lm_k$ are the distinct eigenvalues of $\alpha$. Then the $\lm_j$-eigenspace is $N(\alpha-\lm_j\iota)$. Define the geometric multiplicity of $\lm_j$ to be $g_j = \dim N(\alpha - \lm_j\iota)$. Then $1 \leq  g_j \leq  a_j$. (Note $\alpha$ is diagonalisable if and only if $a_j = g_j$ for all $1 \leq  j \leq  k$.
\end{lemma}

\begin{proof}[\bf Proof]
Since $\lm_j$ is an eigenvalue we have $1 \leq  g_j$. Let $B$ be a basis containing $v_1, \dots, v_{g_j}$ a basis of $N(\alpha - \lm_j\iota)$. Then
\be
[\alpha]_B = \bepm \lm_jI_{g_j} & *\\ 0 & A'\eepm
\ee
so $\chi_\alpha(t) = (\lm_j - t)^{g_j}\chi_{A'}(t)$, so $g_j \leq  a_j$.
\end{proof}

\begin{remark}
If $\chi_A(t) = (-1)^nt^n + a_{n-1}t^{n-1} + \dots + a_0$ then $a_0 = \det A$ and $a_{n-1} = (-1)^{n-1} \tr A$.
\end{remark}

Consider a finite dimensional vector space $V$ over $\C$ and linear maps in $End(V )$. We have seen the diagonal form, but not all matrices are conjugate to such, and the triangular form, but this is not quite sparse enough, i.e. it is not visible whether two matrices in this form are conjugate. We now describe the Jordan Normal Form, which contains eigenvalues along the diagonal, only the entries 0 or 1 just above the diagonal and entries 0 elsewhere.

Define a Jordan block $J(s, \lm)$ as follows:
\be
J(s, \lm) = \bepm \lm & 1 & &  0\\ & \ddots & \ddots & \\ & & \ddots & 1\\ 0 & & & \lm\eepm_{s\times s}.
\ee

Then the characteristic polynomial of $J(s, \lm)$ is $(\lm - t)^s$, the minimal polynomial is $(\lm - t)^s$ and the dimension of the $\lm$-eigenspace is 1.

\begin{theorem}
Suppose $V$ is a vector space over $\C$, let $\alpha \in End(V )$. With resepect to some basis $B$, the matrix $A = [\alpha]_B$ is in JNF, it is of block diagonal form as follows.
\be
A = \bepm B_1 & & 0\\ & \ddots & \\ 0 & &  B_k\eepm
\ee

There is one $a_j \times a_j$ block $B_j$ for each eigenvalue $\lm_j$ where $1 \leq  j \leq  k$. Now fix $\lm = \lm_j$. The corresponding block $B_j$ has block diagonal form
\be
B_j = \bepm C_1 & & 0 \\ & \ddots & \\ 0 & & C_m\eepm 
\ee
where $m = m_j$ and each $C_l = J(n_l, \lm)$ with
\be
n_1 \geq n_2 \geq \dots n_m > 0,\quad \quad n_1 + n_2 + \dots + n_m = 0
\ee
a partition of $a_j$.
\end{theorem}

\begin{remark}
$a_j = \sum^{m_j}_{i=1} n_i$, $g_j = m_j$, $e_j = n_1$.
\end{remark}

\begin{theorem}
Every square matrix over $\C$ is conjugate to a matrix in JNF, and this is unique up to rearranging the $\lm_1, \dots, \lm_k$.
\end{theorem}

\begin{proof}[\bf Proof]
See Groups, Rings \& Modules.
\end{proof}

Firstly, we break up $V$ into generalised eigenspaces $W_j = N(\alpha - \lm_j\iota)^{a_j}$. Then $V = \bigoplus^k_{j=1} W_j$ and taking a basis $B = \bigcup^k_{j=1} B_j$, where $B_j$ is a basis for $W_j$, we obtain
\be
[\alpha]_B = \bepm B_1 & & 0\\ & \ddots & \\ 0 & & Bk \eepm.
\ee

Setting $p_j(t) = (\lm_j-t)^{-a_j} \prod^k_{r=1}(\lm_r-t)^{a_r}$, there exist polynomials $q_j$ with $\sum^k_{j=1} p_jq_j = 1$ and then $W_j = \im(h_j(\alpha))$.

Secondly, note that $\alpha(W_j) \subseteq W_j$, so we may restrict to $W_j$ and $\alpha|_{W_j} \in End(W_j)$. Writing $\lm = \lm_j$, $V = W_j$ and $n = a_j$ , we have $(\alpha - \lm\iota)^n = 0$, so $\alpha - \lm\iota$ is a nilpotent endomorphism. Now break $V$ into cyclic blocks to obtain
\be
\bepm \lm & & & 0\\ 1 & \ddots & & \\ & \ddots & \ddots & \\ 0 & & 1 & \lm\eepm.
\ee

The final vector of the basis is a $\lm$-eigenvector. In fact, if $v_1, \dots, v_m$ is the corresponding part of the basis, then under $\alpha-\lm\iota$, $v_1 \mapsto v_2 \mapsto \dots \mapsto v_m \mapsto 0$. (To obtain the transpose form of the Jordan block, take these vectors in the order order.)

\begin{example}
We list all possible Jordan Normal Forms in the case of $n = 3$ along with their respective characteristic and minimal polynomials.
\be
\ba{llll}
& \bepm 
\lm_1 & & \\ 
& \lm_2 & \\
& & \lm_3 
\eepm
&
\bepm
\lm_1 & & \\
& \lm_2 & 1\\
& & \lm_2
\eepm
&
\bepm
\lm_1 & & \\
& \lm_2 & \\
& & \lm_2
\eepm
\\
\text{char.} & (\lm_1 - t)(\lm_2 - t)(\lm_3 - t) \qquad\qquad & (\lm_1 - t)(\lm_2 - t)^2 \qquad\qquad\qquad & (\lm_1 - t)(\lm_2 - t)^2\\
\text{min.} & (\lm_1 - t)(\lm_2 - t)(\lm_3 - t) \qquad\qquad & (\lm_1 - t)(\lm_2 - t)^2 \qquad\qquad\qquad & (\lm_1 - t)(\lm_2 - t) \\
& & & \\
& \bepm 
\lm & & \\
& \lm & \\
& & \lm
\eepm 
&
\bepm
\lm & 1 & \\
& \lm & \\
& & \lm
\eepm
&
\bepm
\lm & 1 & \\
& \lm & 1 \\
& & \lm
\eepm
\\
\text{char.} & (\lm - t)^3 \qquad & (\lm - t)^3 \qquad & (\lm - t)^3\\
\text{min.} & \lm - t \qquad & (\lm - t)^2 \qquad & (\lm - t)^3 
\ea
\ee
\end{example}

\begin{example}
In the case of $n = 4$, consider matrices with characteristic polynomial $(\lm-t)^4$. The partitions of 4 are 4 = 3+1 = 2+2 = 2+1+1 = 1+1+1+1. Hence the following matrices are possible.

\be
\ba{llll}
& \bepm 
\lm & 1 & & \\ 
& \lm & 1 & \\
& & \lm & 1 \\
& & & \lm 
\eepm
&
\bepm
\lm & 1 & & \\
& \lm & 1 & \\
& & \lm & \\
& & & \lm 
\eepm
&
\bepm
\lm & 1 & & \\
& \lm & & \\
& & \lm & 1\\
& & & \lm
\eepm
\\
\text{min.} & (\lm - t)^4 \qquad\qquad & (\lm - t)^3 \qquad\qquad\qquad & (\lm - t)^2 \\
& & & \\
& \bepm 
\lm & 1 & & \\
& \lm & & \\
& & \lm &\\
& & & \lm
\eepm 
&
\bepm
\lm & & & \\
& \lm & & \\
& & \lm & \\
& & & \lm
\eepm
& \\
\text{min.} & (\lm - t)^2 \qquad & \lm - t \qquad & 
\ea
\ee

In fact, $n((\alpha - \lm\iota)^r)$ for various $r$ will distinguish.
\end{example}

\begin{example}
Consider
\be
A = \bepm 
2 & 0 & 0 & 0 \\
3 & 2 & 0 & -2 \\
0 & 0 & 2 & 0\\ 
0 & 0 & 2 & 2
\eepm
\ee
then $\chi_A(t) = (2 - t)^4$ and
\be
A - 2I = \bepm
0 & 0 & 0 & 0\\
3 & 0 & 0 & -2\\
0 & 0 & 0 & 0\\
0 & 0 & 2 & 0
\eepm
\qquad\qquad 
(A - 2I)^2 =
\bepm
0 & 0 & 0 & 0\\
0 & 0 & -4 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\eepm
\ee

The minimal polynomial of $A$ is $m_A(t) = (2-t)^3$ and $n(A-2I) = 2$. Hence the Jordan Normal Form of $A$ is
\be
JNF = \bepm
2 & 1 & 0 & 0\\
0 & 2 & 1 & 0\\
0 & 0 & 2 & 0\\
0 & 0 & 0 & 2
\eepm.
\ee

To find a Jordan basis, take $v_3 \notin \ker(A - 2I)^2$, e.g. $v_3 = (0, 0, 1, 0)^T$. Under $A - 2I$,
\be
v_3 = \bepm
0\\
0\\
1\\
0\\
\eepm
\mapsto v_2 = \bepm
0\\
0\\
0\\
2\\
\eepm
\mapsto v_1 = \bepm
0\\
-4\\
0\\
0\\
\eepm
\mapsto 0.
\ee

Take $v_4$ to be another eigenvector, e.g. $v_4 = (2, 0, 0, 3)^T$.
\end{example}


\section{Dual spaces}

\begin{definition}
Let $V$ be a finite dimensional vector space over $\F$. Then $V^* = \sL(V, \F)$ is the dual of $V$. The vectors of $V^*$ are the linear functionals on $V$.
\end{definition}

\begin{lemma}
Let $V$ be a vector space over $\F$, let $B = \bra{e_1, \dots, e_n}$ be a basis of $V$. Then $B^* = \bra{\ve_1, \dots, \ve_n}$ is the basis of $V^*$ dual to $B$, where $\ve_j(e_k) = \delta_{jk}$.
\end{lemma}

\begin{proof}[\bf Proof]
If $\sum^n_{j=1} \lm_j\ve_j = 0$ then for all $k = 1, \dots, n$ we have $\lm_k = \bb{\sum^n_{j=1} \lm_j\ve_j}(e_k) = 0$, so $B^*$ is independent. If $\ve\in V^*$ then $\ve = \sum^n_{j=1}\ve(e_j)\ve_j$, so $B^*$ spans $V^*$.
\end{proof}

\begin{remark}
If $\ve = \sum^n_{j=1} a_j\ve_j$, $v = \sum^n_{j=1}  x_je_j$, then
\be
\ve(v) = \sum^n_{j=1} a_jx_j = \bb{a_1 \dots a_n} \bepm
x_1\\
\vdots \\
x_n
\eepm.
\ee

So we can think of the dual of $\F^n$ as the space pf $n$-rows.
\end{remark}

\begin{definition}
If $U \leq V$ let $U^\circ = \bra{\ve \in V^* : \ve(u) = 0, \forall u \in U}$. $U^\circ$ is the annihilator of $U$ in $V^*$.
\end{definition}

\begin{lemma}
\ben
\item [(i)] If $U \leq V$ then $U^\circ \leq  V^*$.
\item [(ii)] If $U \leq  V$ then $\dim U + \dim U^\circ = \dim V$.
\een
\end{lemma}

\begin{proof}[\bf Proof]
\ben
\item [(i)] This is clear.
\item [(ii)] Let $U \leq  V$, let $e_1, \dots, e_k$ be a basis for $U$, let $B = \bra{e_1, \dots, e_k, \dots, e_n}$ be a basis for $V$. We claim $U^\circ = \bsa{\ve_{k+1}, \dots, \ve_n}$, where $\ve_1, \dots, \ve_n$ is the basis of $V^*$ dual to $B$. If $i > k$ then $\ve_i(e_j) = 0$ for $j \leq  k$, so $\ve_i \in U^\circ$. If $\ve \in U^\circ$ then $\ve = \sum^n_{j=1} \lm_j\ve_j$ and then for $j \leq k$ we have $\lm_j = \ve(e_j) = 0$, so $\ve\in \bsa{\ve_{k+1}, \dots, \ve_n}$.
\een
\end{proof}

\begin{lemma}
Let $U, V$ be vector spaces over $\F$, let $U \stackrel{\alpha}{\to} V$ be a linear map. Then the map $V^* \stackrel{\alpha^*}{\to} U^*$ given by $\alpha^*(\ve) = \ve \circ \alpha$ for $\ve \in V^*$ is linear, it is the dual of $\alpha$.
\end{lemma}

\begin{proof}[\bf Proof]
Certainly $\ve \circ \alpha : U \to\F$ is linear, so $\alpha^* \in U^*$. If $\theta_1, \theta_2 \in V^*$ then
\be
\alpha^*(\theta_1 + \theta_2) = (\theta_1 + \theta_2) \circ \alpha = \theta_1 \circ \alpha + \theta_2 \circ \alpha = \alpha^*(\theta_1) + \alpha^*(\theta_2)
\ee
and if $\lm \in \F$, $\theta \in V^*$ then
\be
\alpha^*(\lm\theta) = (\lm\theta) \circ \alpha = \lm(\theta \circ \alpha) = \lm\alpha^*(\theta).
\ee
\end{proof}

\begin{proposition}
Let $U, V$ be vector spaces over $\F$ of finite dimension. Let $B,C$ be their bases, let $B^*$, $C^*$ be bases of $U^*, V^*$ dual to $B,C$. Let $\alpha \in \sL(U, V )$ and let $\alpha^*\in \sL(U^*, V^*)$ be its dual. Then $[\alpha^*]_{C^*,B^*} = [\alpha]^T_{B,C}$.
\end{proposition}

\begin{proof}[\bf Proof]
Let $B = \bra{b_1, \dots, b_n}$, $C = {c_1, \dots, c_m}$ and $B^* = \bra{\beta_1, \dots, \beta_n}$, $C^* = \bra{\gamma_1, \dots, \gamma_m}$. Let $A = [\alpha]_{B,C}$ so that $\alpha(b_j) = \sum^n_{i=1} a_{ij}c_i$. Then
\be
\alpha^*(\gamma_r)(b_s) = \gamma_r(\alpha(b_s)) =  \gamma_r\bb{\sum^n_{i=1} a_{is}c_i} = \sum^n_{i=1} a_{is} \gamma_r(c_i) = \sum^n_{i=1} a_{is}\delta_{ri} = a_{rs} = \bb{\sum^n_{i=1} a_{ri}\beta_i} (b_s)
\ee
for all $s = 1, \dots, n$, so $\alpha^*(\gamma_r) = \sum^n_{i=1} a_{ri}\beta_i$ and hence $[\alpha^*]_{C^*,B^*} = A^T$.
\end{proof}

\begin{corollary}
We have $\det \alpha^* = \det \alpha$, $\chi_{\alpha^*} = \chi_\alpha$, $m_{\alpha^*} = m_\alpha$ since $\det A^T = \det A$ and $p(A^T ) = p(A)^T$ for any polynomial $p$.
\end{corollary}

\begin{lemma}
Let $U, V$ be vector spaces over $\F$ of finite dimension. Let $\alpha \in \sL(U, V )$, let $\alpha^* \in \sL(V^*,U^*)$ its dual. Then $\ker \alpha^* = (\im \alpha)^\circ$. In particular, $\alpha^*$ is injective if and only if $\alpha$ is surjective.
\end{lemma}

\begin{proof}[\bf Proof]
Let $\ve \in V^*$. Then $\ve \in \ker \alpha^*$ iff $\alpha^*(\ve)$ is the zero functional on $U$ iff $\ve \circ \alpha$ is the zero functional on $U$ iff $\ve \in \im(\alpha)^\circ$. In particular, $\alpha^*$ is injective iff $\ker \alpha^* = \bra{0}$ iff $(\im \alpha)^\circ = \bra{0}$ iff $\im \alpha = V$ iff $\alpha$ is surjective.
\end{proof}

\begin{corollary}
If $\alpha \in \sL(U, V )$, then $\rank \alpha = \rank \alpha^*$. If $A \in M_{m,n}(\F)$, then $\rank A = \rank A^T$.
\end{corollary}

\begin{proof}[\bf Proof]
\be
\rank \alpha^* = \dim V^* - n(\alpha^*) = \dim V - \dim(\im \alpha)^\circ = \dim V - (\dim V - \dim\im \alpha) = \rank \alpha
\ee

Note this gives an algebraic proof of the equality between the column rank and the row rank of a matrix.
\end{proof}

\begin{lemma}
We also have $\im \alpha^* = (\ker \alpha)^\circ$.
\end{lemma}

\begin{remark}
The map $V^* \times V \to \F$, $(\ve, v) \mapsto \ve(v)$ is bilinear. Denote this by $\bsa{\ve|v}$. If $U \stackrel{\alpha}{\to} V$, $V^* \stackrel{\alpha^*}{\to} U^*$ then
\be
\bracket{\alpha^*(\ve)}{u} = \bracket{\ve}{\alpha(u)}
\ee
for all $u \in U$, $\ve \in V^*$. We have a map $\wh{}: V \to V^{**}$, $v \mapsto \wh{v}$, $\wh{v}(\ve) = \ve(v)$.
\end{remark}

\begin{theorem}
If $V$ is finite dimensional over $\F$, the map $\wh{}: V \to V^{**}$, $v \mapsto \wh{v}$ with $\wh{v}(\ve) = \ve(v)$ is an isomorphism.
\end{theorem}

\begin{proof}[\bf Proof]
Since $\wh{v} : V^* \to \F$ is linear, $\wh{}$ is linear.
\beast
(\wh{\lm_1v_1 + \lm_2v_2})(\ve) = \ve(\lm_1v_1 + \lm_2v_2) = \lm_1\ve(v_1) + \lm_2\ve(v_2)= \lm_1\wh{v}_1(\ve) + \lm_2\wh{v}_2(\ve) = (\lm_1\wh{v}_1 + \lm_2\wh{v}_2)(\ve)
\eeast
for all $\ve \in V^{**}$. $\wh{}$ is injective (and hence surjective since $V$ is finite dimensional and $\dim V = \dim V^{**}$). If $e_1 \neq 0$, $e_1 \in V$, let $e_1, \dots, e_n$ be a basis for $V$, let $\ve_1, \dots, \ve_n$ be the basis of $V^*$ dual to this. Then $\wh{e}_1(\ve_1) = \ve_1(e_1) = 1$, so $\wh{e}_1 \neq 0$.
\end{proof}

\begin{remark}
This is a natural isomorphism, it is independent of the bases.
\end{remark}

\begin{remark}
If $\ve_1, \dots, \ve_n$ is a basis of $V^{**}$, and $E_1, \dots,E_n$ is the basis of $V^{**}$ dual to this, then $E_j = \wh{e}_j$ for a unique $e_j \in V$ and $\ve_1, \dots, \ve_n$ is the basis of $V^*$ dual to the basis $e_1, \dots, e_n$ of $V$.
\end{remark}

\begin{lemma}
Let $V$ be finite dimensional, let $U \leq  V$. If we identify $V$ and $V^{**}$, then $U = U^{\circ\circ}$. (More precisely, $\wh{U} = U^{\circ\circ}$.)
\end{lemma}

\begin{proof}[\bf Proof]
We first show $U \leq U^{\circ\circ}$. If $u \in U$ then $\ve(u) = 0$ for all $\ve \in U^\circ$, and hence $\wh{u}(\ve) = 0$ for all $\ve \in U^\circ$, whence $\wh{u} \in U^{\circ\circ}$. As also $\dim U = \dim U^{\circ\circ}$, it follows that $U = U^{\circ\circ}$.
\end{proof}

\begin{lemma}
If $U_1,U_2 \leq V$ where $\dim V$ is finite then
\ben
\item [(i)] $(U_1 + U_2)^\circ = U^\circ_1 \cap U^\circ_2$;
\item [(ii)] $(U_1 \cap U_2)^\circ = U^\circ_1 + U^\circ_2$.
\een
\end{lemma}

\begin{remark}
The situation is different when $V$ is not of finite dimension. Consider $V = P(\R)$, the space of real polynomials. Then $V^* = \R^\N$, the space of real sequences, which is not isomorphism to $P(\R) = \bsa{p_0, p_1, \dots}$. Any element $\ve \in V^*$ can be given as $(\ve(p_0), \ve(p_1), \dots)$.
\end{remark}


\section{Bilinear forms}

\begin{definition}
Let $U, V$ be vector spaces over $\F$. The function $\psi: U \times  V \to \F$ is bilinear if it is linear in each coordinate. For fixed $u \in U$, $\psi(u, v)$ is linear in $v$, and for fixed $v \in V$, $(u, v)$ is linear in $u$. (Here we are mainly concerned with the case $U = V$ and bilinear forms on $V$.)
\end{definition}

\begin{example}
\ben
\item [(i)] $\F = \R$, $V = \R^n$, $\psi(x, y) = \sum^n_{i=1} x_iy_i = x^T y$.
\item [(ii)] $V = \F^n$, $A \in M_n(\F)$, $\psi(u, v) = u^TAv$.
\een
\end{example}

\begin{definition}. Let $V$ be a vector space over $\F$ with $\dim V = n$. Let $B = \bra{v_1, \dots, v_n}$ be a basis of $V$. The matrix of the bilinear form $\psi$ on $V$ with respect to $B$ is $A =(\psi (v_i, v_j)) = [\psi]_B$.
\end{definition}

\begin{lemma}
If $\psi$ is a bilinear form on $V$ and $B$ is a basis for $V$, then $\psi(u, v) = [u]^T_B[\psi]_B[v]_B$ for all $u, v \in V$.
\end{lemma}

\begin{proof}[\bf Proof]
Let $B = \bra{v_1, \dots, v_n}$. If $u = \sum^n_{i=1} a_iv_i$, $v = \sum^n_{i=1} b_iv_i$, then
\be
\psi(u, v) = \psi\bb{\sum a_iv_i, \sum b_jv_j} = \sum_{i,j} a_ib_j \psi(v_i, v_j) = \bb{a_1 \dots a_n} [\psi]_B\bepm
b_1\\
\vdots\\
b_n
\eepm = [u]^T_B[\psi]_B[v]_B.
\ee

Moreover, $[\psi]_B$ is the only matrix for which this holds for all $u, v \in V$. If $\psi(u, v) = [u]^T_B A[v]_B$ for all $u, v \in V$, apply this for $u = v_i$, $v = v_j$ to obtain $A_{ij} = \psi (v_i, v_j)$.
\end{proof}

Let $B = \bra{v_1, \dots, v_n}$, $B' = \bra{v'_1 , \dots, v'_n}$ be bases for $V$ and let $P$ be the change of basis matrix from $B$ to $B'$. Then
\be
v'_j = \sum^n_{i=1} p_{ij}v_i,\quad\quad [v]_B = P[v]_{B'}
\ee

\begin{theorem}
$[\psi]_{B'} = P^T [\psi]_BP$.
\end{theorem}

\begin{proof}[\bf Proof]
For all $u, v \in V$, 
\be
\psi(u, v) = [u]^T_B[\psi]_B[v]_B = (P[u]_{B'})^T [psi]_B(P[v]_{B'}) = [u]^T_{B'} P^T [\psi]_B P[v]_{B'},
\ee
so $[\psi]_{B'} = P^T [\psi]_BP$ by uniqueness of $[\psi]_{B'}$.
\end{proof}

\begin{definition}. The real square matrices $A,B$ are congruent if $B = P^TAP$ for some invertible matrix $P$ (i.e. if they represent the same bilinear form with respect to different bases).
\end{definition}

\begin{lemma}
Congruence is an equivalence relation on $M_n(\R)$.
\end{lemma}

\begin{definition}
The rank of a bilinear form is the rank of any matrix representing it. (This is independent of the choice of basis.)
\end{definition}

\begin{definition}
The real bilinear form $\psi$ is symmetric if $\psi(u, v) = \psi(v, u)$ for all $u, v \in V$. This is equivalent to $A = [\psi]_B$ being symmetric, i.e. $A = A^T$.
\end{definition}

\begin{remark}
If $P^TAP = D$ for some non-singular $P$ then $A = A^T$.
\end{remark}

\begin{definition}
Let $V$ be a real vector space. The function $Q : V \to \R$ is a quadratic form on $V$ if $Q(\lm v) = \lm^2Q(v)$ for all $\lm 2\in \R$, $v \in V$, and there exist a symmetric bilinear form $\psi$ on $V$ so that $Q(v) + Q(w) + 2\psi (v,w) = Q(v + w)$ for all $v,w \in V$.
\end{definition}

We can start with a symmetric bilinear form $\psi$ and define $Q(v) = \psi(v, v)$. Conversely, we can start with a quadratic form $Q$ and let $\psi(v,w) = \frac 12 (Q(v + w) - Q(v) - Q(w))$.

\begin{theorem}
Any real symmetric bilinear form can be represented by a diagonal matrix of the form
\be
\bepm
I_p & & 0\\
& -I_q &\\
0 & 0
\eepm.
\ee

Any real symmetric matrix is congruent to a diagonal matrix of this form.
\end{theorem}

\begin{proof}[\bf Proof]
First we prove by induction on $\dim V = n$ that any symmetric bilinear form can be represented by a diagonal matrix. If $\psi(u, v) = 0$ for all $u, v \in V$ then $[\psi]_B = 0$ for any basis $B$. Suppose this is not the case. Then there exists $e\in V$ with $\psi(e, e) \neq 0$. (Otherwise $2 \psi(u, v) =  \psi(u + v, u + v) - \psi (u, u) -\psi (v, v)$ is 0 for all $u, v \in V$.) 

Let $W = \bra{v \in V : \psi (e, v) = 0}$. Then $V = \bsa{e} \oplus W$. (If $v\in V$ then $v = \lm e + (v - \lm e)$ for all $\lm \in\R$. Choose $\lm$ so that $v - \lm e \in W$, by taking $\lm = \frac{\psi (e,v)}{\psi (e,e)}$. Observe $\bsa{e} \cap W = \bra{0}$ since $\psi(e, \lm e) \neq 0$ for $\lm \neq 0$.) Now the restriction $\psi'$ of $\psi$ to $W$ is a symmetric bilinear form, so by induction, there exists a basis $e_2, \dots, e_n$ of $W$ with respect to which $\psi'$ is diagonal, say the matrix representing $\psi'$ is
\be
\bepm
d_2 & & 0\\
& \ddots & \\
0 & d_n
\eepm.
\ee

Let $B' = \bra{e_1, e_2, \dots, e_n}$ where $e_1 = e$. Then $[\psi]_{B'}$ is 
\be
\bepm 
d_1 & 0 &  & 0\\
0 & d_2 & & \\
& & \ddots & \\
0 & & & d_n
\eepm,
\ee
where $d_1 = \psi(e, e)$. This completes the diagonalisation part of the proof.

Now reorder $B_0$ if necessary so that $d_1, \dots, d_p > 0$, $d_{p+1}, \dots, d_{p+q} < 0$ and $d_i = 0$ for all $i > p + q$. Now for $1 \leq  i \leq  p + q$ replace $e_i$ by $\frac 1{\sqrt{\abs{d_i}}} e_i$ to obtain $B$. Then
\be
[\psi]_B = \bepm
I_p & &  0\\
& -I_q & \\
0 & & 0
\eepm.
\ee
\end{proof}

\begin{remark}
Note that $\rank \psi = p + q$.
\end{remark}

\begin{definition}
The signature of $\psi$ is $s(\psi) = p - q$.
\end{definition}

\begin{remark}
We have $p = \frac 12 (r + s)$, $q = \frac 12 (r - s)$. Note that some authors call $(p, q)$ the signature.
\end{remark}

\begin{theorem}[Sylvester's law of inertia]
If the real symmetric bilinear form $\psi$ is represented by
\be
\bepm
I_p & & 0\\
& -I_q & \\
0 & & 0
\eepm,\qquad\qquad
\bepm
I_{p'} & & 0\\
& -I_{q'} & \\
0 & & 0
\eepm
\ee
with respect to different bases then $p = p'$, $q = q'$ and so the rank and signature are well-defined.
\end{theorem}

\begin{proof}[\bf Proof]
Let $B = \bra{v_1, \dots, v_p, v_{p+1}, \dots, v_{p+q}, v_{p+q+1}, \dots, v_n}$ with
\be
[\psi]_B = \bepm
I_p & & 0\\
& -I_q & \\
0 & & 0
\eepm.
\ee

Let $X = \bsa{v_1, \dots, v_p}$, $Y = \bsa{v_{p+1}, \dots, v_n}$. We show that $\psi$ is positive definite on $X$ and that $p$ is the largest dimension of any subspace of $V$ on which $\psi$ is positive definite. $Q\bb{\sum^p_{i=1} \lm_iv_i} = \sum^p_{i=1} \lm^2_i \geq 0$ with equality if and only if $\sum^p_{i=1} \lm_iv_i = 0$, so $\psi$ is positive definite on $X$. If $X' \leq V$ with $\dim X' = p'$ and $\psi$ is positive definite on $X'$ then $X' \cap Y = \bra{0}$ as $\psi$ is negative semi-definite on $Y$. Hence $\dim X' + \dim Y \leq n$. So $p' = \dim X' \leq n - \dim Y = p$.

Similarly, if $N = \bsa{v_{p+1}, \dots, v_{p+q}}$ then $\psi$ is negative definite on $N$ and $q$ is the largest dimension of a subspace of $V$ on which $\psi$ is negative definite.

Hence $p$ and $q$ are independent of the choice of basis.
\end{proof}

\begin{remark}
$p$ is determined by $\psi$, but there may be many subspaces like $X$ of dimension $p$, on which $\psi$ is positive definite. (Similarly for $q$ and $N$.)
\end{remark}

\begin{remark}
Let $t = \min\bra{p, q}$. Then the restriction of $\psi$ to the subspace $\bsa{v_1 + v_{p+1}, \dots, v_t + v_{p+t}, v_{p+q+1}, \dots, v_n}$ is 0, and $n - \max\bra{p, q}$ is the maximal dimension of any subspace on which $\psi$ is 0. For is $\psi = 0$ on $U \leq V$ then $U \cap X = \bra{0} = U \cap N$, so $\dim U \leq n - p$ and $\dim U \leq n - q$.
\end{remark}

\begin{definition}
The kernel of $\psi$ is $\bra{v \in V : \psi(v,w) = 0,\forall w \in V }$. In the above, $\ker \psi = \bsa{v_{p+q+1}, \dots, v_n}$.
\end{definition}

\begin{definition}
The real symmetric bilinear form $\psi$ is non-singular if $\ker \psi = \bra{0}$. Equivalently, $[\psi ]_B$ with respect to any basis $B$ is non-singular. Also equivalently, $n = p + q$.
\end{definition}

\begin{example}
Consider the quadratic form $Q$ on $V = \R^3$ where
\be
Q(x_1, x_2, x_3) = x^2_1 + x^2_2 + 2x^2_3 + 2x_1x_2 + 2x_1x_3 - 2x_2x_3.
\ee

The matrix of $Q$ with respect to the standard basis is
\be
A =\bepm
1 & 1 & 1\\
1 & 1 & -1\\
1 & -1 & 2
\eepm.
\ee

\ben
\item [(i)] (Completing squares.)
\beast
Q(x_1, x_2, x_3) & = & x^2_1 + x^2_2 + 2x^2_3 + 2x_1x_2 + 2x_1x_3 - 2x_2x_3 \\
& = & (x_1 + x_2 + x_3)^2 + x^2_3 - 4x_2x_3\\
& = & (x_1 + x_2 + x_3)^2 + (x_3 - 2x_2)^2 - (2x_2)^2
\eeast

Hence $\rank(Q) = 3$, $s(Q) = 2 - 1 = 1$. Also
\be
P^{-1} = \bepm
1 & 1 & 1\\
0 & -2 & 1\\
0 & 2 & 0
\eepm,\qquad\qquad P^TAP =
\bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & -1
\eepm
\ee

\item [(ii)] (Elementary matrices.) We want $P$ invertible such that $P^TAP$ is diagonal. Apply elementary column operations followed immediately by the corresponding elementary row operation. Then
\be
A \to E^T_1 AE_1 \to \dots E^T_k \dots E^T_1 AE_1 \dots E_k = D
\ee
where $P = E_1 \dots E_k$.

\item [(iii)] (From the proof of Theorem 6.4.) Choose $e_1$ with $Q(e_1) \neq 0$, e.g. $e_1 = (1, 0, 0)^T$, $Q(e_1) = 1$. Set $W = \bra{v \in V : \psi(e, v) = 0} = \bra{(a, b, c)^T : a + b + c = 0}$. Note $e^T_1 A = (1, 1, 1)$. Choose $e_2 \in W$ with $Q(e_2) \neq 0$, e.g. $e_2 = (1, 0,-1)$, $Q(e_2) = 1$.
Now choose $e_3 \in W$ so that $\psi(e_2, e_3) = 0$ so $e_3 = (a, b, c)^T$ with $a + b + c = 0$, $2b - c = 0$ and $e^T_2 A = (0, 2,-1)$, so $e_3 = \frac 12 (-3, 1, 2)^T$, $Q(e_3) = -1$.

\item [(iv)] (See Section 7.) We shall see that $s(Q)$ is the difference of the number of positive eigenvalues and the number of negative eigenvalues.

Now consider bilinear symmetric forms over $\C$. As before in Theorem 6.4, there exists a basis $e_1, \dots, e_n$ such that the matrix representing $\psi$ with respect to this basis is
\be
\bepm
d_1 & & 0\\
& \ddots & \\
0 & & d_n
\eepm.
\ee

Reorder this basis so that $d_1, \dots, d_r \neq 0$ and $d_i = 0$ for all $i > r$. To normalise, replace $e_j$ by $\frac 1{\sqrt{\abs{d_j}}} e_j$ for $1 \leq  j \leq  r$. Then the matrix representing $\psi$ is
\be
\bepm
I_r & 0\\
0 & 0
\eepm.
\ee
\een
\end{example}

\begin{lemma}
Any symmetric complex matrix satisfies $P^TAP = \bepm I_r & 0\\ 0 & 0\eepm$ for some invertible matrix $P$. We have $\rank A = r$.
\end{lemma}

\begin{definition}
Let $V$ be a complex vector space. A complex Hermitian form on $V$ is a function $\psi: V \times V \to \C$ such that
\ben
\item [(i)] for $u \in V$, the function $v \mapsto \psi(u, v)$ is linear;
\item [(ii)] for all $u, v \in V$, $\psi(u, v) = \psi(\ol{v}, u)$.
\een
\end{definition}

\begin{remark}
Note that $\psi$ is not bilinear, it is sesquilinear, i.e.
\beast
\psi (u, \lm_1v_1 + \lm_2v_2) & = & \lm_1 \psi(u, v_1) + \lm_2 \psi(u, v_2)\\
\psi (\lm_1u_1 + \lm_2u_2, v) & = & \ol{\lm}_1 (u_1, v) + \ol{\lm}_2 (u_2, v)
\eeast
and complex-symmetric, i.e. $\psi (u, v) = \psi(\ol{v}, u)$.
\end{remark}

\begin{example}
Inner products of complex inner product spaces are complex Hermitian forms, see Chapter 7.
\end{example}

\begin{remark}
Given a complex Hermitian form $\psi$ on $V$, we can define a complex quadratic form $Q : V \to \C$, $Q(v) = \psi(v, v)$. Note that $Q(v) \in \R$ for all $v \in V$. Here we have $Q(\lm v) = |v|^2Q(v)$. we can recover $\psi$ as follows.
\be
\psi (u, v) = \frac 14 (Q(u + v) - Q(u - v) - iQ(u + iv) + iQ(u - iv))
\ee
If $B = \bra{v_1, \dots, v_n}$ is a basis of $V$, the matrix of $\psi$ with respect to $B$ is $[\psi]_B = (\psi(v_i, v_j)) = A$. Then $\psi (u, v) = [\psi]^T_B[\psi]_B[v]_B$. Note that $A = \ol{A}^T$, the matrix is Hermitian. Finally, a change of basis results in the matrix $\ol{P}^TAP$ with a non-singular
matrix $P$, the change of basis matrix.
\end{remark}

\begin{theorem}
If $\psi$ is a Hermitian form on a complex vector space $V$, there is a basis $B$ of $V$ with respect to which $\psi$ has diagonal matrix
\be
\bepm
I_p & & 0\\
& -I_q & \\
0 & & 0
\eepm
\ee

Moreover, $p$ and $q$ are uniquely determinend by $\psi$, independent of $B$. So if the basis is $v_1, \dots, v_n$ then
\be
Q\bb{\sum^n_{i=1} \xi_iv_i} = \abs{\xi_1}^2 + \dots + \abs{\xi_p}^2 - \abs{\xi_{p+1}}^2 - \dots - \abs{\xi_{p+q}}^2.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
We can prove this as before for Theorem 6.4 and Theorem 6.5.
\end{proof}

\begin{definition}
$\psi$ is positive definite (on $V$) if $p = n$.
\end{definition}

\begin{remark}
The bilinear forms we have considered were mainly symmetric. The next important class are skew-symmetric bilinear forms satisfying $\psi(u, v) = -\psi (v, u)$ for all $u, v \in V$ and hence $\psi (u, u) = 0$ for all $u \in V$. If $A = [\psi]_B$ for some basis $B$ of $V$ then $A^T = -A$, and $A$ is called skew-symmetric.
\end{remark}

\begin{remark}
Any real matrix $A$ can be expressed as the sum of a symmetric and skew-symmetric matrix,
\be
A = \frac 12 (A + A^T ) + \frac 12 (A - A^T )
\ee
\end{remark}

\begin{theorem}
If $\psi$ is a skew-symmetric bilinear form on a real vector space $V$, there exists a basis $v_1,w_1, \dots, v_m,w_m, v_{2m+1}, \dots, v_n$ with respect to which $\psi$ has the block diagonal matrix
\be
\bepm
0 & 1 & & & & & & 0\\
-1 & 0 & & & & & & \\
& & \ddots & & & & & \\
& & & 0 & 1 & & & \\
& & & -1 & 0 & & & \\
& & & & & 0 & & \\
& & & & & & \ddots & \\
& & & & & & & 0 
\eepm.
\ee

Any skew-symmetric matrix is congruent to a matrix of this form.
\end{theorem}

\begin{proof}[\bf Proof] (Sketch). Prove this by induction on $\dim V = n$. If $\psi = 0$ the result is clear. Otherwise take $v_1,w_1$ so that $\psi(v_1,w_1) = 1$, so also $\psi(w_1, v_1) = -1$. Let $U = \bsa{v_1,w_1}$, $W = \bra{v \in V : \psi(v_1, v) = 0 = \psi(w_1, v)}$. Then $V = U \oplus W$. If $v = (av_1 + bw_1) + (v - av_1 - bw_1)$ take $a = \psi (v,w_1)$, $b = \psi(v_1, v)$ to obtain $v - av_1 - bw_1 \in W$. Also $U \cap W = \bra{0}$ since if $av_1 +bw_1 \in W$ then $\psi(av_1 +bw_1, av_1 -bw_1) = a^2 +b^2$. Now finish by working in $W$.
\end{proof}

\begin{remark}
\ben
\item [(i)] If $\psi$ is non-singular skew-symmetric, then $n = 2m$.
\item [(ii)] Reordering the basis $B$ as $v_1, \dots, v_m,w_1, \dots,w_m, v_{2m+1}, \dots, v_n$, we obtain
\be
\bepm
0 & I_m & 0\\
-I_m & 0 & 0\\
0 & 0 & 0
\eepm.
\ee
\een
\end{remark}

\begin{remark} 
(*). Here are some remarks on general bilinear forms on $U \times V$, where $U, V$ are vector spaces over the same field $\F$. We assume $\psi: U \times  V \to \F$ is linear in each coordinate. Examples include
\ben
\item [(i)] $U = V^*$, $\psi : V^* \times V \to\F$, $(\alpha, v) \mapsto \alpha(v)$;
\item [(ii)] $\F = \C$, define $\ol{V}$ to be the space with elements as $V$, addition as in $V$ and multiplication $\cdot$ by $\lm \cdot v = \ol{\lm}v$. (Then sesquilinear forms are just bilinear forms on $\ol{V} \times V$.)
\een
\end{remark}

Suppose $\psi: U \times  V \to\F$ is bilinear. Define
\beast
& & \psi_L : U \to V^*, u \mapsto ( \psi_L(u) : v \mapsto \psi(u, v))\\
& & \psi_R : V \to U^*, v \mapsto ( \psi_R(v) : u \mapsto \psi(u, v))
\eeast
$\psi$ is said to be non-singular if $\ker \psi_L = \bra{0}$, $\ker \psi_R = \bra{0}$. Note that
\ben
\item [(i)] if $\psi$ is non-singular then $\dim U = \dim V$ for $\dim U \leq V^* = \dim V$, $\dim V \leq \dim U^* = \dim U$;
\item [(ii)] if $\dim U = \dim V$, then $\ker \psi_L = \bra{0}$ if and only if $\ker \psi_R = \bra{0}$.
\een

If $\psi$ is a bilinear non-singular form on $U \times V$, let $u_1, \dots, u_n$ be a basis of $U$. Then $\psi_L(u_1), \dots, \psi_L(u_n)$ is a basis of $V^*$. Let $v_1, \dots, v_n$ be the basis of $V$ dual to this. Then $\psi(u_i, v_j) = \delta_{ij}$.

\begin{lemma}
(*). Let $\psi$ be a non-singular bilinear form on $V$. For $W \leq V$, let $W^\perp = \bra{v \in V : \psi (w, v) = 0 ,\forall w \in W}$. Then $W^\perp\leq V$ and $\dim W + \dim W^\perp = \dim V$.
\end{lemma}

\begin{proof}[\bf Proof]
(Sketch). Let $u_1, \dots, u_n$ be a basis of $V$ containing a basis $u_1, \dots, u_m$ of $W$. Let $v_1, \dots, v_n$ be the basis paired to it as above. Then $W^\perp = \bsa{v_{m+1}, \dots, v_n}$.

Or more algebraically, consider
\be
\psi_L : V \to V^*, u \mapsto ( \psi_L(u) : V \to\F, v \mapsto \psi(u, v)).
\ee

This is an isomorphism as $\psi$ is non-singular. Now $W^\perp = (\psi_L(W))^\circ$ as 
\beast
v \in W^\perp & \lra & \psi(w, v) = 0,\forall w \in W\\
& \lra & \psi_L(w)(v) = 0,\forall w \in W\\
& \lra & v \in (\psi_L(W))^\circ.
\eeast

Hence
\beast
\dim W + \dim W^\perp & = & \dim W + \dim(\psi_L(W))^\circ \\
& = & \dim W + \dim V - \dim \psi_L(W) = \dim V.
\eeast
\end{proof}


\section{Inner product spaces}

\begin{definition}
Let $V$ be a real (resp. complex) space. An inner product on $V$ is a positive definite symmetric bilinear (resp. Hermitian) form on $V$. Write $\bsa{v,w}$ for the value of the form on $(v,w) \in V \times V$. A vector space $V$ equipped with an inner product is a real (resp. complex) inner product space, also called a Euclidean (resp. unitary) space. So $\inner{}{}$ is a real symmetric bilinear (resp. Hermitian) form such that $\inner{v}{v} > 0$ for all $v \in V \bs \bra{0}$.
\end{definition}

\begin{definition}
The length of $v$ is defined to be $\dabs{v} =\sqrt{\inner{v}{v}}$. We have $abs{v} > 0$ for $v \neq 0$.
\end{definition}

\begin{lemma}[Schwartz's inequality]
For all $v,w \in V$, $\abs{\inner{v}{w}} \leq  \dabs{v}\dabs{w}$.
\end{lemma}

\begin{proof}[\bf Proof]
If $v = 0$ then the result is clear. So suppose $v \neq 0$ and consider the real and the complex case separately.
\bit
\item (Real case) For all $t \in \R$, we have
\be
0 \leq  \dabs{tv - w}^2 = t^2\dabs{v}^2 - 2t\inner{v}{w} + \dabs{w}^2.
\ee

Set $t = \frac{\inner{v}{w}}{\dabs{v}^2}$ to obtain the result.
\item (Complex case) For all $t \in \C$, we have 
\be
0 \leq  \dabs{tv - w}^2 = t\ol{t} \dabs{v}^2 - \ol{t} \inner{v}{w} - t\ol{\inner{v}{w}} + \dabs{w}^2.
\ee

Set $t = \frac{\inner{v}{w}}{\dabs{v}^2}$, so $\ol{t} = \frac{\inner{v}{w}}{\dabs{v}^2}$ to obtain the result.
\eit
\end{proof}

\begin{lemma}
In the Euclidean case, if $v,w \neq 0$, the angle $\theta$ between them is given by $\cos \theta = \frac{\inner{v}{w}}{\dabs{v}\dabs{w}}$ taking $\theta \in [0, 2\pi)$.
\end{lemma}

\begin{lemma}[Triangle inequality]
For all $v,w \in V$, $\dabs{v + w} \leq  \dabs{v} + \dabs{w}$.
\end{lemma}

\begin{proof}[\bf Proof]
$\dabs{v + w}^2 = \dabs{v}^2 + \inner{v}{w} + \ol{\inner{v}{w}} + \dabs{w}^2 \leq \dabs{v}^2 + 2\dabs{v}\dabs{w} + \dabs{w}^2 = (\dabs{v} + \dabs{w})^2$.
\end{proof}

\begin{remark}
Defining $d(v,w) = \dabs{v - w}$, we obtain a metric on $V$.
\end{remark}

\begin{example}
\ben
\item [(i)] Dot products on $\R^n$, $\C^n$;
\item [(ii)] $V = C[0, 1]$, real or complex valued, $\inner{f}{g} = \int^1_0 \ol{f(t)}g(t) dt$.
\een
\end{example}

\begin{definition}
A set $\bra{e_1, \dots, e_k}$ is orthogonal if $\inner{e_i}{e_j} = 0$ whenever $i \neq j$. It is orthonormal if also $\dabs{e_j} = 1$ for all $j$. It is an orthonormal basis if it is also a basis.
\end{definition}

\begin{lemma}
Orthonormal sets are linearly independent. In fact, if $v = \sum^k_{j=1} \lm_je_j$ then $\lm_j = \inner{e_j}{v}$.
\end{lemma}

\begin{theorem}[Gram-Schmidt]
Let $v_1, \dots, v_n$ be a basis for an inner product space $V$. There exists an orthonormal basis $e_1, \dots, e_n$ of $V$ such that $\bsa{v_1, \dots, v_k} = \bsa{e_1, \dots, e_k}$ for all $1 \leq  k \leq  n$.
\end{theorem}

\begin{proof}[\bf Proof]
Let $e_1 = \frac 1{\dabs{v_1}} v_1$. Suppose we have found $e_1, \dots, e_k$. Now take $e'_{k+1} = v_{k+1} - \sum^k_{j=1} \lm_j e_j$, with $\lm_j$ chosen such that $\inner{e_j}{e'_{k+1}} = 0$ for $1 \leq  j \leq  k$. So $\lm_j = \inner{e_j}{v_{k+1}}$. Then $e'_{k+1} \neq 0$, as $v_1, \dots, v_k, v_{k+1}$ are linearly independent. Let $e_{k+1} = \frac 1{\dabs{e'_{k+1}}} e'_{k+1}$. Then $\inner{e_j}{e_{k+1}} = 0$ for $1 \leq  j \leq  k$, and $\dabs{e_{k+1}} = 1$, and $\bsa{e_1, \dots, e_{k+1}} = \bsa{v_1, \dots, v_{k+1}}$.
\end{proof}

\begin{remark}
In calculations, it may be best to normalise only at the end. But then we have to adjust and take $\lm_j = \frac{\inner{e_j}{v_{k+1}}}{\inner{e_j}{e_j}}$.
\end{remark}

\begin{corollary}
In a finite dimensional inner product space, any orthonormal set of vectors can be extended to an orthonormal basis.
\end{corollary}

\begin{proof}[\bf Proof]
If $e_1, \dots, e_k$ is an orthonormal set, it is linearly independent, so extend it to a basis $e_1, \dots, e_k, v_k+1, \dots, v_n$ of $V$. Apply the Gram-Schmidt process to obtain $e_1, \dots, e_k, e_{k+1}, \dots, e_n$ an orthonormal basis of $V$.
\end{proof}

\begin{corollary}
\ben
\item [(i)] Any real non-singular matrix $A$ can be written as $A = RT$ with $R$ orthogonal and $T$ upper-triangular.
\item [(ii)] Any complex non-singular matrix $A$ can be written as $A = UT$ with $U$ unitary and $T$ upper-triangular.
\een
\end{corollary}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Work in $\R^n$ with the dot product. Let $v_1, \dots, v_n$ be the columns $A^{(1)}, \dots, A^{(n)}$ of $A$. Let $e_1, \dots, e_n$ be the orthonormal basis obtained from this $y$ the Gram-Schmidt process. Let $\R$ be the matrix with columns $R^{(j)} = e_j$. Then $R^TR = I$. Let $v_k = \sum^n_{j=1} t_{jk}e_j$. Since $v_k \in \bsa{e_1, \dots, e_k}$, we see that the matrix $T = (t_{ij})$ is upper-triangular. We have $A = RT$ as $A^{(k)} = \sum^n_{j=1} t_{jk}R(j)$.
\item [(ii)] For a complex non-singular matrix $A$, work over $\C^n$ with the dot product. Replace $R$ by $U$ with $\ol{U}^TU = I$ so $U$ is unitary.
\een
\end{proof}

\begin{definition}
Let $V$ be an inner product space. If $W \leq V$, write $W^\perp = \bra{v \in V : v \perp w,\forall w \in W}$. This is the orthogonal complement for $W$ in $V$.
\end{definition}

\begin{theorem}
If $V$ is a finite dimensional inner product space and $W \leq V$ then $V = W \oplus W^\perp$.
\end{theorem}

\begin{proof}[\bf Proof]
Let $e_1, \dots, e_k$ be an orthonormal basis of $W$, extend this to $e_1, \dots, e_k, e_{k+1}, \dots, e_n$ an orthonormal basis of $V$. (Then $e_{k+1}, \dots, e_n$ is an orthonormal basis for $W^\perp$.) Let $v \in V$. Write $v = \sum^k_{j=1} \lm_je_j + \sum^n_{j=k+1} \lm_je_j$, so $V = W + W^\perp$. And $W \cap W^\perp = \bra{0}$ since if $\inner{v}{v} = 0$ then $v = 0$ as the inner product is positive definite.
\end{proof}

\begin{definition}
Let $V$ be an inner product space, let $W \leq V$. Then the endomorphism $\pi = \pi_W$ of $V$ is an orthogonal projection onto $W$ if $\pi^2 = \pi$, $W = \im \pi$ and $W^\perp = \ker \pi$. 

As above, let $e_1, \dots, e_k$ be an orthonormal basis of $W$, extend this to $e_1, \dots, e_k, e_{k+1}, \dots, e_n$ an orthonormal basis of $V$. If $v =
\sum^n_{j=1} \lm_je_j$ then $\pi_w(v) = \sum^k_{j=1} \lm_je_j$, that is, $\pi_w(v) = \sum^k_{j=1} \inner{e_j}{v} e_j$.
\end{definition}

\begin{remark}
Note that $\iota_V = \pi_W + \pi_{W^\perp}$, $\pi_W\pi_{W^\perp} = 0$.
\end{remark}

\begin{lemma}
If $W \leq V$ as above and $v \in V$ then $\pi_W(v)$ is the vector in $W$ nearest to $v$, that is letting $w_0 = \pi_W(v)$ we have $d(w_0, v) \leq d(w, v)$ for all $w \in W$.
\end{lemma}

\begin{proposition}
Let $V$ be an inner product space of finite dimension, let $\alpha \in End(V )$. There exists a unique endomorphism $\alpha^*$ of $V$, the adjoint of $\alpha$, such that 
\be
\alpha^* : V \to V \qquad\qquad \inner{\alpha v}{w} = \inner{v}{\alpha^*w}
\ee
for all $v,w \in V$. Moreoever, if $B$ is an orthonormal basis of $V$, then $[\alpha^*]_B = \ol{[\alpha]}^T_B$.
\end{proposition}

\begin{proof}[\bf Proof]
Let $B = \bra{e_1, \dots, e_n}$ be an orthonormal basis of $V$. Let $A = [\alpha]_B = (a_{ij})$. Let $\alpha^*$ be the endomorphism such that $[\alpha^*]_B = \ol{A}^T$. Then, writing $C = \ol{A}^T$, for $1 \leq  i, j \leq  n$, we have
\be
\inner{\alpha e_i}{e_j} = \inner{\sum^n_{k=1} a_{ki}e_k}{e_j} = \sum^n_{k=1} \ol{a}_{ki} \inner{e_k}{e_j} = \ol{a}_{ji} = c_{ij} = \sum^n_{k=1} c_{kj}\inner{e_i}{e_k} = \inner{e_i}{\sum^n_{k=1} c_{kj}e_k} = \inner{e_i}{\alpha^*e_j}
\ee

Thus $\inner{\alpha v}{w} = \inner{v}{\alpha^* w}$ for all $v,w \in V$ since this is true on a basis. $\alpha^*$ is unique as $[\alpha^*]_B = \ol{A}^T$ is forced.
\end{proof}

\begin{lemma}
For adjoint endomorphisms, we have $(\alpha + \beta)^* = \alpha^* + \beta^*$, $(\lm\alpha)^* = \ol{\lm}\alpha^*$, $\alpha^{**} = \alpha$, $\iota^* = \iota$, $(\alpha\beta)^* = \beta^*\alpha^*$.
\end{lemma}

\begin{proof}[\bf Proof]
All of these follow from matrices, or directly, e.g. we have $\inner{\alpha^*v}{w} = \inner{v}{\alpha^{**}w}$ but also
\be
\inner{\alpha^*v}{w} = \inner{w}{\alpha^*v} = \ol{\inner{\alpha w}{v}} = \inner{v}{\alpha w}
\ee
so $\inner{v}{\alpha w - \alpha^{**}w} = 0$ for all $v \in V$ so $\alpha w = \alpha^{**} w$ for all $w \in V$, so $\alpha^{**} = \alpha$.
\end{proof}

\begin{definition}. Let $A$ be a real (resp. complex) $n \times n$ matrix.
\bit
\item $A$ is symmetric (resp. Hermitian) if $A^T = A$ (resp. $\ol{A}^T = A$), $A$ is self-adjoint.
\item $A$ is orthogonal (resp. unitary) if $A^T = A^{-1}$ (resp. $\ol{A}^T = A^{-1}$), $A$ is inverse-adjoint.
\eit
Let $V$ be an inner product space over $\R$ (resp. $\C$), let $\alpha \in End(V )$.
\bit
\item $\alpha$ is symmetric (resp. Hermitian) if $\alpha = \alpha^*$, equivalently $\inner{\alpha v}{w} = \inner{v}{\alpha w}$ for all $v,w \in V$.
\item $\alpha$ is orthogonal (resp. unitary) if $\alpha^* = \alpha^{-1}$, equivalently $\inner{\alpha v}{\alpha w} = \inner{v}{w}$, so $\alpha$ is an isometry of $V$.
\item $\alpha$ is normal if $\alpha\alpha^* = \alpha^*\alpha$.
\eit
\end{definition}

\begin{lemma}
If $V$ is an inner product space and $\alpha \in End(V)$, and if $B$ is an orthonormal basis of $V$, then $\alpha$ is Hermitian (unitary, symmetric, or orthogonal) if and only if $[\alpha]_B$ is.
\end{lemma}

\begin{theorem}[Spectral theorem]
Let $V$ be a complex inner product space, let $\alpha \in End(V)$ and assume that $\alpha$ is Hermitian (unitary). There exists an orthonormal basis of $V$ consisting of eigenvectors of $\alpha$. Moreover, each eigenvalue is real (lies on the unit circle).
\end{theorem}

\begin{proof}[\bf Proof]
Since $V$ is a vector space over $\C$, there exists an eigenvalue $\lm \in\C$. Let $e \in V$ with $\alpha(e) = \lm e$ and $\dabs{e} = 1$. Let$ W = \bsa{e}^\perp$. Then $W$ is $\alpha$-invariant. If $w \in W$ then 
\be
\inner{\alpha(w)}{e} = \inner{w}{\alpha^*(e)} = \inner{w}{\alpha(e)} = \inner{w}{\lm e} = \lm \inner{w}{e} = 0
\ee
and so $\alpha(w) \in W$. (If $w \in W$ then
\be
\inner{\alpha(w)}{e} = \inner{w}{\alpha^*(e)} = \inner{w}{\alpha^{-1}(e)} = \inner{w}{\lm^{-1}e} = \lm^{-1}\inner{w}{e} = 0
\ee
and so $\alpha(w) \in W$.) Now $V = \bsa{e} \oplus W$ by Theorem 7.8. Note that $\alpha|_W$ is Hermitian (unitary) and continue in $W$. There exists an orthonormal basis $e_2, \dots, e_n$ of $W$ consisting of eigenvectors of $\alpha$. Then $B = \bra{e_1, e_2, \dots, e_n}$ is an orthonormal basis consisting of eigenvectors of $\alpha$.

Moreover $[\alpha]_B = [\alpha]^T_B$. Now $[\alpha]_B$ is diagonal,
\be
[\alpha]_B = \bepm
\lm_1 & & 0\\
& \ddots & \\
0 & & \lm_n
\eepm,
\ee
so $\ol{\lm}_j = \lm_j$ for all $1 \leq  j \leq  n$, so $\lm_j$ is real. (Or $[\alpha^{-1}]_B = \ol{[\alpha]}^T_B$, so $\ol{\lm}_j = \lm^{-1}_j$ for all $1 \leq  j \leq  n$, so $\abs{\lm_j} = 1$.)
\end{proof}

\begin{lemma}
If $\alpha \in End(V )$ is Hermitian and if $v_1,v_2$ are eigenvectors of $\alpha$ corresponding to distinct eigenvalues $\lm_1, \lm_2$ then $v_1 \perp v_2$.
\end{lemma}

\begin{proof}[\bf Proof]
Consider $\inner{\alpha(v_1)}{v_2}$,
\be
\ol{\lm}_1 \inner{v_1}{v_2} = \inner{\alpha(v_1)}{v_2} = \prod v_1\alpha(v_2) = \lm_2\inner{v_1}{v_2}
\ee
so either $\lm_1 = \ol{\lm}_1 = \lm_2$ or $\inner{v_1}{v_2} = 0$. As $\lm_1 \neq \lm_2$, the result follows.
\end{proof}

\begin{lemma}
If $\alpha$ is a symmetric endomorphism of a real inner product space, then $\alpha$ has real eigenvalues. Also, eigenvalues corresponding to distinct eigenvalues are orthogonal.
\end{lemma}

\begin{proof}[\bf Proof]
Let $B$ be any orthonormal basis of $V$. Then $[\alpha]_B$ is a real symmetric matrix, so also Hermitian (as a complex matrix). Hence the eigenvalues of $[\alpha]_B$, and so of $\alpha$, are real by Theorem 7.13, and the rest follows from Lemma 7.14.
\end{proof}

\begin{theorem}
Let $V$ be a real inner product space, let $\alpha \in End(V )$ be symmetric. There is an orthonormal basis of $V$ consisting of eigenvectors of $\alpha$.
\end{theorem}

\begin{proof}[\bf Proof]
We prove this exactly as before for Theorem 7.13, but use Lemma 7.15 to get started.
\end{proof}

\begin{remark}
If $V$ is a real inner product space and $\alpha \in End(V )$ is orthogonal, there need not be a basis of eigenvectors. But Example Sheet 4 Question 14 shows that if $\alpha$ is orthogonal then there exists an orthonormal basis $B$ of $V$ such that
\be
[\alpha]_B =\bepm
1 & 0 & & & & &\\
0 & 1 & & & & &\\
& & -1 & 0 & & &\\
& & 0 & -1 & & &\\
& & & & \cos \theta_1 & \sin \theta_1 & \\
& & & & -\sin \theta_1 & \cos \theta_1 & \\
& & & & & & \ddots
\eepm
\ee
for some $\theta_i \in \R$.
\end{remark}

\begin{remark}
Let $A$ be a real symmetric (resp. complex Hermitian) matrix. Regard it as an endomorphism on the inner product space $\R^n$ (resp. $\C^n$) with the usual dot product. There exists an orthonormal basis $v_1, \dots, v_n$ of eigenvectors of $A$. The matrix $P = (v_1, \dots, v_n)$ is orthogonal (resp. unitary) and $AP = PD$ with $D$ diagonal. Hence $P^{-1}AP = D = P^TAP$ (resp. $\ol{P}^TAP = D$).
\end{remark}

\begin{proposition}
Let $\psi$ be a real symmetric bilinear (resp. complex Hermitian) form on a real (resp. complex) vector space $V$. Let $A$ be its matrix with respect to any basis $B$ of $V$. Then the signature of $\psi$ is the difference of the number of positive and the number of negative eigenvalues of $A$.
\end{proposition}

\begin{proof}[\bf Proof]
The matrix $A$ is symmetric (resp. Hermitian), so by the previous remark there exists an orthogonal (resp. unitary) matrix $P$ such that $P^{-1}AP = D = P^TAP$ (resp. $\ol{P}^TAP = D$). The assertian now follows.
\end{proof}

\begin{remark}
Let $A$ be a real symmetric $n \times n$ matrix, let $V$ be a real vector space of dimension $n$ with a basis $B$. We can define $\alpha \in End(V )$ with $[\alpha]_B = A$ and a bilinear symmetric form $\psi$ with $[\psi]_B = A$. Changing to a different basis $C$ will do different things to $A$. But let $\inner{}{}$ be the inner product on $V$ such that $B$ is orthonormal. (Define $\inner{e_i}{e_j} = \delta_{ij}$ and extend.) If $C$ is also orthonormal with respect to $\inner{}{}$ and $P$ is the change of basis matrix, then $P^{-1} = P^T$ and $P^{-1}AP = P^TAP$, so $[\alpha]_C = [\psi]_C$.
\end{remark}

\begin{theorem}[Simultaneous diagonalisation of quadratic forms]
Let $\psi$ and $\phi$ be symmetric bilinear (resp. Hermitian) forms on a real (resp. complex) vector space $V$. Assume $\psi$ is positive definite. There exists a basis of $V$ with respect to which both $\psi$ and $\phi$ are diagonal.
\end{theorem}

\begin{proof}[\bf Proof]
Fix any basis and let $A,C$ be the matrices representing $\psi, \phi$. Now diagonalise $\psi$. For some non-singular matrix $P$, $P^TAP = I$ as $\psi$ is positive definite. Now $P^TCP$ is symmetric (resp. Hermitian) so for some orthogonal matrix $Q$, $Q^TP^TCPQ$ is diagonal. Then $Q^TP^TAPQ = Q^T IQ = I$. Write
\be
Q^TP^TCPQ = D = \bepm
d_1 & & 0\\
& \ddots &\\
0 & & d_n
\eepm.
\ee

In fact, the diagonal entries $d_1, \dots, d_n$ of $D$ are the roots of the polynomial $\det(C -tA)$. They are certainly roots of $\det(D - tI)$ and 
\be
\det(D - tI) = \det((PQ)^2(C - tA)(PQ)) = (\det(PQ))^2 \det(C - tA),
\ee
so $\det(D - tI)$ and $\det(C - tA)$ have the same roots.
\end{proof}

We now provide a second proof of Proposition 7.10 in case of a real inner product space $V$.

\begin{proof}[\bf Proof]
Consider $\alpha \in End(V )$, $\alpha^* \in End(V )$, $\inner{\alpha(v)}{w} = \inner{v}{\alpha^*(w)}$. Fix $w \in V$; the map $\phi(w) : V \to \F$, $v \mapsto \ol{\inner{v}{w}}$ is a linear functional on $V$. The map $\ol{V} \to V^*$, $w \mapsto \phi(w)$ is an isomorphism. ($\ol{V}$ is as $V$, but $\lm \cdot v = \ol{\lm}v$.) Hence any linear functional on $V$ can be written for some unique $w' \in V$ as $v\mapsto \inner{v}{w'}$.

Fix $w \in V$; now $v \mapsto \ol{\inner{\alpha(v)}{w}}$ is a linear functional on $V$, so there is a unique $w' = \alpha^*(w)$ with $\inner{\alpha(v)}{w}= \inner{v}{\alpha^*(w)}$ for all $v,w \in V$. Finally, check that $\alpha^*$ is linear.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problems}

\begin{problem}
Let $\R^\R$ be the vector space of all functions $f : \R \to \R$, with addition and scalar multiplication defined pointwise. Which of the following sets of functions form a vector subspace of $\R^\R$?
\ben
\item [(a)] The set $C$ of continuous functions.
\item [(b)] The set $\{f \in C : \abs{f(t)}\leq 1\text{ for all }t \in [0, 1]\}$.
\item [(c)] The set $\{f \in C : f(t) \to 0 \text{ as }t \to \infty\}$.
\item [(d)] The set $\{f \in C : f(t) \to 1 \text{ as }t \to \infty\}$.
\item [(e)] The set of solutions of the differential equation $\ddot{x}(t) + (t^2 - 3)\dot{x}(t) + t^4x(t) = 0$.
\item [(f)] The set of solutions of $\ddot{x}(t) + (t^2 - 3)\dot{x}(t) + t^4x(t) = \sin t$.
\item [(g)] The set of solutions of $(\dot{x}(t))^2 - x(t) = 0$.
\item [(h)] The set of solutions of $(\ddot{x}(t))^4 + (x(t))^2 = 0$.
\een
\end{problem}

\begin{solution}[\bf Solution.]
The $\R^\R$ is a vector space under pointwise addition and multiplication of functions 
\be
(f+g)(x) := f(x) + g(x),\quad\quad (\lm f)(x) := \lm (f(x))
\ee

The with these definitions $(F(\R,\R),+)$ is clearly an abelian group and the distributive laws are satisfied for the multiplication. The remaining vector space axiom are easily checked.

\ben
\item [(a)] $C$: continuous functions from $\R$ to $\R$. Definition continuity $\forall \ve >0$, $\exists \delta >0$, $\abs{x-y}<\delta$, such that $\abs{f(x)-f(y)} <\ve$. 

Given $\abs{f(x)-f(y)}<\frac {\ve}2$ and $\abs{g(x)-g(y)}<\frac {\ve}2$ so that by triangle inequality 
\be
\abs{f(x)+g(x) - f(y)-g(y)} <\frac{\ve}2 + \frac{\ve}2 \ \ra \ \abs{(f+g)(x)- (f+g)(y)} <\ve
\ee

BY similiar argument for $\lm f$ it follows that $C$ is closed. The zero map $0: x\mapsto 0$ is continuous as if $(-f):x\mapsto -f(x)$. So $C$ is a vector space.

\item [(b)] $V:\bra{f\in C:\abs{f}\leq 1, \forall t\in [0,1]}$. Closure under + fails for $f(t)=1$ since $f(t)+f(t) = 2$. So $V$ is not a vector space.

\item [(c)] $V:\bra{f\in C:f(t)\to 0\text{ as }t\to\infty}$. So $V$ is a vector space.
\item [(d)] $V:\bra{f\in C:f(t)\to 1\text{ as }\to \infty}$. $f(t) + 0(t) = f(t)$ where $0(t)=0$ but $0(t)\to 0$ as $t\to \infty$, so no identity function. So $V$ is not a vector space.

\item [(e)] $V:\bra{f\in C:f \text{ solves }\ddot{x}(t) + (t^2-3)\dot{x}(t) + t^4 x(t) = 0}$. If $f\in V$ and $g\in V$ then $\lm f+\mu g\in V$ and $0\in V$ where $0(t)=0$. So $V$ is a vector space.

\item [(f)] $V:\bra{f\in C:f \text{ solves }\ddot{x}(t) + (t^2-3)\dot{x}(t) + t^4 x(t) = \sin t}$. If $f\in V$ then $\lm f \notin V$ since it solves 
\be
(\lm\ddot{x})(t) + (t^2-3)(\lm \dot{x})(t) + t^4 (\lm x)(t) = \lm \sin t.
\ee

So $V$ is not a vector space.

\item [(g)] $V:\bra{f\in C:f \text{ solves }\bb{\ddot{x}(t)}^2 - x(t) = 0}$. If $f\in V$ and $f\neq 0$ then $\exists \lm f\notin V$ since it solves $\lm\bb{\ddot{x}(t)}^2 - x(t) =0$ since non trivial solutions exists. So $V$ is not a vector space.

\item [(h)] $V:\bra{f\in C:f \text{ solves }\bb{\ddot{x}(t)}^4 + \bb{x(t)}^2 = 0}$. Solution are real therefore $\ddot{x} = 0$ and $x(t)=0$. If the function is also real analytic then can use a Taylor expansion to show that the function must be the zero map. Since any continuous function may be approximated arbitrarily closely by a sequence of real analytic functions any continuous solution is approximated arbitrarily closely by a sequence of zero maps and therefore is (by continuity) itself zero. Therefore there are no non-trivial solutions of this equation. So $V=\bra{0}$ is a vector space.
\een
\end{solution}


\begin{problem}
Suppose that the vectors ${\bf e}_1, \dots, {\bf e}_n$ form a basis for $V$. Which of the following are also bases?
\ben
\item [(a)] ${\bf e}_1 + {\bf e}_2, {\bf e}_2 + {\bf e}_3, \dots, {\bf e}_{n-1} + {\bf e}_n, {\bf e}_n$;
\item [(b)] ${\bf e}_1 + {\bf e}_2, {\bf e}_2 + {\bf e}_3, \dots, {\bf e}_{n-1} + {\bf e}_n, {\bf e}_n + {\bf e}_1$;
\item [(c)] ${\bf e}_1 - {\bf e}_2, {\bf e}_2 - {\bf e}_3, \dots, {\bf e}_{n-1} - {\bf e}_n, {\bf e}_n - {\bf e}_1$;
\item [(d)] ${\bf e}_1 - {\bf e}_n, {\bf e}_2 + {\bf e}_{n-1}, \dots, {\bf e}_n + (-1)^n{\bf e}_1$.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(a)] Subtract from RHS $\bbe_n$ vector from $\bbe_{n-1}+\bbe_n$ vector to obtain $\bbe_{n-1}$. Continue to obtain all $n$ standard basis vectors for this $n$ dimensional space. So it is a basis.
\item [(b)] Take alternating sum of the first $n-1$ vectors to obtain
\be
\left\{\ba{ll}
\bbe_1 + \bbe_n \quad\quad & n \text{ even}\\
\bbe_1 - \bbe_n \quad\quad & n \text{ odd}
\ea\right.\qquad\ra\qquad 
\left\{\ba{ll}
\text{linearly dependent} \quad\quad & n \text{ even}\\
\text{basis} \quad\quad & n \text{ odd}
\ea\right.
\ee
\item [(c)] Take sum of the first $n-1$ vectors to obtain $\bbe_1 - \bbe_n$ therefore it is not basis.
\item [(d)] If $n$ is odd then the first and last vectors are anti-parallel. Therefore it is not basis.

If $n$ is even then terms of the form $\bbe_1 \pm \bbe_n, \bbe_2 \pm \bbe_{n-1},\dots $ all occurs so that it is possible to solve in pairs for the standard basis. So it is basis.
\een
\end{solution}


\begin{problem}
Let $V$ be a vector space over a field $\F$.
\ben
\item [(i)] Describe a procedure for picking vectors in $V$ that produces {\bf either} a finite basis for V {\bf or} an infinite linearly independent subset of $V$.
\item [(ii)] Show that $V$ is finite dimensional if and only if every linearly independent subset $S \subset V$ is finite.
\item [(iii)] Deduce that a subspace of a finite dimensional vector space is always finite dimensional.

[Although it is true that every vector space $V$ has a basis, this is only proved in lectures for $V$ finite dimensional. It would not be reasonable to quote the more general result in answering this question.]
\een
\end{problem}

\begin{solution}[\bf Solution.]
Pick a basis vector $\bbe_1$. Pick $\bbe_2$ such that $a_1 \bbe_1 + a_2\bbe_2 = 0$ has no nontrivial solutions. Pick $\bbe_n$ such that $a_1\bbe_1 + \dots + a_n \bbe_n = 0$ has no nontrivial solutions.

If the process terminates because there are no more such vectors to find then the space is finite dimensional and of that dimension by definition. Otherwise the space is infinite dimensional and the process fails to terminate.
\ben
\item [(i)] If follow that every linearly independent set defines a finite subspace. Since every non zero vector is an linearly independent set by itself, it follows that the combined linear span of all the finite subspaces must be the whole infinite dimensional space. Now start choosing vector to build an linearly independent set. Note that this process can be continued inductively and can never terminate. If it terminated then there would be a maximal linearly independent subset which spanned the whole space and the space would be finite dimensional. Therefore we can construct a countable linearly independent set which might not span the space. This contradicts the claim that all linearly independent subsets are finite so the space must also be finite.
\item [(ii)] Evert linearly independent set generates a subspace. If the vector space is finite then it has a basis and all the vectors can be written in terms of this basis. Each elmenet of the linearly independent subset can be written in terms of this basis. If there were more vectors in the linearly independent set than there are in the basis for the whole space then the latter basis would not be linearly independent which is a contradiction. Therefore the linearly independent set is finite and less than or equal in number to the number of vectors in the basis for the whole vector space.
\item [(iii)] Therefore the linearly independent set is finite and less than or equal in number to the number of vectors in the basis for the whole vector space.
\een
\end{solution}


\begin{problem}
Let $T$, $U$ and $W$ be subspaces of $V$.
\ben
\item [(i)] Show that $T \cup U$ is a subspace of $V$ iff either $T \leq U$ or $U \leq T$.
\item [(ii)] Give explicit counter-examples to the following statements:
\be
\text{(a)}\ T + (U \cap W) = (T + U) \cap (T + W);\quad\quad \text{(b)}\ (T + U) \cap W = (T \cap W) + (U \cap W).
\ee
\item [(iii)] Show that each of the equalities in (ii) can be replaced by a valid inclusion of one side in the other.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben 
\item [(i)] Assume $T$ and $U$ are subspace of the vector space $V$ and $T\leq U$. $T\cup U$ is obviously a subspace of $V$.

Assume $T$ and $U$ and $T\cup U$ are subspaces of $V$. Consider $t\in T$ and $u\in U$ $\ra$ $t\in T\cup U$ and $u \in T\cup U$ $\ra$ $t+u \in T\cup U$ since this is a subspace. Hence $t+u \in T$ or $t+u \in U$. Suppose $t+u \in T$ then $t+u - t \in T$ since $T$ is a subspace and so $u\in T$ and $U\leq T$. By symmetry $T\leq U$ or $U\leq T$.

Note: Talk about Venn Diagrams and the difference between set and linear structure.

\item [(ii/iii)] Note that (a) and (b) are true as set theoretic statements with the $+$ interpreted as a set direct sum namely as union. If $T$, $U$ and $W$ are subspaces of the vector space $V$.

\ben
\item [(a)] $T+(U\cap W) \stackrel{?}{=} (T+U)\cap (T+W)$.

Counterexample. Let $U$ be the $x$-axis and $W$ be the $y$-axis in $\R^2$ and $T$ be any other line in $\R^2$ through the origin, say the line $x=y$. Then $T+U = \R^2$ and $T+W = \R^2$, $\ra$ $(T+U)\cap (T+W) = \R^2\cap \R^2 = \R^2$, but 
\be
U\cap W =\bra{0} \ \ra\ T+(U\cap W) = T\ \ra \ \exists x\in (T+U)\cap (T+W) \text{ s.t. }x\notin T \text{so at best containment.}
\ee

Checking Containment. Let $x\in T+(U+W)\ \ra\ x= t+u$ s.t. $t\in T$ and $u\in U\cap W$, then $x\in T+U$ and $x\in T+W$. Hence $T+(U\cap W) \leq (T+U)\cap (T+W)$.

\item [(b)] $(T+U)\cap W \stackrel{?}{=} (T\cap W) + (U\cap W)$.

Counterexample. Let $T$ be the $x$-axis and $U$ be the $y$-axis in $\R^2$ and $W$ be any other line in $\R^2$ through the origin, say the line $x=y$. Then $T+U = \R^2$ then $(T+U)\cap W = W \ \ra \ T\cap W = \bra{0} \text{ and } U\cap W = \bra{0}$
\be
\ra\ \exists x\in (T+U)\cap W =\bra{0} \text{ s.t. } x \notin (T\cap W) + (U \cap W) \text{ so at best containment.}
\ee

Checking Containment. Let $x\in (T\cap W)+(U\cap W)\ \ra\ x= t+u$ s.t. $t\in T$ and $t\in W, u \in U$ and $u \in W \ \ra \ x\in T+U$ and $x\in W$. Hence $(T+U)\cap W \geq (T\cap W)+(U \cap W)$.
\een
\een
\end{solution}


\begin{problem}
For each of the following pairs of vector spaces $(V,W)$ over $\R$, either give an isomorphism $V \to W$ or show that no such isomorphism can exist. [Here $P$ denotes the space of polynomial functions $\R \to \R$, and $C[a, b]$ denotes the space of continuous functions defined on the closed interval $[a, b]$.]
\ben
\item [(a)] $V = \R^4$, $W = \{\bx \in \R^5: x_1 + x_2 + x_3 + x_4 + x_5 = 0\}$.
\item [(b)] $V = \R^5$, $W = \{p \in P: \deg p \leq 5\}$.
\item [(c)] $V = C[0, 1]$, $W = C[-1, 1]$.
\item [(d)] $V = C[0, 1]$, $W = \{f \in C[0, 1] : f(0) = 0, f \text{ continuously differentiable}\}$.
\item [(e)] $V = \R^2$, $W = \{\text{solutions of }\ddot{x}(t) + x(t) = 0\}$.
\item [(f)] $V = \R^4$, $W = C[0, 1]$.
\item [(g)] $V = P$, $W = \R^\N$.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(a)] $V=\R^4$ and $W = \bra{x\in \R^5:x_1+ x_2 + x_3 + x_4 + x_5 = 0}$. 

Claim: There is an isomorphism defined by: Let $\bbe_i$ for $i=1,2,3,4$ be the standard basis for $V$. Let $\bbf_i$ for $i = 1,2,3,4,5$ be the standard basis for $\R^5$. Then $w_1 = f_1 -f_2$, $w_2 = f_1 -f_3$, $w_3 = f_1 -f_4$, $w_4 = f_1 -f_5$ is a basis for $W$. 

The map $\bbe_i \mapsto \bbf_i$ for $i = 1,2,3,4$ is clearly linear one - one and onto i.e. an isomorphism.

\item [(b)] $V=\R^5$ and $W=\bra{p\in P:\deg(p) \leq 5}$

Claim: There is no isomorphism. $\dim V=5$ and $\dim W=6$. Let $\bbf_i$ for $i=1,2,3,4,5$ be the standard basis for $\R^5$.

\item [(c)] $V=C[0,1]$ and $W = C[-1,1]$

Claim: There is a linear isomorphism given by:
\be
\vp:= C[0,1]\to C[-1,1]:f(t)\mapsto f(2t -1),\qquad \vp^{-1}:= C[-1,1] \to C[0,1]: f(t) \mapsto f\bb{\frac{t+1}2}
\ee

\item [(d)] $V = C[0,1]$ and $W = \bra{C[0,1]:f(0)=0, f\text{ is continuously differentiable}}$

Note: Continuously differentiable means is differentiable and the derivative is continuous.

Claim: There is a linear isomorphism given by
\be
\vp:= V\to W:g\mapsto f(x) = \int^x_0 g(t)dt,\qquad \vp^{-1}:= W\to V: f(x) \mapsto \frac{df(x)}{dx}
\ee

By the fundamental theorem of calculus these are inverse functions and the map is one-one. Clearly the map is linear and onto.

\item [(e)] $V=\R^2$ and $W=\bra{C[0,1]:x\text{ solves }\ddot{x}+x(t) = 0}$

Claim: There is a linear isomorphism given by
\be
\vp:= V\to W:\bbe_1 \mapsto \cos t, \bbe_2 \mapsto \sin t\quad\text{where $e_i$ is the standard basis.}
\ee

\item [(f)] $V=\R^4$ and $W=C[0,1]$

Claim: There is no isomorphism. $\dim V = 4$ but $\dim W$ is not finite.

\item [(g)] $V=P$ and $W=\R^\N$ where $\N$ is the set of natural numbers.

Claim: There is no isomorphism.

$\dim P$ is not finite but it is countable but $\dim W$ is not countable. Easiest to see in terms of the standard basis for $P,(1,0,\dots),(0,1,\dots),(0,0,1,\dots),(0,0,0,1,\dots),dots$. Identify each of the basis elements for $P$ with the same sequence in $W$ then the image is isomorphism $P$. Clearly the sequence $(1,1,1,\dots)$ does not lie in the image. To show that $\R^\N$ does not have a countable basis it is easiest to use a diagonal argument which has been slightly modified to guarantee the linear independence of the diagonal sequence.
\een

{\bf Additional exercise}: Prove that $\R^\N$ does not have a countable basis.

Let $(s_1,s_2,s_3,\dots)$ be a sequence in the space of $\R^\N$ of sequences of real numbers so that each $s_i$ is a sequence $(s_{i_1},s_{i_2},s_{i_3},dots)$. Show that one can define a sequence $t=(t_1,t_2,t_3,\dots)$ with the property that, for each $n$, there exists $k$ such that the $k$-tuple $t=(t_1,t_2,t_3,\dots,t_k)$ does not agree with the first $k$ terms of any linear combination of $(s?)$ Deduce that $\R^\N$ does not have a countable basis. (Hard)
\end{solution}


\begin{problem}

\ben
\item [(i)] If $\alpha$ and $\beta$ are linear maps from $U$ to $V$ show that $\alpha + \beta$ is linear. Give explicit counter-examples to
the following statements:
\be
\text{(a)}\ \im(\alpha + \beta) = \im(\alpha) + \im(\beta);\quad\quad \text{(b)}\ \ker(\alpha + \beta) = \ker(\alpha) \cap \ker(\beta).
\ee
Show that each of these equalities can be replaced by a valid inclusion of one side in the other.
\item [(ii)] Let $\alpha$ be a linear map from $V$ to $V$. Show that if $\alpha^2 = \alpha$ then $V = \ker(\alpha)\oplus \Im(\alpha)$. Does your proof still work if $V$ is infinite dimensional? Is the result still true?
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(i)] 
\ben
\item [(a)] Counterexample. $\im (\alpha + \beta) = \im(\alpha) + \im(\beta)$
\be
\alpha = \bepm 
1 & 0\\
0 & 0
\eepm,\qquad
\beta = \bepm
-1 & 0 \\
0 & 1
\eepm,\qquad
\alpha + \beta = \bepm
0 & 0 \\
0 & 1
\eepm
\ee

\be
\im \alpha = \bepm 
x\\
0
\eepm, \ x\text{-axis}\qquad 
\im \beta = \bepm
-x\\
y
\eepm, \ xy\text{-axis} \qquad
\im (\alpha + \beta) =\bepm
0 \\
y
\eepm, \ y\text{-axis}
\ee

But 
\be
\im (\alpha+\beta) \ y\text{-axis} \ \neq \ xy\text{-axis} \im(\alpha) + \im (\beta).
\ee

Claim: $\im (\alpha+\beta) \leq \im (\alpha) + \im(\beta)$

Suppose $v\in \im (\alpha+\beta) \ \ra \ v\in \im (\alpha)\text{ or }v \in \im (\beta) \text{ or both}$, so
\be
\im (\alpha+\beta) \leq \im (\alpha) + \im(\beta)
\ee

Suppose $\alpha v = w$ and $\beta v = -w $, then $\alpha v + \beta v = 0$. So
\be
v \in \im (\alpha) + \im(\beta),\quad v\notin \im (\alpha+\beta).
\ee

\item [(b)] Claim: $\ker (\alpha+\beta) \leq \ker (\alpha) \cap \ker(\beta)$

Suppose $\alpha v = 0$ and $\beta v = 0$, then $\alpha v + \beta v = 0$. So $\ker (\alpha+\beta)\leq \ker (\alpha) \cap \ker(\beta)$.

Suppose $\alpha v = 0$ and $\beta v = -w$, then $\alpha v + \beta v = 0$, so $\ker (\alpha+\beta) \ngeq \ker (\alpha) \cap \ker(\beta)$.

\een
\item [(ii)] Claim: $\ker (\alpha) \oplus \im(\alpha)$

Assume $\alpha^2 = \alpha$. $\ker \alpha \oplus \im \alpha$ iff $\ker \alpha \cap \im \alpha = \bra{0}$ since $\alpha(\alpha v) = \alpha v$ any $v$ in the image of $alpha$ cannot be in the kernel too.

Claim: The result still holds in infinite dimensions.

The result did not require the use of any properties dependent on or defined with respect to finite dimension.
\een
\end{solution}


\begin{problem}
Let
\be
U = \{\bx \in \R^5 : x_1 + x_3 + x_4 = 0,\ 2x_1 + 2x_2 + x_5 = 0\},\quad W = \{\bx \in \R^5 :x_1 + x_5 = 0,\ x_2 = x_3 = x_4\}.
\ee
Find bases for $U$ and $W$ containing a basis for $U \cap W$ as a subset. Give a basis for $U + W$ and show that
\be
U + W = \{\bx \in \R^5 : x_1 + 2x_2 + x_5 = x_3 + x_4\}.
\ee
\end{problem}

\begin{solution}[\bf Solution.]
Basis for $U\cap W$:
\be
x_1+ x_3 + x_4 = 0,\quad x_1 + x_5 = 0, \quad x_1 + x_2 + \frac 12 x_5 = 0,\quad x_2 = x_3 = x_4 \ \ra \ \text{basis }(-2,1,1,1,2)
\ee

Basis for $U$:
\be
x_1 + x_3 + x_4 = 0,\quad x_1 + x_2 + \frac 12 x_5 = 0\ \ra\ \text{basis }(-2,1,1,1,2),(1,-1,0,-1,0),(1,-1,-1,0,0)
\ee

Basis for $W$:
\be
x_1 + x_5 = 0,\quad x_2 = x_3 = x_4 \ \ra \ \text{basis }(-2,1,1,1,2),(-2,0,0,0,2).
\ee

Equation for $U+W$
\be
U + W = \bra{(-2,1,1,1,2),(1,-1,0,-1,0),(1,-1,-1,0,0),(-2,0,0,0,2)}
\ee

Note $\dim U = 5-2 =3$ and $\dim W = 5-3 = 2$ and from above $\dim U\cap W = 1$. Hence $\dim (U+W) = 3+2-1 = 4$ therefore the set is linearly independent.

We want the normal vector to the plane spanned by the 4 vector above. Let $n=(x_1,x_2,x_3,x_4,x_5)$ then
\be
-2x_1+x_2+x_3 + x_4 + 2x_5 = 0,\quad x_1 -x_2 -x_4 = 0,\quad x_1 -x_2 -x_3 = 0,\quad -2x_1 - 2x_5 = 0
\ee

Solving $n=(-1,-2,1,1,-1)$ hence the equation is, as required, $-x_1 - 2x_2 + x_3 + x_4 - x_5 = 0$.\end{solution}


\begin{problem}
Recall that $\F^n$ has standard basis ${\bf e}_1,\dots,{\bf e}_n$. Let $U$ be a subspace of $F^n$. Show that there is a subset $I$ of $\{1, 2, \dots, n\}$ for which the subspace $W = \langle\{{\bf e}_i : i\in I\}\rangle$ is a complementary subspace to $U$ in $F^n$.
\end{problem}

\begin{solution}[\bf Solution.]
Require $U+W = V$. Let $\bra{f_j}_{j\in J}$ be a basis for $U$, then we can extend this by adding $\bra{e_i}_{i\in I}$ to a basis for $V$ so the space spanned by $\bra{e_i}_{i\in I}$ is the required $W$.
\end{solution}


\begin{problem}
Let $\alpha: U \to V$ be a linear map between two finite dimensional vector spaces and let $W$ be a vector subspace of $U$. Show that the restriction of $\alpha$ to $W$ is a linear map $\alpha|_W : W \to V$ which satisfies 
\be
r(\alpha) \geq r(\alpha|_W) \geq r(\alpha) - \dim(U) + \dim(W).
\ee
Give examples (with $W \neq U$) to show that either of the two inequalities can be an equality.
\end{problem}

\begin{solution}[\bf Solution.]
$\alpha|_W v = 0$ implies $\alpha v = 0$ so $\ker\bb{\alpha|_W v}\leq \ker \alpha$ and $r(\alpha) \geq r(\alpha|_W)$ as required.

Use rank-nullity
\be
r(\alpha|_W) + n(\alpha|_W) = \dim W \ \ra\ n(\alpha|_W) = \dim W - r(\alpha|_W),
\ee
\be
r(\alpha) + n(\alpha) = \dim U \ \ra\ n(\alpha) = \dim U - r(\alpha).
\ee

From above we have $n(\alpha) \geq n(\alpha|_W)$ so $\dim W - r(\alpha) \geq \dim W - r(\alpha|_W)$ and $r(\alpha|_W) \geq r(\alpha) + \dim W - \dim U$ as required.\end{solution}


\begin{problem}
Let $\alpha : \R^3 \to \R^3$ be the linear map given by 
\be
\alpha : \bepm
x_1\\
x_2\\
x_3
\eepm \mapsto \bepm
2 & 1 & 0\\
0 & 2 & 1\\
0 & 0 & 2
\eepm
\bepm
x_1\\
x_2\\
x_3
\eepm.
\ee
Find the matrix representing $\alpha$ relative to the basis 
\be
\bepm
1\\
1\\
1
\eepm
, \bepm
1\\
1\\
0
\eepm, \bepm
1\\
0\\
0
\eepm
\ee
for both the domain and the range.

Write down bases for the domain and range with respect to which the matrix of $\alpha$ is the identity.
\end{problem}

\begin{solution}[\bf Solution.]
{\bf Direct method}. $E$ and $F$ are standard bases.
\beast
\text{New basis for domain }\wt{E}:=\bra{\bepm 1 \\ 1 \\ 1\eepm, \bepm 1 \\ 1 \\ 0\eepm, \bepm 1 \\ 0 \\ 0\eepm},\qquad \text{New basis for range }\wt{F}:= \bra{(1,1,1),(1,1,0),(1,0,0)}.
\eeast

These are the coefficients of the new bases wrt the original standard basis. Want $\wt{T} = B^{-1}T A$ where $A=B$ so that 
\be
\bepm
1 & 1 & 1\\
1 & 1 & 0\\
1 & 0 & 0
\eepm = A \bepm
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\eepm = A.
\ee

The induced basis change on rows is given by the inverse,
\be
\bepm
0 & * & *\\
0 & * & *\\
1 & * & *
\eepm \bepm
1 & 1 & 1\\
1 & 1 & 0\\
1 & 0 & 0
\eepm = \bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm\ \ra \ \bepm
0 & 0 & *\\
0 & 1 & *\\
1 & -1 & *
\eepm \bepm
1 & 1 & 1\\
1 & 1 & 0\\
1 & 0 & 0
\eepm = \bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm
\ee

\be
A^{-1} = \bepm
0 & 0 & 1\\
0 & 1 & -1\\
1 & -1 & 1
\eepm 
\ \ra \
\bepm
0 & 0 & 1\\
0 & 1 & -1\\
1 & -1 & 0
\eepm \bepm
2 & 1 & 0\\
0 & 2 & 1\\
0 & 0 & 2
\eepm \bepm
1 & 1 & 1\\
1 & 1 & 0\\
1 & 0 & 0
\eepm= \bepm
2 & 0 & 0\\
1 & 2 & 0\\
0 & 1 & 2
\eepm \ \ra \ \wt{T} = \bepm
2 & 0 & 0\\
1 & 2 & 0\\
0 & 1 & 2
\eepm.
\ee

{\bf Solution explanation}.

\be
\bbe_1 = \bepm
1\\
1\\
1
\eepm,\qquad 
\bbe_2 = \bepm
1\\
1\\
0
\eepm,\qquad
\bbe_3 = \bepm
1\\
0\\
0
\eepm.
\ee

Describe the transformation in terms of the standard basis on the LHS, and in terms of the new basis on the RHS,

\beast
\bepm
2 & 1 & 0 \\
0 & 2 & 1 \\
0 & 0 & 2
\eepm
\bepm
1 \\
1 \\
1
\eepm
& = &
\bepm
3 \\
3 \\
3
\eepm
= (2)\bepm
1 \\
1 \\
1
\eepm + (1)\bepm
1 \\
1 \\
0
\eepm + (0)\bepm
1 \\
0 \\
0
\eepm,\qquad T\bbe_1 = (2)\bbe_1 + (1)\bbe_2 + (0)\bbe_3\\
\bepm
2 & 1 & 0 \\
0 & 2 & 1 \\
0 & 0 & 2
\eepm
\bepm
1 \\
1 \\
0
\eepm
& = &
\bepm
3 \\
2 \\
0
\eepm
= (0)\bepm
1 \\
1 \\
1
\eepm + (2)\bepm
1 \\
1 \\
0
\eepm + (1)\bepm
1 \\
0 \\
0
\eepm,\qquad T\bbe_2= (0)\bbe_1 + (2)\bbe_2 + (1)\bbe_3\\
\bepm
2 & 1 & 0 \\
0 & 2 & 1 \\
0 & 0 & 2
\eepm
\bepm
1 \\
0 \\
0
\eepm
& = &
\bepm
2 \\
0 \\
0
\eepm
= (0)\bepm
1 \\
1 \\
1
\eepm + (0)\bepm
1 \\
1 \\
0
\eepm + (2)\bepm
1 \\
0 \\
0
\eepm,\qquad T\bbe_3 = (0)\bbe_1 + (0)\bbe_2 + (2)\bbe_3
\eeast

Introduce matrix notation in $T\bbe_2 = t_{j_2}\bbe_j = (0)\bbe_1 + (2)\bbe_2 + (1)\bbe_3$ terms of the new basis.
\be
\bbe_1 = \bepm
1\\
0\\
0
\eepm,\qquad 
\bbe_2 = \bepm
0\\
1\\
0
\eepm,\qquad 
\bbe_3 = \bepm
0\\
0\\
1
\eepm. 
\ee

\beast
T_1 \bbe_1 = (2)\bbe_1 + (1)\bbe_2 + (0)\bbe_3\qquad\qquad 
\bepm
* & * & * \\
* & * & * \\
* & * & * 
\eepm
\bepm
1 \\
0 \\
0
\eepm
& = & (2)\bepm
1 \\
0 \\
0
\eepm + (1)\bepm
0 \\
1 \\
0
\eepm + (0)\bepm
0 \\
0 \\
1
\eepm = \bepm
2 \\
1 \\
0
\eepm,\\
T_1 \bbe_1 = (0)\bbe_1 + (2)\bbe_2 + (1)\bbe_3\qquad\qquad 
\bepm
* & * & * \\
* & * & * \\
* & * & * 
\eepm
\bepm
0 \\
1 \\
0
\eepm
& = &
(0)\bepm
1 \\
0 \\
0
\eepm + (2)\bepm
0 \\
1 \\
0
\eepm + (1)\bepm
0 \\
0 \\
1
\eepm= \bepm
0 \\
2 \\
1
\eepm,\\
T_1 \bbe_1 = (0)\bbe_1 + (0)\bbe_2 + (2)\bbe_3\qquad\qquad 
\bepm
* & * & * \\
* & * & * \\
* & * & * 
\eepm
\bepm
0 \\
0 \\
1
\eepm
& = &
(0)\bepm
1 \\
0 \\
0
\eepm + (0)\bepm
0 \\
1 \\
0
\eepm + (2)\bepm
0 \\
0 \\
1
\eepm = \bepm
0 \\
0 \\
2
\eepm.
\eeast

Thus, $\wt{T} = \bepm
2 & 0 & 0 \\
1 & 2 & 0 \\
0 & 1 & 2
\eepm$.

{\bf New basis}. Find $T$ wrt the new basis.

$\wt{T} = B^{-1}TA = B^{-1}TA \ \ra\ B = TA$. Look for any non-singular matrices $A$ and $B$ which satisfy this relation. Let $A=I$ then $B=T$ which is possible because $T$ is non-singular. Thus, $A=I$ and $B=T$.
\be
\text{Domain column basis: }A=I = \bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm\qquad\qquad \text{Range column basis: }B=T = \bepm
2 & 1& 0\\
0 & 2 & 1\\
0 & 0 & 2
\eepm.
\ee
\end{solution}


\begin{problem}
Let $U_1, \dots,U_k$ be subspaces of a vector space $V$ and let $B_i$ be a basis for $U_i$. Show that the following statements are equivalent:
\ben
\item [(i)] $U = \sum_i U_i$ is a direct sum, i.e. every element of $U$ can be written uniquely as $\sum_i u_i$ with $u_i \in U_i$.
\item [(ii)] $U_j \cap \sum_{i\neq j} U_i = \{0\}$ for all $j$.
\item [(iii)] The $B_i$ are pairwise disjoint and their union is a basis for $\sum_i U_i$.
\een
Give an example where $U_i \cap U_j = \{0\}$ for all $i \neq j$, yet $U_1 + \dots + U_k$ is not a direct sum.
\end{problem}

\begin{solution}[\bf Solution.]
Note: Compare with inclusion-exclusion formula in 1A probability course.

(i) $\ra$ (ii). Let $n_i = \dim U_i$ and for fixed $i$ let $\alpha$ run over the $n_i$ basis vectors $\bbe_{\alpha,i}$ for $U_i$. The total number of basis vectors $\bbe_{\alpha,i}$ is $\sum_i n_i = \sum_i \dim U_i$. Use (i) then $\sum_i n_i = \sum_i \dim U_i = \dim \bb{\sum_i U_i}$. Since the number of vectors $\bbe_{\alpha,i}$ which span the space $\sum_i U_i$ is the same as the dimension of the space then the set of vectors must be linearly independent and hence the $\bbe_{\alpha,i}$ are a basis for $\sum_i U_i$.

Assume for contradiction that there are two expressions for a vector $u$ wrt the basis $\bbe_{\alpha,i}$, namely $u=\sum_{\alpha,i} u_{\alpha,i}\bbe_{\alpha,i}\in \sum_i U_i$ and $u = \sum_{\alpha,i} \wt{u}_{\alpha,i}\bbe_{\alpha,i}\in \sum_i U_i$ and for some index pair $u_{\alpha,i} \neq \wt{u}_{\alpha,i}$. Then $0=\sum_{\alpha,i}\bb{\wt{u}_{\alpha,i}-u_{\alpha,i}}e_{\alpha,i}$ so the basis vectors are not linearly independent. Contradiction. Therefore the expression must be unique.

(ii) $\ra$ (iii). Assume for contradiction that $U_j\cap \bb{\sum_{i\neq j}U_j} \neq \bra{0}$ and consider $x\in \bb{U_j\cap \bb{\sum_{i\neq j}U_i}}$. 

Then $x$ may be written, not necessarily uniquely, as $x=\bbe_j + \bbe_i$ with $\bbe_i \in \sum_{i\neq j}U_i$ and $\bbe_j \neq 0$. Then the vector $x = \bbe_j + \bbe_i$ has two distinct expressions, $x=\bbe_j + \bbe_i$ with $\bbe_j + \bbe_i \in U_j$ and also $x=\bbe_j + \bbe_i$ with $\bbe_j \in U_j$ and $\bbe_i \in U_i$ (think in terms of the implicit summation over basis for each subspace which has been suppressed in the notation). The expression is therefore not unique and hence this gives the required contradiction.

(iii) $\ra$ (i). $U_j\cap \bb{\sum_{i\neq j}U_i} = \bra{0}, \forall j$. For each $j$ pick $\bbe_{\alpha,j}$ a basis for $U_j$ and claim that all the $\bbe_{\alpha,j}$ for all the $j$s give a basis for $V$. Let $n_i = \dim U_i$ be the number of $\bbe_{\alpha, i}$ in the basis for $U_i$. Let $B:=\bra{\bbe_{\alpha,i}}$ for all $i$ and all $\alpha$. The number of elements in $B$ is $\sum_i n_i$.

Suppose $\sum_{\alpha,i} a_{\alpha,i} \bbe_{\alpha,i} = 0$. Fix $j$ and resolve (project) onto the $U_j$ subspace then the relation
\be
U_j \cap \bb{\sum_{i\neq j}U_i} = \bra{0}\ \ra\ \sum_\alpha a_{\alpha,j}\bbe_{\alpha,j}= 0 \ \ra \ a_{\alpha,j} =0,\forall \alpha
\ee
since $\bbe_{\alpha,j}$ is a basis for $U_j$. Repeat this for all $j$, $a_{\alpha,j} =0$, $\forall \alpha,j$.

Therefore the elements of $B$ are linearly independent and span a space of $\dim = \sum_i n_i$. Hence 
\be
\dim \bsa{B} = \dim \bb{\sum_i U_i} \ \ra \ \dim\bb{\sum_i U_i} = \sum_i \dim U_i.
\ee

{\bf Countexample}. 
\be
U_1 = \bepm
1\\
0
\eepm,\qquad
U_2 = \bepm
1\\
1
\eepm,\qquad
U_3 = \bepm
1\\
-1
\eepm,\qquad U_1 \cap U_2 = \bra{0},\qquad U_2 \cap U_3 = \bra{0},
\ee

\be
U_1 \cap \sum_{i=2,3}U_i = \bra{v:v=\bepm
\lm\\
0
\eepm = \mu \bepm
1\\
1
\eepm + \rho\bepm
1 \\
-1 
\eepm} = U_1
\ee

Therefore the fourth condition is satisfied but the stronger the third condition fails.
\end{solution}


\begin{problem}
Let $Y$ and $Z$ be subspaces of the finite dimensional vector spaces $V$ and $W$, respectively. Show that $R = \{\alpha \in L(V,W) : \alpha(Y) \leq Z\}$ is a subspace of the space $\sL(V,W)$ of all linear maps from $V$ to $W$. What is the dimension of $R$?
\end{problem}

\begin{solution}[\bf Solution.]
$0\in R$, $\forall x\in Y$, $(\lm \theta + \mu \vp)(x) := \lm \theta (x) + \mu \vp(x)\in Z$ therefore $R$ is subspace.

The dimension of $\R$:
\be
\bepm
*_{m' \times 1}\\
0_{m''\times 1}
\eepm \in Y,\qquad
\bepm
*_{n' \times 1}\\
0_{n''\times 1}
\eepm \in Z,\qquad 
\bepm
*_{n' \times m'} & *_{n' \times m''}\\
0 & *_{n''\times m''} 
\eepm \bepm
*_{m' \times 1}\\
0_{m''\times 1}
\eepm = \bepm
*_{n' \times 1}\\
0_{n''\times 1}
\eepm\in Z.
\ee

Let $\dim Y = m'$, $\dim Z = n'$, $\dim V = m$, $\dim W = n$. Then $m'' = m-m' = \dim V - \dim Y$, $n'' = n - n' = \dim W - \dim Z$. The diagram shows the required form of the matrix.
\be
\dim R = n' \times m' + n' \times m'' + n'' \times m'',
\ee
\be
\dim R = \dim Z \times \dim Y + \dim Z \times (\dim V - \dim Y) + (\dim W - \dim Z)\times (\dim V - \dim Y).
\ee

Thus, $\dim R = \dim Z \times \dim Y + \dim W \times (\dim V - \dim Y)$.
\end{solution}


\begin{problem}
Let $V$ be a vector space over $F$ and let $W$ be a subspace. Show that there is a natural way in which the quotient group $V/W$ is a vector space over $\F$. Show that if the dimension of $V$ is finite, then so is the dimension of $V/W$, and 
\be
\dim V = \dim W + \dim V/W.
\ee
\end{problem}

\begin{solution}[\bf Solution.]
Claim: there is a natural way in which the quotient group $V/W$ is a vector space over $\F$

Define the points, $[x]$, in the quotient by the equivalence classes, $[x] = \bra{x+W}$ where $\bra{x+W}$ is the combined linear span of $x$ with $W$ and where $x$ is any point in $V$.

Claim: The points of the quotient are well defined.
\be
[x] =[y] \text{ iff }\bra{x+W} = \bra{y+W} \text{ iff }x-y \in W, [0]=[W].
\ee

So the identification defines an equivalence relation on the space $V$ and provides a well defined map
\be
q:V\to V/W:x \mapsto [x].
\ee

Notice that the quotient map is linear. Hence
\be
[x+y] = \bra{x+y + W} = \bra{x+W + y + W} = \bra{x+W} + {y+W} = [x] +[y],\quad -[x] = [-x].
\ee

So the quotient forms an additive group with the addition inherited from the larger space. The quotient is natural because the construction was independent of any choice of basis. The only concepts needed were the categorical concepts of vector subspace and linear map.

(If $V$ is a vector space then we can make it into a group by considering rotations about the origin and/or reflections in planes through the origin. Clearly elements which keep all but the space $W$ fixed are closed under composition and form a normal subgroup. So the quotient is well defined.)

Claim: $\dim V/W = \dim V - \dim W$.

Let $\bra{f_i}_{i=1,\dots,m}$ be a basis for $W$ and complete this by adding $\bra{\bbe_i}_{i=m+1,\dots,m+n}$ to a basis for $V$. Then by construction the quotient map gives the basis, $\bra{[e_i]}_{i=m+1,\dots,m+n}$ for $V/W$.

Also $\bra{[f_i]}_{i=1,\dots,m} = [0]$ since all these basis vector are in the kernel of the quotient map. Hence $\dim V/W = \dim V - \dim W$.
\end{solution}


\begin{problem}
Suppose $X$ and $Y$ are linearly independent subsets of a vector space $V$; no member of $X$ is expressible as a linear combination of members of $Y$, and no member of $Y$ is expressible as a linear combination of members of $X$. Is the set $X \cup Y$ necessarily linearly independent? Give a proof or counterexample.
\end{problem}

\begin{solution}[\bf Solution.]
Claim $X\cup Y$ may be linearly dependent
\be
X=\bra{i+j,i+k},\qquad Y = \bra{i-j,i-k},\qquad X\cup Y = \bra{i+j,i+k,i-j,i-k}.
\ee
\end{solution}


\begin{problem}
Show that any two subspaces of the same dimension in a finite dimensional vector space have a common complementary subspace. [You may wish to consider first the case where the subspaces have dimension one less than the space.]
\end{problem}

\begin{solution}[\bf Solution.]
Require $T+W = V$ and $U+W = V$ (definition of direct complement) 

Let $\bbe_i$ be a basis for $T\cap U$ complete with $t_i$ to a basis for $T$ and with $u_i$ to a basis for $U$. Define $W=\Span\bra{t_i + u_i}$. Since $\dim T = \dim U$ this is well defined and $W$ is the required direct complement.

Illustration for Codimension = 1:

Take two hyperplanes $\Pi_1$ and $\Pi_2$ such that $\Pi_1\oplus \bsa{v_1} =V$ and $\Pi_2 \oplus \bsa{v_2} = V$. 

Now if $\bsa{v_2} \nsubseteq \Pi_i$ take $\Pi_1 \oplus \bsa{v_2} = V$ and $\Pi_2 \oplus \bsa{v_2} = V$.

Now if $\bsa{v_2} \subseteq \Pi_i$ take $\bsa{\Pi_1} \oplus \bsa{v_1+v_2} = V$ and $\Pi_2 \oplus \bsa{v_1 +v_2} = V$.

In general,
\be
U_1 = \bsa{\bbe_1,\dots,\bbe_2},\quad V= \bsa{\bbe_1,\dots,\bbe_s}\oplus \bsa{g_{s+1},\dots,g_n},\quad U_2 = \bsa{\bbf_1,\dots,\bbf_2},\quad V= \bsa{\bbf_1,\dots,\bbf_s}\oplus \bsa{h_{s+1},\dots,h_n}
\ee

Now consider $\bsa{g_{s+1}+h_{s+1},g_{s+2} + h_{s+2},\dots, g_n + h_n}$.
\end{solution}


\begin{problem}
(Another version of the Steinitz Exchange Lemma.) Let $\{\bx_1, \bx_2, \dots,\bx_r\}$ and $\{\by_1, \by_2, \dots, \by_s\}$ be linearly independent subsets of a vector space $V$, and suppose $r \leq s$. Show that it is possible to choose distinct indices $i_1, i_2, \dots, i_r$ from $\{1, 2, \dots, s\}$ such that, if we delete each $\by_{ij}$ from $Y$ and replace it by $\bx_j$, the resulting set is still linearly independent. Deduce that any two maximal linearly independent subsets of a finite dimensional vector space have the same size.
\end{problem}

\begin{solution}[\bf Solution.]
Claim:
\ben
\item [(a)] If the sets are disjoint then can replace anything with anything.
\item [(b)] If the sets are not disjoint then use the construction below:

Consider $x_1 = \sum_k a_{1k}y_k$. Pick $y_k$ s.t. $a_{1k} \neq 0$ and $a_{1m} =0$, $\forall m <k$. Let $i_1 = k$ then $a_{1,i_1}y_{i_1} = \bb{\sum_{k\neq i}a_{1k}y_k} - x_1$ .

Define $S_1 = \bra{x_1,y_k \text{ s.t. }k = 1,2,\dots, \wh{i},\dots,s}$ where the $\wh{i}$ means omit $i$. Assume $S_1$ is not linearly independent then $\exists b_{1k}$ s.t. $\bb{\sum_{k\neq i_1} b_{1k}y_k} + b_{1i_1} x_1 = 0$ with the coefficients not all zero. Hence $\bb{\sum_{k\neq i_1} b_{1k}y_k} + b_{1i_1} \sum_k a_{1k} y_k = 0$, then
\be
\bra{y_k \text{ s.t. }k=1,2,\dots s} \text{ are not linearly independent.}
\ee

Contradiction. Then $S_1$ is linearly independent. Continue similarly by induction contructing $S_1,S_2,\dots,S_r$. Then $S_r$ is the required set.
\een

Claim: Maximally linearly independent subsets have the same size.

A maximally linear independent subset forms a basis for the space. Assume for contradiction and wlog that there are more $y_k$ than $x_k$. Then the set $S_r$ obtained at the end of the exchange process contains all the $x_k$ and a few remaining $y_k$, but since the $x_k$ are maximally linearly independent the remaining $y_k$ may be written in terms of the $x_k$ which contradicts the linearly independent of the set $S_r$. Contradiction. Therefore the number of elements in the two sets is the same.
\end{solution}


\begin{problem}
Let $\F_p$ be the field of integers modulo $p$, where $p$ is a prime number. Let $V$ be a vector space of dimension $n$ over $\F_p$. How many vectors are there in $V$? How many (ordered) bases? How many automorphisms does $V$ have? How many $k$-dimensional subspaces are there in $V$?
\end{problem}

\begin{solution}[\bf Solution.]
Claim: There are $p^n$ vectors in $V$?

Any vector may take any one of the $p$ different lengths. There are $n$ dimensions and therefore $p^n$ distinct vectors.

Claim: There are $\frac{(p^n-1)\times (p^n-p)\times (p^n-p^2) \times \dots \times (p^n-p^{n-1})}{n!}$ bases in $V$.

There are $p^n$ vectors in $V$ and therefore $(p^n-1)$ choices for the first vector since we exclude the zero vector. Remove the direction defined by the chosen vector. This leaves $(p^n-p)$  possible choices for the second vector. Remove the direction defined by the chosen vector. This leaves $(p^n-p^2)$ possible choices for the third vector.

Continue inductively obtaining, $(p^n-1)\times (p^n-p)\times (p^n-p^2) \times \dots \times (p^n-p^{n-1})$ possible choices of basis vectors, in a specified order. Therefore divide by the permutations of $n$ objects to obtain, $\frac{(p^n-1)\times (p^n-p)\times (p^n-p^2) \times \dots \times (p^n-p^{n-1})}{n!}$.


Claim: There are $\frac{(p^n-1)\times (p^n-p)\times (p^n-p^2) \times \dots \times (p^n-p^{n-1})}{n!}$ automorphisms in $V$.

The number of automorphisms is the number of possible changes of basis. An automorphism may be defined entirely by its image on any given basis. Pick any basis and the number of automorphism is the number of images of this basis. All are distinct since the bases in the liest are distinct.
\end{solution}


\begin{problem}
\ben
\item [(i)] Let $\alpha : V \to V$ be an endomorphism of a finite dimensional vector space $V$. Show that
\be
V \geq \im(\alpha) \geq \im(\alpha^2) \geq \dots\quad \text{ and }\quad\{0\} \leq \ker(\alpha) \leq \ker(\alpha^2) \leq \dots
\ee
If $r_k = r(\alpha^k)$, deduce that $r_k \geq r_{k+1}$. Show also that $r_k - r_{k+1} \geq r_{k+1} - r_{k+2}$. [Consider the restriction
of $\alpha$ to $\im(\alpha^k)$.] Deduce that if, for some $k \geq 0$, we have $r_k = r_{k+1}$, then $r_k = r_{k+\ell}$ for all $\ell\geq 0$.

\item [(ii)] Suppose that $\dim(V) = 5$, $\alpha^3 = 0$, but $\alpha^2 \neq 0$. What possibilities are there for $r(\alpha)$ and $r(\alpha^2)$?
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(i)] By linearity if $\alpha^k v =0$ then $\alpha^{k+1}v = \alpha (\alpha^k v) = 0$ so $r_k \geq r_{k+1}$.

Consider the JCF for the automorphism. If the matrix has a non-singular restriction to some subspace then those vectors will have non zero diagonal entries in the JCF. The corresponding basis vectors will never be mapped to zero.

The remaining vectors correspond to a restriction of the matrix to a subspace on which it it nilpotent. Those vectors will have zero diagonal entries in the JCF. At every application of the map exactly one vector in the 'Jordan block basis' for each of the nilpotent Jordan blocks is in the kernel. After a finite number of applications any given Jordan block with zero eigenvalues is mapped to the kernel and after that point it contribute no new vectors to the kernel. 

Therefore the number of vectors added to the kernel at any application of the map is exactly the number of zero eigenvalue Jordon blocks with surviving vectors. This number decreases to zero and therefore, $r_k - r_{k+1} \geq r_{k+1} - r_{k+2}$.

\item [(ii)] Illustration:
\be
\alpha = \bepm
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
\eepm , \qquad
\alpha^2 = \bepm
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
\eepm ,\ \qquad
\alpha^3 = \bepm
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
\eepm
\ee
with $\ r(\alpha )= 6, r(\alpha^2 )= 3, \ r(\alpha^3 )= 0$.

If $\dim V = 5$, $r(\alpha^3) = 0$, $r(\alpha^2) \neq 0$. Nilpotent so zeros along the diagonal.
\be
\alpha^2 \neq 0 \text{ so at least one Jordon block with }\alpha = \bepm 0 & 1 & 0 \\ 0 & 0 & 1\\ 0 & 0 & 0 \eepm
\ee

\be
\alpha^3 = 0 \text{ so no bigger Jordon block than }\alpha = \bepm 0 & 1 & 0 \\ 0 & 0 & 1\\ 0 & 0 & 0 \eepm
\ee

\be
\alpha = \bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0
\eepm,\quad 
\bepm
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0
\eepm,\quad
\bepm
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 0 & 0
\eepm,\quad 
\bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 0 & 0
\eepm,\quad
\bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 0 & 0
\eepm.
\ee

No other.
\een

Alternative method.
\be
(r_k - r_{k+1}) = \dim \im (\alpha^k) - \dim \im (\alpha^{k+1}) = \dim \im (\alpha^k) - \dim \alpha(\im(\alpha^k)) = \dim (\ker(\alpha)\cap \im(\alpha^k)).
\ee

But $\dim(\im (\alpha^k)) \geq \dim (\im(\alpha^{k+1}))$ so
\be
\dim \bb{\ker(\alpha)\cap \im(\alpha^k)}\geq \dim(\ker(\alpha)\cap \im(\alpha^{k+1})).
\ee

Hence $(r_k- r_{k+1}) \geq (r_{k+1} - r_{k+2})$.
 
\end{solution}


\begin{problem}
(Another proof of the row rank column rank equality.) Let $A$ be an $m\times n$ matrix of (column) rank $r$. Show that $r$ is the least integer for which $A$ factorises as $A = BC$ with $B \in \text{Mat}_{m,r}(F)$ and $C \in \text{Mat}_{r,n}(F)$.

Using the fact that $(BC)^T = C^TB^T$, deduce that the (column) rank of $A^T$ equals $r$.
\end{problem}

\begin{solution}[\bf Solution.]
Claim: $r$ is the least integer for which $A$ factorises as $A=BC$
\be
\underbrace{U}_{\dim = n} \overbrace{\stackrel{C_{m\times r}}{\to} \underbrace{V}_{\dim = r} \stackrel{B_{r\times n}}{\to} }^{\stackrel{A_{m\times n}}{\longrightarrow}}\underbrace{W}_{\dim = m}
\ee

Since $A$ has rank $r$ the image of $C$ has at least rank $r$ and therefore $C$ is surjective.

Since $A$ has rank $r$ the image of $B$ has at least rank $r$ and therefore $C$ is injective.

Clearly the rank of $A$ is the minimal value for which the factorisation is possible.

Claim: Deduce that the column rank of $A^T$ is $r$. $(BC)^T = C^T B^T$.

\be
\underbrace{W^*}_{\dim = m} \overbrace{\stackrel{B^T_{n\times r}}{\to} \underbrace{V^*}_{\dim = r} \stackrel{C^T_{r\times m}}{\to} }^{\stackrel{A^T_{n\times m}}{\longrightarrow}}\underbrace{W^*}_{\dim = n}
\ee

Since $B^T$ has at least rank $r$ and therefore $B^T$ is surjective.

Since $C^T$ has at least rank $r$ and therefore $C^T$ is injective.

Therefore $A^T$ has at column rank $r$.
\end{solution}


\begin{problem}
Write down the three types of elementary matrices and find their inverses. Show that an $n \times n$ matrix $A$ is invertible if and only if it can be written as a product of elementary matrices. Write the following matrices as products of elementary matrices and hence find their inverses.
\be
\bepm
1 & -1 & 0\\
0 & 0 & 1\\
0 & 3 & -1
\eepm,\quad\quad
\bepm
0 & 1 & 0\\
0 & 2 & 1\\
1 & 3 & 0
\eepm.
\ee
\end{problem}

\begin{solution}[\bf Solution.]
(ia) Claim: Permultiplication of a matrix $E(\lm,r,s) = \delta_{ij} + \lm \delta_{ir}\delta_{js}$.

In general we have an Addition of $\lm row(j)$ to $row(i)$. For example,
\be
E(\lm,1,2)A = \bepm
1 & \lm & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm\bepm
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}
\eepm = \bepm
a_{11} + \lm a_{21} & a_{12} + \lm a_{22} & a_{13} + \lm a_{23}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}
\eepm
\ee

Addition of $\lm row(2)$ to $row(1)$. Inverse $E(\lm,r,s)^{-1} = E(\lm,r,s) = \delta_{ij} - \lm \delta_{ir}\delta_{js}$, thus
\be
\det E(\lm,r,s) = \det \bb{\delta_{ij} + \lm \delta_{ir}\delta_{js}} =1.
\ee

(iia) Premuliplication of matrix $E(\mu,r) = \delta_{ij} + (\mu-1) \delta_{ir}\delta_{jr}$.

In general we have a multiplication of the $r$th row by $\mu$. For example,
\be
F(\mu,2) A = \bepm
1 & 0 & 0\\
0 & \mu & 0\\
0 & 0 & 1
\eepm\bepm
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}
\eepm = \bepm
a_{11} & a_{12} & a_{13}\\
\mu a_{21} & \mu a_{22} & \mu a_{23}\\
a_{31} & a_{32} & a_{33}
\eepm
\ee

Multiplication of the second row by $\mu$. Inverse $F(\mu,r)^{-1} = F(1/\mu,r) = \delta_{ij} + (1/\mu -1)\delta_{ir}\delta_{jr}$,
\be
\det F(\mu,r) = \det\bb{\delta_{ij} + (\mu-1)\delta_{ir}\delta_{jr}} = \mu.
\ee

(iiia) Premuliplication of matrix $S(r,s) = \delta_{ij} + \delta_{ir}\delta_{js} - \delta_{ir}\delta_{is} + \delta_{jr}\delta_{is} - \delta_{jr}\delta_{js}$.

Inverse $S(r,s)^{-1} = S(r,s) =  \delta_{ij} + \delta_{ir}\delta_{js} - \delta_{ir}\delta_{is} + \delta_{jr}\delta_{is} - \delta_{jr}\delta_{js}$.

In general we have a interchange of the $i$th row and $j$th row. For example,
\be
S(1,2) A = \bepm
0 & 1 & 0\\
1 & 0 & 0\\
0 & 0 & 1
\eepm\bepm
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}
\eepm = \bepm
a_{21} & a_{22} & a_{23}\\
a_{11} & a_{12} & a_{13}\\
a_{31} & a_{32} & a_{33}
\eepm
\ee

\be
\det S(r,s) = \det\bb{\delta_{ij} + \delta_{ir}\delta_{js} - \delta_{ir}\delta_{is} + \delta_{jr}\delta_{is} - \delta_{jr}\delta_{js}} = 1.
\ee

(ib) Claim: Permultiplication of a matrix $E(\lm,r,s) = \delta_{ij} + \lm \delta_{ir}\delta_{js}$.

In general we have an Addition of $\lm col(i)$ to $col(j)$. For example,
\be
AE(\lm,1,2) = \bepm
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}
\eepm \bepm
1 & \lm & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm= \bepm
a_{11} & a_{12} + \lm a_{11} & a_{13} \\
a_{21} & a_{22} + \lm a_{21} & a_{23}\\
a_{31} & a_{32} + \lm a_{31} & a_{33}
\eepm
\ee

Addition of $\lm col(1)$ to $col(2)$.

(iib) Premuliplication of matrix $E(\mu,r) = \delta_{ij} + (\mu-1) \delta_{ir}\delta_{jr}$.

In general we have a multiplication of the $r$th row by $\mu$. For example,
\be
AF(\mu,2) = \bepm
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}
\eepm \bepm
1 & 0 & 0\\
0 & \mu & 0\\
0 & 0 & 1
\eepm= \bepm
a_{11} & \mu a_{12} & a_{13}\\
a_{21} & \mu a_{22} & a_{23}\\
a_{31} & \mu a_{32} & a_{33}
\eepm
\ee

Multiplication of the second column by $\mu$. 

(iiib) Premuliplication of matrix $S(r,s) = \delta_{ij} + \delta_{ir}\delta_{js} - \delta_{ir}\delta_{is} + \delta_{jr}\delta_{is} - \delta_{jr}\delta_{js}$.

Inverse $S(r,s)^{-1} = S(r,s) =  \delta_{ij} + \delta_{ir}\delta_{js} - \delta_{ir}\delta_{is} + \delta_{jr}\delta_{is} - \delta_{jr}\delta_{js}$.

In general we have a interchange of the $i$th column and $j$th column. For example,
\be
AS(1,2) = \bepm
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}
\eepm \bepm
0 & 1 & 0\\
1 & 0 & 0\\
0 & 0 & 1
\eepm= \bepm
a_{12} & a_{11} & a_{13}\\
a_{22} & a_{21} & a_{23}\\
a_{32} & a_{31} & a_{33}
\eepm
\ee

Claim: A matrix is invertible iff it can be wriiten as a product of elementary matrices.

Assume $A$ is invertible. Consider the invertible matrix $A = \bepm a_{11} & \dots & a_{1n}\\ \vdots & \ddots & \vdots \\ a_{n1} & \dots & a_{nn}\eepm$. Use the following algorithm to reduce this to the identity matrix.

Step (i) A invertible implies one of $a_{j1},j=1,\dots,n$ is non zero.

Use elementary operation $S$ to place one such row in the position of row 1.

Use elementary operation $F$ to rescale the first row so the leading term is 1.

Use elementary operation $E$ to remove all the remaining non-zero entries in the first column.

This gives a matrix of the form,
\be
A' = \bepm
1 & a_{12} & & a_{1n}\\
0 & a_{22} & & \vdots\\
\vdots & \vdots & \ddots & \\
0 & a_{n2} & \dots & a_{nn}
\eepm.
\ee

Step (ii) A invertible implies one of $a_{j2},j=1,\dots,n$ is non zero.

Use elementary operation $S$ to place one such row in the position of row 2.

Use elementary operation $F$ to rescale the first row so the leading term is 1.

Use elementary operation $E$ to remove all the remaining non-zero entries in the second column.

This gives a matrix of the form,
\be
A'' = \bepm
1 & 0 & a_{13} & \dots & \dots & a_{1n}\\
0 & 1 & a_{23} & \dots & \vdots & a_{2n}\\
0 & 0 & \vdots & & & \\
\vdots & & & \ddots & \\ 
\vdots & \vdots & \ddots & \\
0 & 0 & a_{3n} & \dots & \dots& a_{nn}
\eepm.
\ee

Step (ii) Continue inductively to obtain the identity matrix. Therefore the matrix can be written as a combination of the three row type elementary operations.

Assume the matrix can be written as a combination of the three row type elementary operations.

From above we have explicit inverses for the matrices so all the matrices are non-singular and their composition is also invertible.

Claim: $\bepm 1 & 1 & 3\\ 0 & 1 & -1 \\ -1& 0 & 0\eepm$ has inverse $\bepm 0 & 0 & -1\\ 1/4 & 3/4 & 1/4 \\ 1/4 & -1/4 & 1/4\eepm$.

\be
\bepm 0 & 0 & 1\\ 0 & 1 &0 \\ 1& 0 & 0\eepm \bepm 1 & 1 & 3\\ 0 & 1 & -1 \\ -1& 0 & 0\eepm = \bepm -1 & 0 & 0\\ 0 & 1 & -1 \\ 1& 1 & 3\eepm \ \ra\ \bepm -1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & -1\eepm \bepm -1 & 0 & 0\\ 0 & 1 & -1 \\ 1& 1 & 3\eepm  = \bepm 1 & 0 & 0\\ 0 & -1 & 1 \\ -1 & -1 & -3\eepm
\ee

\be
\bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 1 & 0 & 1\eepm \bepm 1 & 0 & 0\\ 0 & -1 & 1 \\ -1 & -1 & -3\eepm = \bepm 1 & 0 & 0\\ 0 & -1 & 1 \\ 0 & -1 & -3\eepm \ \ra\ \bepm 1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 1\eepm \bepm 1 & 0 & 0\\ 0 & -1 & 1 \\ 0 & -1 & -3\eepm  = \bepm 1 & 0 & 0\\ 0 & 1 & -1 \\ 0 & -1 & -3\eepm
\ee

\be
\bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 1 & 1\eepm\bepm 1 & 0 & 0\\ 0 & 1 & -1 \\ 0 & -1 & -3\eepm = \bepm 1 & 0 & 0\\ 0 & 1 & -1 \\ 0 & 0 & -4\eepm \ \ra\ \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & -1/4\eepm \bepm 1 & 0 & 0\\ 0 & 1 & -1 \\ 0 & 0 & -4\eepm = \bepm 1 & 0 & 0\\ 0 & 1 & -1 \\ 0 & 0 & 1\eepm
\ee

\be
\bepm 1 & 0 & 0\\ 0 & 1 & 1 \\ 0 & 0 & 1\eepm \bepm 1 & 0 & 0\\ 0 & 1 & -1 \\ 0 & 0 & 1\eepm = \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1\eepm
\ee

Hence,
\be
\bepm 1 & 0 & 0\\ 0 & 1 & 1 \\ 0 & 0 & 1\eepm \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & -1/4\eepm \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 1 & 1\eepm \bepm 1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 1\eepm \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 1 & 0 & 1\eepm \bepm -1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & -1\eepm \bepm 0 & 0 & 1\\ 0 & 1 &0 \\ 1& 0 & 0\eepm \bepm 1 & 1 & 3\\ 0 & 1 & -1 \\ -1& 0 & 0\eepm = \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1\eepm
\ee

\be
\bepm 1 & 1 & 3\\ 0 & 1 & -1 \\ -1& 0 & 0\eepm^{-1} = \bepm 1 & 0 & 0\\ 0 & 1 & 1 \\ 0 & 0 & 1\eepm \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & -1/4\eepm \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 1 & 1\eepm \bepm 1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & 1\eepm \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 1 & 0 & 1\eepm \bepm -1 & 0 & 0\\ 0 & -1 & 0 \\ 0 & 0 & -1\eepm \bepm 0 & 0 & 1\\ 0 & 1 &0 \\ 1& 0 & 0\eepm  = \bepm 0 & 0 & -1\\ 1/4 & 3/4 & 1/4 \\ 1/4 & -1/4 & 1/4\eepm  
\ee

Checking 
\be
\bepm 1 & 1 & 3\\ 0 & 1 & -1 \\ -1& 0 & 0\eepm\bepm 0 & 0 & -1\\ 1/4 & 3/4 & 1/4 \\ 1/4 & -1/4 & 1/4\eepm  = \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0& 1\eepm  
\ee

Claim: $\bepm 0 & 1 & 0\\ 0 & 2 & 1 \\ 1 & 3 & 0\eepm$ has inverse $\bepm -3 & 0 & 1\\ 1 & 0 & 0 \\ -2 & 1 & 0\eepm $,

\be
\bepm 0 & 1 & 0\\ 0 & 2 & 1 \\ 1 & 3 & 0\eepm \underrightarrow{\text{interchange row 1 and 3}} \bepm 1 & 3 & 0\\ 0 & 2 & 1 \\ 0 & 1 & 0\eepm \underrightarrow{\text{interchange row 2 and 3}} \bepm 1 & 3 & 0\\ 0 & 1 & 0 \\ 0 & 2 & 1\eepm,   
\ee

\be
\bepm 1 & 3 & 0\\ 0 & 1 & 0 \\ 0 & 2 & 1\eepm \underrightarrow{\text{subtract 3 times row 2 from row 1}} \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 2 & 1\eepm \underrightarrow{\text{subtract 2 times row 2 from row 3}} \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1\eepm.   
\ee

To find the inverse apply the same list in the same order to the identity matrix.
\be
\bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1\eepm \underrightarrow{\text{interchange row 1 and 3}} \bepm 0 & 0 & 1\\ 0 & 1 & 0 \\ 1 & 0 & 0\eepm \underrightarrow{\text{interchange row 2 and 3}} \bepm 0 & 0 & 1\\ 1 & 0 & 0 \\ 0 & 1 & 0\eepm,   
\ee

\be
\bepm 0 & 0 & 1\\ 1 & 0 & 0 \\ 0 & 1 & 0\eepm \underrightarrow{\text{subtract 3 times row 2 from row 1}} \bepm -3 & 0 & 1\\ 1 & 0 & 0 \\ 0 & 1 & 0\eepm \underrightarrow{\text{subtract 2 times row 2 from row 3}} \bepm -3 & 0 & 1\\ 1 & 0 & 0 \\ -2 & 1 & 0\eepm.   
\ee

Checking 
\be
\bepm 0 & 1 & 0\\ 0 & 2 & 1 \\ 1 & 3 & 0\eepm \bepm -3 & 0 & 1\\ 1 & 0 & 0 \\ -2 & 1 & 0\eepm   = \bepm 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0& 1\eepm.
\ee\end{solution}


\begin{problem}
Let $\lm \in F$. Evaluate the determinant of the $n \times n$ matrix $A$ with each diagonal entry equal to $\lm$ and all other entries 1. [Note that the sum of all columns of $A$ has all entries equal.]
\end{problem}

\begin{solution}[\bf Solution.]
Claim: $D = (p+n-1)(p-1)^{n-1}$.
\be
D = \left|\ba{cccccccccccc}
p & 1 & 1 & 1 & 1 & 1 & \dots & 1 & 1 & 1 & 1 & 1 \\
1 & p & 1 & 1 & 1 & 1 & \dots & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & p & 1 & 1 & 1 & \dots & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & p & 1 & 1 & \dots & 1 & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & p & 1 & \dots & 1 & 1 & 1 & 1 & 1 \\
&  &  &  &  &  & \ddots &  &  &  &  &  \\
1 & 1 & 1 & 1 & 1 & 1 & \dots & p & 1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & \dots & 1 & p & 1 & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & \dots & 1 & 1 & p & 1 & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & \dots & 1 & 1 & 1 & p & 1 \\
1 & 1 & 1 & 1 & 1 & 1 & \dots & 1 & 1 & 1 & 1 & p 
\ea\right| \quad \ra\quad  D = \left|\ba{cccccccccccc}
p-1 & 0 & 0 & 0 & 0 & 0 & \dots & 0 & 0 & 0 & 0 & 1 \\
0 & p-1 & 0 & 0 & 0 & 0 & \dots & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & p-1 & 0 & 0 & 0 & \dots & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & p-1 & 0 & 0 & \dots & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & p-1 & 0 & \dots & 0 & 0 & 0 & 0 & 1 \\
&  &  &  &  &  & \ddots &  &  &  &  &  \\
0 & 0 & 0 & 0 & 0 & 0 & \dots & p-1 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & \dots & 0 & p-1 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & \dots & 0 & 0 & p-1 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & \dots & 0 & 0 & 0 & p-1 & 1 \\
1-p & 1-p & 1-p & 1-p & 1-p & 1-p & \dots & 1-p & 1-p & 1-p & 1-p & p 
\ea\right|
\ee
by subtracting the last column from each other column. Add each row to the last row to row $n$,
\be
D = \left|\ba{ccccccccccccc}
p-1 & 0 & 0 & 0 & 0 & 0 & &\dots & 0 & 0 & 0 & 0 & 1 \\
0 & p-1 & 0 & 0 & 0 & 0 & &\dots & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & p-1 & 0 & 0 & 0 & & \dots & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & p-1 & 0 & 0 & &\dots & 0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & p-1 & 0 & &\dots & 0 & 0 & 0 & 0 & 1 \\
&  &  &  &  &  & & \ddots & & &  &  &   \\
0 & 0 & 0 & 0 & 0 & 0 & &\dots & p-1 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & &\dots & 0 & p-1 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & &\dots & 0 & 0 & p-1 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & &\dots & 0 & 0 & 0 & p-1 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & &\dots & 0 & 0 & 0 & 0 & p+n-1 \\
\ea\right| = (p-1)^{n-1}(p+n-1)
\ee
by expanding down column 1 etc.
\end{solution}


\begin{problem}
Let $A$ and $B$ be $n \times n$ matrices over a field $F$. Show that the $2n \times 2n$ matrix
\be
C = \bepm
I & B\\
-A & 0 
\eepm \quad \text{can be transformed into }\quad D = 
\bepm
I & B\\
0 & AB 
\eepm.
\ee
by elementary row operations (which you should specify). By considering the determinants of $C$ and $D$, obtain another proof that $\det AB = \det A \det B$.
\end{problem}

\begin{solution}[\bf Solution.]
Consider $C = \bepm
I & B\\
-A & 0 
\eepm$ and add $a_{11}\times $top row to the $a_{11}$ the row then add $a_{21}\times $ newx row to the $a_{21}$ the row then... continue to clear $A$ in the LHS bottom of the matrix and give the matrix
\be
\wt{C} = \bepm
I & B\\
0 & AB 
\eepm
\ee

To show $\det (AB) = \det A \det B$. The determinant is obtained by expanding along the diagonal so $\det\wt{C} = \det(AB)$. The row operations do not affect the determinant therefore $\det C = \det\wt{C} =\det(AB)$. Interchanging rows (or columns) changes the sign of the determinant. Since $A$ and $B$ are both $n\times n$ blocks
\be
\det C = \det \bepm
I & B\\
-A & 0 
\eepm =\det \bb{(-1)^n\bepm
I & B\\
-A & 0 
\eepm} = \det \bb{(-1)^{2n}\bepm
B & -I\\
0 & A 
\eepm}
\ee

From the expansion rule for determinant the $-I$ cannot contribute. Therefore the only contribution comes from each path throught $B$ multiplied by each path through $A$.
\be
\det C = \det A \det B \ \ra \ \det(AB) = \det A \det B.
\ee
\end{solution}


\begin{problem}
\ben
\item [(i)] Let $V$ be a non-trivial real vector space of finite dimension. Show that there are no endomorphisms $\alpha$, $\beta$ of $V$ with $\alpha\beta - \beta\alpha = id_V$.
\item [(ii)] Let $V$ be the space of infinitely differentiable functions $\R \to \R$. Find endomorphisms $\alpha$, $\beta$ of $V$ which do satisfy $\alpha\beta - \beta\alpha = id_V$.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(i)] Assume for contradiction that there exist two endomorphisms $\alpha$ and $\beta$ such that $\alpha \circ \beta - \beta \circ \alpha = I$ then $\tr(\alpha\circ \beta - \beta \circ \alpha) = \tr I$ where $n$ is the dimension.
\be
\tr(\alpha \circ \beta) =\sum^n_{j=1} \sum^n_{i=1}a_{ij}b_{ji} = \sum^n_{j=1} \sum^n_{i=1} b_{ji}a_{ij} = \tr (\beta\circ \alpha),
\ee
Thus
\be
0 = \tr(\alpha\circ \beta - \beta \circ \alpha) = \tr I = n\text{ (Contradiction)}.
\ee
\item [(ii)] Let $\alpha:=x_i \mapsto ix_{i+1}$, $\beta := x_i \mapsto x_{i-1}$,
\be
\alpha\circ \beta = x_i \mapsto ix_i,\quad \beta\circ \alpha = x_i \mapsto (i-1)x_i,\quad \alpha\circ \beta - \beta\circ \alpha = I.
\ee

Let $\alpha:=f\mapsto \frac{df}{dx}$, $\beta:= f\mapsto xf$,
\be
\alpha\circ \beta = f\mapsto f+x\frac{df}{dx},\quad \beta \circ \alpha = f\mapsto x\frac{df}{dx},\quad \alpha\circ \beta - \beta \circ \alpha = I.
\ee
\een
\end{solution}








\begin{problem}
Compute the characteristic polynomials of the matrices
\be
\bepm
0 & 3 & 0\\
1 & 0 & 0\\
0 & 1 & 0
\eepm, 
\bepm
0 & 3 & 2\\
1 & 0 & 0\\
0 & 1 & 0
\eepm,
\bepm
0 & 3 & 4\\
1 & 0 & 0\\
0 & 1 & 0
\eepm.
\ee
Which of the matrices are diagonalisable over $\C$? Which over $\R$?
\end{problem}

\begin{solution}[\bf Solution.]

\be
\bepm
0 & 3 & 0 \\
1 & 0 & 0\\
0 & 1 & 0
\eepm \ \ra\ \det\bepm
-\lm & 3 & 0 \\
1 & -\lm & 0\\
0 & 1 & -\lm
\eepm  = -\lm\bb{\lm + \sqrt{3}}(\lm - \sqrt{3}) = 0 \ \ra\ \lm = 0,\pm \sqrt{3}
\ee
and $\chi(\lm) = \lm(\lm + \sqrt{3})(\lm - \sqrt{3})$. Diagonalisable over $\C$ and $\R$.

\be
\bepm
0 & 3 & 2 \\
1 & 0 & 0\\
0 & 1 & 0
\eepm \ \ra\ \det\bepm
-\lm & 3 & 2 \\
1 & -\lm & 0\\
0 & 1 & -\lm
\eepm  = -\bb{\lm + 1}^2(\lm - 2) = 0 \ \ra\ \lm = -1,-1,2 
\ee
and $\chi(\lm) = (\lm + 1)^2(\lm - 2)$. The eigenvectors:
\be
\lm = -1 :\ \ra\ \bepm 1 & 3 & 2\\ 1 & 1 & 0 \\ 0 & 1 & 1 \eepm \bepm x\\ y\\ z \eepm = 0 \ \ra \ x=-y =z \ \ra\ \Span_\R\bra{\bepm 1 \\
-1\\ 1\eepm}
\ee

Note diagonalisable over $\C$ and $\R$ because the repeated eigenvalue has algebraic multiplicity 2 but geometric multiplicity 1, and there are only 2 linearly independent eigenvectors.


\be
\bepm
0 & 3 & 4 \\
1 & 0 & 0\\
0 & 1 & 0
\eepm \ \ra\ \det\bepm
-\lm & 3 & 4 \\
1 & -\lm & 0\\
0 & 1 & -\lm
\eepm  = -\lm^3 + 3\lm + 4 = 0 
\ee
and $\chi(\lm) = \lm^3 -3\lm - 4$. From a sketch of the characteristic equation, $\chi(\lm) = 0$ we have, stationary points at (-1,-2) and (1,-4). Therefore there is one real root and a complex conjugate pair of roots. Therefore this cannot be diagonalised over the reals but since all the complex eigenvalues are distinct it can be diagonalised over $\C$.
\end{solution}


\begin{problem}
Find the eigenvalues and give bases for the eigenspaces of the following complex matrices:
\be
\bepm
1 & 1 & 0\\
0 & 3 & -2\\
0 & 1 & 0
\eepm,
\bepm
1 & 1 & -1\\
0 & 3 & -2\\
0 & 1 & 0
\eepm,
\bepm
1 & 1 & -1\\
-1 & 3 & -1\\
-1 & 1 & 1
\eepm,
\bepm
0 & 1 & 0\\
0 & 0 & 1\\
1 & 0 & 0
\eepm.
\ee
The second and third matrices commute; find a basis with respect to which they are both diagonal.
\end{problem}

\begin{solution}[\bf Solution.]
Eigenvalues:
\be
\bepm
1 & 1 & 0\\
0 & 3 & -2\\
0 & 1 & 0
\eepm \ \ra \ \det \bepm
1-\lm & 1 & 0\\
0 & 3-\lm & -2\\
0 & 1 & -\lm
\eepm = -(\lm-2)(\lm-1)^2 \ \ra\ \lm = 1,1,2.
\ee

Eigenspaces:
\be
\lm = 1: \ \ra \ \bepm
0 & 1 & 0\\
0 & 2 & -2\\
0 & 1 & -1
\eepm\bepm
x\\
y\\
z
\eepm = \bepm
0 \\
0\\
0
\eepm \ \ra \ \Span_\C\bra{\bepm 1\\ 0\\ 0\eepm}\quad \ra \ \bepm
0 & 1 & 0\\
0 & 2 & -2\\
0 & 1 & -1
\eepm \bepm x\\ y\\ z\eepm = \bepm 1 \\0 \\0\eepm \ \ra \ \Span_\C \bra{\bepm 0\\ 1\\ 1\eepm}
\ee

\be
\lm = 2:\ \ra \ \bepm
-1 & 1 & 0\\
0 & 1 & -2\\
0 & 1 & -2
\eepm \bepm x\\ y\\ z \eepm = \bepm 0\\ 0\\ 0 \eepm \ \ra\ \Span_\C \bra{\bepm 2\\ 2\\ 1 \eepm}
\ee

Conjugate diagonal: This has two distinct eigenvalues and only two distinct eigenvectors and therefore cannot be diagonalised.
\be
P = \bepm
1 & 0 & \frac 23\\
0 & \frac 1{\sqrt{2}} & \frac 23 \\
0 & \frac 1{\sqrt{2}} & \frac 13
\eepm \ \ra \ \bepm
1 & 1 & 0\\
0 & 1 & 0\\
0 & 0 & 2
\eepm
\ee

Eigenvalues:
\be
\bepm
1 & 1 & -1\\
0 & 3 & -2\\
0 & 1 & 0
\eepm \ \ra \ \det \bepm
1-\lm & 1 & -1\\
0 & 3-\lm & -2\\
0 & 1 & -\lm
\eepm = -(\lm-2)(\lm-1)^2 \ \ra\ \lm = 1,1,2.
\ee

Eigenspaces:
\be
\lm = 1: \ \ra \ \bepm
0 & 1 & -1\\
0 & 2 & -2\\
0 & 1 & -1
\eepm\bepm
x\\
y\\
z
\eepm = \bepm
0 \\
0\\
0
\eepm \ \ra \ \Span_\C\bra{\bepm 1\\ 0\\ 0\eepm}\quad \ra \ \bepm
0 & 1 & -1\\
0 & 2 & -2\\
0 & 1 & -1
\eepm \bepm x\\ y\\ z\eepm = \bepm 1 \\0 \\0\eepm \ \ra \ \Span_\C \bra{\bepm 0\\ 1\\ 1\eepm}
\ee

\be
\lm = 2:\ \ra \ \bepm
-1 & 1 & -1\\
0 & 1 & -2\\
0 & 1 & -2
\eepm \bepm x\\ y\\ z \eepm = \bepm 0\\ 0\\ 0 \eepm \ \ra\ \Span_\C \bra{\bepm 1\\ 2\\ 1 \eepm}
\ee

Conjugate diagonal: This has two distinct eigenvalues and three distinct eigenvectors and therefore can be diagonalised.
\be
P = \bepm
1 & 0 & \frac 1{\sqrt{5}}\\
0 & \frac 1{\sqrt{2}} & \frac 2{\sqrt{5}} \\
0 & \frac 1{\sqrt{2}} & \frac 1{\sqrt{5}}
\eepm \ \ra \ \bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 2
\eepm
\ee

Eigenvalues:
\be
\bepm
1 & 1 & -1\\
-1 & 3 & -1\\
1 & 1 & 1
\eepm \ \ra \ \det \bepm
1-\lm & 1 & -1\\
-1 & 3-\lm & -1\\
-1 & 1 & 1-\lm
\eepm = (2-\lm)^2(1-\lm) \ \ra\ \lm = 1,2,2.
\ee

Eigenspaces:
\be
\lm = 1:\ \ra \ \bepm
0 & 1 & -1\\
-1 & 2 & -1\\
-1 & 1 & 0
\eepm \bepm x\\ y\\ z \eepm = \bepm 0\\ 0\\ 0 \eepm \ \ra\ \Span_\C \bra{\bepm 1\\ 1\\ 1 \eepm}
\ee

\be
\lm = 2: \ \ra \ \bepm
-1 & 1 & -1\\
-1 & 1 & -1\\
-1 & 1 & -1
\eepm\bepm
x\\
y\\
z
\eepm = \bepm
0 \\
0\\
0
\eepm \ \ra \ \Span_\C\bra{\bepm 1\\ 1\\ 0\eepm}\quad \ra \ \bepm
-1 & 1 & -1\\
-1 & 1 & -1\\
-1 & 1 & -1
\eepm \bepm x\\ y\\ z\eepm = \bepm 0 \\0 \\0\eepm \ \ra \ \Span_\C \bra{\bepm 0\\ 1\\ 1\eepm}
\ee

Conjugate diagonal: This has two distinct eigenvalues and three distinct eigenvectors and therefore can be diagonalised.
\be
\text{Conjuate diagonal matrix wrt the new basis }
\bra{\bepm
1 \\
1\\
1
\eepm,\bepm
1 \\
1\\
0
\eepm,\bepm
0 \\
1\\
1
\eepm,} \ \ra \ \bepm
1 & 0 & 0 \\
0 & 2 & 0\\
0 & 0 & 2
\eepm,
\ee

Eigenvalues:
\be
\bepm
0 & 1 & 0\\
0 & 0 & 1\\
1 & 0 & 0
\eepm \ \ra \ \det \bepm
-\lm & 1 & 0\\
0 & -\lm & 1\\
1 & 0 & -\lm
\eepm = (\lm-1)(\lm^2 + \lm + 1) \ \ra\ \lm = 1,\frac{-1\pm i\sqrt{3}}2.
\ee

Eigenspaces:
\be
\lm = 1: \ \ra \ \bepm
-1 & 1 & 0\\
0 & -1 & 1\\
1 & 0 & -1
\eepm\bepm
x\\
y\\
z
\eepm = \bepm
0 \\
0\\
0
\eepm \ \ra \ x=y=z \ \ra\ \Span_\C\bra{\bepm 1\\ 1\\ 1\eepm}
\ee

\be
\lm = \frac{-1 + k i\sqrt{3}}2\ (k=\pm 1):\ \ra \ \bepm
-\lm & 1 & 0\\
0 & -\lm & 1\\
1 & 0 & -\lm
\eepm \bepm x\\ y\\ z \eepm = \bepm 0\\ 0\\ 0 \eepm \ \ra\ \lm^2 x = \lm y = z \ \ra \ \Span_\C \bra{\bepm 1\\ \lm \\ \lm^2 \eepm}
\ee

Conjugate diagonal: This has three distinct eigenvalues and therefore three distinct eigenvectors which form a basis for the space. Therefore it may be diagonalised.
\be
P = \frac 1{\sqrt{3}}\bepm
1 & 1 & 1 \\
1 & \lm & \ol{\lm} \\
1 & \lm^2 & \ol{\lm}^2
\eepm \ \ra \ \bepm
1 & 0 & 0\\
0 & \lm & 0\\
1 & 0 & \ol{\lm}
\eepm
\ee

\be
\bepm
-4 & -4 & -4 \\
2 & 0 & 0 \\
0 & 2 & 2
\eepm \bepm
x\\
y\\
z
\eepm = \bepm
0\\
1\\
-1
\eepm \ \ra \ y = z \text{ and } x=-1 \ \ra \ \Span_\C \bra{\bepm
-1\\
1 \\
1 
\eepm}
\ee

Conjugate trianglar matrix wrt the new basis $\bra{\bepm 2\\ -2 \\ 1\eepm, \bepm 0\\ 1 \\ -1\eepm,\bepm -1\\ 1 \\ 1\eepm} \ \ra \ \bepm 0 & 0 & 0 \\ 0 & 2 & 1 \\ 0 & 0 & 2\eepm$.

Common diagonal basis. The matrix $\bepm 1 & 1 & -1 \\ 0 & 3 & -2 \\ 0 & 1 & 0\eepm$ may be diagonalised using the basis change $P=\bepm 1& 0 & \frac 1{\sqrt{5}}\\ 0 & \frac 1{\sqrt{2}} & \frac 2{\sqrt{5}}\\ 0 & \frac 1{\sqrt{2}} & \frac 1{\sqrt{5}} \eepm$ to give the diagonal form $\bepm 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2\eepm$. 

The matrix $\bepm 1 & 1 & -1\\ -1 & 3 & -1 \\ 1 & 1 & 1\eepm$ may be diagonalised by the basis change $P = \bepm \frac 1{\sqrt{3}} & \frac 1{\sqrt{2}} & 0\\\frac 1{\sqrt{3}} & \frac 1{\sqrt{2}} & \frac 1{\sqrt{2}} \\ \frac 1{\sqrt{3}} & 0 & \frac 1{\sqrt{2}}\eepm$ to give $\bepm 1 & 0 & 0 \\ 0 & 2& 0 \\ 0 & 0 & 2\eepm$.

Lemma. There is a basis in which two matrices may be simultaneously diagonalised iff the matrices commute. 

Therefore there is a basis in which both matrices are diagonal with the above form.
\end{solution}


\begin{problem}
Let $V$ be a vector space, let $\pi_1,\pi_2,\dots, \pi_k$ be endomorphisms of $V$ such that $id_V = \pi_1 + \dots + \pi_k$ and $\pi_i\pi_j = 0$ for any $i \neq j$. Show that $V = U_1 \oplus \dots \oplus U_k$, where $U_j = \im(\pi_j)$.

Let $\alpha$ be an endomorphism on the vector space $V$, satisfying the equation $\alpha^3 = \alpha$. Prove directly that $V = V_0 \oplus V_1 \oplus V_{-1}$, where $V_\lm$ is the $\lm$-eigenspace of $\alpha$.
\end{problem}

\begin{solution}[\bf Solution.]
$x=\iota x = (\pi_1 + \dots + \pi_n)x$, $\pi_j x = \pi_j(\pi_1 + \dots + \pi_n)x = \pi_j^2 x$. Therefore $\pi_j$ acts as the identity on $U_j = \im \pi_j$ and as zero on $V-\im \pi_j$. Hence $\im \pi_j \cap (V-\im \pi_j)= \bra{0}$ and the subspaces are in a direct sum.

$\alpha^3 - \alpha = 0 \ \ra \ \alpha (\alpha - I)(\alpha + I) = 0$. Then this is a minimal polynomial. Eigenspaces consisting of different eigenvalues are distinct hence $V =\ker \alpha \oplus \ker(\alpha - I)\oplus \ker (\alpha + I)$.

Let $x\in V$ then $\alpha (\alpha - I)(\alpha + I) x = 0$. Write $x=u+v$, $u\in \ker(\alpha +I)$, $v\notin \ker(\alpha +I)$ then $\alpha (\alpha - I)v = 0$. Write $v=w+r$, $w \in \ker(\alpha -I)$, $r\notin \ker(\alpha+ I)$ then $\alpha r = 0$.

Hence $x = u + w+ r$ and $V = \ker \alpha + \ker(\alpha - I) + \ker(\alpha + I) $. Let $y\in \ker(\alpha + I)$, $y\in \ker \alpha (\alpha - I)$ then $\alpha y = -y$, $\alpha (\alpha - I)y = -2\alpha y = 2y = 0$. Let $y \in \ker(\alpha -I)$, $y \in \ker\alpha (\alpha +I) $ then $\alpha y = y$, $\alpha (\alpha + I)y = 2\alpha y = 2y  =0$. Let $y\in \ker \alpha$, $\ker(\alpha - I)(\alpha + I) $ then $\alpha y = 0$, $ (\alpha - I)(\alpha + I) y = -y = 0$. Hence $V= \ker \alpha \oplus \ker (\alpha - I)\oplus \ker(\alpha + I).$ 
\end{solution}


\begin{problem}
Let $A$ be a square complex matrix of finite order - that is, $A^m = I$ for some $m$. Show that $A$ can be diagonalised. [You can use a theorem.]
\end{problem}

\begin{solution}[\bf Solution.]
Lemma. Find the JCF representing $\C^2$.

First note that a matrix in JCF is block diagonal therefore need only consider an $n\times n$ block of form $J = \bepm \lm & & & 0\\ 1 & \lm & & \\ & \ddots & \ddots & \\ 0 &  & 1 & \lm \eepm$. Define $A = \bepm 0 & & & \\ 1 & 0 & & \\ & \ddots & \ddots & \\ 0 &  & 1 & 0 \eepm$ i.e. $A = J -\lm I$.

Already have a cyclic basis for this space given by $x,(J-\lm I)x,\dots (J-\lm I)^{n-1}x$. Let $v=\sum^{n-1}_{j=0} a_j (J -\lm I)^j x$ and look for eigenvectors of $J$. From theory if there are any then they must have eigenvalue $\lm$.
\be
Jv = \sum^{n-1}_{j=0} a_j (J-\lm I + \lm I)(J-\lm I)^j x,\quad\quad \lm v = \sum^{n-1}_{j=0} a_j \lm (J-\lm I)^j x.
\ee

Equate $Jv = \lm v \ \ra \ \sum^{n-1}_{j=0}a_j\lm (J-\lm I)^j x = \sum^{n-1}_{j=0} a_j(J-\lm I + \lm I)(J-\lm I)^j x$,
\be
0 = \sum^{n-1}_{j=0} a_j (J-\lm I)(J -\lm I)^j x \ \ra \ v\propto (J-\lm I)^{n-1}x \text{ one eigenvector.}
\ee

Now consider $J^2v = \lm^2 v$. Use $J = A+ \lm I \ \ra \ J^2 = A^2 +2\lm A + \lm^2 I$, $J^2 = (J-\lm I)^2 + 2\lm(J-\lm I) + \lm^2 I$. In matrix form 
\be
J^2 = \bepm
\lm^2 & & & 0\\
2\lm & \lm^2 & & \\
1 & \ddots & \ddots &\\
0 & 1 & 2\lm & \lm^2
\eepm
\ee,

\be
J^2v = \lm^2 v \ \ra \ \sum^{n-1}_{j=0}a_j \bb{(J-\lm I)^2 + 2\lm(J-\lm I) + \lm^2 I}(J-\lm I)^j x = \sum^{n-1}_{j=0}a_j \lm^2 (J-\lm I)^j x
\ee
\be
\sum^{n-1}_{j=0}a_j \bb{(J-\lm I)^2 + 2\lm(J-\lm I) }(J-\lm I)^j x = 0.
\ee

Note that the $a_{n-1}$ drops out of the equation.
\be
\sum^{n-2}_{j=0}\bb{a_j(J-\lm I)^{j+2}x + a_j 2\lm(J-\lm I)^{j+1}x } = 0,\quad \sum^{n}_{j=2}a_{j-2} (J-\lm I)^j x + \sum^{n-1}_{j=1} a_{j-1} 2\lm(J-\lm I)^j x = 0
\ee

\be
\ra\ a_0\lm = 0,\quad \sum^{n-1}_{j=2}a_{j-2} (J-\lm I)^jx + \sum^{n-1}_{j=2} a_{j-1}2\lm(J-\lm I)^jx = 0 \ \ra \ a_{j-2} + a_{j-1} 2\lm = 0 \ \ra \ a_{j-2} = (-2\lm )a_{j-1} \quad (*) 
\ee
for $j= 2,\dots,n-1$. If $\lm \neq 0$ then $a_0 = 0$ and by $(*)$ $a_0 = \dots = a_{n-1} = 0$ and $a_{n-1}$ is unrestricted. If $\lm =0$ then by $(*)$ $a_0 = \dots = a_{n-3} = 0$ and $a_{n-2},a_{n-1}$ are unrestricted.

Therefore the JCF of $T^2$ has the same number of blocks as $T$. All the sizes are the same except for the blocks corresponding to the kernel. The diagonals contain entries of $\lm^2$. The kernel has one extra eigenvector and so the geometric multiplicity of the zero eigenvalue is increased by 1. This is reflected in the sizes of the two blocks. The diagonal piece contains one extra line and the non diagonal piece contains one fewer row.

Claim. $A$ is diagonalisable.

Note that $A$ is invertible and therefore it has no zero eigenvalues. From above we know that the dimensions of the eigenspaces and the sizes of all the non-zero eigenvalue Jordan blocks are preserved by powers of any endomorphism. Therefore if $A$ was not diagonalisable then neither would $A^m$ have a diagonal form. Therefore $A$ must have been diagonalisable.
\end{solution}


\begin{problem}
Let $\alpha$ be an endomorphism of a finite dimensional complex vector space. Show that if $\lm$ is an eigenvalue for $\alpha$ then $\lm^2$ is an eigenvalue for $\alpha^2$. Show further that every eigenvalue of $\alpha^2$ arises in this way. [The corresponding claim fails for real vector spaces.] Are the eigenspaces $\ker(\alpha - \lm I)$ and $\ker(\alpha^2 - \lm^2 I)$ necessarily the same?
\end{problem}

\begin{solution}[\bf Solution.]
Claim. If $\lm$ is an eigenvalue of a JCF $\tau$ then $\lm^2$ is an eigenvalue of $\tau^2$. 

\be
\tau v = \lm v \ \ra \ \tau^2 v = \lm^2v.
\ee

Lemma. Every eigenvalue arises in this way.

Write the endomorphism in JCF. Then $\tau^2$, with respect to the same basis, is triangular with $\lm^2$ down the diagonal. Each $\lm^2$ occurs the same number of times as $\lm$ in the original matrix. Therefore expanding along the diagonal to obtain the characteristic equation shows that the eigenvalues are exactly the squares of the original eigenvalues and they all occur with the same algebraic multiplicity as $\lm$ in the original, but eigenspace dimensions may differ.
\be
\tau = \bepm 0& 0 \\ 1 & 0\eepm \quad \text{The eigenvalue $\lm =0$ has geometric multiplicity = 1}.
\ee
\be
\tau^2 = \bepm 0& 0 \\ 0 & 0\eepm \quad \text{The eigenvalue $\lm =0$ has geometric multiplicity = 2}.
\ee
\end{solution}


\begin{problem}
(Another proof of the Diagonalisability Theorem.) Let $V$ be a vector space of finite dimension. Show that if $\alpha_1$ and $\alpha_2$ are endomorphisms of $V$, then the nullity $n(\alpha_1\alpha_2)$ satisfies $n(\alpha_1\alpha_2) \leq n(\alpha_1) + n(\alpha_2)$. Deduce that if $\alpha$ is an endomorphism of $V$ such that $p(\alpha) = 0$ for some polynomial $p(t)$ which is a product of distinct linear factors, then $\alpha$ is diagonalisable.
\end{problem}

\begin{solution}[\bf Solution.]
Use rank-nullity theorem $V \stackrel{\alpha_2}{\to }V \stackrel{\alpha_1}{\to }V$,
\be
r(\alpha_1) + n(\alpha_1) = \dim V,\quad r(\alpha_2) + n(\alpha_2) = \dim V,\quad r(\alpha_1\alpha_2) + n(\alpha_1\alpha_2) = \dim V,
\ee
\be
n(\alpha_1) + n(\alpha_2) - n(\alpha_1\alpha_2) = \dim V - r(\alpha_1) + r(\alpha_2) + r(\alpha_1\alpha_2).
\ee

Note that
\be
r(\alpha_1 \alpha_2) \geq \min \bra{r(\alpha_1),r(\alpha_2)} - \max\bra{n(\alpha_1),n(\alpha_2)}.
\ee

Relabel if necessary so that $r(\alpha)\leq r(\alpha_2)$ and note that therefore $n(\alpha_1)\leq n(\alpha_2)$ and $r(\alpha_1\alpha_2) \geq r(\alpha_1) - n(\alpha_2)\geq 0$. Hence,
\be
n(\alpha_1) + n(\alpha_2) - n(\alpha_1\alpha_2) \geq \dim V - r(\alpha_1) + r(\alpha_2) - n(\alpha_2), \quad n(\alpha_1) + n(\alpha_2) - n(\alpha_1\alpha_2) \geq 0.
\ee

Let $p(\alpha) = \prod^m_{j=1} (\alpha -\lm_j I)$ and note that the factors are all distinct. Without loss of generality we may assume that this is the minimal polynomial for the endomorphism hence $m\leq \dim V$.

Use $ n(\alpha_1\alpha_2) \leq n(\alpha_1) + n(\alpha_2)$ then
\be
n(p(\alpha)) \leq \sum^m_{j=1}n\bb{\alpha - \lm_j I}.
\ee

We are given that $n(p(\alpha)) = \dim V$ and by definition $n(\alpha - \lm_j I) = \dim E_{\lm_j}$ the dimension of the eigenspace and therefore the number of linearly independent eigenvectors of the corresponding eigenvalue.
\be
\dim V \leq \sum^m_{j=1} \dim E_{\lm_j},\quad\quad \dim V = \sum^m_{j=1} \dim E_{\lm_j}
\ee
since the eigenspaces intersect only at the origin. Therefore there are enough linearly independent eigenvectors to form a basis for the space and therefore the endomorphism has a diagonal form.
\end{solution}


\begin{problem}
Let $C$ be an $n \times n$ matrix over $C$, and write $C = A + iB$, where $A$ and $B$ are real $n \times n$ matrices. By considering $\det(A + \lm B)$ as a function of $\lm$, show that if $C$ is invertible then there exists a real number $\lm$ such that $A + \lm B$ is invertible. Deduce that if two $n \times n$ real matrices $P$ and $Q$ are similar when regarded as matrices over $\C$, then they are similar as matrices over $\R$.
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(a)] Consider the family of matrices $C= A + \lm B$ and note that each element in this family is invertible provided that $\det(A+\lm B)\neq 0$ but for an $n\times n$ matrix $\det(A+\lm B) =0$ has exactly $n$ roots over the complex numbers and therefore there are an infinite number of choices of $\lm$ such that the matrix is invertible.
\item [(b)] Let $C=A+iB$ be such that 
\be
C^{-1}PC = Q \ \ra\ PA + iPB = AQ + iBQ.
\ee

Take real and imaginary parts, $PA = AQ$, $PB = BQ$. Consider 
\be
PA + \lm PB = AQ + \lm BQ \ \ra \ P(A+\lm B) = (A+\lm B)Q.
\ee

By first part it is possible to choose $\lm$ such that $D= A+\lm B$ is invertible and therefore $D^{-1}PD = Q$ as required.
\een
\end{solution}


\begin{problem}
Let $A$ be an $n \times m$ matrix. Prove that if $B$ is an $m \times n$ matrix then
\be
r(AB) \leq \min(r(A), r(B)).
\ee
At the start of each year the jovial and popular Dean of Muddling (pronounced Chumly) College organises $m$ parties for the $n$ students of the College. Each student is invited to exactly $k$ parties, and every two students are invited to exactly one party in common. Naturally $k \geq 2$. Let $P = (p_{ij})$ be the $n \times m$ matrix defined by
\be
p_{ij} = \left\{\ba{ll}
1 \quad\quad & \text{if student $i$ is invited to party $j$}\\
0 & \text{otherwise.}
\ea\right.
\ee
Calculate the matrix $PP^T$ and find its rank. Deduce that $m \geq n$. 

After the Master's cat has been found dyed green, maroon and purple on successive nights, the other fellows insist that next year $k = 1$. Why does the proof above now fail, and what will, in fact, happen next year? (The answer required is mathematical rather than sociological in nature.)
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(a)]
\be
W \overbrace{\stackrel{B}{\to} V \stackrel{A}{\to} }^{\stackrel{AB}{\longrightarrow}}U
\ee

In the best case scenario $\im B \cap \ker A$ is as small as possible.

If $\rank B > \rank A$ then only $\rank A$ vectors can survive to form the image of $AB$.

If $\rank B < \rank A$ then only $\rank B$ vectors can survive to form the image of $AB$.

Hence $\rank(AB) \leq \min \bra{\rank A, \rank B}$.

\item [(b)] Introduce the indicator matrix $p_{ij}$ where the $i$ labels the student and the $j$ labels the party. Each student is invited to $k$ parties so $\sum^m_{j=1} p_{ij} = k, \forall i$.

Each pair of students have exactly one common party so 
\be
\sum^m_{j=1} p_{ij}p_{kj} =  1+ (k-1)\delta_{ik}.
\ee

Calculate $PP^T$.
\be
\bb{PP^T}_{ik} = P_{ij}P_{kj} = 1 + (k-1)\delta_{ik}.
\ee

Rank of $PP^T$. Since $\bb{PP^T}_{ik} = 1 + (k-1)\delta_{ik}$ and the matrix has rank $n$.

\be
n= \rank(PP^T) \leq \min\bra{\rank(P),\rank(P^T)} = \min\bra{m,n} \leq m.
\ee

The matrix $(PP^T)_{ij} =1$ so all the entries are the same and the rank is 1. $1\leq \rank(PP^T) \leq m$. 

All the students go to one party.

ALT: Deduce $m\geq n$.

Suppose that student $A$ goes to the first $k$ parties. Student $B$ goes to the first party and then misses the next $k-1$ and then goes to the next $k-1$ parties.

Student $C$ has two alternatives. Either he can go the first party miss out the next $2k-1$ and then go to the next $k-1$, or he can miss the first go to the next and then the first of the subsequent parties attended by $B$ and then skip the next $k-1$ and then go to the next $k-1$ parties. Illustrated as follow,

\be
\text{ Either }\quad 
\ba{ll}
1  \underbrace{ \ \dots \  }_{k \text{ entries}}\ 1 \ 0 \ \dots & A\\
1 \ 0 \underbrace{\ \dots\ \ }_{k-1\text{ entries}}\ 0 \ 0 \ 1 \underbrace{\ \dots \ }_{k-1\text{ entries}} 1 \ 0   \qquad\qquad & B\\
1 \ 0 \underbrace{\ \dots\ 0\ }_{k-1\text{ entries}}\ 0 \ 0 \ 0 \underbrace{ \ \dots \ }_{k\text{ entries}} 0 \ 0  \qquad\qquad & C
\ea\qquad \text{ or }\quad 
\ba{ll}
1  \underbrace{ \ \dots \  }_{k \text{ entries}}\ 1 \ 0 \ \dots & A\\
1 \ 0 \underbrace{\ \dots\ \ }_{k-1\text{ entries}}\ 0 \ 0 \ 1 \underbrace{\ \dots \ }_{k-1\text{ entries}} 1 \ 0   \qquad\qquad & B\\
0 \ 1 \ 0 \underbrace{\ \dots\ 0\ }_{k-2\text{ entries}}\ 0 \ 1 \underbrace{ \ \dots \ }_{k\text{ entries}} 0 \ 1 \underbrace{ \ \dots \ }_{k-2\text{ entries}}  \qquad\qquad & C
\ea
\ee

From this we can see that the second option generates fewer parties so we look at this alternative for all the students to find the limiting case.

\be
\ba{ll}
s(1): & 1  \underbrace{ \ \dots \  }_{k \text{ entries}}\ 1 \ 0 \dots \\
s(1): & 1 \ 0 \underbrace{ \ \dots \  }_{k-1 \text{ entries}}\ 0 \ 1  \underbrace{ \ \dots \  }_{k-1 \text{ entries}}\ 1 \ 0 \dots \\
s(1): & 0 \ 1 \underbrace{ \ \dots \  }_{k-2 \text{ entries}}\ 0 \ 1 \ 0 \underbrace{ \ \dots \  }_{k-2 \text{ entries}}\ 0 \ 1 \underbrace{ \ \dots \  }_{k-2 \text{ entries}} 1\ 0 \ \dots \\
s(1): & 0 \ 0 \ 1 \underbrace{ \ \dots \  }_{k-3 \text{ entries}} 0 \ 0 \ 1 \ 0 \underbrace{ \ \dots \  }_{k-3 \text{ entries}} 0\ 1 \ 0 \underbrace{ \ \dots \  }_{k-3 \text{ entries}} 0 \ 1 \underbrace{ \ \dots \  }_{k-3 \text{ entries}} 1 \ 0 
\ea
\ee

Therefore the smallest number of parties is given by the formula, 
\be
m = k+(k-1) + (k-2) + \dots + (k-(n-1)).
\ee

We also need, from the final term, $k\geq n-1$. Setting the limiting case $k=(n-1)$ gives, equality in the limiting case,
\be
m \geq \frac{-n^2 + n(2k+1)}2 = \frac{-n^2 + n(2n-1)}2,\quad\quad 2m \geq n(n-1).
\ee

If $n\geq 3$ then $2\geq (n-1)$ and $m\geq n$. If $n=3$ then $2m \geq 6$ and $m\geq 3$ is consistent with $m\geq n$. Hence $m\geq n$ as required.

Case $k =1$. We have $k+1\geq n$ and $n\geq 3$ so $2\geq n$ and $n\geq 3$ inconsistent. For this case the construction above fails at the third step and the only solution is a single party for all students.

\een
\end{solution}


\begin{problem}
Let $f(x) = a_0 + a_1x + \dots + a_nx^n$, with $a_i \in \C$, and let $C$ be the \emph{circulant matrix}
\be
\bepm
a_0 & a_1 & a_2 & \dots & a_n \\
a_n & a_0 & a_1 & \dots & a_{n-1}\\
a_{n-1} & a_n & a_0 & \dots & a_{n-2}\\
\vdots & & & \ddots & \vdots \\
a_1 & a_2 & a_3 & \dots & a_0
\eepm.
\ee

Show that the determinant of $C$ is $\det C = \prod^n_{j=0} f(\zeta^j)$, where $\zeta = \exp(2\pi i/(n + 1))$.
\end{problem}

\begin{solution}[\bf Solution.]
Try $e_1 = \bb{1,1,\dots,1}^T$ then $Ce_1 = \bb{f(1),f(1),\dots,f(1)}^T$ therefore $e_1$ is an eigenvector with eigenvalue $f(1)$.

Try $e_2 = \bb{1,\xi,\xi^2,\dots}^T$ where $\xi = e^{j2\pi i/n}$ for $j=1,\dots n$ because we need $\xi^n = 1$ then
\be
Ce_2 = \bb{f(\xi), f(\xi)\xi, f(\xi)\xi^2,\dots}^T = f(\xi)\bb{1,\xi,\xi^2,\dots}^T = f(\xi)e_2.
\ee

Let $\xi_j = e^{j2\pi i/n}$ for $j = 1,\dots ,n$ then $Ce_j = \bb{f(\xi),f(\xi)\xi, f(\xi)\xi^2,\dots}^T = f(\xi_j)e_j$ from above.

Suppose $\sum^n_{j=1}b_je_j = 0$ with $e_{jk} = e^{kj2\pi i/n} \ \ra\ \sum^n_{j=1} b_j e^{kj2\pi i/n} = 0$ for $k = 1,\dots,n$. Multiply by $e^{-ks2\pi i/n}$ with $s$ fixed then $\sum^n_{k=1}\sum^n_{j=1} b_j e^{k(j-s)2\pi i/n} = 0$. 

First do the sum over $k$,
\be
\sum^n_{j=1}\underbrace{\sum^n_{k=1} b_j e^{k(j-s)2\pi i/n}}_{\text{=0 by $n$th root of unity unless }j=s} = 0 \ \ra\ n \sum^n_{j=1}b_j \delta_{js} = 0,
\ee
$b_s = 0$ for all $s$. Hence $\det C = \prod^{n-1}_{j=0} f\bb{\xi^j}$ since we have all the eigenvalues and vectors for the matrix.
\end{solution}


\begin{problem}
Let $\alpha : V \to V$ be an endomorphism of a real finite dimensional vector space $V$ with $\tr(\alpha) = 0$.
\ben
\item [(i)] Show that, if $\alpha \neq 0$, there is a vector $\bv$ with $\bv$, $\alpha(\bv)$ linearly independent. Deduce that there is a
basis for $V$ relative to which $\alpha$ is represented by a matrix $A$ with all of its diagonal entries equal to 0.
\item [(ii)] Show that there are endomorphisms $\beta$, $\gamma$ of $V$ with $\alpha = \beta\gamma - \gamma\beta$.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(i)] Since $\alpha$ is non-zero there is some vector $v$ such that $\alpha v \neq 0$. Now either $v$ is an eigenvector or it is not. If it is not an eigenvector then $v$ and $\alpha v$ are linearly independent as required.

Therefore it only remains to consider the case where all the vectors are eigenvectors and there is a diagonal matrix representation of $\alpha$. Now either the diagonal matrix for $\alpha$ is block diagonal consisting of a zero sub-matrix and an identity sub-matrix or $\alpha$ has at least two eigenvectors of distinct non-zero eigenvalues. In the latter case the matrix cannot have zero trace and therefore the map $\alpha$ cannot have zero trace. Therefore suppose $\alpha \bbe_\lm = \lm \bbe_\lm$ and $\alpha \bbe_\mu = \mu \bbe_\mu$ and $\lm \neq \mu$. Then $\bbe_\lm + \bbe_\mu$ and $\alpha(\bbe_\lm + \bbe_\mu)$ are linearly independent as required.

\item [(ii)] First choose a basis for the kernel of the map $\alpha$, and extend it to a basis of the whole space. Therefore wlog we can find a basis such that the matrix for $\alpha$ is block diagonal with a zero sub-matrix and a non-singular sub-matrix of zero trace. We need to find a suitable basis for the non-singular matrix restricted to the subspace on which this acts.

From the first part there is a vector $v$ which is linearly independent of $\alpha v$. Form the basis
\be
\bbe_0 = v,\quad \bbe_1 = \alpha v,\quad \bbe_k = \alpha^k v
\ee

Suppose that at $(k+1)$ the new vector is a linear combination of the previous $k$ vectors. Change the basis from the original basis to $\bbe_0,\dots,\bbe_k$ and the remainder of the original basis by using the Steinitz Exchange Lemma. Using this we can reduce the matrix to a block diagonal matrix with $\bbe_0,\dots, \bbe_k$ as the spanning basis for the first block and the rest of the matrix given by the completion of this basis to the whole of the space. 

Notice that on the subspace spanned by $\bbe_0,\dots,\bbe_k$ the matrix representation of $\alpha$ has zeros along the diagonals. Therefore it remains to prove the original statement on the lower dimensional space spanned by the remaining vectors. We can repeat the argument until the whole matrix is of the required form.
\een
\end{solution}

\begin{problem}
Show that none of the following matrices are similar:
\be
\bepm
1 & 1 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\eepm,
\bepm
1 & 1 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm,
\bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm.
\ee
Is the matrix
\be
\bepm
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\eepm
\ee
similar to any of them? If so, which?
\end{problem}

\begin{solution}[\bf Solution.]
All three are in JCF and different therefore they cannot be conjugate.

ALT: All three have eigenvalue = 1 with algebraic multiplicity = 3.

Look for dimensions of eigenspaces. Consider $(M-\lm I)v = 0$ with $\lm =1$.
\beast
\bepm
1 & 1 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\eepm - \bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm =  \bepm
0 & 1 & 0\\
0 & 0 & 1\\
0 & 0 & 0
\eepm \ \ra \ \Span_\R\bra{\bepm
1\\
0\\
0
\eepm}\\
\bepm
1 & 1 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm - \bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm =  \bepm
0 & 1 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\eepm \ \ra \ \Span_\R\bra{\bepm
1\\
0\\
0
\eepm,\bepm
0\\
1\\
0
\eepm}\\
\bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm - \bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm =  \bepm
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\eepm \ \ra \ \Span_\R\bra{\bepm
1\\
0\\
0
\eepm,\bepm
0\\
1\\
0
\eepm,\bepm
0\\
0\\
1
\eepm}
\eeast

Eigenspaces have different dimensions and therefore the matrices are not related by basis change in the vector space i.e. are not related by conjugation. 

Is the matrix conjugate to one of the above?
\be
\bepm
1 & 1 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\eepm - \bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\eepm =  \bepm
0 & 1 & 0\\
0 & 0 & 1\\
0 & 0 & 0
\eepm \ \ra \ \Span_\R\bra{\bepm
1\\
0\\
0
\eepm} \ \ra \ \text{Conjugate to }
\bepm
1 & 1 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\eepm.
\ee

Find transformation
\be
P^{-1}\bepm
1 & 1 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\eepm P = \bepm
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\eepm
\ee

Try $P = \bepm
1 & -2 & 0\\
0 & 1 & -1\\
0 & 0 & 1
\eepm$, $\bepm
1 & 1 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\eepm \bepm
1 & -2 & 0\\
0 & 1 & -1\\
0 & 0 & 1
\eepm = \bepm
1 & -1 & -1\\
0 & 1 & 0\\
0 & 0 & 1
\eepm = \bepm
1 & 2 & 0\\
0 & 1 & -1\\
0 & 0 & 1
\eepm \bepm
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\eepm$.
\end{solution}


\begin{problem}
Find a basis with respect to which $\bepm
0 & -1\\
1 & 2 
\eepm$ is in Jordan normal form. Hence compute $\bepm
0 & -1\\
1 & 2
\eepm^n$.
\end{problem}

\begin{solution}[\bf Solution.]
$\bepm
0 & -1\\
1 & 2
\eepm$, $\lm = 1,1$.
\be
\bepm
-1 & -1\\
1 & 1 
\eepm \bepm
x\\
y \eepm = \bepm
0\\
0\eepm \ \ra \ e_1 = \bepm 
1\\
1
\eepm,\quad\quad \bepm
-1 & -1 \\
1 & 1 
\eepm \bepm
x\\
y
\eepm = \bepm
1\\
-1
\eepm \ \ra \ e_2 = \bepm
-1\\
0
\eepm
\ee

The required basis is $e_1 =\bepm 1\\ -1 \eepm$ and $e_2= \bepm -1\\ 0 \eepm$.
\be
P = \bepm
-1 & -1\\
1 & 0
\eepm \ \ra \ P^{-1} = \bepm
0 & -1\\
-1 & -1
\eepm
\ee

$\bepm 0 & -1 \\ -1 & -1\eepm \bepm 0 & -1\\ 1 & 2 \eepm \bepm -1 & -1 \\ 1 & 0 \eepm = \bepm 1 & 1\\ 0 & 1\eepm$,
\be
\bepm
0 & -1 \\
1 & 2
\eepm^n = \bepm -1 & -1 \\ 1 & 0 \eepm \bepm 1 & 1\\ 0 & 1 \eepm^n \bepm 0 & -1 \\ -1 & -1 \eepm = \bepm -1 & -1 \\ 1 & 0 \eepm \bepm 1 & n\\ 0 & 1 \eepm \bepm 0 & -1 \\ -1 & -1 \eepm = \bepm 1-n & -n \\ n & n+1 \eepm.
\ee
\end{solution}


\begin{problem}
\ben
\item [(a)] Recall that the Jordan normal form of a $3\times 3$ complex matrix can be deduced from its characteristic and minimal polynomials. Give an example to show that this is not so for $4 \times 4$ complex matrices.
\item [(b)] Let $A$ be a $5\times 5$ complex matrix with $A^4 = A^2 \neq A$. What are the possible minimal and characteristic polynomials? How many possible JNFs are there for $A$? 

[There are enough that you probably don't want to list all the possibilities.]
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(a)]
\be
\ba{cc}
\bepm
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\eepm & \qquad \qquad \bepm
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0
\eepm \\
\chi(x) = x^4, m(x) = x^2 & \qquad\qquad \chi(x) = x^4, m(x) = x^2
\ea
\ee
\item [(b)] $A^4 = A^2 \neq A$. Characteristic equation: must be degree 5. Given $A^2 (A^2 -I) = 0$, $A^2 -A = 0$. Possibilities are 
\be
A^3(A-I)(A+I) = 0,\quad A^2(A-I)^2(A+I) = 0,\quad A^2(A-I)(A+I)^2 = 0
\ee
but we need to impose also $A^2$ is not diagonal.

Minimal polynomial.
\be
\ba{c}
\chi: A^3(A-I)(A+I) = 0\\
m : A^3(A-I)(A+I) = 0 \\
\bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm
\ea
\ee

\be
\ba{ccc}
\chi: A^3(A-I)(A+I) = 0 &\qquad \chi: A^2(A-I)^2(A+I) = 0 & \qquad \chi: A^2(A-I)(A+I)^2 = 0\\
m : A^2(A-I)(A+I) = 0 & & \\
\bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm & \bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm & \bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & -1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm
\ea
\ee

\be
\ba{c}
\chi: A^2(A-I)^2(A+I) = 0\\
m : A^2(A-I)^2(A+I) = 0 \\
\bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 1 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm
\ea
\ee

\be
\ba{ccc}
\chi: A^3(A-I)(A+I) = 0 &\qquad \chi: A^2(A-I)^2(A+I) = 0 & \qquad \chi: A^2(A-I)(A+I)^2 = 0\\
m : A(A-I)^2(A+I) = 0 & & \\
\bepm
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 1 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm & \bepm
0 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 1 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm & \bepm
0 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & -1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm
\ea
\ee

\be
\ba{ccc}
\chi: A^3(A-I)(A+I) = 0 &\qquad \chi: A^2(A-I)^2(A+I) = 0 & \qquad \chi: A^2(A-I)(A+I)^2 = 0\\
m : A^2(A-I)(A+I) = 0 & & \\
\bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm & \bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm & \bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & -1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm
\ea
\ee

\be
\ba{c}
\chi: A^3(A-I)(A+I) = 0\\
m : A^2(A-I)(A+I)^2 = 0 \\
\bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 1\\
0 & 0 & 0 & 0 & -1\\
\eepm
\ea
\ee

\be
\ba{ccc}
\chi: A^3(A-I)(A+I) = 0 &\qquad \chi: A^2(A-I)^2(A+I) = 0 & \qquad \chi: A^2(A-I)(A+I)^2 = 0\\
m : A(A-I)(A+I)^2 = 0 & & \\
\bepm
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & -1 & 1\\
0 & 0 & 0 & 0 & -1\\
\eepm & \bepm
0 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & -1 & 1\\
0 & 0 & 0 & 0 & -1\\
\eepm & \bepm
0 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & -1 & 0 & 0\\
0 & 0 & 0 & -1 & 1\\
0 & 0 & 0 & 0 & -1\\
\eepm
\ea
\ee

\be
\ba{ccc}
\chi: A^3(A-I)(A+I) = 0 &\qquad \chi: A^2(A-I)^2(A+I) = 0 & \qquad \chi: A^2(A-I)(A+I)^2 = 0\\
m : A^2(A-I)(A+I) = 0 & & \\
\bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm & \bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & -1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm & \bepm
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & -1\\
\eepm
\ea
\ee

\een
\end{solution}


\begin{problem}
Let $\alpha$ be an endomorphism of the finite dimensional vector space $V$ over $F$, with characteristic polynomial 
\be
\chi_\alpha(t) = (-1)^n t^n + c_{n-1}t^{n-1} + \dots + c_0.
\ee
Show that $\det(\alpha) = c_0$ and $\tr(\alpha) = c_{n-1}$ (or $\det(\alpha) = (-1)^n c_0$ and $\tr(\alpha) = -c_{n-1}$ if $\chi_\alpha(t) = t^n + c_{n-1}t^{n-1} + \dots + c_0$).
\end{problem}

\begin{solution}[\bf Solution.]
$\chi(t) = (-1)^n t^n + c_{n-1}t^{n-1} + \dots + c_0 = (-1)^n (t-a_1)\dots (t-a_n)$ where the $a_j$ are the eigenvalues and may or may not be distinct.
\be
\det \alpha = a_1 \dots a_n = (-1)^n (0-a_1)\dots (0-a_n) = \chi(0) = c_0.
\ee

Any matrix may be written in triangular form and in this form the sum of the diagonals is the trace. It follows that the trace is the sum of the eigenvalues counted with multiplicities.
\be
\chi(t) = (-1)^n t^n + c_{n-1}t^{n-1} + \dots + c_0 = (-1)^n (t-a_1)\dots (t-a_n).
\ee

The sum of the eigenvalues is obtained by taking the term linear in the eigenvalues and in $t^{n-1}$, therefore,
\be
\tr (\alpha) = (-1)^{n-1}\sum^n_{j=1}a_j = c_{n-1}.
\ee
\end{solution}


\begin{problem}
Let $\alpha$ be an endomorphism of the finite-dimensional vector space $V$, and assume that $\alpha$ is invertible. Describe the eigenvalues and the characteristic and minimal polynomial of $\alpha^{-1}$ in terms of those of $\alpha$.
\end{problem}

\begin{solution}[\bf Solution.]
Characteristic equation
\be
\det\bb{A^{-1}-\lm I} = \det\bb{A^{-1}(I-\lm A)} = \det(A^{-1}) \det\bb{(-\lm )\bb{-\frac 1{\lm}I}+A} = \det(A^{-1}) (-\lm )^n \det\bb{A-\frac 1{\lm}I}
\ee

Let $\det(A-\lm I) =a_0 + a_1 \lm + \dots + a_n \lm^n$ then
\beast
\det\bb{A^{-1}-\lm I} & = & (-1)^n \lm^n\det(A^{-1})  \det\bb{a_0 + a_1 \lm^{-1} + \dots + a_n \lm^{-n}} \\
& = & (-1)^n \det(A^{-1})  \det\bb{a_n + a_{n-1} \lm + \dots + a_0 \lm^n}
\eeast

Minimal Polynomial
\be
m(A) = a_0 I + a_1 A + \dots + a_k A^k = 0,\quad k\leq n\qquad\qquad m(A)A^{-k} = a_0 A^{-k} + a_1 A^{-k+1} + \dots + a_k I = 0.
\ee

Assume for contradiction another minimal ploynomial exists then $\wt{m}(A^{-1})A^h = 0$ defines a minimal polynomial for $A$ and gives the required contradiction.
\end{solution}


\begin{problem}
Prove that any square complex matrix is similar to its transpose. [You may want to check it first for a Jordan block matrix.]

Prove that that the inverse of a Jordan block $J_m(\lm)$ with $\lm \neq 0$ has Jordan normal form a Jordan block $J_m(\lm^{-1})$. For an arbitrary non-singular square matrix $A$, describe the Jordan normal form of $A-1$ in terms of that of $A$.
\end{problem}

\begin{solution}[\bf Solution.]
The JCF completely characterises the conjugacy class of the matrix. The transpose of a JCF is still in JCF and is the same JCF up to permutations of basis vectors and therefore represents the same conjugacy class.

Consider a single Jordan block.
\be
J = \bepm
\lm & 0 & &  0\\
0 & \ddots & 1 & \\
& 0 & \ddots & \ddots\\
0 & & 0 & \lm
\eepm
\ee

Note $Jv = \lm v \ \lra \ J^{-1}v = \lm^{-1}v$ so the dimensions of the eigenspaces are the same.

From the result for the characteristic equation the sizes of the whole of the block are also the same.
\be
(J^{-1})^k (Jv -\lm I)^k = (-\lm)^k \bb{J^{-1}-\frac 1{\lm}I}^k
\ee

So the Jordan form of the inverse will be the same size and will have the same number of 1s off the diagonal and the distribution of these 1 will exactly match that of the original Jordan block (because all the dimensions of the images and kernels must coincide.)
\end{solution}


\begin{problem}
Let $V$ be a complex vector space of dimension $n$ and let $\alpha$ be an endomorphism of $V$ with $\alpha^{n-1} \neq 0$ but $\alpha^n = 0$. Show that there is a vector $\bx \in V$ for which $\bx, \alpha(\bx), \alpha^2(\bx), \dots, \alpha^{n-1}(\bx)$ is a basis for $V$. Give the matrix of $\alpha$ relative to this basis.

Let $p(t) = a_0 +a_1 t+ \dots + a_kt^k$ be a polynomial. What is the matrix for $p(\alpha)$ with respect to this basis?

What is the minimal polynomial for $\alpha$? What are the eigenvalues and eigenvectors?

Show that if an endomorphism $\beta$ of $V$ commutes with $\alpha$ then $\beta = p(\alpha)$ for some polynomial $p(t)$.

[It may help to consider $\beta(\bx)$.]
\end{problem}

\begin{solution}[\bf Solution.]
$\alpha^{N-1}\neq 0 \ \ra \ \exists x, \alpha^{N-1}x \neq 0$. Consider $\sum^{N-1}_{i=0}a_i \alpha^i x = 0$ and apply $\alpha^{N-1}$ to both sides. $\ra  0 a_{N-1} = 0$. Continue inductively applying $\alpha^{N-k}$ to show the set is linearly independent.

Represent the basis by 
\be
x= \bepm
1\\
0\\
0\\
\vdots
\eepm,\ \alpha x = \bepm
0\\
1\\
0\\
\vdots
\eepm,\ \alpha x = \bepm
0\\
0\\
1\\
\vdots
\eepm,\dots \quad \ra \quad ,\alpha = \bepm
0 & 0 & 0 & \dots & 0 \\
1 & 0 & 0 & \dots & 0\\
0 & 1 & 0 & \dots & 0\\
\vdots & 0 & 1 & \vdots & 0\\
\vdots & \vdots & 0 & \vdots & 0
\eepm
\ee

Claim: matrix for polynomial $p(\alpha)$,
\be
\alpha = \bepm
0 & 0 & 0 & \dots & 0 \\
1 & 0 & 0 & \dots & 0\\
0 & 1 & 0 & \dots & 0\\
\vdots & 0 & 1 & \vdots & 0\\
\vdots & \vdots & 0 & \vdots & 0
\eepm,\quad \alpha^2 = \bepm
0 & 0 & 0 & \dots & 0 \\
0 & 0 & 0 & \dots & 0\\
1 & 0 & 0 & \dots & 0\\
\vdots & 1 & 0 & \vdots & 0\\
\vdots & \vdots & 1 & \vdots & 0
\eepm, \quad \dots
\ee

$p(\alpha) = a_0 + a_1 \alpha + a_2 \alpha^2 + \dots + a_n \alpha^n$.

\be
p(\alpha) = \alpha = \bepm
a_0 & 0 & 0 & \dots & 0 \\
a_1 & a_0 & 0 & \dots & 0\\
a_2 & a_1 & a_0 & \dots & 0\\
\vdots & a_2 & a_1 & \vdots & 0\\
\vdots & \vdots & a_2 & \vdots & a_0
\eepm
\ee

Diagonals terminate at the $k$th line.

Claim: minimal polynomial $m(p(\alpha)) = \bb{p(\alpha) - a_0 I}^{k+1}$.

There are $k$ off-diagonals which must be shifted to zero by successive applications of the operator $(p(\alpha)-a_0 I)$ therefore we require $k+1$ applications.

Claim: eigenvalues of $p(\alpha)$. Trivially all the eigenvalues are $a_0$.

Claim: eigenvectors of $p(\alpha)$. There are $n-k+1$ eigenvectors. In the above basis they take the form of zeros in all slots except one. They can take the value 1 in any of the $n-k+1$ slots which do not have off-diagonal terms.

Claim: If $\beta$ is a matrix which commutes with $\alpha$ then $\beta$ is a polynomial in $\alpha$.

Let $\beta x = \sum^{n-1}_{i=0} a_i \alpha^i x$ since $\alpha^i x$ for $0\leq i\leq n-1$ is basis. 

Let $v=\sum^{n-1}_{i=0} b_i \alpha^i x$ be an arbitrary vector.

\be
\beta v = \sum^{n-1}_{i=0} b_i \beta \alpha^i x = \sum^{n-1}_{i=0} b_i \alpha^i \beta x = \sum^{n-1}_{i=0} \sum^{n-1}_{j=0} b_i \alpha^i a_j \alpha^j x =  \sum^{n-1}_{j=0} a_j \alpha^j \sum^{n-1}_{i=0} b_i \alpha^i x = \sum^{n-1}_{j=0} a_j \alpha^j v,
\ee
$\ra \ \beta = \sum^{n-1}_{j=0} a_j \alpha^j$ which is a polynomial in $\alpha$.
\end{solution}


\begin{problem}
Let $A$ be an n×n matrix all the entries of which are real. Show that the minimal polynomial of $A$, over the complex numbers, has real coefficients.
\end{problem}

\begin{solution}[\bf Solution.]
Considering $m(A) + \ol{m(A)}$ will do. If $m(A)$ is minimal then $m(A) = a_0 + a_1 A + \dots + a_k A^k = 0$. Take the c.c. and use the reality of $A$, 
\be
\ol{m(A)} = \ol{a_0 + a_1A + \dots + a_k A^k} = \ol{a_0} + \ol{a_1}A + \dots + \ol{a_k}A^k.
\ee

Hence
\be
m(A) + \ol{m(A)} = (a_0 + \ol{a_0}) + (a_1 + \ol{a_1})A + \dots + (a_k + \ol{a_k})A^k
\ee
is also a minimal polynomial and is real.
\end{solution}


\begin{problem}
Let $V$ be a 4-dimensional vector space over $\R$, and let $\{\xi_1, \xi_2, \xi_3, \xi_4\}$ be the basis of $V^*$ dual to the basis
$\{\bx_1, \bx_2, \bx_3, \bx_4\}$ for $V$. Determine, in terms of the $\xi_i$, the bases dual to each of the following:
\ben
\item [(a)] $\{\bx_2, \bx_1, \bx_4, \bx_3\}$;
\item [(b)] $\{\bx_1, 2\bx_2, \frac 12\bx_3, \bx_4\}$ ;
\item [(c)] $\{\bx_1 + \bx_2, \bx_2 + \bx_3, \bx_3 + \bx_4, \bx_4\}$;
\item [(d)] $\{\bx_1, \bx_2 - \bx_1, \bx_3 - \bx_2 + \bx_1, \bx_4 - \bx_3 + \bx_2 - \bx_1\}$.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(a)] $\{\bx_2, \bx_1, \bx_4, \bx_3\} = \bra{\xi_2, \xi_1,\xi_4,\xi_3}$.
\item [(b)] $\{\bx_1, 2\bx_2, \frac 12\bx_3, \bx_4\} = \bra{\xi_1, \frac 12\xi_2,2\xi_3,\xi_4}$ ;
\item [(c)] $\{\bx_1 + \bx_2, \bx_2 + \bx_3, \bx_3 + \bx_4, \bx_4\}$. Start with $\xi_4(\bx_4) = 1$ and $\xi_4(\bx_3 +\bx_4) = 1$, therefore try 
\be
(\xi_4 - \xi_3)(\bx_3 + \bx_4) = 1- 1 =0 \ \ra\ bx_4^* = \xi_4 - \xi_3.
\ee

Similarly $(\xi_3 -\xi_2)(\bx_3 + \bx_4) = 1$ and zero otherwise. Similarly $(\xi_2 -\xi_1)(\bx_2 + \bx_3) = 1$ and zero otherwise. Thus,
\be
\bra{\xi_1, \xi_2 - \xi_1, \xi_3 - \xi_2 + \xi_1, \xi_4 - \xi_3 + \xi_2 - \xi_1}.
\ee
\item [(d)] By inspection of (c), $\bra{\xi_1 + \xi_2, \xi_2 + \xi_3, \xi_3 + \xi_4, \xi_4}$.
\een
\end{solution}


\begin{problem}
Let $P_n$ be the space of real polynomials of degree at most $n$. For $x \in \R$ define $\ve_x \in P^*_n$ by $\ve_x(p) = p(x)$. Show that $\ve_0,\dots, \ve_n$ form a basis for $P^*_n$, and identify the basis of $P_n$ to which it is dual.
\end{problem}

\begin{solution}[\bf Solution.]
$\ve_x(p):= p(x)$. Note that the evaluation maps $\ve_x(p)$ are linear on polynomials
\be
\ve_x(\lm p + \mu q) = (\lm p + \mu q)(x) = \lm p(x) + \mu q(x) = \lm \ve_x (p) + \mu \ve_x(q)
\ee
and therefore are elements of the dual space.

We can take the standard basis on the polynomial space, $1,\bx,\bx^2,\dots, \bx^n$ then any polynomial is a linear combination of these elements. We therefore need a basis of $n+1$ vectors to span the space. $\ve_0,\dots,\ve_n$ will do provided we can show that they are linearly independent.

Define:
\be
p_k (x) = \prod^k_{j=1,j\neq k} \frac{(x-0)(x-1)\dots (x-(k-1))(x-(k+1))\dots (x-n)}{(k-0)(k-1)\dots (k-(k-1))(k-(k+1))\dots (k-n)},\quad k=0,\dots,n.
\ee

$p_k(j) = \delta_{jk}$ so $\ve_j(p_k) := p_k(j) = \delta_{jk}$. Let $\sum^n_{j=0}a_j\ve_j(x) = 0$ then $\sum^n_{j=0} a_j\ve_j(p_k) = a_k = 0$ so the set is linearly independent.

Therefore $\ve_0,\dots, \ve_n$ provide a dual basis to 
\be
p_k (x) = \prod^k_{j=1,j\neq k} \frac{(x-0)(x-1)\dots (x-(k-1))(x-(k+1))\dots (x-n)}{(k-0)(k-1)\dots (k-(k-1))(k-(k+1))\dots (k-n)},\quad k=0,\dots,n.
\ee
\end{solution}


\begin{problem}
\ben
\item [(a)] Show that if $\bx \neq \by$ are vectors in the finite dimensional vector space $V$, then there is a linear functional $\theta\in V^*$ such that $\theta(\bx) \neq \theta(\by)$.
\item [(b)] Suppose that $V$ is finite dimensional. Let $A,B \leq V$. Prove that $A \leq B$ if and only if $A^o \geq B^o$. Show that $A = V$ if and only if $A^o = \{{\bf 0}\}$. Deduce that a subset $F \subset V^*$ of the dual space spans $V^*$ if and only if $\{\bv \in V : f(\bv) = 0\text{ for all }f \in F\} = \{{\bf 0}\}$.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(a)] Let $\bra{x,y_2,\dots,y_n}$ be basis then there exists a dual basis and $f(x) = x^*(x) = 1$.

If $x\neq \lm y$ then $f(x) = x^*(x) = 1$ but $f(y) = x^*(y) = 0$ so that $f(x)\neq f(y)$.

If $x=\lm y$ and $x \neq y$ then by linearity $f(x)\neq f(y)$.

\item [(b)] Claim: $A^\circ \geq B^\circ $ iff $A\leq B$.

Assume: $A\leq B$
\be
A^\circ := \bra{v\in V: (a,v) =0,\forall a\in A},\quad B^\circ := \bra{v\in V: (b,v) =0,\forall b\in B}
\ee

If $x\in B^0$ and $A\leq B$ then $x\in A^\circ$ hence $A^\circ \geq B^\circ$.

Assume: $A^\circ \geq B^\circ$, 
\be
A^\circ := \bra{v\in V: (a,v) =0,\forall a\in A},\quad B^\circ := \bra{v\in V: (b,v) =0,\forall b\in B}
\ee

If $x\in A^\circ$ and $A^\circ \geq B^\circ$ then $x\in B^\circ$ and $A\leq B$.

Claim: $A^\circ = \bra{0}$ iff $A=V$

Assume: $A= V$
\be
A^\circ := \bra{v\in V:(a,v) =0,\forall a \in V}.
\ee

Since $(\ ,\ )$ is non degenerate $(a,v) = 0,\forall a\in V$ implies $v=0$. So $A^\circ = \bra{0}$.

Assume $A^\circ = \bra{0}$
\be
A^\circ := \bra{v\in V:(a,v) = 0,\forall a\in A}
\ee

Suppose for contradiction that $A\neq V$ then $\exists x\in V:x\notin A$. Either $(a,x) =0, \forall a\in A$ and then $x\in A^\circ = \bra{0}$ so $x=0$ or $\exists a\in A: (a,x)\neq 0$ and then $x\notin A^\circ$ but $x\in A \cup A^\circ = V$. Contradiction. So no such non-zero $x$ exists and $A=V$.

Claim: $\bsa{F} = V^*$.

Since $F\subseteq V^*$, $F^\circ := \bra{v\in V:(f,v)=0,\forall f\in F} = V$. Hence $\bsa{F} = V^*$.

\een
\end{solution}


\begin{problem}
Let $V$ be the vector space of all complex sequences $(z_n)$ satisfying the difference equation $z_{n+2} = 3z_{n+1} - 2z_n$ for $n = 1, 2,\dots$. Write down (without solving the difference equation) an obvious basis for $V$ and hence determine its dimension. Show that the "shift" operator which sends a sequence $(z_1, z_2, z_3, \dots)$ to $(z_2, z_3, z_4,\dots)$ is an endomorphism on $V$. Find the matrix which represents this map with respect to your basis. Show that there is a basis for $V$ with respect to which the map is represented by a diagonal matrix.

What happens if we replace the difference equation by $z_{n+2} = 2z_{n+1} - z_n$?
\end{problem}

\begin{solution}[\bf Solution.]
$z_{n+2} = 3z_{n+1} - 2z_n$, $n=1,\dots$

Basis and dimension: Given that $n=1$ and $n=2$ the rest of the sequence is fixed. Impose $z_1 = 0,z_2 =1$ and $z_1 = 1,z_2 = 0$. Thus basis $\bra{(0,1,\dots),(1,0,\dots)}$, $\dim_\C = 2$.

Shift is linear:
\be
S(\lm(z_1,\dots)+\mu(w_1,\dots)) = (\lm z_2 + \mu w_2,\lm_3 + \mu w_3,\dots) = \lm S(z_1,\dots) + \mu S(w_1,\dots).
\ee

Shift is $V$ into $V$:

$S(z) = w$ where $w_n = z_{n+1}$, $w_{n+1} = 3w_n - 2w_{n-1}$, $n>1$. Initial condition $n=1$. Thus, $w_{n+1} = 3w_n - 2w_{n-1}$, $n\geq 1$.

Find matrix repr rel to above basis:

Let $e_1 = (0,1,\dots)$ and $e_2 = (1,0,\dots)$. Note $z=(z_1,z_2,\dots) \ \ra\ Sz=(z_2,z_3,\dots)$. Use $z_3 = 3z_2 -2z_1$. Represent the sequence $z = (z_1,z_2,\dots)$ by the column vector

\be
z = \bepm
z_1\\
z_2
\eepm \ \ra\ Sz = \bepm
0 & 1\\
-2 & 3
\eepm\bepm
z_1\\
z_2
\eepm \ \ra \ S = \bepm
0 & 1\\
-2 & 3
\eepm.
\ee

Find diagonal matrix repr:

Look for eigenvalues. $z= (z_1,z_2,\dots)$, $Sz = (z_2,z_3,\dots) = \lm z$. Therefore 
\be
z_{n+2} = 3z_{n+1} - 2z_n \ \ra\ \lm^2 = 3\lm -2 \ \ra \ \left\{\ba{l}
\lm =1,\ Se_1 = e_1,\ e_1 = (1,1,1,\dots)\\
\lm =2,\ Se_2 = 2e_2,\ e_2 = (1,2,4,\dots)
\ea\right. \ \ra \ S = \bepm
1 & 0\\
0 & 2
\eepm
\ee

ALT: Solve the difference equation. Let 
\be
z_n = \lm^n \ \ra \ \lm^{n+2} = 3\lm^{n+1} - 2\lm^n \ \ra \ \lm = 1,2 \ \ra \ z_n = Az^2 + Bz.
\ee

New equation: $\lm^2 = 2\lm -1$. The eigenvalue equation has a double root. $e_1 = (1,1,1,\dots)$ is still an eigenvector. There are no others. By inspection $e_2 = (1,2,3,\dots)$ is also a solution so there is a 2 dimensional space of solutions.

The matrix $S = \bepm 1 & 1 \\ 0 & 1 \eepm$ does not have a diagonal form.

ALT: Solve the difference equation. Let
\be
z_n = \lm^n \ \ra \ \lm^{n+2} = 2\lm^{n+1} - \lm^n \ \ra \ \lm = 1,1 \ \ra \ z_n = (A+Bn)z^n = (A+Bn)1^n.
\ee

$e_1\mapsto e_1$ and $e_2 \mapsto e_1 + e_2$.
\end{solution}


\begin{problem}
Let $V$ be a vector space of finite dimension over a field $F$. Let $\alpha$ be an endomorphism of $V$ and let $U$ be an $\alpha$-invariant subspace of $V$ (so $\alpha(U) \leq U$). The quotient group $V/U = \{bv + U : \bv \in V \}$ is a vector space under natural operations (called the quotient space). Write $\bar{V} = V/U$, $\bar{\bv} = \bv + U$, and define $\bar{\alpha} \in L(\bar{V})$ by $\bar{\alpha}(\bar{\bv}) = \overline{\alpha(\bv)}$. Check that $\alpha$ is a well-defined endomorphism of $\bar{V}$. Consider a basis $\bv_1,\dots, \bv_n$ of $V$ containing a basis $\bv_1,\dots, \bv_k$ of $U$. Show that the matrix of $\alpha$ with respect to $\bv_1, \dots, \bv_n$ is $A = \bepm
B & D\\
0 & C
\eepm$, with $B$ the matrix of the restriction $\alpha_U$ of $\alpha$ to $U$ with respect to $\bv_1, \dots, \bv_k$, and $C$ the matrix of $\alpha$ with respect to $\overline{\bv_{k+1}}, \dots, \overline{\bv_n}$. Deduce that $\chi_\alpha = \chi_{\alpha U}\chi_{\bar{\alpha}}$.
\end{problem}

\begin{solution}[\bf Solution.]
Choose a basis such that the vectors have the form
\be
\left( {\begin{array}{*{20}c}
  k\times k  &\vline &  k \times (n-k)   \\
\hline
  (n-k)\times k  &\vline &  (n-k) \times (n-k)   \\
\end{array} } \right) 
\left( {\begin{array}{*{20}c}
  k \\
\hline
  (n-k)
\end{array} } \right)
\ee

So that the first $k$ elements in the column vector lie in $U$ and the others provide a completion of the basis to the whole space. 

Since $AU\leq U$ the bottom left hand corner of the matrix is zero.
\be
\left( {\begin{array}{*{20}c}
  k\times k  &\vline &  k \times (n-k)   \\
\hline
  0 &\vline &  (n-k) \times (n-k)   \\
\end{array} } \right) 
\left( {\begin{array}{*{20}c}
  k \\
\hline
  (n-k)
\end{array} } \right)
\ee

Consider $v =\sum^k_{j=1}a_jv_j + \sum^n_{j=k+1}a_j v_j$
\beast
Av & = & A \bb{\sum^k_{j=1}a_jv_j} + \sum^n_{j=k+1} a_j A v_j = A\bb{\sum^k_{j=1}a_j v_j} + \sum^n_{j=k+1}a_j \sum^n_{i=1}b_{ij}v_i\\
 & = & A\bb{\sum^k_{j=1}a_j v_j} + \sum^k_{i=1}\bb{ \bb{b_{ij} \sum^n_{j=k+1}a_j}v_i} + \sum^n_{i=k+1}\bb{\bb{b_{ij} \sum^n_{j=k+1}a_j} v_i}
\eeast

The first term is an element of $U$, it is the image of the part of the original vector restricted to $U$. The central term is the part of the complement of $U$ which is mapped into $U$. The final term is the part of the complement of $U$ which is mapped into the complement of $U$.

\be
A v = A|_U \bb{\sum^k_{j=1}a_j v_j} + \sum^k_{i=1}\bb{\bb{b_{ij}\sum^n_{j=k+1}a_j}v_i} + \ol{A|_{V-U}v}.
\ee

Hence
\be
\left( {\begin{array}{*{20}c}
  B_{k\times k}  &\vline &  D_{k \times (n-k)}   \\
\hline
  0 &\vline &  C_{(n-k) \times (n-k)}   \\
\end{array} } \right) 
\left( {\begin{array}{*{20}c}
  k \\
\hline
  (n-k)
\end{array} } \right)
\ee

where $B$ is the matrix representing the restriction of $A$ to the subspace $U$. $C$ is the matrix representing $\ol{A}$ on $V/U$.

Claim: $\chi_\alpha = \chi_{\alpha(U)} \chi_{\ol{\alpha}}$.

Directly from the form of the matrix we have that the characteristic equation for $A$ is a product of the characteristic equation for $B$ and $C$.
\end{solution}


\begin{problem}
(Another proof of the Cayley Hamilton Theorem.) Assume that the Cayley Hamilton Theorem holds for any endomorphism on any vector space over the field $F$ of dimension less than $n$. Let $V$ be a vector space of dimension $n$ and let $\alpha$ be an endomorphism of $V$. If $U$ is a proper $\alpha$-invariant subspace of $V$, use the previous question and the induction hypothesis to show that $\chi_\alpha(\alpha) = 0$. If no such subspace exists, show that there exists a basis $\bv, \alpha(\bv), \dots \alpha^{n-1}(\bv)$ of $V$. Show that $\alpha$ has matrix
\be
\bepm
0 & & & -a_0\\
1 & \ddots & & -a_1\\
& \ddots & 0 & \vdots\\
& & 1 & -a_{n-1}
\eepm
\ee
with respect to this basis, for suitable $a_i \in F$. By expanding in the last column or otherwise, show that 
\be
\chi_\alpha (t) = t^n + a_{n-1}t^{n-1} + \dots + a_0.
\ee

Show that $\chi_\alpha(\alpha)(\bv) = {\bf 0}$, and deduce that $\chi_\alpha(\alpha)$ is 0 on $V$.
\end{problem}

\begin{solution}[\bf Solution.]
Let $\chi(x) = \det(A-xI) = p(x) \prod_{\lm} (x-\lm)^{a(\lm)}$. The $p(x)$ contains all the factors which cannot be factorised into linear terms.

Note that each $(x-\lm)^{a(\lm)}$ defines an invariant proper subspace of $V$, say $U(\lm)$ via the operator $(A-\lm I)^{a(\lm)}$.

$E(\lm) = \ker(A-\lm I)$ identifies the subspace of $U(\lm)$ of eigenvectors of eigenvalue $\lm$. Suppose the algebraic multiplicity of the eigenvalue $\lm$ is $a(\lm)$ and the geometric multiplicity is $g(\lm)$, then $\dim U(\lm) = a(\lm)$, $\dim E(\lm) = g(\lm)$ so there is a shortfall of $a(\lm) -g(\lm)$ of vectors needed to make a basis for $U(\lm)$.

We claim that $(A-\lm I)^{a(\lm) -g(\lm)+1}$ vanishes on all vectors in $U(\lm)$ and $(A-\lm I)^{a(\lm)-g(\lm)} \neq 0$. If this is the case then $\exists w$ such that $w, (A-\lm I)w, \dots (A-\lm I)^{a(\lm)-g(\lm)}w$ are linearly independent and complete the basis for $U(\lm)$.

Let $\sum^{a(\lm) - g(\lm)}_{j=0} a_j (A-\lm I)^jw = 0$ and act with $(A-\lm I)^{a(\lm)-g(\lm)}$ then continue by induction to see that the set is linearly independent and consists of exactly the number of elements required to complete the basis.

With respect to this basis the matrix of $A$ restricted to $U(\lm)$ takes the form,
\be
B = \bepm
\lm\ \ & 0 & \dots & & & \dots & 0\\
0 \ & \ \ \ddots & & & & & \vdots\\
\vdots\  &\ \ \ddots & \ddots & & & & \\
\ & & 0 &\ \ \ddots & & & \\
& & \vdots & 1 & \ \ \ddots & & \vdots\\
\vdots & & & 0 & \ \ \ddots & \ddots & 0\\
0 & \dots & & \dots & 0 & 1 & \ \lm
\eepm \quad \text{ where }A = \bepm
B& D\\
0 & C
\eepm \text{form Q16}
\ee

Argue by induction, considering the action on the quotient at each step, to reduce the matrix $A$ to a product of matrices of this form with one such matrix of form $\bepm
B& D\\
0 & C
\eepm$ for each of the invariant subspaces including the kernel. Note that a combination of basis elements for each of these subspaces forms a basis for their direct sum. When there are no more proper invariant subspaces the above basis vectors for the invariant subspaces can be completed to a basis for the whole space by adding, $e_1,\dots,e_m$. The images of this basis $Ae_1,\dots,Ae_m$ all include components in the invariant subspaces since the remaining subset is not an invariant subspace. Therefore it it possible to choose basis elements such that the matrix $A$ has the form
\be
A = \bepm
0\ \ & 0 & \dots & & & \dots & -a_0\\
1 \ & \ \ \ddots & & & & &  -a_1\\
\vdots\  &\ \ \ddots & \ddots & & & & \vdots\\
\ & & 1 &\ \ \ddots & & & \\
& & \vdots & 1 & \ \ \ddots & & \vdots\\
\vdots & & & 1 & \ \ \ddots & \ddots & 0\\
0 & \dots & & \dots & 0 & 1 & -a_{n-1}
\eepm
\ee

Now note that
\be
\det(A-xI) = \bevm
-x\ \ & 0 & \dots & & & \dots & -a_0\\
1 \ & \ \ \ddots & & & & &  -a_1\\
\vdots\  &\ \ \ddots & \ddots & & & & \vdots\\
\ & & 1 &\ \ \ddots & & & \\
& & \vdots & 1 & \ \ \ddots & & \vdots\\
\vdots & & & 1 & \ \ \ddots & \ddots & 0\\
0 & \dots & & \dots & 0 & 1 & -x-a_{n-1}
\eevm
\ee

Defines the polynomial $p(x)$ above and by explicit calculation,
\be
p(x) = (-1)^n \chi(x) = x^n + a_{n-1}x^{n-1} + \dots + a_0
\ee

By explicit evaluation note that $p(A) = 0$ on these remaining basis vectors. Therefore the matrix satisfied its own characteristic polynomial.
\end{solution}


\begin{problem}
Show that the dual of the space $P$ of real polynomials is isomorphic to the space $\R^\N$ of all sequences of real numbers, via the mapping which sends a linear form $\xi : P \to \R$ to the sequence $(\xi(1),\xi(t), \xi(t^2), \dots)$.

In terms of this identification, describe the effect on a sequence $(a_0, a_1, a_2, \dots)$ of the linear maps dual to each of the following linear maps $P \to P$:
\ben
\item [(a)] The map $D$ defined by $D(p)(t) = p'(t)$.
\item [(b)] The map $S$ defined by $S(p)(t) = p(t^2)$.
\item [(c)] The map $E$ defined by $E(p)(t) = p(t - 1)$.
\item [(d)] The composite $DS$.
\item [(e)] The composite $SD$.
\een
Verify that $(DS)^* = S^*D^*$ and $(SD)^* = D^*S^*$.
\end{problem}

\begin{solution}[\bf Solution.]
Claim: The map $\theta:(\xi:P\to\R)\mapsto (\xi(1),\xi(t),\dots)$ is linear.
\be
\theta:(\xi:P\to \R) \mapsto (\xi(1),\xi(t),\dots),\quad\quad \theta:(0:P\to \R)\mapsto (0(1),0(t),\dots) = (0,0,\dots)
\ee
by definition of the zero map therefore $\theta(0) = (0,0,\dots)$.
\be
\theta(\lm \xi) = \bb{(\lm \xi)(1), (\lm \xi)(t),\dots } = \bb{\lm \xi(1),\lm \xi(t),\dots} = \lm \bb{\xi(1),\xi(t),\dots} = \lm \theta(\xi).
\ee

\be
\theta(\xi + \eta) = \bb{(\xi+ \eta)(1),(\xi+\eta)(t),\dots} = \bb{\xi(1)+\eta(1),\xi(t)+ \eta(t),\dots} = \theta(\xi) + \theta(\eta).
\ee

Therefore the map $\theta$ is linear.

Claim: The map $\theta:(\xi:P\to \R)\mapsto (\xi(1),\xi(t),\dots)$ is surjective.

Given $\bb{a_0, a_1,\dots}$ define $\xi$ by $\xi(1) =a_0, \xi(t) = a_1,\dots \xi(t^k) =a_k,\dots$. Then since $\xi$ is completely defined by its action on the basis $\bra{1,t,t^2,\dots, t^k,\dots}$ for the space of polynomials then it is well defined in this way.

Claim: The map $\theta: (\xi:P\to \R)\mapsto \bb{\xi(1),\xi(t),\dots}$ is injective.

Suppose $\bb{\xi(1),\xi(t),\dots} = \bb{\eta(1),\eta(t),\dots} \ \ra \ \xi(1)=\eta(1),\xi(t) = \eta(t),\dots, \xi(t^k) = \eta(t^k),\dots$ but $\xi$ is uniquely defined by its action on a basis for $P$ and since $\xi = \eta$ on all elements of the basis for $P$ then $\xi = \eta$. Thus the map is injective.

Hence the map $\theta:(\xi:P\to \R)\mapsto (\xi(1),\xi(t),\dots)$ is a linear isomorphism.

\ben
\item [(a)] $D(p)(t) = p'(t)$. $(D^*q^*)(p) = (q^*)(Dp) = (q^*)(p')$.

Let $q^* = (a_0,a_1,\dots) = (q^*(1),q^*(t),\dots)$ where on the RHS this means dual pairing. $\ra q^* = \sum^N_{k=0}a_k (t^k)^*$.

Let 
\be
p(t) = t^m \ \ra \ p'(t) = mt^{m-1} \ \ra \ q^*(p') = \bb{\sum^N_{k=0} a_k (t^k)^*}(mt^{m-1})  =ma_{m-1}
\ee

Therefore $(D^*q^*)(t^m) = ma_{m-1} (t^m)^*(t^m)$. By linearity,
\be
(D^*q^*)  =\sum^N_{m=1} ma_{m-1}(t^m)^* = (0,a_0,2a_1,3a_2,\dots).
\ee

\item [(b)] $S(p)(t) = p(t^2)$. Use $(S^*q^*)(p) = (q^*)(Sp)$.

Let $q^* = (a_0,a_1,\dots) = (q^*(1),q^*(t),\dots) \ \ra \ q^* = \sum^N_{k=0} a_k (t^k)^*$.

Let 
\be
p(t) = t^m \ \ra \ (Sp)(t) = t^{2m} \ \ra \ q^*(Sp) = \bb{\sum^N_{k=0} a_k (t^k)^*}(t^{2m})  =a_{2m}
\ee

Therefore $(q^*)(S(t^m)) = (S^*q^*)(t^m) = a_{2m} (t^m)^*(t^m) \ \ra \ S^*q^* = \sum^N_{k=0} a_{2k} (t^k)^*$. By linearity,
\be
(S^*q^*)  =\sum^N_{m=1} a_{2m}(t^m)^* = (a_0,a_2,a_4,a_6,\dots).
\ee

\item [(c)] $E(p)(t) = p(t-1)$. Use $(E^*q^*)(p) = (q^*)(Ep)$.

Let $q^* = \sum^N_{k=0} a_k (t^k)^*$. Let 
\be
p(t) = t^m \ \ra \ (Ep)(t) = (t-1)^{m} \ \ra \ q^*(Et^{m}) = \bb{\sum^N_{k=0} a_k (t^k)^*}(t-1)^{m}  = \bb{\sum^N_{k=0} a_k (t^k)^*} \bb{\sum^m_{s=0} \binom{m}{s} (t^s)(-1)^{m-s}} 
\ee

Therefore $(q^*)(E(t^m)) = (E^*q^*)(t^m) = \bb{\sum^m_{s=0} a_s \binom{m}{s} (-1)^{m-s}}(t^m)^*(t^m) $. 
\be
(E^*q^*)  = \sum^N_{k=0} \sum^k_{s=0} a_s \binom{k}{s}(-1)^{k-s}(t^k)^* = (a_0,a_1-a_0,\dots).
\ee

\item [(d)] $DS(p)(t) = (t^2)$. Let $q^* = \sum^N_{k=0} a_k (t^k)^*$ and let 
\be
p(t) = t^m \ \ra \ DS(p)(t) = 2mt^{2m-1} \ \ra \ q^*(DSt^m) = \bb{\sum^N_{k=0} a_k (t^k)^*}(2m t^{2m-1})  =a_{2m-1}2m
\ee

Therefore $(q^*)(DS(t^m)) = ((DS)^*q^*)(t^m) = \sum^N_{k=1} 2k a_{2k-1} (t^k)^*$. Thus,
\be
((DS)^*q^*)  = \sum^N_{k=1} 2ka_{2k-1}(t^k)^* = (0,2a_1,4a_3,6a_5,\dots).
\ee

Claim: $(DS)^* = S^* D^*$, $(q^*) = (a_0,a_1,a_2,a_3,\dots)$ and from above have 
\be
(S^*q^*) = (a_0,a_2,a_4,a_6,\dots),\quad (D^*q^*) = (0,a_0,2a_1,3a_2,\dots),\quad (S^*D^*q^*) = S^*(0,a_0,2a_1,3a_2,\dots) = (0,2a_1,6a_5,\dots).
\ee
as required.

\item [(e)] $SD$ explicitly where $SD(p)(t) = (p')^2$. Let $q^* = \sum^N_{k=0} a_k (t^k)^*$ and let
\be
p(t) = t^m \ \ra \ SD(p)(t) = mt^{2(m-1)} \ \ra \ q^*((SD)t^m) = \bb{\sum^N_{k=0} a_k (t^k)^*}(mt^{2(m-1)})  = m a_{2(m-1)}
\ee

Therefore $(q^*)(SD) = ((SD)^*q^*) = \sum^N_{k=0} k a_{2(k-1)} (t^k)^* = (a_0,2a_2,3a_4,\dots)$.

Claim: $(SD)^* = D^*S^*$. $(q^*) = (a_0,a_1,a_2,a_3,\dots)$ and from above have 
\be
(S^*q^*) = (a_0,a_2,a_4,a_6,\dots),\quad (D^*q^*) = (0,a_0,2a_1,3a_2,\dots),\quad (D^*S^*q^*) = D^*(a_0,a_2,a_4,\dots) = (0,a_0,2a_2,3a_4\dots).
\ee
as required. Thus $(SD)^* = D^*S^*$.

Explanation: If $P\stackrel{f}{\to}Q \stackrel{g}{\to}R$ then take the duals $R^*\stackrel{g^*}{\to}Q^* \stackrel{f^*}{\to}P^*$. $f^*(q)^* \in P^*$ and $g^*(r)^* \in Q^*$,
\be
f^*(g^*(r)^*)(p) = (g^*(r)^*)(f(p)) = (r^*)(g(f(p))) = ((gf)^*(r^*))(p) \ \ra \ f^*g^* = (gf)^*.
\ee

\een
\end{solution}


\begin{problem}
For $A$ an $n \times m$ and $B$ an $m \times n$ matrix over the field $F$, let $\tau_A(B)$ denote $\tr AB$.

Show that, for each fixed $A$, $\tau_A$ is a linear map $\text{Mat}_{m,n}(F) \to F$.

Now consider the mapping $A \mapsto \tau_A$. Show that it is a linear isomorphism $\text{Mat}_{n,m}(F) \to \text{Mat}_{m,n}(F)^*$.
\end{problem}

\begin{solution}[\bf Solution.]
Claim: $\tau_A$ is a linear map.

$\tau_A(B) = \tr(AB)$ but trace is linear. 
\be
\tau_A(0) = \tr(0) = 0,\quad \tau_A(\lm B) = \tr(A\lm B) = \lm \tr(AB) = \lm \tau_A (B),
\ee
\be
\tau_A(B+C) = \tr(AB+AC) = \tr(AB)+ \tr(AC) = \tau_A (B) + \tau_A (C).
\ee

Claim: $T:A\mapsto \tau_A$ is a linear map.
\be
T(0) = \tau_0: B\mapsto \tr(0B) = 0 ,\quad T(\lm A) = \tr(\lm A) : B \mapsto \tr\bb{\lm AB} = \lm \tau_A = \lm T(A),
\ee
\be
T(A+C) = \tau_{A+C}:B \mapsto \tr(AB+CB) = \tr(AB)+ \tr(CB) = \tau_A (B) + \tau_A (C) = T(A) + T(C). 
\ee

Claim: $T:A\mapsto \tau_A$ is an isomorphism.

Surjective: Note that the action of $\tau_A$ on the basis matrices $B^{k,m} = \delta_i^{(k)}\delta_j^{(m)}$ (i.e. $B^{(k,m)} = \delta_i^{(k)}\delta_j^{(m)}$ has $m$ in the $k,m$th slot and zeros elsewhere) fixes all the components of $A$. (To see this consider the illustration)
\be
\left.\tr\bepm
a_{11} & a_{12} & a_{13}\\
a_{21} & 0 & 1\\
0 & 1 & 1
\eepm \bepm
1 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\eepm = \tr\bepm
a_{11} & 0 & 0\\
a_{21} & 0 & 0\\
0 & 0 & 0
\eepm = a_{11},\quad \tr\bepm
a_{11} & a_{12} & a_{13}\\
a_{21} & \dots & \dots\\
0 & \dots & \dots
\eepm \bepm
0 & 0 & 0\\
1 & 0 & 0\\
0 & 0 & 0
\eepm = \tr\bepm
a_{12} & 0 & 0\\
\dots & 0 & 0\\
\dots & 0 & 0
\eepm = a_{12}\rob
\ee

Given $\tau_A$ then $\tau_A (B) = \tr(AB)$ and then the action on the basis $\ra \tau_A(B) = \tr\bb{AB^{k,m}} = A_{m,k}$.

Hence define $T^{-1}(\tau_A) = A$ given by these components.

Injective: Given $\tau_A =\tau_C\ \ra \ \tau_A(B) =\tau_C(B) \ \ra \ \tr(AB) = \tr(AC)$.

Take $B = B^{k,m}$ for each of the basis elements $\ra A_{ij} = C_{ij} \ \ra \ A=C$, thus $T$ is isomorphism.
\end{solution}


\begin{problem}
Let $\alpha : V \to V$ be an endomorphism of a finite dimensional complex vector space and let $\alpha^* : V^* \to V^*$ be its dual. Show that a complex number $\lm$ is an eigenvalue for $\alpha$ if and only if it is an eigenvalue for $\alpha^*$. How are the algebraic and geometric multiplicities of $\lm$ for $\alpha$ and $\alpha^*$ related? How are the minimal and characteristic polynomials for $\alpha$ and $\alpha^*$ related?
\end{problem}

\begin{solution}[\bf Solution.]
Claim: An eigenvalue of $\alpha$ is also an eigenvalue of $\alpha^*$.

Let $\alpha v = \lm v$ then for any $w$,
\be
w^*(\alpha v) = w^* (\lm v) \ \ra \ \bb{\alpha^* w^*}(v) = (\lm w^*)(v) \ \ra \ \bb{\alpha^*w^* - \lm w^*}(v) = 0.
\ee

Now pick $w^* = v^*$ then by definition of dual $bb{\alpha^*v^* - \lm v^*}(e) = 0$ for all $e\neq v$ and by above vanishes on $v$. Therefore $\alpha^{*}v^{*} - \lm v^* = 0$ the zero map.

Repeat the argument for $\alpha^{*}v^{*} = \lm v^* $ to obtain $\alpha^{**}v^{**} =  \lm v^{**}$ and use the natural isomorphism between a vector space and its double dual in finite dimensions. Thus, $\alpha^* v^* = \lm v^*$ iff $\alpha v = \lm v$.

Algebraic multiplicities: Consider $\det(\alpha - \lm I) = 0$. By Cayley-Hamilton $\chi_\alpha (x) = \det(\alpha - xI)$ then $\chi_\alpha(\alpha) = 0$. Consider any pair of vector $w^*\bb{\chi_\alpha(\alpha)v} = 0$ but by definition of dual. Hence $\bb{\chi_\alpha(\alpha)}^* = 0$ but by linearity $\bb{\chi_\alpha (\alpha)}^* = \chi_\alpha (\alpha^*)$. So $\alpha^*$ satisfies the same $n$th order polynomial. Therefore both have the same characteristic equation so the algebraic multiplicities of the eigenvalues are the same.

Geometric multiplicities: Follows from the first part that if $v$ is an eigenvector of eigenvalue $\lm$ of $\alpha$ then $v^*$ is an eigenvector of eigenvalue $\lm$ of $\alpha^*$. Therefore the geometric multiplicities are the same.

Characteristic equation: From above these are the same.

Minimal polynomial: By similar argument as that used above or by dualising the minimal polynomial these are also the same.

Note: In matrix notation the dual is just the transpose so these results are obvious.
\end{solution}

\begin{problem}
The square matrices $A$ and $B$ over the field $F$ are congruent if $B = P^TAP$ for some invertible matrix $P$ over $F$. Which of the following symmetric matrices are congruent to the identity matrix over $\R$, and which over $\C$? (Which, if any, over $\Q$?) Try to get away with the minimum calculation.
\be
\bepm
2 & 0\\
0 & 3
\eepm,\quad
\bepm
0 & 2\\
2 & 0
\eepm,\quad
\bepm
-1 & 0\\
0 & -1
\eepm,\quad
\bepm
4 & 4\\
4 & 5
\eepm.
\ee
\end{problem}

\begin{solution}[\bf Solution.]
Recall that two matrices represent the same bilinear form with respect to different bases iff the matrices are congruent. $B^TAB = I$.

\ben
\item [(ia)] 
\be
\bepm 2 & 0\\ 0 & 3\eepm \ \ra \ \bepm a & b\\ c & d\eepm \bepm 2 & 0\\ 0 & 3\eepm \bepm a & c\\ b & d\eepm \ \ra \ \bepm 1 & 0\\ 0 & 1\eepm \ \ra \ 6\bb{\det B}^2 = 1 \ \ra \ \det B = \pm\frac 1{\sqrt{6}}.
\ee

A product of rational is always rational therefore NO solutions over $\Q$.

\item [(ib,c)] 
\be
\bepm 2a & 3b\\ 2c & 3d\eepm \bepm a & b\\ c & d\eepm = \bepm 2a^2 + 3b^2 &\  2ac + 3bd \\ 2ac + 3bd & \ 2c^2 + 3d^2\eepm = \bepm 1 & 0\\ 0 & 1\eepm \ \ra \ a = \frac 1{\sqrt{2}},\ b = 0,\ c = 0,\ d = \frac 1{\sqrt{3}}.
\ee

Thus congruent over $\R$ and therefore congruent over $\C$.

\item [(iia,b)] 
\be
\bepm 0 & 2\\ 2 & 0\eepm \ \ra \ \bepm a & b\\ c & d\eepm \bepm 0 & 2\\ 2 & 0\eepm \bepm a & c\\ b & d\eepm \ \ra \ \bepm 1 & 0\\ 0 & 1\eepm \ \ra \ (-4)\bb{\det B}^2 = 1
\ee

No real solutions and therefore. Thus no solutions over $\Q$ or $\R$.

\item [(iic)]
\be
\bepm 2b & 2a\\ 2d & 3c \eepm \bepm a & b\\ c & d\eepm = \bepm 4ab &\  2bc + 2ad \\ 2ad + 2bc & \ 4cd\eepm = \bepm 1 & 0\\ 0 & 1\eepm \ \ra \ a = \frac 14,\ b = 1,\ c = -\frac i4,\ d = i.
\ee

Congruent over $\C$.

\item [(iiia,b)] 
\be
\bepm a & b\\ c & d \eepm \bepm -1 & 0 \\ 0 & -1 \eepm \bepm a & b\\ c & d\eepm = -\bepm a^2+b^2 &\  ac + bd \\ ac + bd & \ c^2 + d^2 \eepm = \bepm 1 & 0\\ 0 & 1\eepm \ \ra \ ac + bd = 0,\ a^2 + b^2 = -1,\ c^2 + d^2 = -1.
\ee

The last two equations have no solutions. Thus no congruent over $\Q$ or $\R$.

\item [(iiic)] 
\be
\bepm a & b\\ c & d \eepm \bepm -1 & 0 \\ 0 & -1 \eepm \bepm a & b\\ c & d\eepm = -\bepm a^2+b^2 &\  ac + bd \\ ac + bd & \ c^2 + d^2 \eepm = \bepm 1 & 0\\ 0 & 1\eepm \ \ra \ a=0\, b=i,\ c=i,\ d =0.
\ee
Thus congruent over $\C$.


\item [(iva)] 
\be
\bepm a & b\\ c & d \eepm \bepm 4 & 4 \\ 4 & 5 \eepm \bepm a & b\\ c & d\eepm = -\bepm a^2+b^2 &\  ac + bd \\ ac + bd & \ c^2 + d^2 \eepm = \bepm 1 & 0\\ 0 & 1\eepm \ \ra \ 4\det B^2 = 1 \ \ra \ \det B = \pm \frac 12.
\ee

\be
\bepm 4 & 4 \\ 4 & 5 \eepm \bepm a & b\\ c & d\eepm = 2k\bepm d & -c \\ -b & a \eepm \bepm 1 & 0\\ 0 & 1\eepm \ \ra \ \bepm 4a+4b & 4c+4d \\ 4a+5b & 4c + 5d \eepm = k \bepm 2d & -2c\\ -2b & 2a\eepm, \ k = \pm 1.
\ee

If $k=1$, $\bepm 4a+4b-2d & 6c+4d \\ 4a+5b+ 2b & 4c + 5d-2a \eepm = 0  \ \ra \ b = -\frac{4a}7,\ c = - \frac {4a}7, \ d = \frac {6a}7.$

Thus, congruent over $\Q$ and hence over $\R$ and $\C$ as well.

\een

\end{solution}


\begin{problem}
Find the rank and signature of the following quadratic forms over $\R$.
\be
x^2 + y^2 + z^2 - 2xz - 2yz,\quad x^2 + 2y^2 - 2z^2 - 4xy - 4yz,\quad 16xy - z^2,\quad 2xy + 2yz + 2zx.
\ee
If $A$ is the matrix of the first of these (say), find a non-singular matrix $P$ such that $P^TAP$ is diagonal with entries $\pm 1$.
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(i)] $x^2 + y^2 + z^2 - 2xz - 2yz = (x-z)^2 + (y-z)^2 -z^2$. $\rank = 3$, $\sgn = (2,1) = 2-1 = 1$. Define $X=x-z,Y = y-z,Z=z$. Inverting we have
\be
\left\{\ba{l}
x = X + Z\\
y = Y +Z\\
z=Z
\ea\right. \ \ra \ \bepm
x\\
y\\
z 
\eepm = \bepm
1 & 0 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\eepm \bepm
X\\
Y\\
Z 
\eepm
\ee

Thus, 
\be
\bepm
1 & 0 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\eepm^T \bepm
1 & 0 & -1\\
0 & 1 & -1\\
-1 & -1 & 1
\eepm \bepm
1 & 0 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\eepm = \bepm
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & -1
\eepm.
\ee

\item [(ii)] $x^2 + 2y^2 - 2z^2 - 4xy - 4yz = (x-2y)^2 - 2(y+z)^2 -z^2$. $\rank = 2$, $\sgn = (1,1) = 1-1 = 0$. Define $X=x-2y,Y = y+z,Z=z$. Inverting we have
\be
\left\{\ba{l}
x = X + 2Y -2Z\\
y = Y -Z\\
z=Z
\ea\right. \ \ra \ \bepm
x\\
y\\
z 
\eepm = \bepm
1 & 2 & -2\\
0 & 1 & -1\\
0 & 0 & 1
\eepm \bepm
X\\
Y\\
Z 
\eepm
\ee

Thus, 
\be
\bepm
1 & 2 & -2\\
0 & 1 & -1\\
0 & 0 & 1
\eepm^T \bepm
1 & -1 & 0\\
-1 & 1 & -1\\
0 & -1 & 1
\eepm \bepm
1 & 2 & -2\\
0 & 1 & -1\\
0 & 0 & 1
\eepm = \bepm
1 & 0 & 0\\
0 & -2 & 0\\
0 & 0 & 0
\eepm.
\ee

\item [(iii)] $16xy - z^2 = 4(x+y)^2 - 4(x-y)^2 -z^2$. $\rank = 3$, $\sgn = (1,2) = 1-2 = -1$. Define $X=x+y,Y = x-y,Z=z$. Inverting we have
\be
\left\{\ba{l}
x = \frac 12\bb{X + Y}\\
y = \frac 12\bb{X - Y}\\
z=Z
\ea\right. \ \ra \ \bepm
x\\
y\\
z 
\eepm = \bepm
\frac 12 & \frac 12 & 0\\
\frac 12 & -\frac 12 & 0\\
0 & 0 & 1
\eepm \bepm
X\\
Y\\
Z 
\eepm
\ee

Thus, 
\be
\bepm
\frac 12 & \frac 12 & 0\\
\frac 12 & -\frac 12 & 0\\
0 & 0 & 1
\eepm ^T \bepm
0 & 8 & 0\\
8 & 0 & 0\\
0 & 0 & -1
\eepm \bepm
\frac 12 & \frac 12 & 0\\
\frac 12 & -\frac 12 & 0\\
0 & 0 & 1
\eepm  = \bepm
4 & 0 & 0\\
0 & -4 & 0\\
0 & 0 & -1
\eepm.
\ee

\item [(iv)] Let $X=x+y$, $Y=x-y$. $2xy + 2yz + 2zx = \frac 12 \bb{(X+2Z)^2 - Y^2 - 4Z^2}$. $\rank = 3$, $\sgn = (1,2) = 1-2 = -1$. Define $X'=X+2Z,Y' = Y,Z'=Z$. Inverting we have
\be
\left\{\ba{l}
x = \frac 12\bb{X + Y}\\
y = \frac 12\bb{X - Y}\\
z=Z
\ea\right. \ \ra \ \bepm
x\\
y\\
z 
\eepm = \bepm
\frac 12 & \frac 12 & 0\\
\frac 12 & -\frac 12 & 0\\
0 & 0 & 1
\eepm \bepm
X\\
Y\\
Z 
\eepm,\quad\quad 
\left\{\ba{l}
X = X' - 2Z' \\
Y = Y'\\
Z=Z'
\ea\right. \ \ra \ \bepm
X\\
Y\\
Z 
\eepm = \bepm
1 & 0 & -2\\
0 & 1 & 0\\
0 & 0 & 1
\eepm \bepm
X'\\
Y'\\
Z'
\eepm
\ee

Thus, 
\be
\bepm
1 & 0 & -2\\
0 & 1 & 0\\
0 & 0 & 1
\eepm^T
\bepm
\frac 12 & \frac 12 & 0\\
\frac 12 & -\frac 12 & 0\\
0 & 0 & 1
\eepm ^T \bepm
0 & 1 & 1\\
1 & 0 & 1\\
1 & 1 & 0
\eepm \bepm
\frac 12 & \frac 12 & 0\\
\frac 12 & -\frac 12 & 0\\
0 & 0 & 1
\eepm \bepm
1 & 0 & -2\\
0 & 1 & 0\\
0 & 0 & 1
\eepm = \bepm
\frac 12 & 0 & 0\\
0 & -\frac 12 & 0\\
0 & 0 & -2
\eepm.
\ee
\een

Aside.

\be
\bepm
1 & 0 & 0
\eepm 
\bepm
1 & 0 & -1\\
0 & 1 & -1\\
-1 & -1 & 1
\eepm 
\bepm
1 \\
0 \\
0 
\eepm  = 1,\quad \bepm
1 & 0 & 0
\eepm 
\bepm
1 & 0 & -1\\
0 & 1 & -1\\
-1 & -1 & 1
\eepm 
\bepm
0 \\
1 \\
0 
\eepm  = 0,\quad \bepm
1 & 0 & 0
\eepm 
\bepm
1 & 0 & -1\\
0 & 1 & -1\\
-1 & -1 & 1
\eepm 
\bepm
1 \\
1 \\
1 
\eepm  = 0.
\ee


\be
\bepm
0 & 1 & 0
\eepm 
\bepm
1 & 0 & -1\\
0 & 1 & -1\\
-1 & -1 & 1
\eepm 
\bepm
1 \\
0 \\
0 
\eepm  = 0,\quad \bepm
0 & 1 & 0
\eepm 
\bepm
1 & 0 & -1\\
0 & 1 & -1\\
-1 & -1 & 1
\eepm 
\bepm
0 \\
1 \\
0 
\eepm  = 1,\quad \bepm
0 & 1 & 0
\eepm 
\bepm
1 & 0 & -1\\
0 & 1 & -1\\
-1 & -1 & 1
\eepm 
\bepm
1 \\
1 \\
1 
\eepm  = 0.
\ee

\be
\bepm
0 & 0 & 1
\eepm 
\bepm
1 & 0 & -1\\
0 & 1 & -1\\
-1 & -1 & 1
\eepm 
\bepm
1 \\
0 \\
0 
\eepm  = -1,\quad \bepm
0 & 0 & 1
\eepm 
\bepm
1 & 0 & -1\\
0 & 1 & -1\\
-1 & -1 & 1
\eepm 
\bepm
0 \\
1 \\
0 
\eepm  = 1,\quad \bepm
0 & 0 & 1
\eepm 
\bepm
1 & 0 & -1\\
0 & 1 & -1\\
-1 & -1 & 1
\eepm 
\bepm
1 \\
1 \\
1 
\eepm  = -1.
\ee

\end{solution}


\begin{problem}
\ben
\item [(i)] Show that the function $\psi(A,B) = \tr(AB^T)$ is a symmetric positive definite bilinear form on the space $\text{Mat}_n(\R)$ of all $n \times n$ real matrices. Deduce that $\abs{\tr(AB^T)} \leq \tr(AA^T)^{1/2}\tr(BB^T)^{1/2}$.
\item [(ii)] Show that the map $A \mapsto\tr(A^2)$ is a quadratic form on $\text{Mat}_n(\R)$. Find its rank and signature.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(i)] $\psi(A,B) = \tr(AB^T) = \tr(B^TA) = \tr((B^TA)^T) = \tr(A^TB) = \tr(BA^T) = \psi(B,A)$.

\beast
\psi\bb{(A+\lm a), (B+\mu b)} & = & \tr\bb{(A+\lm a)(B+\mu b)^T} = \tr(AB^T + \mu Ab^T + \lm a B^T + \mu\lm ab^T)  \\
& = & \psi(A,B) + \mu\psi(A,b) + \lm \psi(a,B) + \lm \mu \psi(a,b).
\eeast

Thus, $\psi(A,B)$ is a bilinear form on $n\times n$ matrices.

\be
\psi(A,A) = A_{ij}A_{ji}^T = A_{ij}^2 \ \ra \ \psi \text{ is positive definite.}
\ee

\item [(ii)] The rank of the bilinear form is the rank of the associated quadratic form. (In a given matrix basis they have the same matrix.)

Since $\psi(A,A) = A_{ij}^T$ it vanishes only on matrices with zeros down the diagonal. Therefore nullity $n$, the number of elements along the diagonal of a matrix. The dimension of the space of matrices is $n^2$, hence the rank = $n^2 - n$.

The signature of the bilinear form is the signature of the associated quadratic form. (In a given matrix basis they have the same matrix.) The bilinear form is positive definite and therefore the signature is $n$.
\een
\end{solution}


\begin{problem}
Let $\psi: V \times V \to \C$ be a Hermitian form on a complex vector space $V$.
\ben
\item [(i)] Show that if $n > 2$ then $\psi(u, v) = \frac 1n \sum^n_{k=1} \zeta^k \psi(u + \zeta^k v, u + \zeta^k v)$ where $\zeta = e^{2\pi i/n}$.
\item [(ii)] Find the rank and signature of $\psi$ in the case $V = \C^3$ and
\be
\psi(x, x) = |x_1 + ix_2|^2 + |x_2 + ix_3|^2 + |x_3 + ix_1|^2 - |x_1 + x_2 + x_3|^2.
\ee
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(i)] Convention $\C,\ol{\C}$ linear, 
\beast
\frac 1n \sum^n_{k=1} \zeta^k \psi(u + \zeta^k v, u + \zeta^k v) & = & \frac 1n \sum^n_{k=1} \zeta^k \psi(u,u) + \zeta^k \psi(u,\zeta^k v) + \zeta^k \psi(\zeta^k v, u) + \zeta^k \psi (\zeta^kv ,\zeta^k v)\\
& = & \frac 1n \sum^n_{k=1} \zeta^k \psi(u,u) + \zeta^k\ol{\zeta}^k \psi(u, v) + \zeta^k \zeta^k \psi(v, u) + \zeta^k \bb{\zeta\ol{\zeta}}^k \psi (v,v)\\
& = & \frac 1n \psi(u,u) \sum^n_{k=1} \zeta^k  + \frac 1n \psi(u,v) \sum^n_{k=1} \zeta^k\ol{\zeta}^k + \frac 1n \psi(v,u) \sum^n_{k=1}\zeta^{2k} + \frac 1n\psi(v,v) \sum^n_{k=1} \zeta^k 
\eeast
but $\sum^n_{k=1}\zeta^k = 0$ and since $\zeta^2$ is also a root of unity $\sum^n_{k=1}\zeta^{2k} = 0$.
\be
\frac 1n \sum^n_{k=1} \zeta^k \psi(u + \zeta^k v, u + \zeta^k v)  = \frac 1n \psi(u,v)n = \psi(u,v).
\ee

\item [(ii)] The most direct method is to expand 
\be
\psi(x,x) = \abs{x_1}^2 + \abs{x_2}^2  +\abs{x_3}^2  + x_1\ol{x_2}(-1-i) + \ol{x_1}x_2(-1+i) + x_1\ol{x_3}(-1+i) + x_3\ol{x_1}(-1-i) + \ol{x_3}x_2(-1-i) + x_3\ol{x_2}(-1+i).
\ee

Switching to matrix notation we have
\be
\psi(x,x) = \bepm x_1\\ x_2 \\ x_3 \eepm \bepm
1 & (-1-i) & (-1+i) \\
(-1+i) & 1 & (-1-i) \\
(-1-i) & (-1+i) & 1
\eepm \bepm
\ol{x_1}\\
\ol{x_2}\\
\ol{x_3}
\eepm
\ee

\be
\det\bepm
1 & (-1-i) & (-1+i) \\
(-1+i) & 1 & (-1-i) \\
(-1-i) & (-1+i) & 1
\eepm = 0\ \ra \ \lm^3 - 3\lm^2 - 3\lm + 1 = (\lm+1)(\lm-2)^2 = 0 \ \ra\ \lm = -1,2,2.
\ee

So the signature is (2,1) or 1 and the rank is 3.
\een
\end{solution}


\begin{problem}
Show that the quadratic form $2(x^2 + y^2 + z^2 - xy - yz - zx)$ is positive definite. Compute the basis of $\R^3$ obtained by applying the Gram-Schmidt process to the standard basis.
\end{problem}

\begin{solution}[\bf Solution.]

\be
\det\bepm 2-\lm & -1 & -1\\ -1 & 2-\lm & -1 \\ -1 & -1 & 2-\lm \eepm = 0\ \ra \ \lm(\lm -3)^2 = 0 \ \ra \ \lm =0,3,3.
\ee

We can choose the eigenvectors in the eigenplane to be orthogonal, and then three eigenvectors are,
\be
e_0 = \bepm
1\\
1\\
1
\eepm,\quad e_3 = \bepm
1\\
-1\\
0
\eepm,\quad e_{3(2)} = \bepm
1\\
1\\
-2
\eepm.
\ee

Note we have a symmetric matrix and therefore the matrix for the quadratic form and the associated bilinear form is the same.

Gram-Schmidt Construction
\be
e_1 = \bepm
1\\
0\\
0
\eepm,\quad e_2 = \bepm
0\\
1\\
0
\eepm,\quad e_3 = \bepm
0\\
0\\
1
\eepm,\quad\quad Q= \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm
\ee

Let $\inner{\cdot}{\cdot}$ be the bilinear form associated to $Q$. In matrix form it has the same symmetric matrix as $Q$ wrt any given basis for both.
\be
\bepm
1 & 0 & 0
\eepm \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm \bepm
1 \\
0\\
0
\eepm = 2 \ \ra \ E_1 = \bepm
\frac 1{\sqrt{2}}\\
0\\
0
\eepm \ \ra \ \inner{E_1}{E_1} = 1.
\ee

\beast
E_2 & = & \frac{e_2 - \inner{e_2}{E_1}E_1}{\inner{e_2 - \inner{e_2}{E_1}E_1}{e_2 - \inner{e_2}{E_1}E_1}^{1/2}}\\
& = &  \frac{\bepm
0 \\
1\\
0
\eepm - \bepm
0& 1 & 0
\eepm \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm \bepm
\frac 1{\sqrt{2}}\\
0\\
0
\eepm \bepm
\frac 1{\sqrt{2}}\\
0\\
0
\eepm}{\inner{\bepm
0 \\
1\\
0
\eepm - \bepm
0& 1 & 0
\eepm \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm \bepm
\frac 1{\sqrt{2}}\\
0\\
0
\eepm \bepm
\frac 1{\sqrt{2}}\\
0\\
0
\eepm}{\bepm
0 \\
1\\
0
\eepm - \bepm
0& 1 & 0
\eepm \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm \bepm
\frac 1{\sqrt{2}}\\
0\\
0
\eepm \bepm
\frac 1{\sqrt{2}}\\
0\\
0
\eepm}^{1/2}}\\
& = & \frac{\bepm
\frac 12 \\
1\\
0
\eepm }{\bb{\bepm
0 & 1 & 0
\eepm \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm \bepm
0 \\
1\\
0
\eepm}^{1/2} } = \sqrt{\frac 23}\bepm
\frac 12 \\
1 \\
0\\
\eepm
\eeast


Checking:

\be
\inner{E_1}{E_2} = \bepm
\frac 1{\sqrt{2}}\\
0\\
0
\eepm \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm \sqrt{\frac 23}\bepm
\frac 12 \\
1 \\
0
\eepm = 0
\ee

\be
\inner{E_2}{E_2} = \sqrt{\frac 23}\bepm
\frac 12 & 1 & 0
\eepm \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm \sqrt{\frac 23}\bepm
\frac 12 \\
1 \\
0\\
\eepm = 1
\ee

Also,

\beast
E_3 & = & \frac{e_3 - \inner{e_3}{E_1}E_1 - \inner{e_3}{E_2}E_2}{\inner{e_3 - \inner{e_3}{E_1}E_1 - \inner{e_3}{E_2}E_2}{e_3 - \inner{e_3}{E_1}E_1 - \inner{e_3}{E_2}E_2}^{1/2}}\\
& = &  \frac{\bepm
0 \\
0\\
1
\eepm - \bepm
0& 0 & 1
\eepm \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm \bepm
\frac 1{\sqrt{2}}\\
0\\
0
\eepm \bepm
\frac 1{\sqrt{2}}\\
0\\
0
\eepm - \bepm
0& 0 & 1
\eepm \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm  \sqrt{\frac 23}\bepm
\frac 12 \\
1 \\
0\\
\eepm \sqrt{\frac 23}\bepm
\frac 12 \\
1 \\
0\\
\eepm}{\inner{e_3 - \inner{e_3}{E_1}E_1 - \inner{e_3}{E_2}E_2}{e_3 - \inner{e_3}{E_1}E_1 - \inner{e_3}{E_2}E_2}^{1/2}}\\
& = & \frac{\bepm
1 \\
1\\
1
\eepm }{\bb{\bepm
1 & 1 & 1
\eepm \bepm
2 & - 1& -1 \\
-1 & 2 & -1 \\
- 1 & -1 & 2 
\eepm \bepm
1 \\
1\\
1
\eepm}^{1/2} }  = \frac{\bepm
1 \\
1\\
1
\eepm }{0^{1/2} } 
\eeast

$\inner{E_3}{E_3} = 0 $ because this was the 0 eigenvector.

Note: The G-S construction has to generate the eigenvectors for the 1 dimensional eigenspaces. If the eigenvector is in the kernel then the G-S process will find the direction but cannot scale the vector.
\end{solution}


\begin{problem}
Let $W \leq V$ with $V$ an inner product space. An endomorphism $\pi$ of $V$ is called an \emph{idempotent} if $\pi^2 = \pi$. Show that the orthogonal projection onto $W$ is a self-adjoint idempotent. Conversely show that any self-adjoint idempotent is orthogonal projection onto its image.
\end{problem}

\begin{solution}[\bf Solution.]
\end{solution}


\begin{problem}
Let $S$ be an $n \times n$ real symmetric matrix with $S^k = I$ for some $k \geq 1$. Show that $S^2 = I$.
\end{problem}

\begin{solution}[\bf Solution.]
Since $S$ is real and symmetric it may be written in diagonal form by applying an orthogonal transformation. Since $S^k = I$ all the diagonal entries are roots of unity and since $S$ is real they are $\pm 1$. Hence $S^2 = I$.

Claim $\lm^2 = \pm 1$. $SX^T X = \lm X$. First note that the kernel is zero.
\be
SX^TS = 0 \ \lra \ \bb{S^2X^TX^2}^T = 0 \ \lra \ X=0.
\ee

Now consider the eigenvalue equation.
\be
SX^T S = \lm X,\quad X^T = \lm SXS.
\ee

Use the fact that $S$ is symmetric.
\be
X = \lm (SXS)^T = \lm S^T X^T S^T = \lm SX^T S = \lm S \lm S X SS = \lm^2 X \ \ra \ \lm = \pm 1.
\ee
\end{solution}


\begin{problem}
An endomorphism $\alpha$ of a finite dimensional inner product space $V$ is \emph{positive definite} if it is self-adjoint and satisfies $\inner{\alpha(x)}{x} > 0$ for all non-zero $\bx \in V$.
\ben
\item [(i)] Prove that a positive definite endomorphism has a unique positive definite square root.
\item [(ii)] Let $\alpha$ be an invertible endomorphism of $V$ and $\alpha^*$ its adjoint. By considering $\alpha^*\alpha$, show that $\alpha$ can be factored as $\beta\gamma$ with $\beta$ unitary and $\gamma$ positive definite.
\een
\end{problem}

\begin{solution}[\bf Solution.]
A self-adjoint endomorphism $\alpha$ of a finite dimensional inner product space has a symmetric matrix representation. This matrix may be written in diagonal form wrt the given inner product on the space. This diagonal form must have non-negative entries. Define the positive definite square root as the matrix of positive square roots of the given matrix. This matrix defines the square root endomorphism.

If the matrix representation has exactly $k$ positive entries then the number of possible square roots is the number of ways of assigning '-' signs to the roots of these entries. 
\be
\binom{k}{0} + \dots + \binom{k}{k} = 2^k.
\ee
\end{solution}


\begin{problem}
Let $V$ be a finite dimensional complex inner product space, and let $\alpha$ be an endomorphism on $V$. Assume that $\alpha$ is normal, that is, $\alpha$ commutes with its adjoint: $\alpha\alpha^* = \alpha^*\alpha$. Show that $\alpha$ and $\alpha^*$ have a common
eigenvector $\bv$, and the corresponding eigenvalues are complex conjugates. Show that the subspace $\langle v\rangle^\perp$ is invariant under both $\alpha$ and $\alpha^*$. Deduce that there is an orthonormal basis of eigenvectors of $\alpha$.
\end{problem}

\begin{solution}[\bf Solution.]
The bilinear form which defines the inner product must be positive definite Hermitian.

\be
\inner{\alpha x}{\alpha x} = \inner{x}{\alpha^*\alpha x} = \inner{x}{\alpha\alpha^* x} = \inner{\alpha^* x}{\alpha^* x}.
\ee

Since $\inner{\cdot}{\cdot}$ is positive definite $\inner{x}{x}$ iff $x=0$. If $x\in ker \alpha$ then 
\be
0 = \inner{\alpha x}{\alpha x} = \inner{\alpha^* x}{\alpha^* x}  \ \ra \ \ker \alpha = \ker \alpha^*.
\ee

Let $\alpha x = \lm x$ with $\lm \neq 0$ and $E_\lm$ be the eigenspace wrt $\alpha$. Then
\be
\alpha^* \alpha x = \alpha^* \lm x \ \ra \ \alpha (\alpha^* x) = \lm (\alpha^* x) \ \ra \ \alpha^* E_\lm \subseteq E_\lm
\ee
and similarly $\alpha \wt{E}_\lm \subseteq \wt{E}_\lm$ where $\wt{E}_\lm$ is the eigenspace wrt $\alpha^*$. Thus, $\alpha$ and $\alpha^*$ have the same eigenspaces.


The characteristic polynomial of $\alpha$ can be factorised over the complex numbers and therefore each eigenvalue has at least one eigenvector.

Let $\alpha x = \lm x$ with $\lm \neq 0$.
\beast
\inner{\alpha^* x- \lm^* x}{\alpha^* x- \lm^* x} & = & \inner{\alpha^* x}{\alpha^* x} - \inner{\lm^* x}{\alpha^* x} -  \inner{\alpha^* x}{\lm^* x} + \inner{\lm^*x}{\lm^*x}\\
& = & \inner{x}{\alpha \alpha^* x} - \lm^*\inner{x}{\alpha^* x} -  \lm \inner{\alpha^* x}{x} + \lm^*\lm \inner{x}{x}\\
& = & \inner{x}{\alpha^* \alpha x} - \lm^*\inner{\alpha x}{x} -  \lm \inner{x}{\alpha x} + \lm^*\lm \inner{x}{x}\\
& = & \lm^*\lm \inner{x}{x} - \lm^*\lm\inner{x}{x} -  \lm \lm^*\inner{x}{x} + \lm^*\lm \inner{x}{x} =0
\eeast

$\alpha^* x- \lm^* x = 0$ as required. Thus, $\alpha$ and $\alpha^*$ have the same eigenvectors and complex conjugate eigenvalues.

\be
\inner{v}{\alpha w} = \inner{\alpha^* v}{w} = \inner{\lm^* v}{w} = \lm^*\inner{v}{w} = 0,\quad \inner{v}{\alpha^* w} = \inner{\alpha v}{w} = \inner{\lm v}{w} = \lm\inner{v}{w} = 0
\ee

So both $\alpha^* w\in v^{\perp}$ and $\alpha w\in v^\perp$ as required. $v^\perp = \bra{w\in X:\inner{w,v} =0}$ is invariant under both $\alpha$ and $\alpha^*$.

Claim: There is an orthonormal basis of eigenvectors for $\alpha$.

By induction on the number of eigenspaces.

Let $e_1$ be an eigenvector of $\alpha$ and define the space $\bsa{e_1}$ and the corresponding orthogonal subspace, $\bsa{e_1}^\perp$. Then we have a direct sum decomposition.
\be
X = \bsa{e_1}\oplus \bsa{e_1}^\perp.
\ee

Define the restrictions, $X_2 = \bsa{e_1}^\perp$ and $\alpha_2 = \alpha|_{X_2}$ and repeat construction to find eigenvectors for $\alpha_2 = \alpha|_{X_2}$ on $X_2 = \bsa{e_1}^\perp$ and form the next pair of restrictions. Since the space is finite dimensional the process terminates and generate an orthogonal set of eigenvectors for the space.
\end{solution}


\begin{problem}
Find a linear transformation which reduces the pair of real quadratic forms
\be
2x^2 + 3y^2 + 3z^2 - 2yz,\quad\quad x^2 + 3y^2 + 3z^2 + 6xy + 2yz + 6zx
\ee
to the forms
\be
X^2 + Y^2 + Z^2,\quad\quad \lm X^2 + \mu Y^2 + \nu Z^2
\ee
for some $\lm,\mu,\nu\in \R$ (which should turn out in this example to be integers). 

Does there exist a linear transformation which reduces the pair of real quadratic forms $x^2 - y^2$, $2xy$ simultaneously to diagonal forms?
\end{problem}

\begin{solution}[\bf Solution.]

\be
Q = \bepm 2 & 0 & 0 \\ 0 & 3 & -1 \\ 0 & -1 & 3\eepm,\quad S= \bepm 1 & 3 & 3 \\ 3 & 3 & 1 \\ 3 & 1 & 3\eepm ,\quad QS =  \bepm 2 & 6 & 6 \\ 6 & 8 & 0 \\ 6 & 0 & 8\eepm = SQ.
\ee

Therefore the forms are simultaneously diagonalisable. The eigenvalues are 
\be
\det \bepm 2-x & 0 & 0 \\ 0 & 3-x & -1 \\ 0 & -1 & 3-x\eepm = 0 \ \ra \ x = 2,2,4,\quad \det \bepm 1-x & 3 & 3 \\ 3 & 3-x & 1 \\ 3 & 1 & 3-x\eepm = 0 \ \ra \ x = 2,2,7
\ee
Then the eigenvectors are 
\be
e_{2,1} = \bepm
1\\
0\\
0
\eepm,\quad 
e_{2,2} = \bepm
0\\
1\\
1
\eepm,\quad 
e_{4} = \bepm
0\\
1\\
-1
\eepm.
\ee

We are not interested in preserving lengths only orthogonality, therefore define $P$ as below,
\be
Q' = P^TQ P = \bepm 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 1 & -1\eepm\bepm 2 & 0 & 0 \\ 0 & 3 & -1 \\ 0 & -1 & 3\eepm\bepm 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 1 & -1\eepm = \bepm 2 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 8\eepm
\ee

Now rescale to obtain the desired form,
\be
Q'' = D^T P^T Q PD = \bepm 1/\sqrt{2} & 0 & 0 \\ 0 & 1/2 & 0 \\ 0 & 0 & 1/\sqrt{8} \eepm \bepm 2 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 8\eepm\bepm 1/\sqrt{2} & 0 & 0 \\ 0 & 1/2 & 0 \\ 0 & 0 & 1/\sqrt{8} \eepm = \bepm 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \eepm
\ee

Apply the same transformation to the other form,
\beast
S'' & = & \bepm 1/\sqrt{2} & 0 & 0 \\ 0 & 1/2 & 0 \\ 0 & 0 & 1/\sqrt{8} \eepm \bepm 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 1 & -1\eepm \bepm 1 & 3 & 3 \\ 3 & 3 & 1 \\ 3 & 1 & 3\eepm \bepm 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 1 & -1\eepm \bepm 1/\sqrt{2} & 0 & 0 \\ 0 & 1/2 & 0 \\ 0 & 0 & 1/\sqrt{8} \eepm\\
& = & \bepm 1/\sqrt{2} & 0 & 0 \\ 0 & 1/2 & 0 \\ 0 & 0 & 1/\sqrt{8} \eepm \bepm 1 & 6 & 0 \\ 6 & 1 & 0 \\ 0 & 0 & 4\eepm \bepm 1/\sqrt{2} & 0 & 0 \\ 0 & 1/2 & 0 \\ 0 & 0 & 1/\sqrt{8} \eepm = \bepm 1/2 & 3/\sqrt{2} & 0 \\ 3/\sqrt{2} & 1/4 & 0 \\ 0 & 0 & 1/2 \eepm.
\eeast

\beast
S''' & = & \bepm \cos\theta & \sin \theta & 0 \\ -\sin \theta & \cos\theta & 0 \\ 0 & 0 & 1\eepm \bepm 1/2 & 3/\sqrt{2} & 0 \\ 3/\sqrt{2} & 1/4 & 0 \\ 0 & 0 & 1/2 \eepm\bepm \cos\theta & -\sin \theta & 0 \\ \sin \theta & \cos\theta & 0 \\ 0 & 0 & 1\eepm \\
& = & \bepm \bb{\frac{1+\cos2\theta}4 + \frac{3\sin 2\theta}{\sqrt{2}}+ \frac{1-\cos2\theta}8}  & \bb{-\frac{\sin 2\theta}8 + \frac{3\cos2\theta}{\sqrt{2}}} & 0 \\ \bb{-\frac{\sin 2\theta}8 + \frac{3\cos2\theta}{\sqrt{2}}} & \bb{\frac{1+\cos2\theta}4 - \frac{3\sin 2\theta}{\sqrt{2}}+ \frac{1-\cos2\theta}4} & 0 \\ 0 & 0 & 1/2\eepm 
\eeast

Let $\tan 2\theta = \frac{-24/\sqrt{578}}{\sqrt{2}/\sqrt{578}}$, $\cos2\theta = - \frac{24}{\sqrt{578}}$, $\sin 2\theta = \frac{\sqrt{2}}{\sqrt{578}}$.

\be
S''' = \bepm \bb{\frac 38 + \frac 3{\sqrt{578}} - \frac{24}{8\sqrt{578}}}  & 0 & 0 \\ 0 &  \bb{\frac 38 - \frac 3{\sqrt{578}} + \frac{24}{8\sqrt{578}}}  & 0 \\ 0 & 0 & 1/2\eepm = \bepm 3/8 & 0 & 0 \\ 0 & 3/8 & 0 \\ 0 & 0 & 1/2 \eepm
\ee

The required linear transformation is 
\be
T = \bepm 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 1 & -1\eepm \bepm 1/\sqrt{2} & 0 & 0 \\ 0 & 1/2 & 0 \\ 0 & 0 & 1/\sqrt{8} \eepm \bepm \cos\theta & \sin \theta & 0 \\ -\sin \theta & \cos\theta & 0 \\ 0 & 0 & 1\eepm .
\ee

Claim: It is not possible to simultaneously diagonalise the second pair $x^2 - y^2$, $2xy$.

Note that the matrices do not commute.
\be
\bepm
1 & 0\\ 
0 & -1
\eepm\bepm
0 & 1\\ 
1 & 0
\eepm - \bepm
0 & 1\\ 
1 & 0
\eepm \bepm
1 & 0\\ 
0 & -1
\eepm = \bepm
0 & 1\\ 
-1 & 0
\eepm - \bepm
0 & -1\\ 
1 & 0
\eepm \neq 0.
\ee
\end{solution}


\begin{problem}
Let $f_1, \dots, f_t, f_{t+1}, \dots, f_{t+u}$ be linear functionals on the finite dimensional real vector space $V$. Show that $Q(\bx) = f_1(\bx)^2 + \dots + f_t(\bx)^2 - f_{t+1}(\bx)^2 - \dots - f_{t+u}(\bx)^2$ is a quadratic form on $V$. Suppose $Q$ has rank $p + q$ and signature $p - q$. Show that $p \leq t$ and $q \leq u$.
\end{problem}

\begin{solution}[\bf Solution.]
p343 S.MacLane and G.Birkoff Algebra (Macmillan 1979 2nd Edition)

Definition. If $V$ is a finite dimensional vector space over a field $\F$ of characteristic not equal to 2 then a quadraatic form on $V$ is a function $q:V\to \F$ such that $q(-v) = q(v)$ for all $v$ and such that the function $h$ defined by $2h(u,v) = q(u+v) -q(u) - q(v)$ gives a bilinear form $h:V\times V \to \F$. The rank of $q$ is defined as the rank of $h$.

Claim: $Q$ is quadratic form.

\be
Q(x) = f_1(x)^2 + \dots + f_t(x)^2 - f_{t+1}(x)^2 - \dots - f_{t+u}(x)^2,
\ee
\beast
Q(-x) & = & f_1(-x)^2 + \dots + f_t(-x)^2 - f_{t+1}(-x)^2 - \dots - f_{t+u}(-x)^2 \\
& = & (-1)^2\bb{ f_1(x)^2 + \dots + f_t(x)^2 - f_{t+1}(x)^2 - \dots - f_{t+u}(x)^2} = Q(x).
\eeast

$2h(x,y) = Q(x+y) - Q(x) - Q(y)$ is clearly bilinear as required.

Claim: Suppose that $Q$ has rank $p + q$ and signature $p - q$. Then $p \leq t$ and $q \leq u$.

By Sylvester's laq of Inertia we can find a basis such that $Q$ is diagonal with $1$s, $-1$s and $0$s only.

Let $P$ be the number of $+1$s. Let $N$ be the number of $-1$s. Let $Z$ be the number of $0$s.
\be
P-N = p-q,\ P+N = p+q,\ \ra\ P = p-q, \ N =q.
\ee

In the dual basis to the linear functionals chosen for the vector space the 'matrix form' of the quadratic form takes the diagonal form corresponding to the $f$s as given above. We therefore have $P\leq t$, $N\leq u$ to allow for possible $f$s which are the zero map. Hence $p\leq t$ and $q\leq u$.
\end{solution}


\begin{problem}
Suppose that $Q$ is a non-degenerate quadratic form on $V$ of dimension $2m$. Suppose that $Q$ vanishes on $U \leq V$ with $\dim U = m$. What is the signature of $Q$? Establish the following.
\ben
\item [(i)] There is a basis with respect to which $Q$ has the form $x_1x_2 + x_3x_4 + \dots + x_{2m-1}x_{2m}$.
\item [(ii)] We can write $V = U \oplus W$ with $Q$ also vanishing on $W$.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(i)] Claim: $V=U\oplus W$

By Sylvester's law of Inertia there is a basis in which $Q$ takes diagonal form with $P$ enties of $+1$ and $N$ entries of $-1$ and no 0s because the form is non-singular. $P+N =2m$.

Note that the form restricted to the positive definite subspace is positive on all vectors and restricted to the negative subspace is negative definite. On the restriction to an indefinite 2-space we can find two distinct null vectors, namely 
\be
\bepm
1 & 0 \\
0 & -1
\eepm,\quad \bepm
1 \\
1
\eepm,\quad 
\bepm
1 \\
-1
\eepm
\ee

The signature of the form is therefore zero because the diagonal two spaces each have the form below and there is no 0-space.

Note that the two vectors are not orthogonal with respect to the bilinear form associated with this quadratic form. On the positive and negative definite subspaces the quadratic form is a multiple of the identity so within each of these subspaces we can make any basis change without altering the matrix of the quadratic form.

Therefore we may assume, by symmetry, wlog that the first element we choose from the total null subspace is $\bepm 1 & 0 & \dots & 1 & 0& \dots\eepm^T$. Clearly any linear combination of this with any other null vector with components in this 2-space will yield a vector which is not null.

Therefore the other elements of $U$ must each lie in their own 2-space and that 2-space is disjoint from all the other 2-spaces occupied by linearly independent elements of $U$. Each of these two spaces is definite and each contains another null vector which is not othogonal to $U$ and is not $U$. Let $W$ be the span of all these vectors. By construction $W$ is totally null and also of dimension $m$. By construction the intersection of these two totally null subspaces is the origin and since they are, by construction, linearly independent they span $V$ between then. So $V = U \oplus W$.

\item [(ii)] Claim: There is a basis with respect to which $Q$ has the form $x_1x_2 + x_3x_4 + \dots + x_{2m-1}x_{2m}$.

Consider the 2-space and null basis
\be
\bepm
1 & 0 \\
0 & -1
\eepm,\quad \frac 14\bepm
1 \\
1
\eepm,\quad 
\frac 14\bepm
1 \\
-1
\eepm
\ee

With respect to the null basis the matrix takes the form
\be
\frac 12\bepm
0 & 1 \\
1 & 0
\eepm,\quad \bepm
1 \\
0
\eepm,\quad
\bepm
0 \\
1
\eepm
\ee

Therefore wrt the null basis correctly scaled the quadratic form becomes $x_1x_2 + x_3x_4 + \dots + x_{2m-1}x_{2m}$.

Note: Compare this to Minkowski metric in $SR$.

\een
\end{solution}


\begin{problem}
Suppose that $\alpha$ is an orthogonal endomorphism on the finite-dimensional real inner product space $V$. Prove that $V$ can be decomposed into a direct sum of mutually orthogonal $\alpha$-invariant subspaces of dimension 1 or 2. Determine the possible matrices of $\alpha$ with respect to orthonormal bases in the cases where $V$ has dimension 1 or dimension 2.
\end{problem}

\begin{solution}[\bf Solution.]
Let the endomorphism have a matrix representation $A$ with respect to the basis in which the matrix $Q$ representing the inner product on the space is diagonal.

Since the space is an inner product space the Quadratic form representing the inner product on the space is real and positive definite.

The orthogonal endomorphism must satisfy $A^TQA = Q$.

The characteristic equation for $A$ has roots in complex conjugate pairs or real roots. Each real root has at least one real eigenvector. The complex conjugate pairs have no real eigenvectors.

Note that $\det A = \pm 1$ so the kernel of $A$ is zero.

Claim: Given a real eigenvalue $\lm$, there is a real eigenvector, $e_\lm$, and the space $V$ may be decomposed into a direct sum of orthogonal subspaces, $V= \bsa{e_\lm} \oplus \bsa{e_\lm}^\perp$ with respect to $Q$.

Let $\lm$ be a real eigenvector of $A$ then can form $e_\lm$ a real eigenvector corresponding to the eigenvalue. From the orthogonal complement of $\bsa{e_\lm}$ the real span this eigenvector with respect to the inner product $Q$, call this $\bsa{e_\lm} := \bra{v\in V:Q(e_lm,v)}$. Then $V=\bsa{e_\lm}\oplus \bsa{e_\lm}^\perp$ is a direct sum decomposition of the space.

Claim: The endomorphism preserves the orthogonal decomposition $V = \bsa{e_\lm}\oplus \bsa{e_\lm}^\perp$.

$Q(Ae_\lm,e_\lm) = \lm Q(e_\lm,e_\lm) \neq 0$. Consider $Q(w,e_\lm) =0$ then $\lm Q(Aw,e_\lm) = Q(Aw,Ae_\lm) = Q(w,e_\lm) = 0$ .

Claim: The vector space $V$ may be decomposed into a direct sum of eigenspaces with real eigenvalues and a component with only complex non real eigenvalues.

From $V = \bsa{e_\lm} \oplus \bsa{e_\lm}^\perp$ as above and consider the restrictions. $V_2 := V|_{\bsa{e_\lm}^\perp}$ and $\alpha|_{\bsa{e_\lm}^\perp}$ and $Q|_{\bsa{e_\lm}^\perp}$ and repeat the above construction. Continue inductively until all the real eigenvalues have been used up.

Claim: A complex eigenvalue provides a decomposition into a real 2-space and its orthogonal complement with respect to the inner product $Q$.

Let $\lm e^{i\theta}$ be a complex eigenvalue of $A$ then we can form $w = (x+iy)$ a complex eigenvector corresponding to the eigenvalue.
\be
Aw = \lm e^{i\theta} w = (x+iy)\lm (\cos\theta + i\sin \theta), \quad\quad A\bepm x\\ y \eepm = \bepm \cos\theta & -\sin \theta \\ \sin \theta & \cos\theta \eepm \bepm x\\ y \eepm.
\ee

\be
Aw^* = \lm e^{-i\theta} w^* = (x-iy)\lm (\cos\theta - i\sin \theta), \quad\quad A\bepm x\\ y \eepm = \bepm \cos\theta & \sin \theta \\ -\sin \theta & \cos\theta \eepm \bepm x\\ y \eepm.
\ee

\beast
Aw + Aw^* & = & \lm \bb{\bb{x\cos\theta - y \sin \theta} + i\bb{x\sin\theta + y\cos\theta}} +  \lm \bb{\bb{x\cos\theta - y \sin \theta} - i\bb{x\sin\theta + y\cos\theta}} \\
& = & 2\lm \bb{x\cos\theta - y\sin \theta}
\eeast

\be
Aw - Aw^* = 2\lm \bb{x\sin\theta + y\cos \theta}
\ee

Hence $Ax =\lm \bb{x\cos\theta - y\sin \theta}$, $Ay =\lm \bb{x\sin\theta + y\cos \theta}$.

It follows that there is a real 2-space of $V$ which lies inside the real 4-space obtained by taking the complex space corresponding to a pair of complex conjugate eigenvalues of $A$.

On this 2-space the endomorphism acts as a positive rescaling and a rotation. Analogously to the real case we can now form the orthonal complement to the span of this 2-space with respect to $Q$. The 2-space is invariant under the action of $A$ by construction.

We can now continue inductively to obtain a full decomposition of the space into the required direct sum.

Example. $V$ has dimension 1.
\be
A^TQ A = Q, \ \det A = k=\pm 1, \ A \text{ real, } Q \text{ is positive definite} \ \ra \ A = \pm 1.
\ee

Example. $V$ has dimension 2.
\be
A^TQA = Q,\ \det A = k = \pm 1 , \ A \text{ real, } Q \text{ is positive definite}
\ee

\be
\bepm a & c\\ b & d\eepm \bepm \lm & 0\\ 0 & \mu\eepm \bepm a & b\\ c & d\eepm = \bepm \lm & 0\\ 0 & \mu\eepm \quad\quad ad -bc = k.
\ee
\be
\bepm a & c\\ b & d\eepm \bepm \lm & 0\\ 0 & \mu\eepm  = k \bepm \lm & 0\\ 0 & \mu\eepm \bepm d & -b\\ -c & a\eepm 
\ee

$\lm a = k\lm d$, $\mu c = -k\lm b$, $\lm b = -k\mu c$, $\mu d = k\mu a$. Thus,
\be
a = \pm k \frac{\cos\theta}{\sqrt{\mu}}, \quad b = \pm \frac{\sin\theta}{\sqrt{\lm}},\quad a = \mp k \frac{\sqrt{\lm}\sin\theta}{\mu},\quad d = \pm\frac{\cos\theta}{\sqrt{\mu}}.
\ee

Note this reduces to Euclidean isometries for the Euclidean metric.
\end{solution}


\begin{problem}
Show that if $A$ is an $m\times n$ real matrix of rank $n$ then $A^TA$ is invertible. Is there a corresponding result for complex matrices?
\end{problem}

\begin{solution}[\bf Solution.]
{\bf Claim:} $A^TA$ is invertible.

The matrix $A$ has $m$ rows and $n$ columns. Since the matrix has rank $n$ these $n$ columns must be linearly independent since they represent the images of the $n$ standard basis vector. Therefore the number or rows is greater than or equal to $n$. We choose a basis for the image such that the standard basis elements are mapped to the image basis labelled in the same order. With respect to these bases the matrix consists of an $n\times n$ identity matrix at the top and a zero in any remaining rows below the first $n$. This is because the codomain is $m$ dimensional but the range is $n$ dimensional and spanned by the first $n$ basis vectors of the codomain. Therefore we have,
\be
A^T A = \bepm I_n & 0 \eepm \bepm I_n\\ 0 \eepm.
\ee

So in these bases the composition is represented on the original domain by the identity matrix and therefore it must be invertible.

The result is entirely algebraic in nature and therefore will hold for the complex version of this expression. Note that in the complex place, if we are working with the usual map to the complex 'dual' we would expect the transpose to be replaced by the conjugate transpose for compatibility with the usual inner product. The above result of course still holds because the matrix representation chosen is explicitly real.
\end{solution}


\begin{problem}
Prove Hadamard's Inequality: if $A$ is a real $n \times n$ matrix with $\abs{a_{ij}}\leq k$, then 
\be
\abs{\det A} \leq k^n n^{n/2}.
\ee
\end{problem}

\begin{solution}[\bf Solution.]
Let $A=  (A_1,A_2,\dots,A_n)$ be the columns of the matrix $A$. By applying a Gram-Schmidt construction to these vectors we can obtain, for each $k=1,\dots,n$ an orthonormal basis $(e_1,e_2,\dots, e_k)$ such that
\be
\Span(A_1,A_2,\dots, A_k) = \Span(e_1,e_2,\dots,e_k).\quad (*)
\ee

Let $B = \bb{e_1,e_2,\dots,e_n}$ be the orthonormal matrix make up of the column vectors of the orthonormal basis. $B$ is orthogonal.

For any vector $v\in \R^n$ we have $v = \sum^n_{i=1}\inner{v}{e_i}e_i$ and $\dabs{v}^2 =\sum^n_{i=1} \inner{v}{e_i}^2$.

Apply this to the columns of $A$ to obtain $\dabs{A_j}^2 = \sum^j_{i=1}\inner{A_j}{e_i}^2$ where the sume terminates after the first $j$ terms because of ($*$).

Define the matrix $C = (c_{km}) = \left\{\ba{ll} c_{km} = \inner{A_1}{e_k} \quad\quad & 1\leq k\leq m \\ c_{km} = 0 & 1< k\leq m\ea\right.$.

$C$ is upper triangular and 
\be
C = \bepm
\inner{A_1}{e_1} & \inner{A_1}{e_2} & \dots & \dots \\
0 & \inner{A_2}{e_2} & \dots & \dots \\
\vdots & 0 & \ddots & \vdots \\
\vdots & \vdots & 0 & \inner{A_n}{e_n} 
\eepm
\ee

Note also that $A=BC$,
\beast
\bb{\det A}^2 & = & \det(A^TA) = \det\bb{C^TB^TBC} = \det\bb{C^TC} = \det C^2 = \prod^n_{j=1}\inner{A_j}{e_j} \leq \prod^n_{j=1} \sum^j_{m=1} \abs{\inner{A_m}{e_m}}^2 \\
& \leq & \prod^n_{j=1}\dabs{A_j}^2 \leq \prod^n_{j=1}nk^2 = n^nk^{2n},\quad k>0
\eeast

Hence $\abs{\det A} \leq k^n n^{n/2}$.
\end{solution}


\begin{problem}
Let $P_n$ be the ($n + 1$-dimensional) space of real polynomials of degree $\leq n$. Define
\be
\inner{f}{g} = \int^{+1}_{-1} f(t)g(t)dt .
\ee
Show that $\inner{\cdot}{\cdot}$ is an inner product on $P_n$ and that the endomorphism $\alpha : P_n \to P_n$ defined by
\be
\alpha(f)(t) = (1 - t^2)f''(t) - 2tf'(t)
\ee
is self-adjoint. What are the eigenvalues of $\alpha$?

Let $s_k \in P_n$ be defined by $s_k(t) = \frac{d^k}{dt^k} (1 - t^2)^k$. Prove the following.
\ben
\item [(i)] For $i \neq j$, $\inner{s_i}{s_j} = 0$.
\item [(ii)] $s_0, \dots, s_n$ forms a basis for $P_n$.
\item [(iii)] For all $1 \leq k \leq n$, $s_k$ spans the orthogonal complement of $P_{k-1}$ in $P_k$.
\item [(iv)] $s_k$ is an eigenvector of $\alpha$. (Give its eigenvalue.)
\een
What is the relation between the $s_k$ and the result of applying Gram-Schmidt to the sequence 1, $x$, $x^2$, $x^3$ and so on? (Calculate the first few terms?)
\end{problem}

\begin{solution}[\bf Solution.]
Definition. An inner product space (also called a Euclidean vector space) is a vector space over $\R$ equipped with a symmetric bilinear form $h$ whose associated quadratic form is positive definite.

p352 S.MacLane and G.Birkoff Algebra (Macmillan 1979 2nd Edition)

The bilinear form defines the inner product and therefore the inner product is linear and symmetric and the induced norm which is given by the positive square root of the inner product of a vector with itself is non-negative. (By symmetry we need only check left linear.)

Claim: $\inner{f}{g}:=\int^1_{-1}f(t)g(t)dt$ is an inner product on $P_n$.

\be
\inner{f+ah}{g} = \int^1_{-1}(f(t)+ ah(t))g(t)dt = \int^1_{-1}f(t)g(t)dt + a\int^1_{-1}h(t)g(t)dt = \inner{f}{g} + a\inner{h}{g}.
\ee

\be
\inner{f}{g} = \int^1_{-1}f(t)g(t)dt = \int^1_{-1}g(t)f(t)dt = \inner{g}{f},\quad \inner{f}{f} = \int^1_{-1}f^2(t)dt \geq 0.
\ee

Claim; $(\alpha(f))(t):=(1-t^2)f''(t) - 2tf'(t)$ is self-adjoint.

\beast
\inner{g}{\alpha f} & = & \int^1_{-1}g(t)(1-t^2)f''(t) - g(t)2tf'(t)dt \\
& = & \bb{g(t)(1-t^2)f'(t) - g(t)2tf(t)}|^1_{-1} - \int^1_{-1}\bb{g'(t)(1-t^2)-2tg(t)} f'(t) - \bb{g'(t)2t+ g(t)2}f(t)dt\\
& = & \bb{- g(t)2tf(t)}|^1_{-1} - \bb{\bb{g'(t)(1-t^2)-2tg(t)} f(t)}|^1_{-1} + \int^1_{-1}\bb{g''(t)(1-t^2) - 2tg'(t) - 2tg'(t) - 2g(t)} f(t) dt \\
& & \qquad\qquad + \int^1_{-1} \bb{g'(t)2t+ g(t)2}f(t)dt\\
& = & \bb{- g(t)2tf(t)}|^1_{-1} - \bb{\bb{g'(t)(1-t^2)-2tg(t)} f(t)}|^1_{-1} + \int^1_{-1}\bb{g''(t)(1-t^2) - 2tg'(t)}f(t)dt\\
& = &  \int^1_{-1}\bb{g''(t)(1-t^2) - 2tg'(t)}f(t)dt = \inner{\alpha g}{f}
\eeast

Therefore $\inner{g}{\alpha f} = \inner{\alpha g}{f}$ as required.

Find eigenvalues of $\alpha$ where $(\alpha(f))(t):=(1-t^2)f''(t) - 2tf'(t)$. Consider 
\be
(\alpha(f))(t):=(1-t^2)f''(t) - 2tf'(t) = \lm f(t).
\ee

Note that this is the defining differential equation for Legendre polynomials. Let $f(t) = \sum^n_{k=0}a_k t^k$.
\be
(\alpha(f))(t) = (1-t^2)\sum^n_{k=0} k(k-1)a_k t^{k-2} - 2t \sum^n_{k=0}ka_kt^{k-1} = \lm \sum^n_{k=0}a_k t^k.
\ee

\beast
0 & = & \sum^n_{k=2} k(k-1)a_k t^{k-2} - \sum^n_{k=2} k(k-1)a_kt^{k} - \sum^n_{k=1} 2k a_k t^{k} - \lm \sum^n_{k=0}a_k t^k \\
& = & \sum^n_{k=2,s=k-2} (s+2)(s+2-1)a_{s+2} t^{s} - \sum^n_{k=2} k(k-1)a_kt^{k} - \sum^n_{k=1} 2k a_k t^{k} - \lm \sum^n_{k=0}a_k t^k \\
& = & \sum^n_{k=0} (k+2)(k+1)a_{k+2} t^{k} - \sum^n_{k=2} k(k-1)a_kt^{k} - \sum^n_{k=1} 2k a_k t^{k} - \lm \sum^n_{k=0}a_k t^k
\eeast

Thus, 
\beast
k = 0: & & 2a_2 - \lm a_0 = 0 \ \ra \ a_0 = a_2 = 0\\
k = 1: & & 6a_3 - 2a_1 -\lm a_1 = 0 \ \ra \ a_1 = a_3 = 0\\
k= n-1 : & & -(n-1)(n-2)a_{n-1}  - 2(n-1)a_{n-1} -\lm a_{n-1} = 0 \ \ra \ a_{n-1} = 0\text{ or } \lm = -n(n-1)\\
k= n : & & -n(n-1)a_n  - 2n a_n -\lm a_n = 0 \ \ra \ \lm = - n(n+1) \text{ or } a_n = 0
\eeast

$1<k<n-1$, $(k+2)(k+1)a_{k+2}  - k(k-1)a_k - 2k a_k - \lm a_k = 0$, then
\be
a_{k+2} = \frac{k(k+1)+\lm}{(k+2)(k+1)}a_k \text{ or }a_k = a_{k+2} = 0.
\ee

Therefore we have one solution for each degree of polynomial. So if $n$ is even then
\be
a_0 = 0,\ a_1 = 1,\ \lm = -n(n+1),\ a_{k+2} = \frac{k(k+1)+\lm}{(k+2)(k+1)}a_k .
\ee

If $n$ is odd then
\be
a_0 = 1,\ a_1 = 0,\ \lm = -n(n+1),\ a_{k+2} = \frac{k(k+1)+\lm}{(k+2)(k+1)}a_k .
\ee

\ben
\item [(i)] Note that $p_k(t) = \frac{(-1)^k}{2^k k!}\frac{d^k}{dt^k} (1-t^2)^k = \frac{(-1)^k}{2^k k!}s_k(t)$ are Legendre polynomials.

\beast
\inner{s_i}{s_j} & = & \int^1_{-1}\frac{d^i}{dt^i}(1-t^2)^i\frac{d^j}{dt^j}(1-t^2)^j dt \\
& = & \left.\underbrace{\frac{d^i}{dt^i}(1-t^2)^i}_{=0 \text{ by bcs}}\frac{d^j}{dt^j}(1-t^2)^j\right|^1_{-1} - \int^1_{-1} \frac{d^{i-1}}{dt^{i-1}}(1-t^2)^i \frac{d^{j+1}}{dt^{j+1}}(1-t^2)^j dt \\
& = &  - \int^1_{-1} \frac{d^{i-1}}{dt^{i-1}}(1-t^2)^i \frac{d^{j+1}}{dt^{j+1}}(1-t^2)^j dt = \dots = (-1)^i \int^1_{-1} (1-t^2)^i \frac{d^{j+1}}{dt^{j+1}}(1-t^2)^j dt
\eeast

Wlog we assume that $i>j$ and then
\be
\inner{s_i}{s_j} = (-1)^i \int^1_{-1} (1-t^2)^i 0 dt = 0.
\ee

\item [(ii)] First verify that the $s_k\in P_n$ are linearly independent. Suppose that $\sum^n_{j=0} a_j s_j = 0$ then 
\be
\inner{s_i}{\sum^n_{j=0} a_j s_j} = \sum^n_{j=0} a_j \inner{s_i}{s_j} = a_i\inner{s_i}{s_i} = 0,
\ee
\beast
\inner{s_i}{s_i} & = & \int^1_{-1}\frac{d^i}{dt^i}(1-t^2)^i\frac{d^i}{dt^i}(1-t^2)^i dt = \dots = (-1)^i \int^1_{-1} (1-t^2)^i (2i)! (-1)^i dt \\
& = & (2i)! \int^1_{-1} (1-t)^i (1+t)^i dt = (2i)! \bb{\left.(1-t)^i\frac{(1+t)^{i+1}}{i+1}\right|^1_{-1} + \int^1_{-1}i(1-t)^{i-1}\frac{(1+t)^{i+1}}{i+1} dt}\\
& = & (2i)! \frac{i}{i+1} \int^1_{-1}(1-t)^{i-1}(1+t)^{i+1} dt = \dots = (2i)! \frac{i!}{(i+1)\dots (2i)} \int^1_{-1}(1+t)^{2i} dt  = (i!)^2 \frac{2^{2i+1}}{2i+1}.
\eeast

Therefore $a_i = 0$ for all $i$ and the set is linearly independent.

Note that the space is $n+1$ dimensional and there are $n+1$ polynomials and therefore they form a basis for the space.

\item [(iii)] By (i) $s_k$ is orthogonal to all the other basis vectors in $P_k$ and therefore $\forall 1\leq k\leq n$, $s_k$ spans the orthogonal complement of $P_{k-1}$ in $P_k$.

\item [(iv)] Since $\alpha$ is self-adjoint it has a unique o/n basis of eigenvectors wrt the given inner product. But induction on the dimension of the polynomial space and part (iii) we see that $s_k$ is an eigenvector of $\alpha$ with eigenvalue $\lm = -k(k+1)$.

\item [(v)] Claim: The Gram-Schmidt construction applied to the monomials will generate the $s_k$.

This follows trivially form the above.
\een

Illustration. By applying the Gram-Schmidt process to the basis $1,x,x^2,x^3$, find an o/n basis for the vector space of real polynomials of degree at most 3, relative to the inner product
\be
\inner{f}{g} = \int^1_{-1}f(t)g(t)dt.
\ee

Let $e_1 = 1$, $e_2 =x $, $e_3 = x^2$, $e_4 = x^3$,
\be
\inner{e_1}{e_1} =\int^1_{-1}1dt = t|^1_{-1} = 2\ \ra \ f_1 = \frac{e_1}{\inner{e_1}{e_1}^{1/2}} = \frac 1{\sqrt{2}}.
\ee

\be
\inner{f_1}{e_2} = \int^1_{-1} \frac 1{\sqrt{2}} t dt = \frac 1{\sqrt{2}} \left.\frac{t^2}2\right|^1_{-1} = 0,\quad \inner{e_2}{e_2} =\int^1_{-1}t^2dt = \frac 23 .
\ee

\be
f_2 = \frac{e_2-\inner{e_2}{f_1}f_1}{\inner{e_2-\inner{e_2}{f_1}f_1}{e_2-\inner{e_2}{f_1}f_1}^{1/2}} = x\sqrt{\frac32}.
\ee

\be
\inner{f_1}{e_3} = \int^1_{-1} \frac 1{\sqrt{2}} t^2 dt = \frac 1{\sqrt{2}} \left.\frac{t^3}3\right|^1_{-1} = \frac{\sqrt{2}}3,\quad \inner{f_2}{e_3} = \int^1_{-1} t \sqrt{\frac32} t^2 dt = 0,\quad  \inner{e_3}{e_3} =\int^1_{-1}t^4dt = \frac 25 .
\ee

\be
f_3 = \frac{e_3-\inner{e_3}{f_1}f_1- \inner{e_3}{f_2}f_2}{\inner{e_3-\inner{e_3}{f_1}f_1- \inner{e_3}{f_2}f_2}{e_3-\inner{e_3}{f_1}f_1- \inner{e_3}{f_2}f_2}^{1/2}} = \frac{\sqrt{5}}{2\sqrt{2}}(3x^2 -1).
\ee

\be
\inner{f_1}{e_4} = \int^1_{-1} \frac 1{\sqrt{2}} t^3 dt = 0 ,\quad \inner{f_2}{e_4} = \int^1_{-1} t \sqrt{\frac32} t^3 dt = \frac{\sqrt{6}}5,\quad  \inner{f_3}{e_4} = \int^1_{-1} \frac{\sqrt{5}}{2\sqrt{2}}(3t^2 -1) t^3 dt = 0 .
\ee

\be
\inner{e_4}{e_4} =\int^1_{-1}t^6dt = \frac 27
\ee

\be
f_4 = \frac{e_4-\inner{e_4}{f_1}f_1- \inner{e_4}{f_2}f_2 -\inner{e_4}{f_3}f_3}{\inner{e_4-\inner{e_4}{f_1}f_1- \inner{e_4}{f_2}f_2 -\inner{e_4}{f_3}f_3}{e_4-\inner{e_4}{f_1}f_1- \inner{e_4}{f_2}f_2 -\inner{e_4}{f_3}f_3}^{1/2}} = \frac{\sqrt{7}}{2\sqrt{2}}(5x^3 -3x).
\ee

Thus,
\be
f_1 = \frac 1{\sqrt{2}},\quad f_2 = x\sqrt{\frac32},\quad f_3 = \frac{\sqrt{5}}{2\sqrt{2}}(3x^2 -1),\quad f_4 = \frac{\sqrt{7}}{2\sqrt{2}}(5x^3 -3x).
\ee
\end{solution}


\begin{problem}
Let $a_1, a_2, \dots, a_n$ be real numbers such that $a_1 + \dots + a_n = 0$ and $a^2_1 +\dots + a^2_n = 1$. What is the maximum value of $a_1a_2 + a_2a_3 + \dots + a_{n-1}a_n + a_na_1$?
\end{problem}

\begin{solution}[\bf Solution.]
Consider the variables 
\be
\vp = a_1a_2 + \dots + a_n a_1\quad \text{s.t.}\quad \sum^n_{j=1} a_j = 0, \ \sum^n_{j=1}a_j^2 = 1.
\ee

Use the method of Lagrange multipliers. Form 
\be
F(a_1,a_2,\dots, a_n) = a_1a_2 + \dots+ a_na_1 + \lm \bb{a_1^2 + \dots + a^2_n -1} + \mu\bb{a_1 + \dots + a_n}
\ee

\be
dF(a_1,a_2,\dots, a_n) = (a_n + a_2)da_1 + (a_1 + a_2) da_3 + \dots + (a_{n-1} + a_1)da_n + 2\lm \bb{a_1da_1 + \dots + a_nda_n} + \mu\bb{da_1 + \dots + da_n}.
\ee

From the constraint equations we have $da_1 + \dots + da_n = 0$, $a_1 da_1 + \dots + a_n da_n = 0$. Hence
\beast
a_n + a_2 + 2\lm a_1 +\mu & = & 0\\
a_1 + a_3 + 2\lm a_2 +\mu & = & 0\\
& \vdots &\\
a_{n-1} + a_1 + 2\lm a_n +\mu & = & 0
\eeast

Adding all the $n$ equations gives 
\be
0 + 0 + 2\lm 0 + n\mu = 0 \ \ra \ \mu = 0,\quad a_{j-1} + 2\lm a_j + a_{j+1} = 0.
\ee

Solving the difference equaiton we have
\be
a_j = x^j \ \ra \ x^{-1} + 2\lm + x = 0 \ \ra\ \lm = -\frac{x+x^{-1}}2.
\ee

From the two constraint equations
\be
a_1 + \dots + a_n = 0,\quad a_1^2 + \dots a_n^2 = 1.
\ee

We also have
\be
x+x^2 + \dots + x^n = 0,\quad x^2 + x^4 + \dots + x^{2n} = 1.
\ee

The first of these has exactly $n$ solutions, these are the $n$ complex roots of unity.
\be
x = e^{i\theta} \text{ with }\theta = \frac{2\pi m}n\text{ with }m=0,\dots, n-1.
\ee

Since we require real solutions we take the real and imaginary parts, giving,
\be
\cos\theta + \cos2\theta + \dots+ \cos n\theta = 0,\quad \sin\theta + \sin2\theta + \dots+ \sin n\theta = 0,\quad \lm = -\frac{x+x^{-1}}2 = -\cos\frac{2\pi m}n.
\ee

Therefore the solution is either $x=\cos \frac{2\pi m}n$ or $x=\sin \frac{2\pi m}n$. The second constraint gives
\be
x^2 + x^4 + \dots + x^{2n} = 1.
\ee

Again take the real and imaginary parts to obtain real solutions,
\be
\cos \frac{4\pi m}n + \cos \frac{8\pi m}n + \dots + \cos \frac{4n\pi m}n = 1,\quad\sin \frac{4\pi m}n + \sin \frac{8\pi m}n + \dots + \sin \frac{4n\pi m}n = 1.
\ee

Only the first of these two equations corresponds to the required second constraint, therefore the required solutions are 
\be
\lm = -\cos \frac{2\pi m}n,\quad x = \cos \frac{2\pi m}n,\quad \text{with }m =0,\dots,n-1.
\ee
\be
f(a_1,a_2,\dots,a_n) = a_1a_2 + \dots + a_n a_1 = x^3 + x^5 + \dots + x^{n+1} = x\bb{x^2 + x^4 + \dots + x^n}.
\ee

From the second constraint we have $f(a_1,a_2,\dots,a_n) = x$.

Note that $m=0$ gives the value 1 which does not satisfy the second constraint. Consider the graph of the cosine function over 0 to $2\pi$ the next largest value for $x$ occurs for $m=1$. 
\be
f(a_1,a_2,\dots,a_n) = \cos\frac{2\pi}n.
\ee

Checking: $n=2$,
\be
f(a_1,a_2,\dots,a_n) = a_1a_2 + a_2 a_1, \quad a_1^2 + a_2^2 = 1,\quad a_1 + a_2 = 0
\ee
\be
f(a_1,a_2,\dots,a_n) = -1, \quad a_1^2 = 1/2,\quad a_1 = -a_2.
\ee

\be
f(a_1,a_2,\dots,a_n) = \cos \frac{2\pi}n = -1 \text{ as required.}
\ee

$n=3$,
\be
f(a_1,a_2,\dots,a_n) = a_1a_2 + a_2a_3 + a_3 a_1, \quad a_1^2 + a_2^2 + a_3^2 = 1,\quad a_1 + a_2 + a_3= 0
\ee
\be
f(a_1,a_2,\dots,a_n) = \cos \frac{2\pi}3 = -\frac 12 \text{ as required.}
\ee
\end{solution}


\begin{problem}
Let $A$ be a $2n \times 2n$ alternating matrix over a field $F$. Show that the determinant of $A$ is a square. In fact $\det(A) = pf(A)^2$ where $pf(A)$ is a homogeneous polynomial of degree $n$ in the entries of $A$ (called the \emph{Pfaffian} of $A$). Assuming this fact, show that every matrix in the \emph{symplectic group }
\be
Sp_{2n}(F) = \left\{P \in GL_{2n}(F) | P^T JP = J\right\}, \quad \text{where }J = \bepm
0 & I_n\\
-I_n & 0
\eepm,
\ee
has determinant +1.
\end{problem}

\begin{solution}[\bf Solution.]
Aside: In mathematics, the determinant of a skew-symmetric matrix can always be written as the square of a polynomial in the matrix entries. This polynomial is called the Pfaffian of the matrix. The Pfaffian is non vanishing only for $2n\times 2n$ skew-symmetric matrices, in which case it is a polynomial of degree $n$. (The degree of the polynomial is $(2n-1)n$, the same as the number of coefficients in an antisymmetric matrix?)

REF: http://en.wikipedia.org/wiki/Pfaffian

Let $A= (a_{ij})$ be a $2n\times 2n$ skew-symmetric matrix. The Pfaffian of $A$ is defined by the equation
\be
Pf(A) = \frac 1{2^nn!}\sum_{\sigma \in S_{2n}}\sgn(\sigma) \prod^n_{i=1} a_{\sigma(2i-1),\sigma(2i)}
\ee
where $S_{2n}$ is the symmetric group and $\sgn(\sigma)$ is the signature of $\sigma$.

Lemma 1. An alternating matrix is congruent to a product of hyperbolic two planes.

$A =-A^T$. Since $v^TA v = (v^T A v)^T = v^T A^T v = - v^T A v$ we see that $A$ vanishes on all vectors. Let $Af_m = 0$, $m=1,\dots,M$ be a basis for the kernel of $A$.

Now suppose we choose $Ae_1 \neq 0$ then
\be
f_k^T A e_1 = (f_k^TA e_1)^T = e_1^T A^T f_k = - e_1^T A f_k = 0
\ee

So the space can be decomposed into a direct sum of the kernel and the image.

Now consider $e_2 = Ae_1$ then $e_1^T e_2 = e_1^T A e_1 = 0$ and $e_1^T A e_2 = e^T_2 A^T e_1 = -e_2^T e_2$. So if our image space has dimension two then the matrix $A$, can by a suitable, non geometry preserving, basis change be put into the form of a hyperbolic two plane. So we have 
\be
P^TAP = \bepm
0 & 1\\
-1 & 0
\eepm
\ee

Now note that if the dimension is even and greater than two then we can choose a basis for the complement of the subspace spanned by $\bra{e_1,e_2}$ so that,
\be
Av \neq 0,\quad Ae_3 = v + \alpha e_1 + \beta e_2,
\ee
\be
e_1 Ae_3 = -v^T A e_1 - \underbrace{\alpha e_1^TA e_1 }_{=0} -\beta e_2^TA e_1  
\ee

\be
e_2 Ae_3 = -v^T A e_2 - \alpha e_2^TA e_1 - \underbrace{\beta e_2^TA e_2  }_{=0} 
\ee

So we can choose the constants so that $v^T Ae_1 = 0 = v^T A e_2$. Continuing inductively we can construct a basis such that the space is a direct sum of the kernel and hyperbolic 2-space and the rest of the image. Now continuing by induction again we can construct a basis such that the matrix is clearly a direct sum of the kernel and sum of hyperbolic two planes.

Claim: The determinant is the square of a polynomial in the matrix entries. 

From lemma 1 we have the congruence relation. $P^TAP = J_{2n}$ for the non-singular case. (The singular case is clearly trivially the same but multiplied by zeros.)

Hence
\be
A = (P^T)^{-1}J_{2n}P^{-1} = (P^{-1})^T J_{2n}P^{-1} \ \ra \ \det A = \det (P^{-1})\det(J_{2n}) \det(P^{-1}) = (-1)^{2n}\bb{\det(P^{-1})}^2 = \bb{\det(P^{-1})}^2.
\ee

The left hand side is clearly a polynomial in the coefficients of the matrix and the right hand side is clearly a square. The if polynomial factorisation is assumed to be unique for polynomial with coefficients in $\F$, we have the required result.

Claim: Every symplectic group element has determinant 1.

$P^TJP = J$ for elements $P$ of the symplectic group. Clearly $\det(P)^2 =1$. 

While the notion of the Pfaffian is particular to skew-symmetric matrices, it is very general in the sense that these matrices may have entries from any commutative ring.

The two fundamental results needed for the proof are stated below. (For further details see E.Artin, Geometric Algebra, Interscience tracts in pure and applied mathematics. Interscience Publishers Inc., New York, 1957).

\bit
\item For any even integer $n>2$, there is a polynomial in $n(n-1)/2$ variables with integer coefficients, denoted by Pf, with the following property. For any $n\times n$ skew-symmetric matrix $K$ (with entries in any commutative ring), the number $Pf (K)$ obtained by evaluating the polynomial $Pf$ at the upper triangular entries of $K$ (i.e. $k_{ij}$ for $i<j$) satisfies 
\be
\det K = Pf(K)^2.
\ee

Modulo a certain normalizing condition, the polynomial $Pf$ is unique. Note that $Pf(J) \neq 0$, since $J$ is non-singular.

\item Congruence tranformations preserve skew-symmetry, and Pfaffians behave nicely with respect to congruences. For any $A\in \F^{n\times n}$ and any $n\times n$ skew-symmetric $K$, we have 
\be
Pf(A^TKA) = \det A Pf(K).
\ee

It can now be shown very quickly that any symplectic matrix, with entries from any field, has determinant $+1$. Recall that $A\in \spa (2n,\F)$ then $A^TJA = J$. Then $Pf(J) = Pf(A^TJA) = \det A Pf(J)$ hence $\det A = 1$.
\eit
\end{solution}


\begin{problem}
An inner product space (also called a Euclidean vector space) is a vector space over $\R$ equipped with a symmetric bilinear form $h$ whose associated quadratic form is positive definite. p352 S. MacLane and G.Birkoff Algebra (Macmillan 1979 2nd Edition)
\end{problem}

\begin{solution}[\bf Solution.]
We can choose a basis, $\bra{w_i}$ for the subspace $W$ and apply a G-S construction to choose to be an o/n basis with respect to the inner product on the space. We can extend this basis to a basis, $\bra{w_i,u_j}$ for the whole space which by a further G-S construction may be chosen to be an o/n basis on the whole space. Any vector in the space may be written as $v=a_iw_i + b_ju_j$ with a summation convention. In this basis orthogonal projection onto the subspace $W$ is given by 
\be
p(v) = v - \inner{v}{u_j}u_j = a_i w_i + b_ju_j - b_j u_j = a_i w_i.
\ee

$p$ acts as the identity on $W$ and as zero on $U$ and since the only vector in the intersection of the two spaces is the zero vector we actually have $V=U \oplus W$ so the matrix for the orthogonal projection has a block diagonal form and in particular wrt the chosen basis the form,
\be
p(v) = \bepm I & 0 \\ 0 & 0 \eepm \text{ on the basis }v\leftrightarrow \bb{\frac WU}.
\ee

A self adjoint operator is represented by a symmetric matrix and therefore the orthogonal projection is self-adjoint.

Conversely any self-adjoint projection must act as the identity on the image space which we will call $W$, by definition of projection. Since the map is a projection and the space is positive definite we can find an o/n basis and therefore the projection must act as zero on the orthogonal complement of $W$.

\be
p(v) = \bepm I & 0 \\ * & 0 \eepm \text{ on the basis }v\leftrightarrow \bb{\frac WU}.
\ee

Since the projection is self-adjoint the correponding matrix representation must be symmetric and therefore must have the form $p(v) = \bepm I & 0 \\ 0 & 0 \eepm $ with basis $v\leftrightarrow \bb{\frac WU}$. And therefore the space splits into a direct sum and the projection is orthogonal.
\end{solution}


\begin{problem}
Extra question.
\end{problem}

\begin{solution}[\bf Solution.]
Since $S$ is symmetric we can work in the basis in which $S$ is diagonal. 

Consider $DX^TD = \lm X$ then for all $i,j$ we have $x_{ji}\lm_i \lm_j = \lm x_{ij}$,
\be
DX^TD =\bepm
\lm_1\lm_1 x_{11} & \lm_1\lm_2 x_{21} & \dots & \lm_1\lm_n x_{n1}\\
\lm_1\lm_2 x_{12} & \lm_2\lm_2 x_{22} & \dots & \\
\vdots & & & \\
\lm_1\lm_n x_{1n} & \dots & \dots \lm_n\lm_n x_{nn}
\eepm.
\ee

By inspection there are $n$ eigenvalues of the form
\be
DX^T D = \bepm
\lm_1\lm_1 & 0 & \dots \\
0 & & \dots & \\
\vdots & & 
\eepm = \lm \bepm
1 & 0 & \dots \\
0 & & \dots & \\
\vdots & & 
\eepm
\ee
where the eigenmatrix has a 1 on one diagonal entry, say the $jj$th and zeros elsewhere and the corresponding eigenvalue is $\lm^2_j$.

By inspection there are $\frac {n^2 - n}2$ eigenvalues of the form
\be
DX^TD = \bepm
& \lm_1\lm_2 & 0 & \\
\lm_1\lm_2 & & \dots & \\
0 & & & \\
\vdots & & & 
\eepm = \lm \bepm
0 & 1 & \dots \\
1 & & \dots & \\
0 & & & \\
\vdots & & &
\eepm
\ee
where the eigenmatrix has a 1 in two symmetrically positioned off diagonal entries, say the $jk$th and the $kj$th places and zeros elsewhere and the corresponding eigenvalue is $\lm_j\lm_k$.

By inspection there are $\frac{n^2 -n}2$ eigenvalues of the form
\be
DX^TD = \bepm
& \lm_1\lm_2 & 0 & \\
-\lm_1\lm_2 & & \dots & \\
0 & & & \\
\vdots & & & 
\eepm = \lm \bepm
0 & -1 & \dots \\
1 & & \dots & \\
0 & & & \\
\vdots & & &
\eepm
\ee
where the eigenmatrix has a 1 and a -1 in two symmetrically positioned off diagonal entries, say the $jk$th and the $kj$th places and zeros elsewhere and the corresponding eigenvalue is $-\lm_j\lm_k$.

This gives a total of $n+\frac{n^2 - n}2 + \frac{n^2 - n}2 = n^2$ eigenvalues and eigenvectors and therefore there can be no more.
\end{solution}


\begin{problem}
Extra question.
\end{problem}

\begin{solution}[\bf Solution.]
{\bf Claim:} $SS^*$ is self-adjoint.

Work in any matrix representation and note that the map must be equal to its conjugate transpose.
\be
(SS^*)^* = S^{**}S^* = SS^*.
\ee

{\bf Claim:} $\inner{SS^*x}{x} \geq 0$ for all x.

\be
\inner{SS^*x}{x} = \inner{S^*x}{S^*x} \geq 0.
\ee

Aside: Show $T$ is positive iff all its eigenvalues are non negative.

Counterexample:
\be
T = \bepm 1 & 1 \\ 0 & 1 \eepm,\quad T^* = \bepm 1 & 0 \\ 1 & 1 \eepm,\quad \lm_T = 1,1\quad T \neq T^*.
\ee

{\bf Claim:} If all the eigenvalues are non-negative then $T$ is positive on the eigenspace.
\ben
\item [(i)] $\inner{Tx}{x} = \lm \inner{x}{x} \geq 0$.
\item [(ii)] $\inner{(T-T^*)x}{x} = \lm \inner{x}{x} - \lm^*\inner{x}{x} = 0$. So $T=T^*$ on the eigenspaces.
\een

The counterexample shows that full result fails if either the $T$ is not known to be self-adjoint or not diagonalisable.

{\bf Claim:} If $T$ is positive then all the eigenvalues are non-negative.

If $T$ has a negative eigenvalue then $\inner{Tx}{x} = \inner{\lm x}{x} =\lm \inner{x}{x}>0$ which contradicts the definition of positive.

{\bf Claim:} If $T$ is positive then there is a positive square root.

Choose the triangular matrix representation for $T$ and use the fact $T$ is self-adjoint to note that $T$ is diagonalisable. Define $S$ in this basis to be the diagonal matrix of positive or zero square roots of the eigenvalues of $T$. Then by construction $S^2 = T$.

{\bf Claim:} The positive square root is unique.

Suppose that the positive square root is not unique then there is another square root $\wt{S}$ which is not diagonal, since the diagonal form is clearly unique, but whose square is $T$. Write this matrix $\wt{S}$ in Jordan canonical form and notice that on squaring this matrix all the non kernel spaces in the direct sum retain the same dimension and the matrix squared has the same number of off diagonal terms as the matrix itself in all subspaces except the kernel, (which has one extra eigenvector.). 

Therefore the square cannnot be diagonalised so $\wt{S}$ cannot exist.

{\bf Claim:} Any $T$ may be written as a product of unitary and a positive matrix.

$TT^*$ is positive and has a unique square root. (From above.) Call this root $T:=\abs{T}$. Try $T:= \abs{T}\phi$ then
\be
T^* := \phi^* \abs{T}^* \ \ra \ TT^* := \abs{T}\phi\phi^*\abs{T}^* = \abs{T}^2.
\ee

Now define $\phi:= T\abs{T}^{-1}$ on the non zero domain and zero on the kernel.
\end{solution}


