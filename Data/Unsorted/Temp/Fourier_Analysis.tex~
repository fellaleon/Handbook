\chapter{Fourier Analysis}

\section{Waves in strings} It is said that Pythagoras
was the first to realise that the notes emitted by
struck strings of lengths $l$. $l/2$, $l/3$ and so on
formed particularly attractive harmonies for the human ear.
From this he concluded, it is said, that all is number
and the universe is best understood in terms of mathematics
--- one of the most outrageous and
most important leaps of faith in human history.

Two millennia later the new theory of mechanics
and the new method of mechanics enabled mathematicians
to write down a model for a vibrating string.
Our discussion will be exploratory with no
attempt at rigour.
Suppose that the string is in tension $T$ and
has constant density $\rho$. If the graph of
the position of the string at time $t$ is given
by $y=Y(x,t)$ where $Y(x,t)$ is always very
small then, working to the first order in $\delta x$, 
the portion of the string between $x$ and $x+\delta x$
experiences a force parallel to the $y$-axis of
\[T\left(\frac{\partial Y}{\partial x}(x+\delta x,t)
-\frac{\partial Y}{\partial x}(x,t)\right)
=T\delta x\frac{\partial^{2} Y}{\partial x^{2}}.\]
Applying Newton's second law we obtain
(still working to first order)
\[\rho\delta x\frac{\partial^{2} Y}{\partial t^{2}}
=T\delta x\frac{\partial^{2} Y}{\partial x^{2}}.\]
Thus we have the exact equation
\[\rho\frac{\partial^{2} Y}{\partial t^{2}}
=T\frac{\partial^{2} Y}{\partial x^{2}}.\]
For reasons which will become apparent later,
it is usual to write $c$ for the positive square root of
$T/\rho$ giving our equation in the form
\begin{equation*}\tag*{$\bigstar$}
\frac{\partial^{2} Y}{\partial t^{2}}
=c^{2}\frac{\partial^{2} Y}{\partial x^{2}}.
\end{equation*}
Equation $\bigstar$ is often called `the wave equation'.

Let us try and solve the wave equation for a string
fixed at $0$ and $l$ (that is, with $Y(0,t)=Y(0,l)=0$
for all $t$). Since it is rather ambitious to try
and find \emph{all} solutions let us try and find some
solutions. A natural approach is to seek solutions
of the particular form $Y(x,t)=X(x)T(t)$. Substitution
in $\bigstar$ gives
\[X(x)T''(t)=c^{2}X''(x)T(t)
\ \text{which we can rewrite as}
\ \frac{T''(t)}{T(t)}=c^{2}\frac{X''(x)}{X(x)}.\]
Since a quantity which depends only on $x$ and
only on $t$ must be constant $X''(x)/X(x)$
must be constant on $(0,l)$. Thus 
$\frac{X''(x)}{X(x)}$ must take a constant value $K$.

If $K=-\omega^{2}$ with $\omega>0$, then
\[X(x)=Ae^{\omega x}+Be^{-\omega x}\]
for appropriate constants $A$ and $B$.
If $K=0$, then
\[X(x)=A+Bx\]
for appropriate constants $A$ and $B$.
If $K=-\omega^{2}$ with $\omega>0$, then
\[X(x)=A\cos \omega x+B\sin \omega x\]
for appropriate constants $A$ and $B$.
However, since $Y(0,t)=Y(0,l)=0$,
we must have $X(0)=X(l)=0$. (We ignore the
uninteresting possibility $T(t)=0$ for all $t$.)
The only way to obtain a non-trivial solution
for $X$ is to take $K=-(n\pi/l)^{2}$ with $n$
a strictly positive integer. This yields
\[X(x)=B\sin{n\pi x/l}
\ \text{and}\ T''(t)+(n\pi c/l)^{2}T(t)=0\]
and gives us the particular solutions
\[Y(x,t)=(a_{n}\cos(n\pi ct/l)+b_{n}\sin(n\pi ct/l))
\sin{n\pi x/l}.\]

The wave equation is \emph{linear} in the sense that
if $Y_{1}$ and $Y_{2}$ are solutions of $\bigstar$
then so is $\lambda_{1}Y_{1}+\lambda_{2}Y_{2}$.
Subject to appropriate conditions 
\[Y(x,t)=\sum_{n=1}^{\infty}
(a_{n}\cos(n\pi ct/l)+b_{n}\sin(n\pi ct/l))
\sin{n\pi x/l}\]
will be a solution of our problem. It is natural
to ask if this is the \emph{most general} solution.
More specifically, it is natural to ask, whether
if $u,\,v:[0,l]\rightarrow{\mathbb R}$
are well behaved functions with $u(0)=v(0)=u(l)=v(l)$
we can find $a_{n}$ and $b_{n}$ such that,
\[Y(x,0)=u(x)\ \text{and}\ \frac{\partial Y}{\partial t}(x,0)=v(x).\]
Without worrying too much about rigour, our question reduces to
asking whether we can find $a_{n}$ and $b_{n}$
such that
\[\sum_{n=1}^{\infty}a_{n}\sin{n\pi x/l}=u(x)
\ \text{and}
\ \sum_{n=1}^{\infty}\frac{n\pi c}{l}b_{n}\sin{n\pi x/l}=v(x).\]

The next lemma does not answer our question but
indicates the direction our answer might take.
\begin{lemma}\label{L, Expand string} 
(i) $\sin\theta\sin\phi=\tfrac{1}{2}
\big(\cos(\theta-\phi)-\cos(\theta+\phi)\big)$.

(ii) ${\displaystyle
\int_{0}^{l}\sin\frac{n\pi x}{l}\sin\frac{m\pi x}{l}\,dx=0}$
if $n$ and $m$ are distinct integers.

(iii) ${\displaystyle
\int_{0}^{l}\sin\frac{n\pi x}{l}\sin\frac{n\pi x}{l}\,dx=\frac{l}{2}}$
if $n$ is an integer.

(iv) The {\bf formal} solution of
$\sum_{n=1}^{\infty}a_{n}\sin\frac{n\pi x}{l}=u(x)$
is given by 
\[a_{n}=\frac{2}{l}\int_{0}^{l}u(x)\sin\frac{n\pi x}{l}\,dx.\]
\end{lemma}
In this course we will investigate to what extent
a general function can be written in the kind of way
suggested {\bf formally} by lemma~\ref{L, Expand string} 

D'Alambert came up with another very elegant way of solving
the wave equation. Here we deal with an infinite string.
\begin{lemma} (i) If we write $\sigma=x+ct$
and $\tau=x-ct$ then the wave equation
\[\frac{\partial^{2} Y}{\partial t^{2}}
=c^{2}\frac{\partial^{2} Y}{\partial x^{2}}
\ \text{becomes}
\  \frac{\partial^{2} Y}{\partial \tau\partial \sigma}=0.\]

(ii) The general solution of the wave equation is
\[Y(x,t)=f(x-ct)+g(x+ct).\]
\end{lemma}
Note how the solution can be interpreted as two `signals'
traveling with velocity $c$ in two directions. 
\section{Approximation by polynomials}\label{S, Polynomial}
If you have met Fourier Analysis before you have probably
met it as as the study of `decompositions into sines and cosines'.
We shall treat it as `decomposition into functions of
the form $\exp i\lambda x=\cos\lambda x+i\sin\lambda x$'.
Technically, this is a rather trivial change but you are
entitled to ask `why bring complex numbers into the study
of real objects?' In the first two sections I will try
to convince you that there can be genuine advantages
in such a procedure. Neither section is directly related
to Fourier Analysis but this course is a ramble through
fine scenery rather than a forced march to some distant
goal.

Our first topic will be approximation by polynomials.
Complex numbers and trigonometric functions will only make
a brief (but useful) appearance at the very end.

We start by recalling some useful results from algebra.
\begin{lemma}\label{L, Division algorithm}
(i) If $P$ is a polynomial of degree $n\geq 1$
and $a$ is constant then there exists a polynomial $Q$
of degree $n-1$ and a constant $r$ such that
\[P(x)=(x-a)Q(x)+r.\]

(ii) If $P$ is a polynomial of degree $n\geq 1$
and $a$ is a zero of $P$ then there exists a polynomial $Q$
of degree $n-1$ such that
\[P(x)=(x-a)Q(x).\]

(iii) If $P$ is a polynomial of degree at most $n$ which
vanishes at $n+1$ distinct points then $P=0$.
\end{lemma}

Suppose that $b>a$, that $x_{0}$, $x_{1}$, \dots, $x_{n}$
are distinct points of $[a,b]$ and that 
$f:[a,b]\rightarrow{\mathbb R}$ is a given function.
We say that at real polynomial $P$ interpolates $f$
at the points $x_{0}$, $x_{1}$, \dots, $x_{n}$
if $f(x_{j})=P(x_{j})$ for $0\leq j\leq n$.

Lemma~\ref{L, Division algorithm}~(iii) gives the following useful fact.
\begin{lemma}\label{L, interpolation unique} 
With the notation just introduced, there can exist
at most one polynomial $P$ of degree at most $n$ which
interpolates $f$ at the points $x_{0}$, $x_{1}$, \dots, $x_{n}$.
\end{lemma}
This uniqueness result is complemented by an existence
result.
\begin{lemma} We use the notation introduced above.

(i) If we set
\[e_{j}(x)=\prod_{k\neq j}\frac{x-x_{k}}{x_{j}-x_{k}},\]
the $e_{j}$ is a polynomial of degree $n$ such that
$e_{j}(x_{k})=0$ for $k\neq j$ and $e_{j}(x_{j})=1$.

(ii) There exists a polynomial $P$ of degree at most $n$ which
interpolates $f$ at the points $x_{0}$, $x_{1}$, \dots, $x_{n}$.
\end{lemma}

We have thus shown that there is unique interpolating polynomial $P$
of degree at most $n$ which agrees with $f$ at $n+1$ points.
How good an approximation is $P$ to $f$ at other points?
If $f$ is reasonably smooth, an ingenious use of Rolle's
theorem gives a gives a partial answer.
\begin{theorem}{\bf (Rolle's Theorem.)} If
$F:[a,b]\rightarrow{\mathbb R}$ is continuous on $[a,b]$
and differentiable on $(a,b)$ 
and $F(a)=F(b)$, then there exists a $c\in(a,b)$
such that $F'(c)=0$.
\end{theorem}
\begin{lemma}\label{L, start interpolation error} 
Suppose that $f$ is $n+1$ times differentiable. 
With the notation of this section, let
$t$ be a point distinct from the $x_{j}$. Set
\[E(t)=f(t)-P(t)\]
(so $E(t)$ is the `error at point $t$) and write
\[g(x)=f(x)-P(x)-E(t)
\prod_{k=0}^{n}\frac{x-x_{k}}{t-x_{k}}.\]

(i) The function $g$ is $n+1$ times differentiable and
vanishes at $n+2$ distinct points on $[a,b]$.

(ii) The function $g'$ is $n$ times differentiable and 
vanishes at $n+1$ distinct points on $(a,b)$.

(iii) If $1\leq r\leq n+1$ then
the function $g^{(r)}$ is $n+1-r$ times differentiable and 
vanishes at $n+2-r$ distinct points on $(a,b)$.

(iv) There exists a $\zeta\in(a,b)$ such that
$g^{(n+1)}(\zeta)=0$.

(v) There exists a $\zeta\in(a,b)$ such that
\[f^{(n+1)}(\zeta)=(n+1)!E(t)\prod_{k=0}^{n}(t-x_{k})^{-1}.\]
\end{lemma}
Part~(v) of Lemma~\ref{L, start interpolation error}
gives us the required estimate.
\begin{theorem}\label{T, interpolation error}  
Suppose $f:[a,b]\rightarrow{\mathbb R}$
is $n+1$ times differentiable and $|f^{(n+1)}(x)|\leq M$
for all $x\in[a,b]$. If $x_{0}$, $x_{1}$, \dots, $x_{n}$
are distinct points of $[a,b]$ and $P$ is the unique polynomial
of degree $n$ or less such that $P(x_{j})=f(x_{j})$
$[0\leq j\leq n+1]$ then
\[|P(t)-f(t)|\leq \frac{M}{(n+1)!}\prod_{k=0}^{n}(t-x_{k}).\]
\end{theorem}

In order to exploit the inequality of 
Theorem~\ref{T, interpolation error} fully we need a polynomial
$\prod_{k=0}^{n}(t-x_{k})$ which is small for all $t\in[a,b]$.
Such a polynomial was found by Tchebychev.
We start by recalling De~Moivre's theorem.
\begin{theorem}{\bf (De~Moivre's Theorem.)}
If $\theta$ is real and $n$ is a positive integer
\[\cos n\theta+i\sin n\theta=(\cos\theta+i\sin\theta)^{n}.\]
\end{theorem}
Taking real parts in the De~Moivre formula we obtain the following result.
\begin{lemma}\label{L, Tchebychev} There is
a real polynomial of degree $n$ such that
\[T_{n}(\cos\theta)=\cos n\theta
\ \text{for all real $\theta$.}\]
If $n\geq 1$,

(i) $T_{n+1}(t)=2tT_{n}(t)-T_{n-1}(t)$ for all $t$.

(ii) The coefficient of $t^{n}$ in $T_{n}(t)$ is $2^{n-1}$.

(iii) $|T_{n}(t)|\leq 1$ for all $|t|\leq 1$.

(iv) $T_{n}$ has $n$ distinct roots all in $(-1,1)$.
\end{lemma}

We call $T_{n}$ the $n$th Tchebychev polynomial.
\begin{theorem}
Let $x_{0}$, $x_{1}$, \dots, $x_{n}$ be the $n+1$ roots
of the $n+1$st Tchebychev polynomial. 
If $f:[-1,1]\rightarrow{\mathbb R}$
is $n+1$ times differentiable, $|f^{(n+1)}(x)|\leq M$
for all $x\in[-1,1]$ and $P$ is the unique polynomial
of degree $n$ or less such that $P(x_{j})=f(x_{j})$
$[0\leq j\leq n+1]$ then
\[|P(t)-f(t)|\leq \frac{M}{2^{n}(n+1)!}\]
for all $t\in[-1,1]$.
\end{theorem}
The practical use of this result is restricted
by the fact that the size of the $n$th derivative
of apparently well behaved function may increase explosively
as $n$ in increases. 

\section{Cathode ray tubes and cellars}\label{S, Electron}
The path of an electron of mass $m$ and charge $e$
in an electric field ${\mathbf E}$
and a magnetic field ${\mathbb B}$ is given by
\[m\ddot{\mathbf x}=e({\mathbf E}+\dot{\mathbf x}\times {\mathbf B}).\]
In the simple case when the fields are constant
and ${\mathbf E}=(O,me^{-1}E,0)$, ${\mathbf B}=(0,0,me^{-1}B)$
the equation can be written coordinate-wise as
\begin{align*}
\ddot{x}&=B\dot{y}\\
\ddot{y}&=E-B\dot{x}\\
\ddot{z}&=0.
\end{align*}

It is one thing to write down a set of equations like this.
It is quite another thing to solve them. We can obtain
$z=z_{0}+w_{0}t$ for some constants $z_{0}$ and $w_{0}$
from the third equation. We can simplify the first
two equations by setting $u=\dot{x}$ and $v=\dot{y}$
to obtain
\beast
\dot{u}&=Bv\\
\dot{v}&=E-Bu.
\eeast
If we set $U=u+B^{-1}E$ and $V=v$ these equations
take the simpler form
\beast
\dot{U}&=BV\\
\dot{V}&=-BU
\eeast
but we still have to solve them.

To do this we introduce complex numbers by
adding $i$ times the second equation to the first
to obtain the single equation
\[\frac{d\ }{dt}(U+iV)=B(V-iU).\]
If we set $\Phi=U+iV$ this equation takes the form
\[\dot{\Phi}=-iB\Phi.\]
This is an equation that we can solve to obtain
\[\Phi(t)=Ae^{-iBt}\]
where $A$ is a fixed complex number.

How does this solution fit in with our original problem.
recall that we can write
\[A=r\exp i\alpha\]
where $r$ is real and positive and $\alpha$ is real.
We can thus write
\[\Phi(t)=re^{i(\alpha-Bt)}.\]
Taking real and imaginary parts this gives us
\beast
U&=r\cos(Bt-\alpha)\\
V&=-r\sin(Bt-\alpha).
\eeast
Since $\dot{x}=U-B^{-1}E$ and $\dot{y}=V$
we obtain
\beast
x&=x_{0}-R\cos(Bt-\alpha)\\
y&=y_{0}-R\sin(Bt-\alpha)
\eeast
where $R=r/B$ and $x_{0}$ and $y_{0}$ are constants.

Putting everything together we see that
\begin{align*}
x&=x_{0}-R\cos(Bt-\alpha)-B^{-1}Et\\
y&=y_{0}-R\sin(Bt-\alpha)\\
z&=z_{0}+w_{0}t
\end{align*}
so that the electron follows a spiral path.
We note that small perturbations of the electron will have little
effect on its path.

Here is another example of the use of complex numbers
which brings us closer to the main topic of this course.
Consider the temperature $\theta$ at a depth $x$ in the ground.
The equation for heat conduction is
\be
\frac{\partial\theta}{\partial t} =K\frac{\partial^{2} \theta}{\partial x^{2}}.
\ee
It is natural to seek a solution for the case
$\theta(0,t)=A \cos\omega t$ $[\omega>0]$
in which the surface is periodically
warmed and cooled (consider the surface temperature during a day).
Let us try and solve the related complex problem
\begin{equation*}\tag*{$\bigstar$}
\frac{\partial Y}{\partial t}
=K\frac{\partial^{2} Y}{\partial x^{2}}.
\end{equation*}
$Y(0,t)=Ae^{i\omega t}$. A natural guess is
to try $Y(x,t)=f(x)e^{i\omega t}$. Substitution in $\bigstar$
yields
\[Kf''(x)=i\omega f(x)\]
and this in turn shows that
\[f(x)=a_{1}e^{-\alpha x}+a_{2}e^{\alpha x}\]
with
\[\alpha=(\omega/K)^{1/2}e^{i\pi/4}=\left(\frac{\omega}{K}\right)
^{1/2}\frac{1+i}{2^{1/2}}\]
(where we take positive square roots of positive numbers).

Now we observe that $e^{-\alpha x}\rightarrow 0$ as 
$x\rightarrow\infty$ but $|e^{\alpha x}|\rightarrow \infty$.
Thus the only physically plausible solutions for $f$
will have $a_{2}=0$. Our initial guess thus gives
\[Y(x,t)=Ae^{-\alpha x}e^{i\omega t}
=A\exp\left(-\left(\frac{\omega}{2K}\right)^{1/2}x\right)
\exp\left(i\left(\omega t-\left(\frac{\omega}{2K}\right)^{1/2}x\right)
\right)\]
as the solution of the complex problem.
Taking real parts we obtain a solution for our
original problem 
\[\theta(x,t)=A\exp\left(\left(-\frac{\omega}{2K}\right)^{1/2}x\right) 
\cos\left(\omega t-\left(\frac{\omega}{2K}\right)^{1/2}x\right).\]

We can read off all sorts of interesting facts from this solution.
First we note that the effects of periodic heating drop
off exponentially with depth. Thus the annual heating
and cooling of the arctic surface leaves the permafrost
unaffected. We note also that the typical length in the
exponential decrease is $(2K/\omega)^{1/2}$ so that low frequency
effects are longer range than high frequency. The effects of
daily heating only extend for 10's of centimetres below the surface
but those of annual heating extend a few metres. Since
a similar equation governs the penetration of radio-waves in
water submarines can only be contacted by very low frequency
radio waves. For similar reasons
it would not make sense to use high frequencies in
a microwave oven. It is worth noting the time lag of
$(\omega/2K)^{1/2}$ which means that, for example,
the soil temperature at a depth of about 2 metres is
higher in winter than in summer.

\section{Radars and such-like} We work in the $(x,y)$ plane.
Consider an array of $2N+1$ radio transmitters broadcasting
at frequency $\omega$. Let the $k$th transmitter be
at $(0,kl)$ $[k=-N,\ -N+1,\ \dots,\ 0,\ 1,\ \dots,\ N]$.
It is reasonable to take the signal at $(x,y)$
due to the $k$th transmitter to be
\[A_{k}r_{k}^{-2}\exp(i(\omega t-\lambda^{-1}r_{k}-\phi_{k}))\]
where $\lambda$ is the wavelength (thus $\omega\lambda=c$
the speed of light)
and $r_{k}^{2}=x^{2}+(y-kl)^{2}$.
The total signal at $(x,y)$ is
\[S(x,y,t)=\sum_{k=-N}^{N}
A_{k}r_{k}^{-2}\exp(i(\omega t-\lambda^{-1}r_{k}-\phi_{k})).\]
\begin{lemma}\label{Distant beam}
If $x=R\cos\theta$, $y=R\sin\theta$ where
$R$ is very large then to a very good approximation
\[S(R\cos\theta,R\sin\theta,t)=R^{-2}\exp(i(\omega t-\lambda R)Q(u)\]
where
\[Q(u)=\sum_{k=-N}^{N}A_{k}\exp(i(ku-\phi_{k})),\]
and $u=\lambda^{-1}l\sin\theta$.
\end{lemma}
In the discussion that follows we use the notation
of Lemma~\ref{Distant beam} and the discussion that
preceded it. We set
\[P(u)=\sum_{k=-N}^{N}A_{k}\exp(iku),\]
that is $P=Q$ with $\phi_{k}=0$ for all $k$.
Since we could take the $A_{k}$
to be complex, there was no real increase
in generality in allowing $\phi_{k}\neq 0$ but
I wished to make the following points.
\begin{lemma}\label{stearable}
(i) Given $\theta_{0}$ we can find
$\phi_{k}$ such that
\[Q(u)=P(u-u_{0}).\]

(ii) $S(-x,-y,t)=S(x,y,t)$.

(iii) If $l>\lambda\pi/2$ then there exist $0<\theta_{1}<\pi/2$
such that
\[S(R\cos\theta_{1},R\sin\theta_{1},t)=S(0,R,t).\]
\end{lemma}

Bearing in mind that the equations governing the reception
of signal at $(x,y)$ transmitted from our array are
essentially the same as those governing the reception
of signal at  our array, Lemma~\ref{stearable}~(i)
talks about electronic stearability of radar beams,
and Lemma~\ref{stearable}~(ii) and~(iii) deal with
ambiguity. It is worth noting that in practice
the signal received by a radar corresponds to $|Q(u)|$.

Let us look at $P(u)$ in two interesting cases.
\begin{lemma}\label{discrimination}
(i) If $A_{k}=(2N+1)^{-1}$ then,
writing $P_{N,l}(u)=P(u)$,
\[P_{N,l}(u)=
\frac{1}{2N+1}\frac{\sin((N+\tfrac{1}{2})u)}{\sin(\tfrac{1}{2}u)}.\]

(ii) With the notation of (i)
\[P_{N,a/N}((a/N)^{-1}v)\rightarrow\frac{\sin av}{2av}\]
as $N\rightarrow\infty$.
\end{lemma}
Lemma~\ref{discrimination} is usually interpreted
as saying that a radar cannot discriminate between
two targets if their angular distance is of the
order of the size of $\lambda/a$ where $\lambda$
is the wave length used and $a$ is the length of the array.
It is natural to ask if a cleverer choice of $A_{k}$
might enable us to avoid this problem. We shall see
that, although the choice in Lemma~\ref{discrimination} may
not be the best, there is no way of avoiding
the $\lambda/a$ rule.
\section{Towards rigour} I hope it is obvious that, so far,
we have made no attempt at rigour. However, the deeper
study of Fourier analysis makes little sense unless it
is pursued rigorously. Much of what is often called
`a second course in analysis' was invented to aid the
rigorisation of  Fourier analysis and related topics.

Although I have tried to make the course accessible
to that part of my audience unfamiliar with the
ideas that follow, some parts will only become
fully rigorous for those familiar with the following ideas.
\emph{If you are not familiar with these ideas do
not worry and do not spend much time thinking about them.
It is more important to reflect on the ideas of Fourier
analysis and leave the details until later.} However,
you should try to understand the notion of the
uniform norm given in Definition~\ref{D, uniform norm}.

The discussion that follows is thus intended to jog
the memories of those who already know these ideas
and (apart from  Definition~\ref{D, uniform norm})
may be ignored by the others.

\begin{lemma} If $f:[a,b]\rightarrow{\mathbb R}$ is continuous
on the closed bounded interval $[a,b]$ then $f$
is bounded and attains its bounds. In other words,
we can find $x_{1},\,x_{2}\in[a,b]$ such that
\[f(x_{1})\geq f(x)\geq f(x_{2})
\ \text{for all $x\in[a,b]$}.\]
\end{lemma}
\begin{lemma}\label{L, before uniform norm}
If $f:[a,b]\rightarrow{\mathbb C}$ is continuous
then we can find an $x_{1}\in[a,b]$ such that
\[|f(x_{1})|\geq |f(x)|.\]
\end{lemma}
\begin{definition}\label{D, uniform norm} Using the notation
of Lemma~\ref{L, before uniform norm} we set
\[\|f\|_{\infty}=|f(x_{1})|.\]
In other words $\|f\|_{\infty}$ is the least $K$ such that
\[|f(x)|\leq K\ \text{for all $x\in[a,b]$}.\]
\end{definition}
Note that $\|f-g\|_{\infty}$ may be considered as the
(or, more properly, a) distance between two continuous functions
$f$ and $g$.

We shall use various results about the uniform
norm of which the following is the most important.
(It is equivalent to the results known as `the general principle
of uniform convergence' and `the completeness of the uniform norm'.)
\begin{lemma} If the functions
$f_{n}:[a,b]\rightarrow{\mathbb C}$ are continuous
and $\sum_{n=1}^{\infty}\|f_{n}\|_{\infty}$
converges, then there exists a continuous function
$f:[a,b]\rightarrow{\mathbb C}$ such that
\[\left\|\sum_{n=1}^{N}f_{n}-f\right\|_{\infty}\rightarrow 0\]
as $N\rightarrow\infty$.
\end{lemma}
\section{Why is there a problem?} We now turn to to the question
of whether Fourier expansions are always possible. It turns out
to be simplest to work on the circle 
${\mathbb T}={\mathbf R}/2\pi{\mathbb Z}$ that is to work
`modulo $2\pi$' so that $x+2n\pi=x$. We ask whether a
continuous function $f:{\mathbb T}\rightarrow{\mathbb C}$
can be represented in the form
\[f(t)\overset{?}{=}\sum_{n=-\infty}^{n=\infty}a_{n}\exp int.\]
Since
\begin{equation*}
\frac{1}{2\pi}\int_{\mathbb T}(\exp int)(\exp -imt)\,dt=
\begin{cases}1&\text{if $n=m$,}\\
0&\text{otherwise,}
\end{cases}
\end{equation*}
the same arguments as we used in Lemma~\ref{L, Expand string}
show that we should ask whether
\begin{equation}
\tag*{$\bigstar$}
f(t)\overset{?}{=}\sum_{n=-\infty}^{n=\infty}\hat{f}(n)\exp int.
\end{equation}
where
\[\hat{f}(n)=
\frac{1}{2\pi}\int_{\mathbb T}\exp(-int)f(t)\,dt.\]
Over the last two centuries we have learnt that the
formula $\bigstar$ can be interpreted in many different
ways and that each way gives rise to new set of questions
and answers but for the moment let us take the most
obvious interpretation and ask whether
\[\sum_{n=-N}^{n=N}\hat{f}(n)\exp int
\overset{?}{\rightarrow}f(t)\]
as $N\rightarrow\infty$ for each $t\in{\mathbb T}$?

Observe that
\begin{align*}
\sum_{n=-N}^{n=N}\hat{f}(n)\exp int
&=\sum_{n=-N}^{n=N}
\frac{1}{2\pi}\int_{\mathbb T}\exp(-inx)f(x)\,dx\exp(int)\\
&=\frac{1}{2\pi}\int_{\mathbb T}\sum_{n=-N}^{n=N}\exp(in(t-x))f(x)\,dx.
\end{align*}
The same algebra that we used when considering the radar problem
now gives us the result of the next lemma.
\begin{lemma}{\bf (Dirichlet's kernel.)}\label{L, Dirichlet's kernel}  
(i) If $f:{\mathbb T}\rightarrow{\mathbb C}$ is continuous then
\[\sum_{n=-N}^{n=N}\hat{f}(n)\exp int=
\frac{1}{2\pi}\int_{\mathbb T}D_{N}(t-x)f(x)\,dx\]
where
\[D_{N}(s)=\sum_{n=-N}^{n=N}\exp(ins).\]

(ii) We have
\begin{equation*}
D_{N}(s)=
\begin{cases}2N+1&\text{if $s=0$,}\\
\frac{\sin((N+\frac{1}{2})s)}{\sin(\frac{1}{2}s)}&\text{otherwise.}
\end{cases}
\end{equation*}

(iii) ${\displaystyle \frac{1}{2\pi}\int_{\mathbb T}D_{N}(x)\,dx=1.}$
\end{lemma}
We call $D_{N}$ the Dirichlet kernel.

If we look at the graphs of $D_{N}(s)$ and $D_{N}(t-x)f(x)$
(where $t$ is fixed) we see that
$\frac{1}{2\pi}\int_{\mathbb T}D_{N}(t-x)f(x)\,dx$
may indeed tend to $f(t)$ as $N\rightarrow\infty$ but that,
if it does so, it appears that this is the result of some quite
complicated cancellation.

In order to make this statement more precise we need
results which you may already know.
\begin{lemma} (i) If
$f:[1,\infty)\rightarrow{\mathbb R}$ is a decreasing
function then
\[\sum_{n=1}^{N-1}f(n)\geq\int_{1}^{N}f(x)\,dx
\geq \sum_{n=2}^{N}f(n).\]

(ii) ${\displaystyle \frac{1}{\log N}\sum_{n=1}^{N}\frac{1}{n}
\rightarrow 1}$ as $N\rightarrow\infty$.
\end{lemma}
\begin{lemma} If $0\leq x\leq \pi/2$ then
\[\frac{2x}{\pi}\leq \sin x\leq x.\]
\end{lemma}

Observing that
\begin{align*}
\frac{1}{2\pi}\int_{\mathbb T}|D_{N}(x)|\,dx&
\geq \frac{1}{\pi}\sum_{r=1}^{2N}\int_{r\pi/(2N+1)}^{(r+1)\pi/(2N+1)}
\left|\frac{\sin\frac{(2N+1)x}{2}}{\sin\frac{x}{2}}\right|\,dx\\
&\geq \frac{1}{\pi}\sum_{r=1}^{2N}\int_{r\pi/(2N+1)}^{(r+1)\pi/(2N+1)}
\frac{|\sin\frac{(2N+1)x}{2}|}{x}\,dx\\
&\geq \frac{1}{\pi}\sum_{r=1}^{2N}\frac{2N+1}{r+1}
\int_{r\pi/(2N+1)}^{(r+1)\pi/(2N+1)}
\left|\sin\frac{(2N+1)x}{2}\right|\,dx.,
\end{align*}
we obtain the next lemmas.
Here and elsewhere we adopt the the abbreviation
\[S_{N}(f,t)=\sum_{n=-N}^{n=N}\hat{f}(n)\exp int.\]

\begin{lemma}\label{L Dirichlet large} 
There exists a constant $A>0$ such that
\[\frac{1}{\log N}\left(\frac{1}{2\pi}\int_{\mathbb T}|D_{N}(x)|\,dx\right)
\geq A\]
for all $N\geq 1$.
\end{lemma}
\begin{lemma}\label{L, start divergence} 
There exists a constant $B>0$ such that,
given any $N\geq 1$ we can find a continuous function
$f:{\mathbb T}\rightarrow{\mathbb R}$ with 
$\|f\|_{\infty}\leq 1$ and 
\[|S_{N}(f,0)|\geq B\log N.\]
\end{lemma}

It thus comes as no surprise that the following
theorem holds.
\begin{theorem} There exists a continuous function
$f:{\mathbb T}\rightarrow{\mathbb R}$ such that
$S_{N}(f,0)$ fails to converge as $N\rightarrow\infty$.
\end{theorem}
The full details of the proof require a good grasp of
uniform convergence so I leave them as an exercise to be
done (if it is done at all) after completion of the next section.
\section{Fej\'{e}r's theorem} It is, of course, true that,
as we shall see later, the Fourier sum $S_{N}(f,t)\rightarrow f(t)$
for all sufficiently well behaved functions $f$ but
the fact that this result fails for some continuous $f$
remained a serious bar to progress until the beginning
of the 20th century. Then a young Hungarian 
mathematician realised that, although Fourier sums might
behave badly, their averages
\[\sigma_{N}(f,t)=(N+1)^{-1}\sum_{m=0}^{N}S_{m}(f,t)
=\sum_{r=-N}^{N}\frac{N+1-|r|}{N+1}\hat{f}(r)\exp irt\]
behave much better. (We call $\sigma_{N}(f,t)$ the
$N$th Fej\'{e}r sum.) 

We use the same procedure to study Fej\'{e}r sums
as we did Fourier sums. The following algebraic
identity plays a very useful role.
\[\left(\sum_{r=0}^{N}\exp\left( i(r-\frac{N}{2})s\right)\right)^{2}
=\sum_{r=-N}^{N}(N+1-|r|)\exp irt.\]
The next result and its proof should be compared carefully
with Lemma~\ref{L, Dirichlet's kernel} and its proof.


\begin{lemma}{\bf (Fej\'{e}r's kernel.)}\label{L, Fejer's kernel}  
(i) If $f:{\mathbb T}\rightarrow{\mathbb C}$ is continuous then
\[\sigma_{N}(f,t)=\sum_{n=-N}^{N}\frac{N+1-|n|}{N+1}\hat{f}(n)\exp int=
\frac{1}{2\pi}\int_{\mathbb T}K_{N}(t-x)f(x)\,dx\]
where
\[K_{N}(s)=\sum_{n=-N}^{n=N}\frac{N+1-|n|}{N+1}\exp(ins).\]

(ii) We have
\begin{equation*}
K_{N}(s)=
\begin{cases}N+1&\text{if $s=0$,}\\
\frac{1}{N+1}\left(\frac{\sin(\tfrac{N+1}{2}s)}
{\sin(\tfrac{1}{2}s)}\right)^{2}&\text{otherwise.}
\end{cases}
\end{equation*}

(iii) ${\displaystyle \frac{1}{2\pi}\int_{\mathbb T}K_{N}(x)\,dx=1.}$

(iv) $K_{n}(s)\geq 0$ for all $s$.

(v) If $\eta>0$ then $K_{n}\rightarrow 0$ uniformly
for $|t|\geq \eta$
as $n\rightarrow\infty$.
\end{lemma}
We call $K_{N}$ the Fej\'{e}r kernel.
Conditions~(iv) and, to a lesser extent,~(v)
give the key differences between the Dirichlet
and the Fej\'{e}r kernels. 
If we look at the graphs of $K_{N}(s)$ and $K_{N}(t-x)f(x)$
(where $t$ is fixed) we see that
$\frac{1}{2\pi}\int_{\mathbb T}K_{N}(t-x)f(x)\,dx$
will indeed tend to $f(t)$ as $N\rightarrow\infty$
without any need for cancellation.

Using Lemma~\ref{L, Fejer's kernel} we see that, if $0<\eta<\pi$
\begin{align*}
|\sigma_{N}(f,t)-f(t)|&\leq
\left|\frac{1}{2\pi}\int_{\mathbb T}K_{N}(t-x)f(x)\,dx-f(t)\right|\\
&=\left|\frac{1}{2\pi}\int_{\mathbb T}K_{N}(x)f(t-x)\,dx-f(t)\right|\\
&=\left|\frac{1}{2\pi}\int_{\mathbb T}(K_{N}(x)f(t-x)-f(t))\,dx\right|\\
&\leq\frac{1}{2\pi}\int_{\mathbb T}K_{N}(x)|f(t-x)-f(t)|\,dx\\
&=\frac{1}{2\pi}\int_{|x|\leq\eta}K_{N}(x)|f(t-x)-f(t)|\,dx\\
&\ \ \ 
\ +\frac{1}{2\pi}\int_{|x|>\eta}K_{N}(x)|f(t-x)-f(t)|\,dx\\
&\leq\sup_{|x|\leq\eta}|f(t-x)-f(t)|
\frac{1}{2\pi}\int_{|x|\leq\eta}K_{N}(x)\,dx\\
&\ \ \
\ +\sup_{|x|\geq\eta}K_{N}(x)\frac{1}{2\pi}
\int_{|x|>\eta}|f(t-x)-f(t)|\,dx\\
&\leq\sup_{|x|\leq\eta}|f(t-x)-f(t)|+
2\|f\|_{\infty}\sup_{|x|\geq\eta}K_{N}(x).
\end{align*}
This calculations immediately gives us the required result.
\begin{theorem} If $f:{\mathbb T}\rightarrow{\mathbb C}$ 
is continuous, then
$\sigma_{N}(f,t)\rightarrow f(t)$ as $N\rightarrow\infty$
for each $t\in{\mathbb T}$.
\end{theorem}
A little thought gives a still stronger and more useful
theorem.
\begin{theorem} {\bf (Fej\'{e}r's theorem.)}
If $f:{\mathbb T}\rightarrow{\mathbb C}$ 
is continuous then
\[\|\sigma_{N}(f)-f\|_{\infty}\rightarrow 0\]
as $N\rightarrow\infty$.
\end{theorem}
Here $\sigma_{N}(f)(t)=\sigma_{N}(f,t)$.

Fej\'{e}r's theorem has many important consequences.
\begin{theorem} {\bf (Uniqueness.)}
If $f,\,g:{\mathbb T}\rightarrow{\mathbb C}$ 
are continuous and $\hat{f}(n)=\hat{g}(n)$ for
all $n$ then $f=g$.
\end{theorem}
\begin{theorem}\label{T, absolutely convergent}
If $f:{\mathbb T}\rightarrow{\mathbb C}$ 
is continuous and 
$\sum_{n=-\infty}^{\infty}|\hat{f}(n)|$ converge then
$\|S_{N}(f)-f\|_{\infty}\rightarrow 0$ as $N\rightarrow\infty$.
\end{theorem}
The steps in the proof of Theorem~\ref{T, absolutely convergent}
are set out in the next lemma.
\begin{lemma} Suppose that
$f:{\mathbb T}\rightarrow{\mathbb C}$ 
is continuous and 
$\sum_{n=-\infty}^{\infty}|\hat{f}(n)|$ converges.

(i) $\sum_{n=-N}^{N}\hat{f}(n)\exp int$
converges uniformly to a continuous function $g$.

(ii) The function $g$ of~(i) satisfies $\hat{g}(n)=\hat{f}(n)$
for all $n$ and so by the uniqueness of the Fourier coefficients
$g=f$.
\end{lemma}
\section{The trigonometric polynomials are uniformly dense}
The 20th century made it clear that for many purposes
convergence is less important than approximation. 
Fej\'{e}r's theorem tells us that the trigonometric
polynomials are uniformly dense in the continuous functions.
\begin{theorem}\label{T, density} 
(i) If $f:{\mathbb T}\rightarrow{\mathbb C}$ 
is continuous then, given any $\epsilon>0$ we can find a 
trigonometric polynomial 
\[P(t)=\sum_{n=-N}^{N}a_{n}\exp int\]
such that $\|P-f\|_{\infty}<\epsilon$.

(ii) If $g:{\mathbb T}\rightarrow{\mathbb R}$ 
is continuous then, given any $\epsilon>0$ we can find a 
real trigonometric polynomial $Q$ 
such that $\|Q-f\|_{\infty}<\epsilon$.
\end{theorem}

Here is a typical use of this result.
\begin{theorem}\label{T, mean square} Suppose that
$f:{\mathbb T}\rightarrow{\mathbb C}$ 
is continuous.

(i) $\sum_{n=-\infty}^{\infty}|\hat{f}(n)|^{2}$ converges
and 
\[\sum_{n=-\infty}^{\infty}|\hat{f}(n)|^{2}=
\frac{1}{2\pi}\int_{\mathbb T}|f(t)|^{2}\,dt.\]

(ii) The expression
\[\frac{1}{2\pi}\int_{\mathbb T}\left|f(t)-\sum_{n=-N}^{N}a_{n}e^{int}\right|^{2}\,dt\]
has a unique minimum (over choices of $a_{n}$)  when $a_{n}=\hat{f}(n)$.

(iii) We have
\[\frac{1}{2\pi}\int_{\mathbb T}\left|f(t)-\sum_{n=-N}^{N}\hat{f}_{n}e^{int}\right|^{2}\,dt
\rightarrow 0\]
as $N\rightarrow\infty$.
\end{theorem}
In other words the Fourier sums are the best `mean square' approximations
and converge in `mean square' to the original function. The following
pretty formula (Parseval's identity) can be deduced from Theorem~\ref{T, mean square}
or proved by using the same ideas.
\begin{lemma} Suppose that
$f,\,g:{\mathbb T}\rightarrow{\mathbb C}$ 
are continuous. Then 
\[\sum_{n=-N}^{N}\hat{f}(n)\hat{g}(n)^{*}\rightarrow
\frac{1}{2\pi}\int_{\mathbb T}f(t)g(t)^{*}\,dt\]
as $N\rightarrow\infty$.
\end{lemma}  

Here is a beautiful application due to Weyl
of Theorem~\ref{T, density}. If $x$ is real
let us write $\langle x\rangle$ for the fractional part
of $x$, that is, let us write
\[\langle x\rangle=x-[x].\]
\begin{theorem}\label{Weyl} If $\alpha$ is an irrational
number and $0\leq a\leq b\leq 1$, then
\[\frac{\card\{1\leq n\leq N \mid \langle n\alpha\rangle
\in [a,b]\}}{N}
\rightarrow b-a\]
as $N\rightarrow\infty$. The result is false
if $\alpha$ is rational.
\end{theorem}

The proof of Weyl's theorem can be split into stages as follows.
\begin{lemma} The following statements are equivalent.

(i) If $\alpha$ is an irrational
number and $0\leq a\leq b\leq 1$, then
\[\frac{\card\{1\leq n\leq N \mid \langle n\alpha\rangle
\in [a,b]\}}{N}
\rightarrow b-a\]
as $N\rightarrow\infty$.

(ii) If $\alpha$ is an irrational
number and $0\leq a\leq b\leq 2\pi$, then
\[\frac{\card\{1\leq n\leq N \mid  2\pi n\alpha
\in [a,b]\}}{N}
\rightarrow \frac{b-a}{2\pi}\]
as $N\rightarrow\infty$. 

(iii) If $\alpha$ is an irrational
number and $f:{\mathbb T}\rightarrow{\mathbb C}$
is continuous then
\[\sum_{n=0}^{N}f(2\pi n\alpha)\rightarrow
\frac{1}{2\pi}\int_{\mathbb T}f(x)\,dx\]
as $N\rightarrow\infty$.
\end{lemma}
\begin{lemma} If $\alpha$ is an irrational
number and $f:{\mathbb T}\rightarrow{\mathbb C}$
is continuous let us write
\[J_{N}f=\sum_{n=0}^{N}f(2\pi n\alpha)
\ \text{and}
\ If=\frac{1}{2\pi}\int_{\mathbb T}f(x)\,dx.\]

(i) $J_{N}$ and $I$ are linear maps from $C({\mathbb T})$
to ${\mathbb C}$ with $|J_{N}f|\leq\|f\|_{\infty}$
and $|If|\leq\|f\|_{\infty}$.

(ii) If we define $e_{n}:{\mathbb T}\rightarrow{\mathbb R}$
by $e_{n}(t)=\exp int$ then
\[J_{N}e_{n}\rightarrow Ie_{n}\]
as $N\rightarrow\infty$ for all $n\in{\mathbb Z}$.
\end{lemma}

Much of this work can be extended to more dimensions. I shall probably
leave the proofs of the following results as exercises for those
who want to do them.
\begin{lemma}\label{ L, many start}
If we define $\tilde{K}:{\mathbb T}^{m}\rightarrow{\mathbb R}$
by $\tilde{K}(t_{1},t_{2},\dots,t_{m})=\prod_{j=1}^{m}K(t_{j})$
then we have the following results.

(i) $\frac{1}{(2\pi)^{m}}\int_{{\mathbb T}^{m}}
\tilde{K}_{n}({\mathbf t})\, d{\mathbf t}=1$.

(ii) If $\eta>0$ then
$\int_{|{\mathbf t}|\geq\eta}\tilde{K}_{n}({\mathbf t})
\,d{\mathbf t}\rightarrow 0$
as $n\rightarrow\infty$.

(iii) $\tilde{K}_{n}({\mathbf t})\geq 0$ for all ${\mathbf t}$.

(iv) $\tilde{K}_{n}$ is a (multidimensional) trigonometric
polynomial.

(v) If $f:{\mathbb T}^{m}\rightarrow{\mathbb C}$ is continuous then
\[\frac{1}{(2\pi)^{m}}\int_{{\mathbb T}^{m}}
\tilde{K}_{N}({\mathbf t}-{\mathbf x})f({\mathbf x})\,d{\mathbf x}
\rightarrow f({\mathbf t})\]
uniformly as $N\rightarrow\infty$.
\end{lemma}
\begin{lemma} 
If $f:{\mathbb T}^{m}\rightarrow{\mathbb C}$ 
is continuous then, given any $\epsilon>0$ we can find a 
trigonometric polynomial 
\[P(t)=\sum_{|j(r)|\leq N}a_{j(1),j(2),\dots,j(m)}
\exp \left(i\sum_{r=1}^{m}j(r)t_{r}\right)\]
such that $\|P-f\|_{\infty}<\epsilon$.
\end{lemma}

We immediately obtain a striking generalisation of
Weyl's theorem (Theorem~\ref{Weyl}).
\begin{lemma}\label{T, Weyl many} Suppose
that $\alpha_{1}$, $\alpha_{2}$, \dots, $\alpha_{m}$
are real numbers. A necessary and sufficient condition
that
\[\frac{\card\{1\leq n\leq N \mid
(\langle n\alpha_{1}\rangle,\langle n\alpha_{2}\rangle,
\dots,\langle n\alpha_{m}\rangle
\in \prod_{j=1}^{m}[a_{j},b_{j}]\}}{N}
\rightarrow \prod_{j=1}^{m}(b_{j}-a_{j})\]
as $N\rightarrow\infty$ whenever $0\leq a_{j}\leq b_{j}\leq 1$
is that
\begin{equation*}
\sum_{j=1}^{m} n_{j}\alpha_{j}\notin{\mathbb Z}
\ \text{for integer $n_{j}$ not all zero}. \tag*{$\bigstar$}
\end{equation*}
\end{lemma}
If $\alpha_{1}$, $\alpha_{2}$, \dots, $\alpha_{m}$
satisfy $\bigstar$ we say that they are independent.
The multidimensional version of Weyl's theorem has
an important corollary.
\begin{theorem}{\bf (Kronecker's theorem.)}\label{Kronecker's theorem}
Suppose
that $\alpha_{1}$, $\alpha_{2}$, \dots, $\alpha_{m}$
are independent real numbers. Then given real
numbers $\beta_{1}$, $\beta_{2}$, \dots, $\beta_{m}$
and $\epsilon>0$ we can find integers
$N$, $r_{1}$, $r_{2}$, \dots, $r_{m}$ such that
\[|N\alpha_{j}-\beta_{j}-r_{j}|<\epsilon\]
for each $1\leq j\leq M$.

The result is false if
$\alpha_{1}$, $\alpha_{2}$, \dots, $\alpha_{m}$
are not independent.
\end{theorem}
\section{First thoughts on Fourier transforms} We have
seen that it very useful to look at functions
$f:{\mathbb T}\rightarrow{\mathbb C}$ in the form
\[f(t)=\sum_{n=-\infty}^{n=\infty}a_{n}\exp int,\]
that is functions $f:{\mathbb T}\rightarrow{\mathbb C}$ which
are weighted sums of simple waves (exponentials).  
However, many problems involve functions
$g:{\mathbb R}\rightarrow{\mathbb C}$ so it is natural to 
investigate those $g:{\mathbb R}\rightarrow{\mathbb C}$
which are weighted \emph{integrals} of simple waves (exponentials),
that which can be written
\[g(t)=\int_{-\infty}^{\infty}G(\lambda)\exp(-i\lambda t)\,d\lambda.\]
(The minus sign is inserted for consistency with our other
conventions.)
We say that $g$ is the \emph{Fourier transform} 
of $G:{\mathbb R}\rightarrow{\mathbb C}$.

Unfortunately the rigorous treatment of `integrals over
an infinite range' raises certain problems.
\begin{example} (i) If we set 
\begin{equation*} 
a_{rs}=\begin{cases}2^{-r}&\text{if $2^{r}+1\leq s\leq 2^{r+1}$,}\\
-2^{-r-1}&\text{if $2^{r+1}+1\leq s\leq 2^{r+2}$,}\\
0&\text{otherwise,}
\end{cases}
\end{equation*}
then
\[\sum_{r=1}^{\infty}\left(\sum_{s=1}^{\infty}a_{rs}\right)=0
\neq 1=\sum_{s=1}^{\infty}\left(\sum_{r=1}^{\infty}a_{rs}\right).\]

(ii) We can find a continuous function 
$f:{\mathbb R}^{2}\rightarrow{\mathbb R}$ with
$f({\mathbf x})\rightarrow 0$ as $\|{\mathbf x}\|\rightarrow\infty$
with all the integrals in the next inequality well defined but
\[\int_{-\infty}^{\infty}\left(\int_{-\infty}^{\infty}f(x,y)\,dx\right)\,dy
\neq
\int_{-\infty}^{\infty}\left(\int_{-\infty}^{\infty}f(x,y)\,dy\right)\,dx.\]
\end{example}

Provided the functions fall away sufficiently fast towards infinity
the problem raised by the previous example will not occur.
\begin{lemma}  If
$f:{\mathbb R}^{2}\rightarrow{\mathbb R}$ is such that we can find
a constant $A$ with
\[|f(x,y)|\leq\frac{A}{(1+x^{2})(1+y^{2})}\]
for all $(x,y)\in{\mathbb R}^{2}$ then
all the integrals in the next equality are well defined and
\[\int_{-\infty}^{\infty}\left(\int_{-\infty}^{\infty}f(x,y)\,dx\right)\,dy
=
\int_{-\infty}^{\infty}\left(\int_{-\infty}^{\infty}f(x,y)\,dy\right)\,dx.\]
\end{lemma}

In developing the theory of Fourier transforms we shall
assume various theorems which depend on reasonably rapid decay towards 
infinity.
\section{Fourier transforms} We now start the study
of Fourier transforms in earnest.

\begin{definition} If $f:{\mathbb R}\rightarrow{\mathbb C}$
is reasonably well behaved, we define
\[\hat{f}(\lambda)
=\int_{-\infty}^{\infty}f(t)e^{-i\lambda t}\,dt,\]
and call the function
$\hat{f}:{\mathbb R}\rightarrow{\mathbb C}$
the \emph{Fourier transform}.
\end{definition}
As we said in the previous section we require
a certain amount of good behaviour from $f$.
The condition that $f$, $f'$ and $f''$
are continuous and
$t^{2}f(t),\ t^{2}f'(t),\ t^{2}f''(t)\rightarrow 0$
as $|t|\rightarrow\infty$ are amply sufficient
for our purpose (much less is required,  but there
always has to be some control over behaviour towards
infinity).

The following results form part of the grammar of
Fourier transforms.
\begin{lemma}\label{L, grammar} 
(i) If $a\in{\mathbb R}$, let us write
$f_{a}(t)=f(t-a)$. Then
\[\hat{f}_{a}(\lambda)=e^{-ia\lambda}\hat{f}(\lambda).\]
(Translation on one side gives phase change on other.)

(ii) If $K\in{\mathbb R}$ and $K>0$, let us write
$f_{K}(t)=f(Kt)$. Then
\[\hat{f}_{K}(\lambda)=K^{-1}\hat{f}(\lambda/K).\]
(Narrowing on one side gives broadening on the other.)

(iii) $\hat{f}(\lambda)^{*}=(f^{*})\hat{\ }(-\lambda)$.

(iv) $(\hat{f})'(\lambda)
=-i\hat{F}(\lambda)$ where $F(t)=tf(t)$.

(v) $(f')\hat{\ }(\lambda)=i\lambda\hat{f}(\lambda)$.
\end{lemma}

It is natural to hope that we could obtain results on Fourier
transforms as limits (in some sense) of Fourier sums.
This is not impossible and we shall see in Section~\ref{S, Poisson}
that there is a very elegant link between Fourier
transforms and Fourier sums. However, there are technical 
difficulties and it is more straightforward to start afresh.

We pay particular attention to the Gaussian (or heat, or error)
kernel $E(x)=(2\pi)^{-1/2}\exp(-x^{2}/2)$.
\begin{lemma}\label{L, Fourier of Gauss}
(i) The Fourier transform of $E$ obeys the partial differential
equation
\[\hat{E}'(\lambda)=-\lambda\hat{E}(\lambda).\]

(ii) $\hat{E}(\lambda)=(2\pi)^{1/2}E(\lambda)$
\end{lemma}
(The fact that $\hat{E}(0)=1$ is derived from a formula
that is probably known to you. If not, consult
Exercise~\ref{E, Liouville}.)

We use the following neat formula.
\begin{lemma}\label{L neat} If $f,g:{\mathbb R}\rightarrow{\mathbb C}$
are well behaved then
\[\int_{-\infty}^{\infty}\hat{g}(x)f(x)\,dx
=\int_{-\infty}^{\infty}g(\lambda)\hat{f}(\lambda)\,d\lambda.\]
\end{lemma}

By taking $g(x)=E_{R}(x)=E(Rx)$ and allowing $R\rightarrow 0+$
we obtain an inversion formula.
\begin{lemma}\label{L start inversion} 
If $f:{\mathbb R}\rightarrow{\mathbb C}$
is well behaved then
\[f(0)=\frac{1}{(2\pi)^{1/2}}
\int_{-\infty}^{\infty}\hat{f}(\lambda)\,d\lambda.\]
\end{lemma}
Using Lemma~\ref{L, grammar}, we see that translation gives us
our full inversion result.
\begin{theorem}\label{T, inversion transform}
If $f$ is well behaved,
then $\Hat{\Hat{f}}(t)=2\pi f(-t)$.
\end{theorem}
(If we write ${\mathcal F}(f)=(2\pi)^{-1/2}\hat{f}$, $Jf(t)=f(-t)$
and $If=f$ then subject to appropriate conditions we obtain
${\mathcal F}^{2}=J$ and ${\mathcal F}^{4}=I$.)

The inversion formula gives a uniqueness result which
is often more useful than the inversion formula itself.
\begin{theorem}{\bf (Uniqueness.)} If $f$ and $g$
are well behaved and
$\hat{f}=\hat{g}$ then $f=g$.
\end{theorem}
One of the reasons that Fourier analysis works so well
is that, although the inversion theorem does require
some sort of good behaviour, it turns out that the
uniqueness theorem is (almost) endlessly extendable.

Combining the inversion formula with Lemma~\ref{L neat}
we obtain an elegant
analogue of Theorem~\ref{T, mean square}.
\begin{lemma} If $f:{\mathbb R}\rightarrow{\mathbb C}$
is well behaved, then
\[\int_{-\infty}^{\infty}|f(t)|^{2}\,dt
=\frac{1}{2\pi}\int_{-\infty}^{\infty}|\hat{f}(\lambda)|^{2}\,d\lambda.\]
\end{lemma}

Fourier transforms are closely linked with the
important operation of convolution.
\begin{definition} If $f,\ g:{\mathbb R}\rightarrow{\mathbb C}$
are well behaved, we define their \emph{convolution}
$f*g:{\mathbb R}\rightarrow{\mathbb C}$ by
\[f*g(t)=\int_{-\infty}^{\infty}f(t-s)g(s)\,ds.\]
\end{definition}
\begin{lemma} If $f$ and $g$ are well behaved, 
$\widehat{f*g}(\lambda)=\hat{f}(\lambda)\hat{g}(\lambda)$.
\end{lemma}
For many mathematicians and engineers, Fourier transforms are
important because they convert convolution into multiplication
and convolution is important because it is transformed
by Fourier transforms into multiplication.
We shall see that convolutions occur naturally in the
study of differential equations.
It also occurs in probability theory where the sum $X+Y$
of two independent random variables $X$ and $Y$ with
probability densities $f_{X}$ and $f_{Y}$ is
$f_{X+Y}=f_{X}*f_{Y}$. In the next section
we outline the connection of convolution with signal
processing.
\section{Signals and such-like}\label{Green} Suppose we have a black box
${\mathcal K}$. If we feed in a signal 
$f:{\mathbb R}\rightarrow{\mathbb C}$ we will get out
a transformed signal 
${\mathcal K}f:{\mathbb R}\rightarrow{\mathbb C}$.
Simple black boxes will have the following properties:

(1) \emph{Time invariance} If ${\mathcal T}_{a}f(t)=f(t-a)$,
then ${\mathcal K}({\mathcal T}_{a}f)(t)=({\mathcal K}f)(t-a)$.
In other words, ${\mathcal K}{\mathcal T}_{a}
={\mathcal T}_{a}{\mathcal K}$.

(2) \emph{Causality} If $f(t)=0$ for $t<0$, then
$({\mathcal K}f)(t)=0$ for $t<0$. (The response
to a signal cannot precede the signal.)

(3) \emph{Stability} Roughly speaking, the black box
should consume rather than produce energy. Roughly
speaking, again, if there exists a $R$ such that
$f(t)=0$ for $|t|\geq R$, then we should have
$({\mathcal K}f)(t)\rightarrow 0$ as $t\rightarrow\infty$.
If conditions like this do not apply, both our mathematics
and our black box have a tendency to explode. 
(Unstable systems may be investigated using a close
relative of the Fourier transform called the Laplace transform.)

(4) \emph{Linearity} In order for the methods of this course
to work, our black box must be linear, that is
\[{\mathcal K}(af+bg)=a{\mathcal K}(f)+b{\mathcal K}(g).\]
(Engineers sometimes spend a lot of effort converting
non-linear systems to linear for precisely this reason.)

As our first example of such a system, let us consider
the differential equation
\begin{equation*}
\tag*{$\bigstar$}
F''(t)+(a+b)F'(t)+ab F(t)=f(t)
\end{equation*}
(where $a>b>0$), subject to the boundary condition
$F(t),\ F'(t)\rightarrow 0$ as $t\rightarrow -\infty$. 
We take ${\mathcal K}f=F$.

Before we can solve the system using Fourier transforms
we need a preliminary definition and lemma.
\begin{definition} The Heaviside function
$H:{\mathbb R}\rightarrow{\mathbb R}$ is given by
\begin{alignat*}{2}
H(t)&=0&&\qquad\text{for $t<0$},\\
H(t)&=1&&\qquad\text{for $t\geq 0$.}
\end{alignat*}
\end{definition}
\begin{lemma} Suppose that $\Re \alpha<0$. Then, if we set
$e_{\alpha}(t)=e^{\alpha t}H(t)$, we obtain
\[\hat{e}_{\alpha}(\lambda)=\frac{1}{i\lambda-\alpha}.\]
\end{lemma}
(Some applied mathematicians would leave out
the condition $\Re \alpha<0$ in the lemma
just given and most would write $\hat{H}(\lambda)=1/(i\lambda)$.
The study of Laplace transforms reveals why this reckless
behaviour does not lead to disaster.)

\begin{lemma}\label{Lemma, big star}
The solution $F={\mathcal K}f$ of
\begin{equation*}
\tag*{$\bigstar$}
F''(t)+(a+b)F'(t)+ab F(t)=f(t)
\end{equation*}
(where $a,\ b>0$), subject to the boundary condition
$F(t),\ F'(t)\rightarrow 0$ as $t\rightarrow -\infty$,
is given by
\[{\mathcal K}f=K\star f
\ \text{where}\ K(t)=\frac{e^{-bt}-e^{-at}}{a-b}H(t).\]
\end{lemma}

Observe that $K(t)=0$ for $t\leq 0$ and so, if $f(t)=0$
for $t\leq 0$, we have
\begin{alignat*}{2}
{\mathcal K}f(t)&=K\star f(t)=0\qquad\text{for $t\leq 0$},\\
{\mathcal K}f(t)&=K\star f(t)=\int_{0}^{t}f(s)K(t-s)\,ds
\qquad\text{for $t>0$}.
\end{alignat*}
Thus ${\mathcal K}$ is indeed causal.

There is another way of analysing black boxes. Let
$g_{n}$ be a sequence of functions such that

(i) $g_{n}(t)\geq 0$ for all $t$,

(ii) ${\displaystyle \int_{-\infty}^{\infty}g_{n}(t)\,dt=1}$,

(iii) $g_{n}(t)=0$ for $|t|>1/n$.

\noindent In some sense, the $g_{n}$ `converge' towards
the `idealised impulse function' $\delta$ whose
defining property runs as follows.
\begin{definition} If $f:{\mathbb R}\rightarrow{\mathbb R}$
is a well behaved function then
\[\int_{-\infty}^{\infty}f(t)\delta(t)\,dt=f(0).\]
\end{definition}
If the black box is well behaved we expect ${\mathcal K}g_{n}$
to converge to some function $E$. We write
\[{\mathcal K}\delta=E\]
and say that the response of the black box to the delta
function is the elementary solution $E$. Note that,
since our black box is causal, $K(t)=0$ for $t<0$.

If $f$ is a ordinary function, we define its translate
by some real number $a$ to be $f_{a}$ where
$f_{a}(t)=f(t-a)$. In the same way, we define the
translate by $a$ of the delta function by $a$ to be
$\delta_{a}$ where $\delta_{a}(t)=\delta(t-a)$ or,
more formally, by
\[\int_{-\infty}^{\infty}f(t)\delta_{a}(t)\,dt=
\int_{-\infty}^{\infty}f(t)\delta(t-a)\,dt=f(a).\]

Since our black box is time invariant, we have
\[{\mathcal K}\delta_{a}=E_{a}\]
and, since it is linear,
\[{\mathcal K}\sum_{j=1}^{n}\lambda_{j}\delta_{a_{j}}(t)=
\sum_{j=1}^{n}\lambda_{j}E_{a_{j}}(t).\]
In particular, if $F$ is a well behaved function,
\begin{align*}
{\mathcal K}\sum_{j=-MN}^{MN}N^{-1}F(j/N)\delta_{j/N}(t)&=
\sum_{j=-MN}^{MN}N^{-1}F(j/N)E_{j/N}(t)\\
&=\sum_{j=-MN}^{MN}N^{-1}F(j/N)E(t-j/N).
\end{align*}
Crossing our fingers and allowing $M$ and $N$ to tend
to infinity, we obtain
\[{\mathcal K}F(t)=\int_{-\infty}^{\infty}F(s)E(t-s)\,ds,\]
so
\[{\mathcal K}F=F*E.\]

Thus the response of the black box to a signal $F$
is obtained by convolving $F$ with the response of
the black box to the delta function. (This is why
the acoustics of concert halls are tested by letting off
starting pistols.) We now understand the importance
of convolution, delta functions and elementary solutions
in signal processing and the study of partial differential
equations. (The response of
the black box to the delta function is often called
the Green's function.)

We use two methods to find out what happens in our specific case.

\noindent\emph{First method} Suppose
\begin{equation*}
\tag*{$\bigstar$}
E''(t)+(a+b)E'(t)+ab E(t)=\delta(t)
\end{equation*}
and $E$ is well behaved. Then we have
\[E''(t)+(a+b)E'(t)+ab E(t)=0\]
for $t>0$ so $E(t)=Ae^{-at}+Be^{-bt}$ for $t>0$ where
$A$ and $B$ are constants to be determined. We also
have 
\[E''(t)+(a+b)E'(t)+ab E(t)=0\]
for $t<0$ and the condition that $E(t),\ E'(t)\rightarrow 0$
as $t\rightarrow-\infty$ gives $E(t)=0$ for $t<0$.
When a bat hits a ball the velocity of the ball changes
(almost) instantaneously but the position does not.
We thus expect $E$ to be continuous at $0$ even though
$E'$ is not. The continuity of $E$ gives
\[0=E(0-)=E(0+)=A+B\]
so $A=-B$ and $E(t)=Ae^{-at}-Ae^{-bt}$ for $t>0$.

Next we observe that, if $\eta>0$
\[\int_{-\eta}^{\eta}\big(E''(t)+(a+b)E'(t)+ab E(t)\big)\,dt=
\int_{-\eta}^{\eta}\delta(t)\,dt=1\]
so
\[\left[E'(t)+(a+b)E(t)\right]_{-\eta}^{\eta}
+ab\int_{-\eta}^{\eta}E(t)\,dt=1.\]
Allowing $\eta\rightarrow 0+$ and using the continuity
of $E$ we get
\[E'(0+)-E'(0-)=1\]
so $(b-a)A=1$ and $A=(b-a)^{-1}$.

\noindent\emph{Second method} Use Fourier transforms
in the style of our treatment of Lemma~{Lemma, big star}.

Fortunately both methods give the same result.
\begin{lemma}\label{Lemma, Big elementary}
The solution $E={\mathcal K}\delta$ of
\begin{equation*}
\tag*{$\bigstar$}
E''(t)+(a+b)E'(t)+ab E(t)=\delta(t)
\end{equation*}
(where, $a,\ b>0$), subject to the boundary condition
$E(t),\ E'(t)\rightarrow 0$ as $t\rightarrow -\infty$,
is given by
\[E(t)=\frac{e^{-bt}-e^{-at}}{a-b}H(t).\]
\end{lemma}

Observe that Lemma~\ref{Lemma, Big elementary}
implies Lemma~\ref{Lemma, big star} and vice versa.
\section{Heisenberg} If we strike a note on the piano
the result can not be a pure tone (frequency) since
it is limited in time. More generally, as hinted in our discussion
of radar signals, we can not shape a function very
sharply if it is made up of a limited band of frequencies.
(Crudely `thin functions' have `fat transforms'.)
Their are many different ways of expressing this insight.
In this section we give one which has the advantage
(and, perhaps, the disadvantage) of being mathematically
precise.

We shall need to recall the notion of an inner product.
\begin{definition} If $V$ is a vector space over ${\mathbb C}$
we say that the map from $V^{2}$ to ${\mathbb C}$ given by
$({\mathbf x},{\mathbf y})\mapsto
\langle {\mathbf x},{\mathbf y}\rangle$ is an inner product if

(i) $\langle {\mathbf x},{\mathbf x}\rangle\geq 0$ with equality
if and only if ${\mathbf x}={\mathbf 0}$,

(ii) $\langle{\mathbf x},{\mathbf y}\rangle
=\langle{\mathbf y},{\mathbf x}\rangle^{*}$,

(iii) $\langle(\lambda{\mathbf x}),{\mathbf y}\rangle=
\lambda\langle{\mathbf x},{\mathbf y}\rangle$,

(iv) $\langle\langle{\mathbf x}+{\mathbf y}),{\mathbf z}\rangle
=\langle{\mathbf x},{\mathbf z}\rangle+
\langle{\mathbf y},{\mathbf z}\rangle.$
\end{definition}
As the audience has probably realised long ago we
have met at least two inner products in the study
of Fourier analysis.
\begin{lemma} (i) If we set
\[\langle f,g \rangle=\frac{1}{2\pi}\int_{\mathbb T}f(t)g(t)^{*}\,dt\]
then $\langle\ ,\ \rangle$ is an inner product on $C({\mathbb T})$.

(ii) If we set
\[\langle f,g \rangle=\int_{-\infty}^{\infty}f(t)g(t)^{*}\,dt,\]
then $\langle\ ,\ \rangle$ is an inner product on the space 
of well behaved functions on ${\mathbb R}$.
\end{lemma}
The reason that I draw this to the readers attention is
that we need the Cauchy-Schwarz inequality.
\begin{lemma}{\bf (The Cauchy-Schwarz inequality).}%
\label{Cauchy-Schwarz}
If $\langle\ ,\ \rangle$ is an inner product on a vector space $V$
over ${\mathbb C}$
then 
\[|\langle{\mathbf x},{\mathbf y}\rangle|\leq
\|{\mathbf x}\| \|{\mathbf y}\|.\]
where we write $\|{\mathbf a}||$ for the positive square root
of $\langle{\mathbf a},{\mathbf a}\rangle$
\end{lemma}

Using Lemma~\ref{L, grammar}~(v) (Fourier transform of a derivative), 
Theorem~\ref{T, mean square} (Parseval), the Cauchy-Schwarz inequality
and a certain amount of ingenuity we see that, if $f$ is well behaved.
\begin{align*}
\frac{1}{2\pi}\int_{-\infty}^{\infty}x^{2}|f(x)|^{2}\,dx
\int_{-\infty}^{\infty}\lambda^{2}|\hat{f}(\lambda)|^{2}\,d\lambda
=&\frac{1}{2\pi}\int_{-\infty}^{\infty}|xf(x)|^{2}\,dx
\int_{-\infty}^{\infty}|\lambda \hat{f}(\lambda)|^{2}\,d\lambda\\
=&\frac{1}{2\pi}\int_{-\infty}^{\infty}|xf(x)|^{2}\,dx
\int_{-\infty}^{\infty}|\widehat{f'}(\lambda)|^{2}\,d\lambda\\
=&\int_{-\infty}^{\infty}|xf(x)|^{2}\,dx
\int_{-\infty}^{\infty}|f'(x)|^{2}\,dx\\
\geq&\left(\int_{-\infty}^{\infty}|xf(x)f'(x)|\,dx\right)^{2}\\
\geq&\left(\int_{-\infty}^{\infty}
\frac{x}{2}(f'(x)f^{*}(x)+f(x){f^{*}}'(x))\,dx\right)^{2}\\
=&\frac{1}{4}\left(\int_{-\infty}^{\infty}
x\left(\frac{d\ }{dx}|f(x)|^{2}\right)\,dx\right)^{2}\\
=&\frac{1}{4}\left(\int_{-\infty}^{\infty}
|f(x)|^{2}\,dx\right)^{2}\\
=&\frac{1}{8\pi}\int_{-\infty}^{\infty}|f(x)|^{2}\,dx
\int_{-\infty}^{\infty}|\hat{f}(\lambda)|^{2}\,d\lambda
\end{align*}
Rewriting the result we obtain the desired theorem.
\begin{theorem}{\bf (Heisenberg's inequality.)}
If $f:{\mathbb R}\rightarrow{\mathbb C}$
is well behaved and non-trivial, then
\[\frac{\int_{-\infty}^{\infty}x^{2}|f(x)|^{2}\,dx}
{\int_{-\infty}^{\infty}|f(x)|^{2}\,dx}
\frac{\int_{-\infty}^{\infty}\lambda^{2}|\hat{f}(\lambda)|^{2}\,d\lambda}
{\int_{-\infty}^{\infty}|\hat{f}(\lambda)|^{2}\,d\lambda}
\geq\frac{1}{4}.\]
\end{theorem}
Thus, if $f$ is concentrated near the origin $\hat{f}$
cannot be.
\section{Poisson's formula}\label{S, Poisson} 
The circle ${\mathbb T}$ is just
the real line ${\mathbb R}$ rolled up. By reflecting on this
we are led to a remarkable formula. 
\begin{theorem}\label{T, Poisson}{\bf (Poisson's formula.)}
Suppose that
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a continuous
function such that $\sum_{m=-\infty}^{\infty}|\hat{f}(m)|$
converges and $\sum_{n=-\infty}^{\infty}|f(2\pi n+x)|$
converges uniformly on $[-\pi,\pi]$. Then
\[\sum_{m=-\infty}^{\infty}\hat{f}(m)=
2\pi\sum_{n=-\infty}^{\infty}f(2\pi n).\]
\end{theorem}
It is possible to adjust the hypotheses on $f$
in Poisson's formula in various ways
though some hypotheses there must be. The following
rather simple lemma suffices for many applications.
\begin{lemma}\label{L, simple needs}
If $f:{\mathbb R}\rightarrow{\mathbb C}$
is a twice continuously differentiable function such
that $\int_{-\infty}^{\infty}|f(x)|\,dx$,
$\int_{-\infty}^{\infty}|f'(x)|\,dx$
and $\int_{-\infty}^{\infty}|f''(x)|\,dx$
converge whilst $f'(x)\rightarrow 0$
and $x^{2}f(x)\rightarrow 0$ as $|x|\rightarrow\infty$,
then $f$ satisfies the conditions of Theorem~\ref{T, Poisson}.
\end{lemma}

To prove Theorem~\ref{T, Poisson} we observe that
$\sum_{n=-\infty}^{\infty}f(2\pi n+x)$ converges uniformly
to a continuous $2\pi$ periodic function $g(x)$.
We can now define a continuous function $G:{\mathbb T}\rightarrow{\mathbb C}$
by setting $G(x)=g(x)$ for $-\pi<x\leq \pi$. We now observe that
\begin{align*}
\hat{G}(m)&=\frac{1}{2\pi}\int_{-\pi}^{\pi}g(x)\exp(-imx)\,dx\\
&=\frac{1}{2\pi}\sum_{n=-\infty}^{\infty}
\int_{-\pi}^{\pi}f(x+2n\pi)\exp(-imx)\,dx\\
&=\frac{1}{2\pi}\int_{-\infty}^{\infty}f(x)\exp(-imx)\,dx
=\frac{\hat{f}(m)}{2\pi}.
\end{align*}
Theorem~\ref{T, absolutely convergent} on absolutely convergent
Fourier series now gives us the following result.
\begin{lemma}\label{L, Poisson two}
Suppose that
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a continuous
function such that $\sum_{m=-\infty}^{\infty}|\hat{f}(m)|$
converges and $\sum_{n=-\infty}^{\infty}|f(2\pi n+x)|$
converges uniformly on $[-\pi,\pi]$. Then
\[\sum_{m=-\infty}^{\infty}\hat{f}(m)\exp imt=
2\pi\sum_{n=-\infty}^{\infty}f(2\pi n+t).\]
\end{lemma}
Setting $t=0$ gives Theorem~\ref{T, Poisson}.
\begin{exercise}\label{E, Poisson both ways}
Prove Lemma~\ref{L, Poisson two} from
Theorem~\ref{T, Poisson}.
\end{exercise} 
\begin{exercise}\label{E invers via Poisson} 
Suppose that $f$ satisfies the conditions
of Lemma~\ref{L, simple needs}. 
(i) Show that, if $K>0$, then
\[K\sum_{m=-\infty}^{\infty}\hat{f}(Km)=
2\pi\sum_{n=-\infty}^{\infty}f(2\pi n/K).\]
What formula do you obtain if $K<0$?

(ii) By allowing $K\rightarrow 0+$ obtain a new proof
of the inversion formula
\[f(0)=\frac{1}{(2\pi)^{1/2}}
\int_{-\infty}^{\infty}\hat{f}(\lambda)\,d\lambda.\]
Deduce in the usual way that
\[\Hat{\Hat{f}}(t)=2\pi f(-t)\]
for all $t$.
\end{exercise}
\section{Shannon's theorem}
Poisson's formula has a particularly interesting consequence.
\begin{lemma} If $g:{\mathbb R}\rightarrow{\mathbb C}$
is twice continuously differentiable and
$g(t)=0$ for $|t|\geq \pi$ then $g$ is completely
determined by the values of $\hat{g}(m)$ for
integer $m$.
\end{lemma}
Taking $g=\hat{f}$ and remembering the inversion formula
we obtain the following result.
\begin{theorem}\label{before Shannon}
If $f:{\mathbb R}\rightarrow{\mathbb C}$
is a well behaved function with $\hat{f}(\lambda)=0$
for $|\lambda|\geq\pi$
then $f$ is determined by its values at integer
points.
\end{theorem}
The object of this final section is to give a constructive
proof of this theorem.

The simplest approach is via the \emph{sinc function}
\[\sinc(x)=\frac{1}{2\pi}\int_{-\pi}^{\pi}\exp(-ix\lambda)\,d\lambda.\]
We state the most immediately useful properties
of  $\sinc$.
\begin{lemma}
(i) $\sinc(0)=1$,

(ii) $\sinc(n)=0$ if $n\in{\mathbb Z}$ but $n\neq 0$.
\end{lemma}

(We note also that although, strictly speaking,
$\widehat{\sinc}(\lambda)$ is not defined
for us, since $\int|\sinc (x)|\,dx=\infty$
we are strongly tempted to say that
$\widehat{\sinc}(\lambda)=1$ if $|\lambda|<\pi$
and
$\widehat{\sinc}(\lambda)=0$ if $|\lambda|>\pi$.)

We can, at once, prove that Theorem~\ref{before Shannon}
is best possible.
\begin{lemma}\label{Shannon best}
If $\epsilon>0$ then we can find a
well behaved non-zero $f$ such that
$\hat{f}(\lambda)=0$ for $|\lambda|>\pi+\epsilon$
but $f(n)=0$ for all $n\in{\mathbb Z}$.
\end{lemma}

We now show how to recover the function of
Theorem~\ref{before Shannon} from its values
at integer points.
\begin{theorem}\label{Shannon constructive}
Suppose $f:{\mathbb R}\rightarrow{\mathbb C}$
is a continuous function with
$\int_{-\infty}^{\infty}|f(t)|\,dt<\infty$.
If $\hat{f}(\lambda)=0$
for $|\lambda|\geq\pi$
then
\[\sum_{n=-N}^{N}f(n)\sinc(t-n)\rightarrow f(t)\]
uniformly as $N\rightarrow\infty$.
\end{theorem}

To prove this we proceed as follows. Set $F=\hat{f}$.
By the inversion theorem 
\[f(t)=\frac{1}{2\pi}\int_{-\infty}^{\infty}F(\lambda)\exp(i\lambda t)\,dt
=\frac{1}{2\pi}\int_{-\pi}^{\pi}F(\lambda)\exp(i\lambda t)\,dt\]
and, more particularly,
\[f(n)=\frac{1}{2\pi}\hat{F}(-n).\]
This enables us to use results on ${\mathbb T}$ such as 
Schwarz's inequality and Parseval's equality as follows.
\begin{align*}
\left|\sum_{n=-N}^{N}\right.&\left.\vphantom{\sum_{n=-N}^{N}}
f(n)\sinc(t-n)-f(t)\right|\\
&=\left|\sum_{n=-N}^{N}\frac{1}{2\pi}\hat{F}(-n)
\frac{1}{2\pi}\int_{-\pi}^{\pi}\exp(i(t-n)\lambda)\,d\lambda
-\frac{1}{2\pi}\int_{-\pi}^{\pi}F(\lambda)\exp(i\lambda t)\,d\lambda\right|\\
&=\frac{1}{2\pi}\left|\int_{-\pi}^{\pi}
\left(\frac{1}{2\pi}\sum_{n=-N}^{N}\hat{F}(-n)\exp(i\lambda(t-n))
-F(\lambda)\exp(i\lambda t)\right)\,d\lambda\right|\\
&\leq \frac{1}{2\pi}\int_{-\pi}^{\pi}
\left|\frac{1}{2\pi}\sum_{n=-N}^{N}\hat{F}(-n)\exp(-i\lambda n)-F(\lambda)
\right|\,d\lambda\\
&=\frac{1}{2\pi}\int_{-\pi}^{\pi}
\left|F(\lambda)-\frac{1}{2\pi}\sum_{n=-N}^{N}\hat{F}(n)\exp(i\lambda n)
\right|\,d\lambda\\
&\leq\left(
\frac{1}{2\pi}\int_{-\pi}^{\pi}
\left|F(\lambda)-\frac{1}{2\pi}\sum_{n=-N}^{N}\hat{F}(n)\exp(i\lambda n)
\right|^{2}\,d\lambda\right)^{1/2}\\
&=\frac{1}{2\pi}\left(\sum_{|n|\geq N+1}|\hat{F}(n)|^{2}\right)^{1/2}
\rightarrow 0
\end{align*}
as $N\rightarrow\infty$.

[At the level that this course is given we could have avoided
the last two steps by assuming that $f$ and thus $F$ is 
sufficiently well behaved that
$\frac{1}{2\pi}\sum_{n=-N}^{N}\hat{F}(n)\exp(i\lambda n)
\rightarrow F(\lambda)$ uniformly, but the proof given here
applies more generally.]
We restate Theorem~\ref{before Shannon} in a very slightly
generalised form.
\begin{theorem}{\bf (Shannon's Theorem)}~\label{Shannon}
Suppose $f:{\mathbb R}\rightarrow{\mathbb C}$
is a continuous function with
$\int_{-\infty}^{\infty}|f(t)|\,dt<\infty$
and that $K>0$.
If $\hat{f}(\lambda)=0$
for $|\lambda|\geq K$
then then $f$ is determined by its values at points of the form
$n\pi K^{-1}$ with $n\in{\mathbb Z}$.
\end{theorem}
We call $\pi K^{-1}$ the `Nyquist rate'. Since
electronic equipment can only generate, transmit
and receive in a certain band of frequencies
and sampling more frequently than the Nyquist
rate produces, in principle, no further information
it is reasonable to suppose that the rate of transmission
of information is is proportional to the Nyquist
rate. We thus have
\[\frac{\text{\rm rate of transmission of information}}
{\text{\rm band width of signal}}\leq\text{\rm constant}\]
where the constant can be improved a little by elegant
engineering but must remain of the same order of magnitude.
Fibre optics gives a much broader bandwidth and therefore
allows a much faster rate of information transmission than earlier
systems.

We saw earlier that radio contact with a submerged submarine
requires the use of very low frequencies and this means that
the rate of transmission of information is very low indeed
(reportedly more that a minute to transmit a couple
of letters of Morse code). 

The human ear is only sensitive to limited band of frequencies.
Thus provided the sampling rate is high enough and the sampling done
to sufficient precision sound can be recorded in digital form.
This is the principle of the compact disc. (It is a comment
on the ingenuity of engineers that the sampling for a CD
is done fairly close to the appropriate Nyquist rate.)

\section{Distributions on ${\mathbb T}$} 
For the moment we shall work
on  the circle ${\mathbb T}$.  In Section~\ref{Green}
we introduced the notion of the delta function as
follows. Let $h_{n}:{\mathbb T}\rightarrow{\mathbb R}$
be a sequence of continuous functions such that

(i) $h_{n}(t)\geq 0$ for all $t\in{\mathbb T}$,

(ii) ${\displaystyle \int_{\mathbb T}h_{n}(t)\,dt=1}$,

(iii) $h_{n}(t)\rightarrow 0$ uniformly for all $\eta\leq |t|\leq \pi$
whenever $\eta>0$.

\noindent Then we take $\int_{\mathbb T} f(t)\delta (t)\,dt$
to be the limit of $\int_{\mathbb T} f(t)h_{n}(t)\,dt$.
The strengths of this approach are illustrated in the first
part of the next exercise (done as Exercise~\ref{Exercise, good kernel})
and the weaknesses in the second part.
\begin{exercise}\label{example delta}
(i) If $h_{n}$ has the properties
stated in the previous paragraph and
$f:{\mathbb T}\rightarrow{\mathbb C}$ is continuous, then
\[\int_{\mathbb T} f(t)h_{n}(t)\,dt\rightarrow f(0)\]
as $n\rightarrow\infty$.

(ii) Consider the functions
$k_{n},\,l_{n}:{\mathbb T}\rightarrow{\mathbb R}$
given by
\begin{equation*}
k_{n}(t)=
\begin{cases}
4(1-nt)/3&\text{if $0\leq t\leq n^{-1}$,}\\
4(1-2nt)/3&\text{if $-2n^{-1}\leq t<0$,}\\
0&\text{otherwise,}
\end{cases}
\end{equation*}
and $l_{n}(t)=k_{n}(-t)$.
Show that $k_{n}$ and $l_{n}$
satisfy the condition placed on $h_{n}$
in the previous paragraph. Show, however,
that if we define $f:{\mathbb T}\rightarrow{\mathbb R}$
by
\begin{equation*}
f(t)=
\begin{cases}
\pi-t&\text{if $0<t\leq \pi$,}\\
-\pi-t&\text{if $-\pi<t<0$,}\\
0&\text{if $t=0$,}
\end{cases}
\end{equation*}
then
\[\int_{\mathbb T} f(t)k_{n}(t)\,dt\rightarrow
-\frac{\pi}{3}\ \text{and}
\ \int_{\mathbb T} f(t)l_{n}(t)\,dt\rightarrow
\frac{\pi}{3}\]
as $n\rightarrow \infty$.
\end{exercise}
Our approach and the more general approach via measure
theory also fails to assign any meaning to the
`derivative $\delta'$ of the delta function' a concept
used with considerable success by the physicist Dirac.

Exercise~\ref{example delta} seems to show that the
delta function `can be integrated against well behaved
functions but not against less well behaved functions'.
Laurent Schwarz had the happy idea of only pairing
very well behaved objects with objects which
(at least from the view point of classical analysis)
might be rather badly behaved.

We first need a class of well behaved objects.
\begin{definition} We let ${\mathcal D}$
(read `curly D') be the set of infinitely differentiable
functions $f:{\mathbb T}\rightarrow{\mathbb C}$.
\end{definition}
In order to do analysis we need a notion of convergence.
\begin{definition} If $f$ and $f_{n}$ lie in
${\mathcal D}$, we say that $f_{n}\arrowD f$
if, for each fixed $r\geq 0$, we have $f_{n}^{(r)}\rightarrow f^{(r)}$
uniformly on ${\mathbb T}$.
\end{definition}
I suggest that the reader does to following short exercise
to check that she understands the quantifiers in the
definition just given.
\begin{exercise} Let $f_{n}(x)=2^{-n}\sin nx$. Show
that $f_{n}\arrowD 0$ but that $f_{n}^{(r)}(x)$ does not
converge uniformly to $0$ for $x\in {\mathbb T}$
and $r\geq 0$ as $n\rightarrow\infty$.
\end{exercise}

We now have our collection of good objects ${\mathcal D}$
together with a notion of convergence and must use them to
define our `less classically good' objects.
\begin{definition} We write ${\mathcal D}'$
for the set of linear maps $T:{\mathcal D}\rightarrow{\mathbb C}$
which are continuous in the sense that
if $f$ and $f_{n}$ lie in
${\mathcal D}$ and $f_{n}\arrowD f$,
then $Tf_{n}\rightarrow Tf$.
\end{definition}
We call the set ${\mathcal D}'$ the space of distributions
and ${\mathcal D}$ the space of test functions. We
often write
\[Tf=\langle T,f\rangle.\]
To find out what a distribution $T$ does we take
a test function $f$ and look at the value of
$Tf=\langle T,f\rangle$.
\begin{exercise} Show that, if we set
\[\langle \delta,f\rangle=f(0)\]
for all $f\in{\mathcal D}$ then $\delta\in{\mathcal D}'$.
\end{exercise}
The following is a simple but important observation.
\begin{lemma}\label{smooth distributions}
If $\phi:{\mathbb T}\rightarrow {\mathbb C}$
and we set
\[\langle T_{\phi},f\rangle=
\frac{1}{2\pi}\int_{\mathbb T}\phi(t) f(t)\,dt \]
then $T_{\phi}\in{\mathcal D}'$.
\end{lemma}
We shall write $\langle \phi,f\rangle=\langle T_{\phi},f\rangle$.

Whenever we define a new object $T$ which we hope
will be a distribution we must check that:-

(A) $\langle T,\lambda f+\mu g\rangle=
\lambda \langle T,f\rangle
+\mu \langle T,g\rangle$.

(B) $f_{n}\arrowD f$ implies $f_{n}\arrowD f$.

(C) Our definition is consistent when we use ordinary
functions $\phi$ as distributions.

\noindent Conditions~(A) and~(B) are simply the definition.
of a distribution. The meaning of condition~(C) becomes
clearer if we look at the following example.
\begin{lemma}\label{sum distributions}
Let $T$ and $S$ be distributions,
$\lambda,\,\mu\in{\mathbb C}$ and
let $\tau\in{\mathcal D}$. Then we may define distributions
$\lambda T+\mu S$ and $FT$ by

(i) $\langle\lambda T+\mu S,f\rangle=\lambda\langle T,f\rangle
+\mu \langle S,f\rangle$.

(ii) $\langle FT,f\rangle=\langle T,Ff\rangle$.
\end{lemma}
In order to check condition~(C) we must prove the following lemma.
\begin{lemma} We use the definitions and notations of
Lemmas~\ref{smooth distributions} and~\ref{sum distributions}.

(i) If $\phi,\,\psi\in{\mathcal D}$ and
$\lambda,\,\mu\in{\mathbb C}$ then
\[T_{\lambda\phi+\mu\psi}=\lambda T_{\phi}+\mu T_{\psi}\].

(ii) If $F,\, \phi\in{\mathcal D}$ then
\[T_{F\phi}=FT_{\phi}.\]
\end{lemma}

An even clearer example of the use of condition~(C) occurs
when we seek to define the derivative of a distribution.
Observe that if $\phi,\,f\in{\mathcal D}$ then
\begin{align*}
\langle \phi',f\rangle&
=\frac{1}{2\pi}\int_{-\pi}^{\pi}\phi'(t)f(t)\,dt\\
&=\frac{1}{2\pi}\left[\phi(t)f(t)\right]_{-\pi}^{\pi}
-\frac{1}{2\pi}\int_{-\pi}^{\pi}\phi(t)f'(t)\,dt\\
&=-\frac{1}{2\pi}\int_{-\pi}^{\pi}\phi(t)f'(t)\,dt\\
&=-\langle \phi,f'\rangle.
\end{align*}
This fixes the form of our definition.
\begin{definition} If $T\in{\mathcal D}'$ then
\[\langle T',f\rangle=-\langle T,f'\rangle\]
for all $f\in{\mathcal D}$.
\end{definition}
\begin{lemma} (i)
If $T\in{\mathcal D}'$ then $T'\in{\mathcal D}'$.

(ii) If $T,\,S\in{\mathcal D}'$ and $\lambda,\,\mu\in{\mathbb C}$
then $(\lambda T+\mu S)'=\lambda T'+\mu S'$.
\end{lemma}
\begin{exercise}
If
\begin{equation*}
f(t)=
\begin{cases}
\pi-t&\text{if $0<t\leq \pi$,}\\
-\pi-t&\text{if $-\pi<t<0$,}\\
0&\text{if $t=0$,}
\end{cases}
\end{equation*}
then
\[T_{f}'=-T_{1}+2\pi \delta\]
or, more concisely
\[f'=-1+2\pi \delta.\]
\end{exercise}

We have defined convergence on ${\mathcal D}$ but not on
${\mathcal D}'$. The next definition remedies this deficiency.
\begin{definition} Let $T_{n},\,T\in{\mathcal D}'$
we say that $T_{n}\arrowd T$ if
\[\langle T_{n},f\rangle\rightarrow \langle T,f\rangle\]
for all $f\in{\mathcal D}$.
\end{definition}
\begin{exercise} If $g,\,g_{n}\in C({\mathbb T})$ and
$g_{n}\rightarrow g$ uniformly, show that
$g_{n}\arrowd g$.
\end{exercise}
\begin{exercise} (i) If $T_{n},\,T\in{\mathcal D}'$
and $T_{n}\arrowd T$ show that $T_{n}'\arrowd T'$.

(ii) Define $g_{n}:{\mathbb T}\rightarrow{\mathbb R}$
by $g_{n}(x)=A_{n}(x^{2}-n^{-2})$ for $|x|\leq n^{-1}$
and $g_{n}(x)=0$ otherwise where $A_{n}$ is chosen so that
$\int_{\mathbb T}g_{n}(t)\,dt=1$. Explain why
$g_{n}\arrowd \delta$.

(iii) By (i) we have $g_{n}'\arrowd \delta'$.
Sketch the function $g_{n}'$ for various values of $n$.
Physicists like this way of visualising $\delta'$.
However the space ${\mathcal D}'$ contains much
odder objects such as the `distributional derivative'
of classically nowhere differentiable functions.
\end{exercise}

We have seen that we can do many things with distributions.
However we can not do everything that we wish. For example
we can not (at least in the standard theory of distributions
developed here) always multiply distributions. To see why
this should be so, let us ask what meaning we should
assign to $\delta^{2}$ the square of the delta function.
If $h_{n}$ is the function discussed in the first paragraph
of this section, then $h_{n}\arrowd \delta$ so, presumably,
we would want $h_{n}^{2}\arrowd \delta^{2}$. But, if
$f:{\mathbb T}\rightarrow{\mathbb R}$ is continuous
with $f(0)>0$, then
it is easy to see (Exercise~\ref{explosion}) that
\be
\int_{\mathbb T}h_{n}^{2}(x)f(x)\,dx\rightarrow\infty\quad\quad \text{as }n\to\infty.
\ee

\section{Distributions and Fourier series}
[This was the last lecture and some proofs were merely sketched.
None of the proofs are particularly hard but some require 
accurate argument.]

We require Theorem~\ref{Very absolute} which in turn requires Theorem~\ref{Can differentiate} which we just quote. 

\begin{theorem}\label{Can differentiate}
Let $g_{j}:{\mathbb T}\rightarrow{\mathbb C}$ be differentiable
with continuous derivative. If $\sum_{j=1}^{n}g_{j}(x)$
converges for each $x$ and
$\sum_{j=1}^{n}g_{j}'$ converges uniformly as
$n\rightarrow\infty$, then $\sum_{j=1}^{\infty}g_{j}$ is
differentiable and
\[\frac{d\ }{dx}\left(\sum_{j=1}^{\infty}g_{j}(x)\right)
=\sum_{j=1}^{\infty}g_{j}'(x).\]
\end{theorem}
\begin{theorem}\label{Very absolute}
(i) If $f\in{\mathcal D}$ then $n^{k}\hat{f}(n)\rightarrow 0$
as $|n|\rightarrow\infty$ for every integer $k\geq 0$.

(ii) If $a_{n}\in{\mathbb C}$ and $n^{k}a_{n}\rightarrow 0$ as $|n|\rightarrow\infty$ for every integer $k\geq 0$ then there exists a unique $f\in{\mathcal D}$ such that $\hat{f}(n)=a_{n}$.
\end{theorem}

Theorems~\ref{T, absolutely convergent},
\ref{Can differentiate} and~\ref{Very absolute}
give us the following useful result.
(As usual we write $S_{n}(f,t)=\sum_{j=-n}\hat{f}(n)\exp (ijt)$.)
\begin{lemma}\label{Fourier sum distribution} 
If $f\in{\mathcal D}$ then
\[S_{n}(f)\arrowD f\]
as $n\rightarrow\infty$.
\end{lemma}
(This result appears to put our previous results on
convergence in the shade but it must be remembered
that the space ${\mathcal D}$ is a very small subset
of the classes of function that we considered earlier.)

How should we define the Fourier coefficients of a distribution.
We observe that, if $\phi\in C({\mathbb T})$, then
\[\hat{\phi}(n)=\frac{1}{2\pi}\int_{\mathbb T}\phi(t)e_{-n}(t)\,dt
=(2\pi)^{-1}\langle \phi,e_{-n}\rangle\]
where $e_{m}(t)=\exp(imt)$. Our principle~(C)
thus leads us to the following definition.
\begin{definition} If $T\in{\mathcal D}'$ then
\[\hat{T}(n)=\langle T,e_{-n}\rangle.\]
\end{definition}
Lemma~\ref{Fourier sum distribution} now tells us that
\[\sum_{-n}^{n}\hat{T}(-j)\hat{f}(j)
=\langle T,S_{n}f\rangle\rightarrow \langle T,f\rangle\]
as $n\rightarrow\infty$ whenever $T\in{\mathcal D}'$
and $f\in{\mathcal D}$. A little reflection gives
the following theorem.
\begin{theorem}\label{Distribution is series}
(i) If $T\in{\mathcal D}$ then there exists an integer $K\geq 0$
such that
$n^{-K}\hat{T}(n)\rightarrow 0$
as $|n|\rightarrow\infty$.

(ii) If $b_{n}\in{\mathbb C}$ and there exists an integer $K\geq 0$
such that $n^{-K}b_{n}\rightarrow 0$
as $|n|\rightarrow\infty$ for every integer $k\geq 0$
then there exists a unique $T\in{\mathcal D}'$ such that
$\hat{T}(n)=b_{n}$.

(iii) If $T\in{\mathcal D}'$
and $f\in{\mathcal D}$ then
\[\langle T,f\rangle=\sum_{j=-\infty}^{\infty}\hat{T}(-j)\hat{f}(j).\]
The sum on the right is absolutely convergent.
\end{theorem}
\begin{exercise} If $T\in{\mathcal D}'$ show that
\[\sum_{j=-n}^{n}\hat{T}(n)e_{n}\arrowD T.\]
\end{exercise}

What about convolution? We have not defined convolution
on ${\mathbb T}$ even for ordinary functions but it
is clear (apart from a choice of constants) what the
definition should be in this case. 
\begin{definition} If $\phi,\,\psi\in C({\mathbb T})$,
we set
\[\phi*\psi(t)=\frac{1}{2\pi}\int_{\mathbb T}\phi(t-s)\psi(s)\,ds.\]
\end{definition}

Thus, if
$\phi,\,\psi\in C({\mathbb T})$ and $f\in{\mathcal D}$, then
\begin{align*}
\langle \phi*\psi,f\rangle
&=\frac{1}{2\pi}\int_{\mathbb T}
\left(\frac{1}{2\pi}\int_{\mathbb T}\phi(t-s)\psi(s)\,ds\right)
f(t)\,dt\\
&=\frac{1}{(2\pi)^{2}}\int_{\mathbb T}\int_{\mathbb T}
\phi(t-s)\psi(s)f(t)\,ds\,dt\\
&=\frac{1}{(2\pi)^{2}}\int_{\mathbb T}\int_{\mathbb T}
\phi(t-s)\psi(s)f(t)\,dt\,ds\\
&=\frac{1}{(2\pi)^{2}}\int_{\mathbb T}\int_{\mathbb T}
\phi(u)\psi(s)f(u+s)\,du\,ds\\
&=\langle \phi(u),\langle\psi(s),f(u+s)\rangle\rangle.
\end{align*}
Principle~(C) thus suggests the following definition.
\begin{definition} If $T,\,S\in{\mathcal D}'$
we define $T*S\in{\mathcal D}'$ by
\[ \langle T*S,f\rangle=\langle T(u),\langle S(s),f(u+s)\rangle\rangle.\]
\end{definition}
Here our notation is slightly informal with $s$ and $u$
acting as \emph{dummy variables}. It requires a fair amount
of work to show that this definition actually makes sense
but the reader may either take this on trust or do
Exercise~\ref{work convolution}. 

We have the following satisfying result.
\begin{lemma} If $T,\,S\in{\mathcal D}'$ then
\[\widehat{T*S}(n)=\hat{T}(n)\hat{S}(n).\]
\end{lemma}

Theorem~\ref{Distribution is series} is very satisfactory
but raises the possibility that a distribution might
be `merely a formal trigonometric series'. The reason
that this is not the case is that, although it makes
no sense to talk about the value of a distribution at a 
point, distributions actually have locality.
\begin{exercise}\label{live}
Show that if $f\in{\mathcal D}$ and there
exists an $\eta>0$ such that $f(t)=0$ for $|t|\leq \eta$
then
\[\langle \delta',f\rangle=0\]
\end{exercise}
Thus it makes sense to think of $\delta'$ as `living at $0$'.
The next exercise shows the need for caution when using this idea.
\begin{exercise} Let $f(x)=\sin x$. Observe that that $f\in{\mathcal D}$
and $f(0)=0$ but
\[\langle \delta',f\rangle=-1.\]
Why does this not contradict Exercise~\ref{live}?
\end{exercise}

We need a little topology (see Exercise~\ref{support}) to exploit the idea
of locality to the full but the following lemma and its proof 
give the main idea.
\begin{lemma}\label{start support} Suppose $T\in{\mathcal D}'$
is such that $\langle T,f_{j}\rangle =0$ whenever $f_{j}\in\mathcal D$
and $f_{j}(x)=0$ for $x\notin[a_{j}-\eta,b_{j}+\eta]$
for $j=1,\,2$ and some $\eta>0$.
Then $\langle T,f\rangle =0$ whenever $f\in\mathcal D$
and $f(x)=0$ for $x\notin[a_{1},b_{1}]\cup[a_{2},b_{2}].$ 
\end{lemma}
\begin{proof} Choose $E_{j}\in{\mathcal D}$ such that
$1\geq E_{j}(x)\geq 0$ for all $x$,
$E_{j}(x)=1$ for all $x\in [a_{j},b_{j}]$,
$E_{j}(x)>0$ for all $x\in (a_{j}-\eta,b_{j}+\eta)$
and $E_{j}(x)=0$ for all $x\notin[a_{j}-\eta,b_{j}+\eta]$ $[j=1,\,2]$.
Choose $E_{3}\in{\mathcal D}$ such that
$1\geq E_{3}(x)\geq 0$ for all $x$,
$E_{3}(x)=1$ for all 
$x\notin (a_{1}-\eta,b_{1}+\eta)\cup(a_{2}-\eta,b_{2}+\eta)$,
$E_{j}(x)>0$ for all $x\notin[a_{1},b_{1}]\cup[a_{2},b_{2}]$
and $E_{j}(x)=0$ for all $x\in[a_{1},b_{1}]\cup[a_{2},b_{2}]$.
(See  Exercises~\ref{start bump} and~\ref{end bump}.)

We observe that $E_{1}(x)+E_{2}(x)+E_{3}(x)>0$ for all $x$
and so we may set
\[G_{k}(x)=\frac{E_{k}(x)}{E_{1}(x)+E_{2}(x)+E_{3}(x)}\]
obtaining  $G_{k}\in{\mathcal D}$ for $1\leq k\leq 3$. Note
that
\[G_{1}+G_{2}+G_{3}=1\]
and so
\[\langle T,f\rangle= \langle T,(G_{1}+G_{2}+G_{3})f\rangle
=\sum_{k=1}^{3}\langle T,G_{k}\rangle.\]
If $f(x)=0$ for $x\notin[a_{1},b_{1}]\cup[a_{2},b_{2}]$,
then $G_{3}f=0$ so $\langle T,G_{3}\rangle=0$. Also
$f_{j}=G_{j}f$ vanishes outside $x\notin[a_{j}-\eta,b_{j}+\eta]$
so $\langle T,G_{j}f\rangle=\langle T,f_{j}\rangle=0$
for $j=1$ and for $j=2$. Thus $\langle T,f\rangle=0$
as stated.
\end{proof}
\section{Distributions on ${\mathbb R}$} So far as physicists
and engineers are concerned ${\mathbb T}$ is a `toy space'.
What happens if we try to extend the theory of distributions
to ${\mathbb R}$? The short answer is that the theory does extend
but the fact that ${\mathbb R}$
is \emph{unbounded} (or to speak more correctly,
but more technically, \emph{non-compact}) means that
matters are less straightforward.

One way forward in inspired by Theorem~\ref{Distribution is series}.
\begin{definition} We let ${\mathcal S}$
(the `Schwartz space') be the set of infinitely differentiable
functions $f:{\mathbb R}\rightarrow{\mathbb C}$
such that
\[(1+x^{2})^{m}f^{r}(x)\rightarrow 0\]
as $|x|\rightarrow \infty$ for all positive integer $r$ and $m$.
(We say that $f$ and all its derivatives `decrease
faster than polynomial'.)

If $f$ and $f_{n}$ lie in
${\mathcal S}$ we say that $f_{n}\arrowS f$
if, for each fixed pair of positive integers $r$ and $m$, 
we have $(1+x^{2})^{m}(f_{n}^{(r)}(x)-f^{(r)}(x))\rightarrow 0$
uniformly on ${\mathbb R}$.
\end{definition}

It turns out that the Schwartz space is beautifully adapted
to the Fourier transform.
\begin{theorem}\label{good S} 
If $f\in{\mathcal S}$, let us write
\[{\mathcal F}f(\lambda)=\hat{f}(\lambda)
=\int_{-\infty}^{\infty}f(t)e^{-i\lambda t}\,dt.\]
Then ${\mathcal F}f$ is a well defined element of 
${\mathcal S}$.

The map ${\mathcal F}:{\mathcal S}\rightarrow{\mathcal S}$
is linear and
\[{\mathcal F}^{2}=2\pi J\]
where $Jf(x)=f(-x)$. Thus 
${\mathcal F}:{\mathcal S}\rightarrow{\mathcal S}$
is a bijection. Further ${\mathcal F}$ is continuous
in the sense that $f_{n}\arrowS f$ implies 
${\mathcal F}f_{n}\arrowS{\mathcal F}f$.
\end{theorem}

In is easy to define the appropriate space ${\mathcal S}'$
of distributions (called the Schwartz space of \emph{tempered distributions}).
\begin{definition}
We write ${\mathcal S}'$
for the set of linear maps $T:{\mathcal D}\rightarrow{\mathbb C}$
which are continuous in the sense that
if $f$ and $f_{n}$ lie in
${\mathcal S}$ and $f_{n}\arrowS f$,
then $Tf_{n}\rightarrow Tf$. We write
$Tf=\langle T,f\rangle$.

If $T_{n},\,T\in{\mathcal S}'$
we say that $T_{n}\arrows T$ if
\[\langle T_{n},f\rangle\rightarrow \langle T,f\rangle\]
for all $f\in{\mathcal S}$.
\end{definition} 

Much of of what we did for ${\mathcal D}$ such as the
definition of the derivative of a distribution transfers directly.
What about Fourier transforms?
Writing $e_{\lambda}(t)=\exp i\lambda t$ we observe that
$e_{\lambda}\notin{\mathcal S}$ so the expression
$ \langle T,e_{\lambda}\rangle$ makes no sense in this context.
However, we recall from Lemma~\ref{L neat} 
that, if $f,\,g:{\mathbb R}\rightarrow{\mathbb C}$
are well behaved, then
\[\int_{-\infty}^{\infty}\hat{g}(x)f(x)\,dx
=\int_{-\infty}^{\infty}g(\lambda)\hat{f}(\lambda)\,d\lambda,\]
and this hint gives us a way to define Fourier transforms
of tempered distributions.
\begin{definition} If $T\in{\mathcal S}'$ then we
define $\hat{T}\in{\mathcal S}'$ by the formula
\[\langle \hat{T},f\rangle=\langle T,\hat{f}\rangle.\]
\end{definition}
\begin{exercise} Check that the definition works correctly.
\end{exercise}
 
Theorem~\ref{good S} which tells us that the Fourier transform
works well on ${\mathcal S}$ now implies 
that the Fourier transform
works well on ${\mathcal S}'$.
\begin{theorem} 
If $T\in{\mathcal S}'$ let us write
${\mathcal F}T=\hat{T}$.
The map ${\mathcal F}:{\mathcal S}'\rightarrow{\mathcal S}'$
is linear and
\[{\mathcal F}^{2}=2\pi J\]
where $\langle JT,f\rangle=\langle T(x),f(-x)\rangle$. Thus 
${\mathcal F}:{\mathcal S}'\rightarrow{\mathcal S}'$
is a bijection. Further ${\mathcal F}$ is continuous
in the sense that $T_{n}\arrows T$ implies 
${\mathcal F}T_{n}\arrows {\mathcal F}T$.
\end{theorem}
The following result is in accordance with a formula 
often used in formal manipulation of Fourier transforms.
\begin{exercise} If we work in ${\mathcal S}'$ then
\[\hat{\delta}=1\ \text{and}
\ \hat{1}=2\pi\delta.\]
\end{exercise}

A major difference between ${\mathbb T}$ and the
unbounded (that is non-compact) space ${\mathbb R}$
is that it is no longer possible to convolve every
pair of distributions. Observe that no part of
the formula
\[\widehat{1*1}\overset{?}{=}\hat{1}\hat{1}\overset{?}{=}
(2\pi)^{2}\delta^{2}\]
makes any sense in our theory
since $\int_{-\infty}^{\infty}1\,dt$ diverges.
Convolution remains an important operation
but we have to impose conditions on the
objects convolved to make sure that we
can perform it.

From the point of view of applications the space
of tempered distributions is too small to deal with
the functions of exponential growth which occur in the
theory of differential equations.
\begin{exercise} If $\phi(t)=\exp(t)$ then setting
$f(t)=\exp(-(1+t^{2})^{1/2}$ we have $f\in{\mathcal F}$
but $\int_{-\infty}^{\infty}\phi(t)f(t)\,dt$ diverges.
\end{exercise}  
To get round this problem, Schwartz used a smaller
space of test functions ${\mathcal D}({\mathbb R})$, 
obtaining a larger space of distributions
${\mathcal D}'({\mathbb R})$ .
But that is another story recounted, for example,
in~\cite{Friedlander}. 

\section{Further reading} On the whole, interesting books on
Fourier analysis are at a higher level than this course.
The excellent book of Dym and McKean~\cite{Dym} is, perhaps the
most in the spirit of this course and I have also written
a book~\cite{Korner1} on Fourier analysis. There are two 
superb introductions to the study of Fourier Analysis
for its own sake by Helson~\cite{Helson} and
by Katznelson~\cite{Katznelson}.
\begin{thebibliography}{9}
\bibitem{Dym} H.~Dym and H.~P.~McKean
\emph{Fourier Series and Integrals}
Academic Press, 1972.
\bibitem{Friedlander} F.~G.~Friedlander
\emph{Introduction to the Theory of Distributions}
CUP, 1982. [There is a second edition 
also published by CUP in 1998 with
an additional chapter by M.~Joshi.]
\bibitem{Helson} H.~Helson
\emph{Harmonic Analysis}
Adison--Wesley, 1983.
\bibitem{Katznelson} Y.~Katznelson
\emph{An Introduction to Harmonic Analysis}
Wiley, 1963. [There is a Dover reprint, CUP hope to
bring out a second edition.]
\bibitem{Korner1} T.~W.~K\"{o}rner
\emph{Fourier Analysis}
CUP, 1988.
\end{thebibliography}
\section{Exercises} Here are some exercises. They are at various
levels and you are not expected to do all of them. Just do the
ones that interest you.
\begin{question} (i) Let $f:{\mathbb R}\rightarrow{\mathbb R}$
be $n+1$ times differentiable. Show that there is a unique
polynomial (to be exhibited) $P$ of degree $n$ such that
$P^{(r)}(0)=f^{(r)}(0)$, for all $0\leq r\leq n$.

(ii) Let $t>0$. Set
\[E(t)=f(t)-P(t)\]
(so $E(t)$ is the `error at point $t$) and write
\[g(x)=f(x)-P(x)-E(t)\left(\frac{x}{t}\right)^{n+1}.\]
By repeated use of Rolle's theorem show that
there exists a $c_{r}\in(0,t)$ such that
\[g^{(r)}(0)=g^{(r)}(c_{r})\]
for $1\leq r\leq n$. Deduce that there exists a $c\in(0,t)$
such that we have the following `Taylor theorem'
\[f(t)=P(t)+\frac{f^{(n+1)}(c)t^{n+1}}{(n+1)!}.\]
\end{question}

\begin{question}
Suppose that $f:{\mathbb R}\rightarrow{\mathbb R}$
is $2n+2$ times differentiable. By considering polynomials
of the form $x^{k}(1-x)^{l}$, or otherwise, show that
there is a unique polynomial $P$ of degree $2n+1$
such that
\[P^{(r)}(0)=f^{(r)}(0)\ \text{and}\ P^{(r)}(1)=f^{(r)}(1)
\ \text{for all $0\leq r\leq n$}.\]
Show that the error $E(y)=f(y)-P(y)$ at $y\in[0,1]$
is given by
\[E(y)=\frac{f^{(2n+2)}(c)}{(2n+2)!}y^{n+1}(y-1)^{n+1},\]
for some $c\in(0,1)$.
\end{question}

\begin{question} By taking imaginary parts in the de Moivre formula,
or otherwise,
show that there is a polynomial
$U_{n}$ of degree $n$ such that 
$U_{n}(\cos\theta)\sin\theta=\sin(n+1)\theta$.

Show that $T_{n}'(x)=nU_{n-1}(x)$ for $n\geq 1$.
\end{question}
\begin{question} By looking at the real part of 
$\sum_{n=0}^{\infty}t^{n}e^{in\theta}$,
or otherwise, show that
\[\frac{1-tx}{1-2tx+t^{2}}=\sum_{n=0}^{\infty}T_{n}(x)t^{n}\]
for all $|t|<1$ and $|x|\leq 1$.
\end{question}

\begin{question} If $f:{\mathbb T}\rightarrow{\mathbb C}$
is continuous, we write
\[P_{r}(f,\theta)=\sum_{n=-\infty}^{\infty}r^{|n|}\hat{f}(n)\exp in\theta\]
for all $\theta\in{\mathbb T}$ and all real $r$ with $0<r<1$.
By modifying the proof of the Fej\'{e}r theorem,
show that $P_{r}(f,\theta)\rightarrow f(\theta)$ as
$r\rightarrow 1$ from below.
\end{question}

\begin{question} The ideas behind the vibrating string
may be generalised. For example the equation of a two
dimensional vibrating drum is
\[\nabla^{2}\phi=K\frac{\partial^{2}\phi}{\partial t^{2}}\]
where, by definition,
\[\nabla^{2}\phi=\frac{\partial^{2}\phi}{\partial x^{2}}
+\frac{\partial^{2}\phi}{\partial y^{2}}.\]
Suppose we have a circular drum of radius $a$.
It is natural to seek a solution of the form
\[\phi=f(r)(A\cos\omega t+B\sin\omega t)\]
where $r$ is the distance from the centre and $f(a)=0$.

By using the chain rule, show that
\[\nabla^{2}f(r)=f''(r)+\frac{1}{r}f'(r)\]
and deduce that
\[f''(r)+\frac{1}{r}f'(r)+\omega^{2}f(r)=0,\ f(a)=0.\]
\end{question}

\begin{question} Improve on Lemma~\ref{L Dirichlet large}
by showing that
\[\frac{1}{\log N}\left(\frac{1}{2\pi}
\int_{\mathbb T}|D_{N}(x)|\,dx\right)\]
tends to a limit and find that limit. (This requires some thought.)
\end{question}
\begin{question} Let $a_{1}$, $a_{2}$, \dots be a sequence
of complex numbers.

(i) Show that, if $a_{n}\rightarrow a$ then
\[\frac{a_{1}+a_{2}+\dots+a_{n}}{n}\rightarrow a\]
as $n\rightarrow\infty$.

(ii) By taking an appropriate sequence of $0$s and $1$s,
or otherwise, find a sequence $a_{n}$ such that
$a_{n}$ does not tend to a limit as $n\rightarrow\infty$
but $(a_{1}+a_{2}+\dots+a_{n})/n$ does.

(iii) By taking an appropriate sequence of $0$s and $1$s,
or otherwise, find a bounded sequence $a_{n}$ such that
$(a_{1}+a_{2}+\dots+a_{n})/n$ does not tend to a limit
as $n\rightarrow\infty$.
\end{question}
\begin{question} (i) Show that if $f:{\mathbb T}\rightarrow{\mathbb R}$ 
is continuous and $f(t)=f(-t)$ for all $t$,
then, given any $\epsilon>0$ we can find a 
trigonometric polynomial 
\[P(t)=\sum_{n=-N}^{N}a_{n}\cos nt\]
with $a_{n}$ real such that $\|P-f\|_{\infty}<\epsilon$.

(ii) By using Tchebychev polynomials 
(see Lemma~\ref{L, Tchebychev}),
prove the theorem of Weierstrass which states that if
$F:[0,1]\rightarrow{\mathbb R}$ is continuous 
then, given any $\epsilon>0$ we can find a 
polynomial 
\[Q(t)=\sum_{n=0}^{N}b_{n}t^{n}\]
with $b_{n}$ real such that $\|Q-F\|_{\infty}<\epsilon$.

(iii) Suppose that $g:[0,1]\rightarrow{\mathbb R}$ is continuous
and $\int_{0}^{1}g(t)t^{n}\,dt=0$ for all $n\geq 0$.
Show that $g=0$.
\end{question}
\begin{question} Consider the heat equation on the circle.
In other words consider well behaved functions
$\theta:{\mathbb T}\times[0,\infty)\rightarrow{\mathbb C}$
satisfying the partial differential equation
\[
\frac{\partial\theta}{\partial t}
=K\frac{\partial^{2} \theta}{\partial x^{2}}.
\]
for all $t>0$ and all $x\in{\mathbb T}$.
Try to find solutions using separation of variables and
then use the same kind of arguments as we used for the
vibrating string to suggest that the general solution
is
\[\theta(x,t)=\sum_{n=-\infty}^{\infty}a_{n}e^{inx}e^{-Kn^{2}t}.\]
What happens as $t\rightarrow \infty$?
\end{question}

\begin{question}\label{Exercise, good kernel}
Suppose $L_{n};{\mathbb T}\rightarrow{\mathbb R}$
is continuous and

(A) $\frac{1}{2\pi}\int_{\mathbb T}L_{n}(t)\, dt=1$,

(B) If $\eta>0$ then $L_{n}\rightarrow 0$ uniformly
for $|t|\geq \eta$
as $n\rightarrow\infty$,

(C) $L_{n}(t)\geq 0$ for all $t$.

(i) Show that if $f:{\mathbb T}\rightarrow{\mathbb T}$
is continuous, then
\[\frac{1}{2\pi}\int_{\mathbb T}L_{n}(t)f(x-t)\, dt\rightarrow f(x)\]
uniformly as $n\rightarrow\infty$.

(ii) Show that condition (C) can be replaced by

(C') There exists a constant $A>0$ such that
\[\int_{\mathbb T}|L_{n}(t)|\, dt\leq A.\]
\end{question}
\begin{question}
(In this question we write $S_{n}(f,t)=\sum_{r=-n}^{n}\hat{f}(n)\exp int$.)

(i) Use Theorem~\ref{T, mean square} to show that, if
$f:{\mathbb T}\rightarrow{\mathbb C}$
is continuous, then $\hat{f}(n)\rightarrow 0$
as $|n|\rightarrow\infty$ (this is the very simplest
version of the `Riemann-Lebesgue lemma').

(ii)  Suppose that $f_{1},\, g_{1}:{\mathbb T}\rightarrow{\mathbb C}$
are continuous
and $f_{1}(t)=g_{1}(t)\sin t$ for all $t$.
Show that
$\hat{f}_{1}(j)=(\hat{g}_{1}(j-1)-\hat{g}_{1}(j+1)/2$
and deduce that $S_{n}(f_{1},0)\rightarrow 0$
as $n\rightarrow\infty$.

(iii) Suppose that $f_{2}:{\mathbb T}\rightarrow{\mathbb C}$
is continuous,
$f_{2}(n\pi)=0$ and $f_{2}$ is differentiable at $0$ and $\pi$.
Show that there exists a
continuous $g_{2}:{\mathbb T}\rightarrow{\mathbb C}$
such that $f_{2}(t)=g_{2}(t)\sin t$ for all $t$
and deduce that $S_{n}(f_{2},0)\rightarrow 0$
as $n\rightarrow\infty$.

(iv) Suppose that $f_{3}:{\mathbb T}\rightarrow{\mathbb C}$
is continuous,
$f_{3}(0)=0$ and $f_{3}$ is differentiable at $0$.
Write $f_{4}(t)=f_{3}(2t)$.
Compute $\hat{f}_{4}(j)$
in terms of the Fourier coefficients of $f_{3}$.
Show that $S_{n}(f_{4},0)\rightarrow 0$
and deduce that $S_{n}(f_{3},0)\rightarrow 0$
as $n\rightarrow\infty$.

(v) Suppose that $f:{\mathbb T}\rightarrow{\mathbb C}$
is continuous,
and $f$ is differentiable at some point $x$.
Show that $S_{n}(f,x)\rightarrow f(x)$
as $n\rightarrow\infty$.
\end{question}
\begin{question} (i) Use Lemma~\ref{L, start divergence}
to show that given any $\epsilon_{1}>0$ and any $K_{1}>0$ we can find
a continuous function
$f_{1}:{\mathbb T}\rightarrow{\mathbb R}$ with 
$\|f_{1}\|_{\infty}\leq \epsilon_{1}$ and and integer $M_{1}>0$ 
\[|S_{M_{1}}(f_{1},0)|\geq K_{1}.\]

(ii) Use part~(i) to show that,
given any $\epsilon_{2}>0$ and 
any $K_{2}>0$, we can find
a real trigonometric polynomial
$P_{2}:{\mathbb T}\rightarrow{\mathbb R}$ with 
$\|P_{2}\|_{\infty}\leq \epsilon_{2}$ and and integer $M_{2}>0$ 
\[|S_{M_{2}}(P_{2},0)|\geq K_{2}.\]

(iii) Use part~(ii) to show that, given any $\epsilon_{3}>0$, 
any $K_{3}>0$ and any integer $m_{3}>0$, we can find
a real trigonometric polynomial
$P_{3}:{\mathbb T}\rightarrow{\mathbb R}$ with 
$\|P_{3}\|_{\infty}\leq \epsilon_{3}$,
$\hat{P}_{3}(r)=0$ for $|r|\leq m_{3}$ and and integer $M_{3}>0$ 
\[|S_{M_{3}}(P_{3},0)|\geq K_{3}.\]

(iv) Show that we can find a sequence of real trigonometric
polynomials $P_{n}$ and integers $q(n)$, $M(n)$ and $m(n)$ such that

\ \ (a) $\|P_{n}\|_{\infty}\leq 2^{-n}$ for all $n$.

\ \ (b) $\hat{P}_{n}(r)=0$ if $|r|\leq m{n}$ or $|r|\geq q(n)$.

\ \ (c) $|S_{M(n)}(P_{n},0)|\geq 2^{n}$.

\ \ (d) $q(n-1)\leq m(n)\leq M_{n}<q(n)$

\noindent for all $n\geq 0$. (We take $q(0)=0$.)

(v) Show carefully that $\sum_{n=1}^{\infty}P_{n}$ is uniformly
convergent to some continuous function $f$ and that
$\hat{f}(r)=\hat{P}_{n}(r)$ if $m(n)\leq r\leq q(n)$.

(vi) Deduce that $|S_{M(n)}(f,0)-S_{m(n)}(f,0)|\geq 2^{n}$
for all $n\geq 1$ and that $S_{N}(f,0)$ can not converge as
$N\rightarrow\infty$.
\end{question}
\begin{question} (i) Show that, if $f:{\mathbb T}\rightarrow{\mathbb C}$,
then $\hat{f}(n)\rightarrow 0$ as $|n|\rightarrow\infty$. 

(i) If $\kappa(n)>0$ and
$\kappa(n)\rightarrow \infty$ as $n\rightarrow\infty$,
show that we can find $n(j)\rightarrow\infty$
such that $\sum_{j=1}^{\infty}2^{j}/\kappa(n(j))$ converges.
Deduce that
we can find a continuous function $f$
such that $\limsup_{n\rightarrow\infty}\kappa(n)\hat{f}(n)=\infty$.
\end{question}
\begin{question} Show that
\[\frac{\card\{1\leq n\leq N \mid \langle \log_{10}n\rangle
\in [0,1/2]\}}{N}\]
does not tend to limit as $N\rightarrow\infty$. Show
however that given any $\epsilon>0$ and any $x\in[0,1]$
we can find a positive integer $n$ such that
\[|\langle \log_{10}n\rangle-x|<\epsilon\]
as $N\rightarrow\infty$. 

Prove the same results with $\log_{10}$ replaced by $\log_{e}$.
\end{question}
\begin{question} Using the kind of ideas behind
the proof of of Weyl's theorem (Theorem~\ref{Weyl}),
or otherwise, prove the following results.

(i) If $f:{\mathbb T}\rightarrow{\mathbb C}$
is continuous, then $\hat{f}(n)\rightarrow 0$
as $|n|\rightarrow\infty$. (This is a version of
the Lebesgue--Riemann lemma.)

(ii) If $f:{\mathbb T}\rightarrow{\mathbb R}$
is continuous, then
\[\int_{0}^{2\pi}f(t)|\sin nt|\,dt
\rightarrow \frac{2}{\pi}\int_{0}^{2\pi}f(t)\,dt\]
as $n\rightarrow\infty$.
\end{question}
\begin{question} Let $R$ be a rectangle cut up into
smaller rectangles $R(1)$, $R(2)$, \dots, $R(k)$ 
each of which has sides parallel to the sides of $R$.
Then, if each $R(j)$ has at least one pair of sides
of integer length, it follows that $R$ has 
at least one pair of sides of integer length.

First try and prove this without using Fourier analysis.

Then try and prove the result using what is, in effect,
a Fourier transform
\[\iint_{R(j)}\exp\big(2\pi i(x+y)\big)\,dx\,dy.\]
\end{question} 
\begin{question}If $f,\ g:{\mathbb T}\rightarrow{\mathbb C}$
are well behaved, let us define their \emph{convolution}
$f*g:{\mathbb T}\rightarrow{\mathbb C}$ by
\[f*g(t)=\frac{1}{2\pi}\int_{\mathbb T}f(t-s)g(s)\,ds.\]
Show that 
$\widehat{f*g}(n)=\hat{f}(n)\hat{g}(n)$.

Show that if $P$ is a trigonometric polynomial
$P*f$ is a trigonometric polynomial. Identify
$D_{N}*f$ and $K_{N}*f$ where $D_{N}$ and $K_{N}$
are the Dirichlet and Fej\'{e}r kernels.

Suppose that $L_{N}(t)=A_{N}K_{N}^{2}(t)$ with
$A_{N}$ chosen so that $\frac{1}{2\pi}\int_{\mathbb T}L_{N}(t)\,dt=1$.
Show that, if $f$ is continuous, $L_{N}*f(t)\rightarrow f(t)$. 
\end{question} 
\begin{question}\label{E, Liouville}.
Let $E(x)=(2\pi)^{-1/2}\exp(-x^{2}/2)$.
Show, by changing to polar coordinates, that
\begin{align*}
\left(\int_{-\infty}^{\infty}E(x)\,dx\right)^{2}
&=\int_{-\infty}^{\infty}E(x)\,dx\int_{-\infty}^{\infty}E(y)\,dy\\
&=\frac{1}{2\pi}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
\exp(-(x^{2}+y^{2}))\,dx\,dy\\
&=\int_{0}^{\infty}r\exp(-r^{2}/2)\,dr=1.
\end{align*}
Kelvin once asked his class if they knew what a mathematician was.
He wrote the formula
\[\int_{\infty}^{\infty}e^{-x^{2}/2}\,dx=\sqrt{\pi}\]
and the board and said. 
`A mathematician is one to whom that 
is as obvious as that twice two makes four is to you. 
Liouville was a mathematician.'
\end{question}
\begin{question}\label{E, differentiable everywhere}
(i) Explain why
\[\sum_{j=-N}^{N}|a_{j}b_{j}|\leq 
\left(\sum_{j=-N}^{N}|a_{j}|^{2}\right)^{1/2} 
\left(\sum_{j=-N}^{N}|b_{j}|^{2}\right)^{1/2}\]
for all $a_{j},\,b_{j}\in{\mathbb C}$.

(ii) Use~(i) to show that, if
$\sum_{j=-\infty}^{\infty}|a_{j}|^{2}$ and
$\sum_{j=-\infty}^{\infty}|b_{j}|^{2}$ converges, then
$\sum_{j=-\infty}^{\infty}|a_{j}b_{j}|$ converges and
\[\sum_{j=-\infty}^{\infty}|a_{j}b_{j}|\leq 
\left(\sum_{j=-\infty}^{\infty}|a_{j}|^{2}\right)^{1/2}
\left(\sum_{j=-\infty}^{\infty}|b_{j}|^{2}\right)^{1/2}.\]

(iii) If $f:{\mathbb T}\rightarrow{\mathbb C}$ is continuously 
differentiable, explain why
\[\sum_{j=-\infty}^{\infty}j^{2}|\hat{f}(j)|^{2}
\leq \frac{1}{2\pi}\int_{\mathbb T}|f'(t)|^{2}\,dt.\]

(iv) Use~(ii)
to show that $\sum_{j=-\infty}^{\infty}|\hat{f}(j)|$ converges.
Deduce that $\sum_{j=-\infty}^{\infty}\hat{f}(j)\exp ijt$
converges uniformly to $f(t)$.
\end{question}
\begin{question} (i) If $u:{\mathbb T}\rightarrow{\mathbb R}$
is once continuously differentiable and
$\frac{1}{2\pi}\int_{\mathbb T}u(t)\,dt=0$, show that
\[\frac{1}{2\pi}\int_{\mathbb T}(u(t))^{2}\,dt
\leq \frac{1}{2\pi}\int_{\mathbb T}(u'(t))^{2}\,dt\]
with equality if and only if
$u(t)=C\cos(t+\phi)$ for some constants $C$ and $\phi$.

(ii) Use~(i) to show that, if $v:[0,\pi/2]\rightarrow{\mathbb R}$
is once continuously differentiable with $v(0)=0$ and
$v'(\pi/2)=0$ then
\[\int_{0}^{\pi/2}(v(t))^{2}\,dt
\leq  \int_{0}^{\pi/2}(v'(t))^{2}\,dt\]
with equality if and only if
$u(t)=C\sin t$ for some constant $C$.

(iii) By approximating $w$ by functions of the type considered
in~(iii) show that, if $w:[0,\pi/2]\rightarrow{\mathbb R}$
is once continuously differentiable with $w(0)=0$, then
\[\int_{0}^{\pi/2}(w(t))^{2}\,dt
\leq  \int_{0}^{\pi/2}(w'(t))^{2}\,dt.\]
(This is Wirtinger's inequality.)
\end{question}

\begin{question} (i) By applying Poisson's formula to the function
$f$ defined by $f(x)=\exp(-t|x|/2\pi)$ show that
\[2(1-e^{-t})^{-1}
=\sum_{n=-\infty}^{\infty}2t(t^{2}+4\pi^{2}n^{2})^{-1}.\]

(ii) By expanding $(t^{2}+4\pi n^{2})^{-1}$ and
interchanging sums (justifying this, if you can, just interchanging,
if not) deduce that
\[2(1-e^{-t})^{-1}=1+2t^{-1}+\sum_{m=0}^{\infty}c_{m}t^{m}\]
where $c_{2m}=0$ and
\[c_{2m+1}=a_{2m+1}\sum_{n=1}^{\infty}n^{-2m}\]
for some value of $a_{2m+1}$ to be given explicitly.

(iii) Hence obtain Euler's formula
\[\sum_{n=1}^{\infty}n^{-2m}=
(-1)^{m-1}2^{2m-1}b_{2m-1}\pi^{2m}/(2m-1)!\]
for $m\geq 1$, where the $b_{m}$ are defined by the formula
\[(e^{y}-1)^{-1}=y^{-1}-2^{-1}+\sum_{n=1}^{\infty}b_{n}y^{n}/n!\]
(The $b_{n}$ are called Bernoulli numbers.)
\end{question}
\begin{question}{\bf (The Gibbs Phenomenon.)}\label{E, Gibbs}
Ideally you should first look at what happens
when we try to reconstruct a reasonable
discontinuous function from its Fourier sums
and then use this question to explain what you see.
There are a number of questions linked to this one
but you need not do them to understand what is going on.

We have only discussed Fourier
series for continuous functions in this course.
It is possible to use what we already know
to discuss `well behaved' discontinuous
functions. Let $F:{\mathbb T}\rightarrow{\mathbb R}$
be defined by
\begin{equation*}
F(t)=
\begin{cases}
\pi-t&\text{for $0\leq t\leq\pi$}\\
0&\text{for $t=0$}\\
\-pi-t&\text{for $-\pi<t\leq 0$}
\end{cases}
\end{equation*}

(i) Suppose that  $f:{\mathbb T}\rightarrow{\mathbb R}$
is continuously differentiable
on ${\mathbb T}\setminus\{0\}$ and that both the
function $f$ and its derivative have left and right
limits at $0$. Show that we can find a $\lambda$
and  and a
continuous function $g:{\mathbb T}\rightarrow{\mathbb R}$
such that $g'$ exists and is continuous
except possibly at $0$ and that
$g$ has left and right derivatives at $0$ which
are the left and right limits of $g'$ at that point
and
\[f=g+\lambda F.\]
Since the Fourier sums of $g$ behave extremely well
(see Exercise~\ref{E, Kahane nice} or take my word for it)
it follows that any bad behaviour will be due to $F$
and study of the Fourier sums of $F$ will tell us all
we need to know about well behaved functions with a
single well behaved discontinuity at $0$. Why
will this also tell us all
we need to know about well behaved functions with a
single well behaved discontinuity? Can the same
idea be made to work for well behaved functions with a
finite number of well behaved discontinuities?

(ii) Show that the $n$th Fourier sum of $F$
\[S_{n}(F,t)=\sum_{r=-n}^{n}\hat{F}(r)\exp(irt)
=2\sum_{r=1}^{n}\frac{1}{r}\sin rt.\]

(iii) Explain why
\[S_{n}(F,\tau/n)
=2\frac{\tau}{n}\sum_{r=1}^{n}\frac{1}{\tfrac{r\tau}{n}}
\sin\tfrac{r\tau}{n}\rightarrow\int_{0}^{\tau}\frac{\sin x}{x}\,dx\]
as $n\rightarrow\infty$.

(iv) Sketch the behaviour of the function
\[G(\tau)=\int_{0}^{\tau}\frac{\sin x}{x}\,dx.\]
(The information in questions~\ref{Q, infinite Dirichlet 1}
and~\ref{Q, infinite Dirichlet 2} including the fact that
$\int_{0}^{\infty}\frac{\sin x}{x}\,dx=\pi/2$ is useful
but not essential.)

(v) Sketch the behaviour of $S_{n}(F,\tau/n)$ for small
$\tau$ and large $n$.

\noindent[General theorems show that $S_{n}(F,t)\rightarrow F(t)$
when $t$ is fixed but if the the reader is unwilling to
take my word for this they can do Question~\ref{E, Fourier}.]
\end{question}
\begin{question}\label{E, Kahane nice}
(This is just an extension of Question~\ref{E, differentiable everywhere}) 
(i) Suppose that  $f:[-\pi,\pi]\rightarrow{\mathbb C}$
is continuous. We define
\[\hat{f}(r)=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(t)\exp(irt)\,dt.\]
Show that
\[\sum_{n=-N}^{N}|\hat{f}(n)|^{2}\leq
\frac{1}{2\pi}\int_{-\pi}^{\pi}|f(t)|^{2}\,dt.\]

(ii) Suppose that  $f:[-\pi,\pi]\rightarrow{\mathbb C}$
has continuous first derivative (we use left and right
derivatives at end points) and that $f(\pi)=f(-\pi)$. Show that
\[\widehat{f'}(r)=ir\hat{f}(r).\]
Show that
\[\sum_{r=-n}^{n}|\hat{f}(r)|\leq |\hat{f}(0)|
+2\sum_{r=1}^{n}r^{-2}\frac{1}{2\pi}\int_{-\pi}^{\pi}|f'(t)|^{2}\,dt.\]
Conclude that $\sum_{r=-\infty}^{\infty}|\hat{f}(r)|$ converges.

(iii) Suppose that  $g:{\mathbb T}\rightarrow{\mathbb C}$
is continuous, that $g'$ exists and is continuous
except possibly at one point $\alpha$ and that
$g$ has left and right derivatives at $\alpha$ which
are the left and right limits of $g'$ at that point.
Show that $\sum_{r=-\infty}^{\infty}|\hat{g}(r)|$ converges
and deduce, using Theorem~\ref{T, absolutely convergent}, that
$\|S_{N}(g)-g\|_{\infty}\rightarrow 0$ as $N\rightarrow\infty$..
\end{question}
\begin{question}\label{Q, infinite Dirichlet 1}
(i) Explain why the function $f:[0,\infty)\rightarrow{\mathbb R}$
defined by $f(x)=(\sin x)/x$ for $x\neq 0$, $f(0)=1$ is continuous
at $0$. It is traditional to write $f(x)=(\sin x)/x$
and ignore the fact that, strictly speaking, $(\sin 0)/0$
is meaningless. Sketch $f$.

(ii) If ${\displaystyle I_{n}=\int_{0}^{n\pi}\frac{\sin x}{x}\,dx}$,
show, by using the alternating series test, that $I_{n}$
tends to a strictly positive limit $L$, say. Deduce carefully
that ${\displaystyle \int_{0}^{\infty}\frac{\sin x}{x}\,dx}$
exists with value $L$.

(iii) Let ${\displaystyle
I(t)=\int_{0}^{\infty}\frac{\sin tx}{x}\,dx}$ for all $t\in{\mathbb R}$.
Show using (i), or otherwise, that
$I(t)=L$ for all $t>0$, $I(0)=0$, $I(t)=-L$ for $t<0$.

(iv) Find a  continuous function $g:[0,\pi]\rightarrow{\mathbb R}$
such that $g(t)\geq 0$ for all $t\in[0,\pi]$, $g(\pi/2)>0$
and
\[\left|\frac{\sin x}{x}\right|\geq \frac{g(x-n\pi)}{n}\]
for all $n\pi\leq x\leq(n+1)\pi$ and all integer $n\geq 1$.
Hence, or otherwise, show that $\int_{0}^{\infty}|(\sin x)/x|\,dx$
fails to converge.
\end{question}
\begin{question}\label{Q, infinite Dirichlet 2}
Although the existence of the infinite integral
$\int_{0}^{\infty}\frac{\sin x}{x}\,dx$ is very important,
its actual value is less important. It is, however, reasonably
easy to find using our knowledge of the Dirichlet
kernel, in particular the fact that
\[2\pi=\int_{-\pi}^{\pi}\frac{\sin\big((n+\tfrac{1}{2})x\big)}
{\sin \tfrac{x}{2}}\,dx\]
(see Lemma~\ref{L, Dirichlet's kernel}~(iii)).

(i) If $\epsilon>0$, show that
\[\int_{-\epsilon}^{\epsilon}\frac{\sin \lambda x}{x}\,dx
\rightarrow
\int_{-\infty}^{\infty}\frac{\sin x}{x}\,dx,\]
as $\lambda\rightarrow\infty$.

(ii) If $\pi\geq \epsilon>0$, show, by using the
estimates from the alternating series test, or otherwise, that
\[\int_{-\epsilon}^{\epsilon}
\frac{\sin\big((n+\tfrac{1}{2})x\big)}
{\sin \tfrac{x}{2}}\,dx\rightarrow
\int_{-\pi}^{\pi}\frac{\sin\big((n+\tfrac{1}{2})x\big)}
{\sin \tfrac{x}{2}}\,dx=2\pi\]
as $n\rightarrow\infty$.

(iii) Show that
\[\left|\frac{2}{x}-\frac{1}{\sin \tfrac{1}{2}x}\right|\rightarrow 0\]
as $x\rightarrow 0$.
and deduce that
\[\int_{0}^{\infty}\frac{\sin x}{x}\,dx=\frac{\pi}{2}.\]
\end{question}
\begin{question}\label{E, Fourier} 
This question refers back to Question~\ref{E, Gibbs}.
There we discussed the behaviour of $S_{n}(F,t)$ when $t$ is small
but did not show that $S_{n}(F,t)$ behaves well when $t$ is far from $0$.
This follows from general theorems but we shall prove it directly.
This brings us into direct contact with Fourier since he used
$F$ as a test case for his statement that any
function\footnote{We would now say any `reasonable function'
but Fourier and his contemporaries had a narrower view
of what constituted a function.} had a Fourier expansion.

(i) Show that
\[S_{n}(F,t)=\int_{0}^{t}\sum_{r=1}^{n}\cos rx\,dx\]
and
\[\sum_{r=1}^{n}\cos rx=\frac{\sin(n+\tfrac{1}{2})x}{2\sin\tfrac{x}{2}}.\]

(ii) Deduce that
\[S_{n}(F,t)=2\int_{0}^{t}\frac{\sin(n+\tfrac{1}{2})x}{x}\,dx
-t+\int_{0}^{t}g(x)\sin(n+\tfrac{1}{2})x\,dx\]
where $g(0)=0$
and
\[g(x)=\frac{x-2\sin\tfrac{x}{2}}{x\sin\tfrac{x}{2}}.\]
for $0<|x|<\pi$.

(iii) Show that $g$ is continuous at $0$. Show that $g$ is
differentiable at $0$ and find its derivative there. Show
that $g$ is continuously differentiable on $(-T,T)$ for
all $0<T<\pi$. Note that, in particular, $g$ and $g'$
are bounded on any interval $[-|t|,|t|]$ with
$|t|<\pi$. Use integration by parts to show that
\begin{align*}
\int_{0}^{t}g(x)&\sin(n+\tfrac{1}{2})x\,dx\\
&=-\frac{1}{n+\tfrac{1}{2}}g(t)\cos(n+\tfrac{1}{2})x
+\frac{1}{n+\tfrac{1}{2}}
\int_{0}^{t}g'(x)\cos(n+\tfrac{1}{2})x\,dx\\
&\rightarrow 0
\end{align*}
as $n\rightarrow \infty$ for all $0<|t|<\pi$.
(This really just another instance of the Riemann--Lebesgue
lemma.


(iv) Show, using Question~\ref{Q, infinite Dirichlet 2},
that
\[2\int_{0}^{t}\frac{\sin(n+\tfrac{1}{2})x}{x}\,dx
=2\int_{0}^{(n+\tfrac{1}{2})t}\frac{\sin x}{x}\,dx
\rightarrow \pi\]
and deduce that
\[S_{n}(F,t)\rightarrow F(t)\]
as $n\rightarrow\infty$ whenever $0<|t|<\pi$. Show directly
that the result is true when $t=0$ and $t=\pi$
and so holds for all $t$.

(v) What does the result of~(iv) tell us about the behaviour
of the Fourier sums of the function $f$ described in
part~(i) of Question~\ref{E, Gibbs}?
\end{question}
\begin{question}\label{explosion}
Suppose that $h_{n}:{\mathbb T}\rightarrow{\mathbb R}$
be a sequence of continuous functions such that

(i) $h_{n}(t)\geq 0$ for all $t\in{\mathbb T}$,

(ii) ${\displaystyle \int_{\mathbb T}h_{n}(t)\,dt=1}$,

(iii) $h_{n}(t)\rightarrow 0$ uniformly for all $\eta\leq |t|\leq \pi$
whenever $\eta>0$.

\noindent If $K>0$, let us write
\[E_{n}=\{x\in {\mathbb T}\,:\,h_{n}(x)\geq K\}.\]
Show that
\[\int_{E_{n}}h_{n}(t)\,dt\rightarrow 1\]
as $n\rightarrow\infty$. Deduce that there exists an $N(K)$ such that
\[\int_{E_{n}}h_{n}(t)^{2}\,dt\geq \frac{K}{2}\]
for all $n\geq N(K)$.

If $f:{\mathbb T}\rightarrow{\mathbb R}$ is continuous
with $f(0)>0$, deduce that
\[\int_{\mathbb T}h_{n}(x)^{2}f(x)\,dx\rightarrow\infty\]
as $n\rightarrow \infty$.
\end{question}

\begin{question}\label{work convolution}
(i) By using the mean value theorem
or some other appropriate version of Taylors theorem,show that,
if $f\in{\mathcal D}$,
\[\frac{f(x+h)-f(x)}{h}\rightarrow f'(x)\]
uniformly in $x$ as $h\rightarrow 0$.

(ii) If $h_{n}\neq 0$, $h_{n}\rightarrow 0$ and
$f\in{\mathcal D}$, show that
\[\frac{f(x+h_{n})-f(x)}{h_{n}}\arrowD f'(x).\]
Deduce that, if $S\in{\mathcal D}'$ and we write
\[g(x)=\langle S(s),f(x+s)\rangle\]
then
\[\frac{g(x+h_{n})-g(x}{h_{n}}\rightarrow\langle S(s),f'(s+x)\rangle\]
as $n\rightarrow\infty$.

(iii) If $f\in{\mathcal D}$ and $S\in{\mathcal D}'$,
show that
\[\frac{\langle S(s),f(x+h+s)\rangle-\langle S(s),f(x+s)\rangle}{h}
\rightarrow \langle S(s),f'(s+x)\rangle\]
as $h\rightarrow 0$.

(iv) If $f\in{\mathcal D}$ and $S\in{\mathcal D}'$,
show that, if $g(x)=\langle S(s),f(x+s)\rangle$
then $g\in{\mathcal D}$. Deduce that, if $T\in{\mathcal D}'$
$\langle T(u),\langle S(s),f(u+s)\rangle$
is a well defined object.

(v) If $T,\,S\in{\mathcal D}'$
we set
\[\langle T*S,f\rangle=\langle T(u),\langle S(s),f(u+s)\rangle.\]
for all $f\in {\mathcal D}$. Show that $T*S\in{\mathcal D}'$.
\end{question}

\begin{question}\label{start bump} Consider the function
$E:{\mathbb R}\rightarrow{\mathbb R}$ defined by
\begin{alignat*}{2}
E(0)&=0\\
E(x)&=\exp(-1/x^{2})&&\qquad\text{otherwise}.
\end{alignat*}

(i) Prove by induction, using the standard rules of differentiation,
that $E$ is infinitely differentiable at all points $x\neq 0$
and that, at these points,
\[E^{(n)}(x)=P_{n}(1/x)\exp(-1/x^{2})\]
where $P_{n}$ is a polynomial which need not be found explicitly.

(ii) Explain why $x^{-1}P_{n}(1/x)\exp(-1/x^{2})\rightarrow 0$
as $x\rightarrow 0$.

(iii) Show by induction, using the definition of differentiation,
that $E$ is infinitely differentiable at $0$
with $E^{(n)}(0)=0$ for all $n$.
[Be careful to get this part of the argument
right.]

(iv) Show that
\[E(x)=\sum_{j=0}^{\infty}\frac{E^{(j)}(0)}{j!}x^{j}\]
if and only if $x=0$. (The reader may prefer to say
that `The Taylor expansion of $E$ is only valid at $0$'.)

(v) If you know some version of Taylor's theorem examine
why it does not apply to $E$.
\end{question}
\begin{question}\label{end bump}
The hard work for this question was done in 
Exercise~\ref{start bump}.

(i) Let $F:{\mathbb R}\rightarrow{\mathbb R}$ be defined by
$F(x)=0$ for $x<0$, $F(x)=E(x)$ for $x\geq 0$ where
$E$ is the function defined in Exercise~\ref{start bump}.
Show that $F$ is infinitely differentiable.

(ii) Sketch the functions $f_{1},\,f_{2}:{\mathbb R}\rightarrow{\mathbb R}$
given by $f_{1}(x)=F(1-x)F(x)$ and $f_{2}(x)=\int_{0}^{x}f_{1}(t)\,dt$.

(iii) Show that given $a<\alpha<\beta<b$ we can find an
infinitely differentiable function $f:{\mathbb R}\rightarrow{\mathbb R}$
with $1\geq f(x)\geq 0$ for all $x$, $f(x)=1$ for all $x\in[\alpha,\beta]$,
$f(x)>0$ for $x\in(a,b)$
and $f(x)=0$ for all $x\notin [a,b]$.
\end{question}
\begin{question}\label{support} (This requires elementary topology,
in particular knowledge of compactness and/or the Heine--Borel
theorem.)
(i) Let $T\in{\mathcal D}'$. We say that an open interval
$(a,b)\in A$ if we can find an $\eta>0$ such that,
if $f\in{\mathcal D}$ and $f(x)=0$ whenever 
$x\notin(a-\eta,b+\eta)$ then $\langle T,f\rangle=0$.

Let $U=\bigcup_{(a,b)\in A}(a,b)$ and $\supp T={\mathbb T}\setminus U$.
Explain why $\supp T$ is closed.
Show, by using compactness and an argument along the lines
of our proof of Lemma~\ref{start support}, that
if $K$ is closed set with $K\cap\supp T=\emptyset$,
$f\in{\mathcal D}$ and $f(x)=0$ for all $x\notin K$,
then $\langle T,f\rangle=0$.

(ii) We continue with the notation of~(i).
Suppose $L$ is a closed set with the property
that, if $K$ is closed set with $K\cap L=\emptyset$,
$f\in{\mathcal D}$ and $f(x)=0$ for all $x\notin K$,
then $\langle T,f\rangle=0$. Show that $L\supseteq \supp T$.

(iii) If $S,\,T\in{\mathcal D}'$ show that
\[\supp(T+S)\subseteq \supp T \cup \supp S.\]

(iv) If $T\in{\mathcal D}'$ show that
\[\supp T'\subseteq \supp T.\]

(v) If $f\in C({\mathbb T})$ show that
$\supp T_{f}$ (or, more briefly, $\supp f$
is the closure of $\{x\,:\,f(x)\neq 0\}$.

If $f\in{\mathcal D}$ and $T\in{\mathcal D}'$ show that
\[\supp fT\subseteq \supp T\cap  \supp f.\]
\end{question}
\begin{question} (Only if you know about metric spaces.)

(i) Show that, if we set
\[d(f,g)=\sum_{r=0}^{\infty}
\frac{2^{-r}\|f^{(r)}-g^{(r)}\|_{\infty}}{1+\|f^{(r)}-g^{(r)}\|_{\infty}},\]
then $({\mathcal D},d)$ is a metric space.

(ii) Show that $f_{n}\arrowD f$ if and only if $d(f_{n},f)\rightarrow 0$.

(iii) (Only if you know what this means.)
Show that $({\mathcal D},d)$ is complete.

(iv) Find a metric $\rho$ on ${\mathcal S}$ such that
$f_{n}\arrowS f$ if and only if $\rho(f_{n},f)\rightarrow 0$

\end{question}
\begin{question} Show that the following equality holds
in the space of tempered distributions
\[2\pi\sum_{n=-\infty}^{\infty}\delta_{2\pi n}
=\sum_{m=-\infty}^{\infty}e_{m}\]
where $\delta_{2\pi n}$ is the delta function at $2\pi n$ and $e_{n}$
is the exponential function given by $e_{n}(t)=\exp(int)$.
What formula results if we take the Fourier transform
of both sides?
\end{question}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Some notes of explanation} Since the birth of the
Lebesgue integral it has been clear that it is a much
more powerful tool for studying Fourier analysis
than the Riemann integral. However I shall try to
make the course accessible to those who have not
done measure theory (though they may have to take
the statement of certain results on trust). If
either those who know Lebesgue integration, or
those who do not, feel that this leads to any problems
they should raise them with me.

Because of the strong number theoretic bias of this
course, I will not have the time to devote to
the Fourier transform that, ideally, I would have wished. 
The reader must be aware that
she is seeing only a limited number of aspects
of Fourier analysis. Although I intend to reach the end
of Section~\ref{S;prime number}, I am not sure that
I will I have time for the final two sections.

\emph{The exercises do not form part of the course.}
I hope that those who attempt them will
find them reasonably easy, instructive
and helpful both in understanding the course and
helping the reader towards `mathematical maturity'
--- but I may well be wrong.
          
\section{Fourier series on the circle} We work on the circle
${\mathbb T}={\mathbb R}/2\pi{\mathbb Z}$ (that is on
the interval $[0,2\pi]$ with the two ends $0$ and $2\pi$
identified). If $f:{\mathbb T}\rightarrow{\mathbb C}$
is integrable\footnote{That is 
to say Lebesgue integrable
or Riemann integrable according to the reader's
background.} we write
\[\hat{f}(n)=\frac{1}{2\pi}\int_{\mathbb T}f(t)\exp -int\,dt.\]
We shall see (Lemma~\ref{Unique}) that $f$ is uniquely
determined by its Fourier coefficients $\hat{f}(n)$.
Indeed it is clear that there is a `natural identification'
(where natural is deliberately used in a vague sense)
\[f(t)\sim\sum_{r=-\infty}^{\infty}\hat{f}(r)\exp irt.\]
However, we shall also see that, even when $f$ is continuous,
$\sum_{r=-\infty}^{\infty}\hat{f}(r)\exp irt$ may
fail to converge at some points $t$.

Fej\'{e}r discovered that, although
\[S_{n}(f,t)=\sum_{r=-n}^{n}\hat{f}(r)\exp irt\]
may behave badly as $n\rightarrow\infty$, the
average
\[\sigma_{n}(f,t)=(n+1)^{-1}\sum_{m=0}^{n}S_{m}(f,t)
=\sum_{r=-n}^{n}\frac{n+1-|r|}{n+1}\hat{f}(r)\exp irt\]
behaves much better. (We call $\sigma_{n}(f,t)$ the
Fej\'{e}r sum. We also write $S_{n}(f,t)=S_{n}(f)(t)$
and $\sigma_{n}(f,t)=\sigma_{n}(f)(t)$.)

\begin{exercise} Let $a_{1}$, $a_{2}$, \dots be a sequence
of complex numbers. 

(i) Show that, if $a_{n}\rightarrow a$, then
\[\frac{a_{1}+a_{2}+\dots+a_{n}}{n}\rightarrow a\]
as $n\rightarrow\infty$.

(ii) By taking an appropriate sequence of $0$s and $1$s
or otherwise find a sequence $a_{n}$ such that
$a_{n}$ does not tend to a limit as $n\rightarrow\infty$
but $(a_{1}+a_{2}+\dots+a_{n})/n$ does.

(iii) By taking an appropriate sequence of $0$s and $1$s
or otherwise find a bounded sequence $a_{n}$ such that
$(a_{1}+a_{2}+\dots+a_{n})/n$ does not tend to a limit
as $n\rightarrow\infty$.
\end{exercise}

In what follows we define
\[f*g(t)=\frac{1}{2\pi}\int_{\mathbb T}f(t-s)g(s)\,ds\]
(for appropriate $f$ and $g$).
\begin{lemma} If $f$ is integrable we have
\begin{align*}
S_{n}(f)&=f*D_{n}\\
\sigma_{n}(f)&=f*K_{n}.
\end{align*}
where
\begin{align*}
D_{n}(t)&=\frac{\sin((n+\tfrac{1}{2})t)}{\sin(\tfrac{1}{2}t)}\\
K_{n}(t)&=\frac{1}{n+1}\left(\frac{\sin(\tfrac{n+1}{2}t)}
{\sin(\tfrac{1}{2}t)}\right)^{2}
\end{align*}
for $t\neq 0$.
\end{lemma}

The key differences between the Dirichlet kernel $D_{n}$
and the Fej\'{e}r kernel $K_{n}$ are illustrated
by the next two lemmas.

\begin{lemma} (i) 
${\displaystyle \frac{1}{2\pi}\int_{\mathbb T}D_{n}(t)\, dt=1}$.

(ii) If $t\neq \pi$, then $D_{n}(t)$ does not tend to
a limit as $n\rightarrow\infty$.

(iii) There is a constant $A>0$ such that
\[\frac{1}{2\pi}\int_{\mathbb T}|D_{n}(t)|\, dt\geq A\log n\] 
for $n\geq 1$.
\end{lemma}

\begin{lemma}\label{Fejer positive}
(i) $\displaystyle{\frac{1}{2\pi}\int_{\mathbb T}K_{n}(t)\, dt=1}$.

(ii) If $\eta>0$, then $K_{n}\rightarrow 0$ uniformly
for $|t|\geq \eta$
as $n\rightarrow\infty$.

(iii) $K_{n}(t)\geq 0$ for all $t$.
\end{lemma}

The properties set out in Lemma~\ref{Fejer positive}
show why Fej\'{e}r sums work so well.

\begin{theorem}~\label{Fejer convergence} 
(i) If $f:{\mathbb T}\rightarrow{\mathbb C}$
is integrable and $f$ is continuous at $t$, then
\[\sigma_{n}(f,t)\rightarrow f(t)\]
as $n\rightarrow\infty$.

(ii) If $f:{\mathbb T}\rightarrow{\mathbb C}$
is continuous, then
\[\sigma_{n}(f)\rightarrow f\]
uniformly as $n\rightarrow\infty$.
\end{theorem}
\begin{exercise} Suppose that
$L_{n}:{\mathbb T}\rightarrow{\mathbb R}$
is continuous (if you know Lebesgue theory you
merely need integrable) and

(A) ${\displaystyle \frac{1}{2\pi}\int_{\mathbb T}L_{n}(t)\, dt=1}$,

(B) If $\eta>0$, then $L_{n}\rightarrow 0$ uniformly
for $|t|\geq \eta$
as $n\rightarrow\infty$,

(C) $L_{n}(t)\geq 0$ for all $t$.

(i) Show that, if $f:{\mathbb T}\rightarrow{\mathbb C}$
is integrable and $f$ is continuous at $t$, then
\[L_{n}*f(t)\rightarrow f(t)\]
as $n\rightarrow\infty$.

(ii) Show that, if $f:{\mathbb T}\rightarrow{\mathbb C}$
is continuous, then
\[L_{n}*f\rightarrow f\]
uniformly as $n\rightarrow\infty$.

(iii) Show that condition (C) can be replaced by

(C') There exists a constant $A>0$ such that
\[\frac{1}{2\pi}\int_{\mathbb T}|L_{n}(t)|\, dt\leq A\]
in parts (i) and (ii). [You need only give the proof in
one case and say that the other is `similar'.]
\end{exercise}
\begin{exercise} Suppose that
$L_{n}:{\mathbb T}\rightarrow{\mathbb R}$
is continuous but that
\[\sup_{n}\frac{1}{2\pi}
\int_{\mathbb T}|L_{n}(t)|\, dt=\infty.\]
Show that we can find a sequence of continuous functions
$g_{n}:{\mathbb T}\rightarrow{\mathbb R}$ with
$|g_{n}(t)|\leq 1$ for all $t$, $L_{n}*g_{n}(0)\geq 0$
for all $n$ and 
\[\sup_{n}L_{n}*g_{n}(0)=\infty.\]

(i) If you know some functional analysis deduce the
existence of a continuous function $f$ such that
\[\sup_{n}L_{n}*f(0)=\infty.\]

(ii) Even if you can obtain the result of (i) by
slick functional analysis there is some point
in obtaining the result directly. 

(a) Suppose that we have defined positive integers
$n(1)<n(2)<\dots<n(k)$, a continuous function
$g_{k}$ and a real number $\epsilon(k)$ with
$2^{-k}>\epsilon(k)>0$.
Show that there is an $\epsilon(k+1)$ with
$\epsilon(k)/2>\epsilon(k+1)>0$ such that
whenever $g$ is a continuous function with
$\|g-g_{k}\|_{\infty}<2\epsilon(k+1)$
we have $|L_{n(j)}*g(0)-L_{n(j)}*g_{k}(0)|\leq 1$.
for $1\leq j\leq k$.

(b) Continuing with the notation of (a), show that
there exists an $n(k+1)>n(k)$ and a continuous
function $g_{k+1}$ with $\|g_{k+1}-g_{k}\|_{\infty}\leq\epsilon(k+1)$
such that $|L_{n(k+1)}*g_{k+1}(0)|>2^{k+1}$.

(c) By carrying out the appropriate induction
and considering the uniform limit of $g_{k}$
obtain (i). 
 

(iii) Show that there exists a continuous function
$f$ such that $S_{n}(f,0)$ fails to converge as
$n\rightarrow\infty$. (We shall obtain a stronger result
later in Theorem~\ref{Kahane and Katznelson}.)
\end{exercise}

Theorem~\ref{Fejer convergence} has several very useful 
consequences.
\begin{theorem}[Density of trigonometric polynomials]%
\label{density} The trigonometric polynomials are
uniformly dense in the continuous functions on ${\mathbb T}$.
\end{theorem}
\begin{lemma}[Riemann-Lebesgue lemma] If $f$ is an
integrable function on ${\mathbb T}$, then
$\hat{f}(n)\rightarrow 0$ as $|n|\rightarrow\infty$.
\end{lemma}
\begin{theorem}[Uniqueness]\label{Unique}
If $f$ and $g$ are
integrable functions on ${\mathbb T}$ with 
$\hat{f}(n)=\hat{g}(n)$ for all $n$, then $f=g$.
\end{theorem}
\begin{lemma} If $f$ is an
integrable function on ${\mathbb T}$ and
$\sum_{j}|\hat{f}(j)|$ converges, then $f$ is continuous
and $f(t)=\sum_{j}\hat{f}(j)\exp ijt$.
\end{lemma}
As a preliminary to the next couple of results we
need the following temporary lemma (which will be immediately
superseded by Theorem~\ref{Parseval}).
\begin{lemma}[Bessel's inequality]
If $f$ is
a continuous function on ${\mathbb T}$, then
\[\sum_{n=-\infty}^{\infty}|\hat{f}(n)|^{2}
\leq\frac{1}{2\pi}\int_{\mathbb T}|f(t)|^{2}\,dt.\]
\end{lemma}
\begin{theorem} [Mean square convergence]%
\label{mean square} If $f$ is
a continuous function on ${\mathbb T}$, then
\[
\frac{1}{2\pi}\int_{\mathbb T}
| f(t)-S_{n}(f,t) |^{2}\,dt\rightarrow 0
\]
as $n\rightarrow\infty$.
\end{theorem}
\begin{theorem} [Parseval's Theorem]\label{Parseval} 
If $f$ is
a continuous function on ${\mathbb T}$, then
\[\sum_{n=-\infty}^{\infty}|\hat{f}(n)|^{2}
=\frac{1}{2\pi}\int_{\mathbb T}|f(t)|^{2}\,dt.\]
More generally, if $f$ and $g$ are continuous
\[\sum_{n=-\infty}^{\infty}\hat{f}(n)\hat{g}(n)^{*}
=\frac{1}{2\pi}\int_{\mathbb T}f(t)g(t)^{*}\,dt.\]
\end{theorem}


(The extension to all $L^{2}$ functions of Theorems~\ref{mean square}
and~\ref{Parseval} uses easy measure
theory.)
\begin{exercise} 
If you use Lebesgue integration, 
state and prove Theorems~\ref{mean square}
and~\ref{Parseval} for $(L^{2}({\mathbb T}),\|\ \|_{2})$.

If you use Riemann integration, extend and prove
Theorems~\ref{mean square} and~\ref{Parseval}
for all Riemann integrable function.
\end{exercise}
Note the following complement to the Riemann-Lebesgue lemma.
\begin{lemma} If 
$\kappa(n)\rightarrow \infty$ as $n\rightarrow\infty$,
then we can find a continuous function $f$
such that $\limsup_{n\rightarrow\infty}\kappa(n)\hat{f}(n)=\infty$.
\end{lemma}

The proof of the next result is perhaps more interesting
than the result itself.
\begin{lemma}\label{pointwise}
Suppose that $f$ is an
integrable function on ${\mathbb T}$ such that there
exists an $A$ with $|\hat{f}(n)|\leq A|n|^{-1}$ for
all $n\neq 0$. If $f$ is continuous at $t$, then
$S_{n}(f,t)\rightarrow f(t)$ as $n\rightarrow\infty$.
\end{lemma}
\begin{exercise} Suppose that $a_{n}\in{\mathbb C}$
and there
exists an $A$ with $|a_{n}|\leq A|n|^{-1}$ for
all $n\geq 1$. Write
\[s_{n}=\sum_{r=0}^{n}a_{r}.\]
Show that, if
\[\frac{s_{0}+s_{1}+\dots+s_{n}}{n+1}\rightarrow s\]
as $n\rightarrow\infty$, then $s_{n}\rightarrow s$
as $n\rightarrow\infty$. (Results like this are
called Tauberian theorems.)

\end{exercise}
\begin{exercise} (i)
Suppose that $f:[-\pi,\pi)\rightarrow{\mathbb R}$
is increasing and bounded. Write $f(\pi)=\lim_{t\rightarrow 0}
f(\pi -t)$.
Show that
\[\int_{-\pi}^{\pi}f(t)\exp it\, dt=
\int_{0}^{\pi}(f(t)-f(t-\pi))\exp it\, dt\]
and deduce that $|\hat{f}(1)|\leq 
(f(\pi)-f(-\pi))/2\leq(f(\pi)-f(-\pi))$.

(ii) Under the assumptions of (i) show that
\[|\hat{f}(n)|\leq (f(\pi)-f(-\pi))/|n|\]
for all $n\neq 0$.

(iii) (Dirichlet's theorem) Suppose that $g=f_{1}-f_{2}$
where $f_{k}:[-\pi,\pi)\rightarrow{\mathbb R}$
is increasing and bounded $[k=1,2]$. (It can be
shown that functions $g$ of this form are the,
so called, functions of bounded variation.)
Show that if $g$ is continuous at $t$, then
$S_{n}(g,t)\rightarrow f(t)$ as $n\rightarrow\infty$. 
\end{exercise}

Most readers will already be aware of the next fact.
\begin{lemma} If $f:{\mathbb T}\rightarrow{\mathbb C}$
is continuously differentiable, then
\[(f')\hat{\ }(n)=in\hat{f}(n).\]
\end{lemma}
This means that Lemma~\ref{pointwise} applies,
but we can do better.
\begin{lemma}\label{once is}
If $f:{\mathbb T}\rightarrow{\mathbb C}$
is continuously differentiable, then
\[\sum_{n=-\infty}^{\infty}|\hat{f}(n)|<\infty.\]
\end{lemma}

Here is a beautiful application due to Weyl
of Theorem~\ref{density}. If $x$ is real,
let us write $\langle x\rangle$ for the fractional part
of $x$, that is, let us write
\[\langle x\rangle=x-[x].\]
\begin{theorem}\label{Weyl} If $\alpha$ is an irrational
number and $0\leq a\leq b\leq 1$, then
\[\frac{\card\{1\leq n\leq N \mid \langle n\alpha\rangle
\in [a,b]\}}{N}
\rightarrow b-a\]
as $N\rightarrow\infty$. The result is false
if $\alpha$ is rational.
\end{theorem}
(Of course this result may be deduced from the ergodic
theorem and Theorem~\ref{density} itself can be deduced
from the Stone-Weierstrass theorem but the techniques
used can be extended in directions not covered by
the more general theorems.)

Hurwitz used Parseval's theorem in a neat proof of the 
isoperimetric inequality.
\begin{theorem} Among all smooth closed
non-self-intersecting curves of given length, 
the one which encloses greatest area is the circle.
\end{theorem}
(Reasonably simple arguments show that the requirement of smoothness
can be dropped.) 
\section{A Theorem of Kahane and Katznelson}
We need to recall (or learn) the following definition.
\begin{definition} A subset $E$ of ${\mathbb T}$
has (Lebesgue) measure zero if, given $\epsilon>0$,
we can find intervals $I_{j}$ of length $|I_{j}|$
such that $\bigcup_{j=1}^{\infty}I_{j}\supseteq E$
but $\sum_{j=1}^{\infty}|I_{j}|<\epsilon$.
\end{definition}

There is a deep and difficult theorem of Carleson
which tells us that if $f:{\mathbb T}\rightarrow{\mathbb C}$
is continuous (or even $L^{2}$), then the set
\[E=\{t\in {\mathbb T}\mid S_{n}(f,t)\nrightarrow f(t)
\ \text{as $n\rightarrow\infty$}\}\]
has measure $0$. (We shall neither prove nor
make use of this result which is included
for information only.) Kahane and Katznelson
proved a converse which though much easier to prove
is still remarkable.

\begin{theorem}[Kahane and Katznelson]%
~\label{Kahane and Katznelson} Given
any  subset $E$ of ${\mathbb T}$ with
measure zero, we can find a continuous function $f$
such that
\[\limsup_{n\rightarrow\infty}|S_{n}(f,t)|\rightarrow\infty\]
for all $t\in E$.
\end{theorem}

The theorem follows relatively simply from its
`finite version'.
\begin{lemma}\label{Finite Katznelson}
Given any $K>0$, we can find a $\epsilon(K)>0$
such that if $J_{1}$, $J_{2}$, \dots $J_{N}$ is any
finite collection of intervals with 
$\sum_{r=1}^{N}|J_{r}|<\epsilon(K)$
we can find a trigonometric polynomial $P$
such that $\|P\|_{\infty}\leq 1$ but
\[\sup_{n}|S_{n}(P,t)|\geq K\]
for all $t\in \bigcup_{r=1}^{N}J_{r}$.
\end{lemma}

It is the proof of Lemma~\ref{Finite Katznelson}
which contains the key idea. This is given
in the next lemma.
\begin{lemma} Let us define $\log z$ on
${\mathbb C}\setminus\{x\in{\mathbb R}\,:\, x\leq 0\}$
so that $\log x$ is real when $x$ is real and positive.
Suppose that $1>\delta>0$ and that 
$\theta_{1},\ \theta_{2},\ \dots,\ \theta_{N}\in{\mathbb R}$.
If we set
\[\phi(z)=\log\left(N^{-1}\sum_{n=1}^{N}
\frac{1+\delta}{1+\delta-ze^{-i\theta_{n}}}\right),\]
then $\phi$ is a well defined analytic function
on $\{z\mid |z|<1+\delta/2\}$ such that

(i) $|\Im \phi(z)|<\pi$ for all $|z|<1+\delta/2$,

(ii) $\phi(0)=0$,

(iii) $|\Re \phi(e^{i\theta})|\geq \log(\delta^{-1}/4N)$
for all $|\theta-\theta_{n}|\leq\delta/2$
and $1\leq n\leq N$.
\end{lemma}
\section{Many Dimensions} The extension of these
ideas to higher dimensions can be either trivial
or very hard. If $f:{\mathbb T}^{m}\rightarrow{\mathbb C}$
we define
\[\hat{f}({\mathbf n})=\frac{1}{(2\pi)^{m}}
\int_{\mathbb T}\dots \int_{\mathbb T}
f({\mathbf t})\exp(-i{\mathbf n}.{\mathbf t})
\,dt_{1}\dots dt_{m}.\]
Very little is known about the convergence of
\[\sum_{u^{2}+v^{2}\leq N}\hat{f}(u,v)\exp(i(ux+vy))\]
as $N\rightarrow\infty$ even when $f$ is continuous.
(Of course, under stronger conditions, such as those
in Exercise~\ref{trivial circular} below the matter
becomes much easier.)


However the treatment of the sums of type
\[\sum_{|u|,|v|\leq N}\hat{f}(u,v)\exp(i(ux+vy))\]
is a straightforward. The following results
are part of the course but will be left as exercises.
(Of course, if you have trouble with them you
can ask the lecturer to do them. If you are using
Lebesgue integration work with $L^{\infty}$
rather than $L^{1}$ functions.)
\begin{lemma}\label{many start}
If we define $\tilde{K}_{n}:{\mathbb T}^{m}\rightarrow{\mathbb R}$
by 
\[\tilde{K}_{n}(t_{1},t_{2},\dots,t_{m})=\prod_{j=1}^{m}K_{n}(t_{j}),\]
then we have the following results.

(i) ${\displaystyle \frac{1}{(2\pi)^{m}}\int_{{\mathbb T}^{m}}
\tilde{K}_{n}({\mathbf t})\, d{\mathbf t}=1}$.

(ii) If $\eta>0$, then 
\[\frac{1}{(2\pi)^{m}}\int_{|{\mathbf t}|\geq\eta}\tilde{K}_{n}({\mathbf t})
\,d{\mathbf t}\rightarrow 0\] 
as $n\rightarrow\infty$.

(iii) $\tilde{K}_{n}({\mathbf t})\geq 0$ for all ${\mathbf t}$.

(iv) $\tilde{K}_{n}$ is a (multidimensional) trigonometric
polynomial.
\end{lemma}
\begin{lemma}
If $f:{\mathbb T}^{m}\rightarrow{\mathbb C}$ is integrable
and $P:{\mathbb T}^{m}\rightarrow{\mathbb C}$ is a trigonometric
polynomial, then
\[P*f({\mathbf x})=\frac{1}{(2\pi)^{m}}\int_{{\mathbb T}^{m}}
P({\mathbf x}-{\mathbf t})f({\mathbf t})\, d{\mathbf t}\]
is a trigonometric polynomial in ${\mathbf x}$.
\end{lemma}
\begin{theorem}[Density of trigonometric polynomials]%
The trigonometric polynomials are
uniformly dense in the continuous functions on ${\mathbb T}^{m}$.
\end{theorem}
\begin{lemma}[Riemann-Lebesgue lemma] If $f$ is an
integrable function on ${\mathbb T}^{m}$, then
$\hat{f}({\mathbf n})\rightarrow 0$ 
as $|{\mathbf n}|\rightarrow\infty$.
\end{lemma}
\begin{theorem}[Uniqueness] If $f$ and $g$ are
integrable functions on ${\mathbb T}^{m}$ with 
$\hat{f}({\mathbf n})=\hat{g}({\mathbf n})$ 
for all ${\mathbf n}$, then $f=g$.
\end{theorem}
\begin{lemma} If $f$ is an
integrable function on ${\mathbb T}$ and
$\sum_{{\mathbf j}}|\hat{f}({\mathbf j})|$ converges, 
then $f$ is continuous
and $f({\mathbf t})=\sum_{{\mathbf j}}\hat{f}({\mathbf j})
\exp i{\mathbf j}.{\mathbf t}$.
\end{lemma}
\begin{exercise}\label{trivial circular}
Suppose that
$f:{\mathbb T}^{m}\rightarrow{\mathbb C}$
is integrable and 
$\sum_{(u,v)\in{\mathbb Z}^{2}}|\hat{f}(u,v)|<\infty$.
Show that
\[\sum_{u^{2}+v^{2}\leq N}\hat{f}(u,v)\exp(i(ux+vy))
\rightarrow f(x,y)\]
uniformly as $N\rightarrow\infty$.
\end{exercise}
\begin{theorem} [Parseval's Theorem]\label{many end}
If $f$ is
a continuous function on ${\mathbb T}^{m}$, then
\[\sum_{n\in{\mathbb Z}^{m}}|\hat{f}({\mathbf n})|^{2}
=\frac{1}{(2\pi)^{m}}\int_{{\mathbb T}^{m}}
|f({\mathbf t})|^{2}\,d{\mathbf t}.\]
More generally, if $f$ and $g$ are continuous,
\[\sum_{n\in{\mathbb Z}^{m}}
\hat{f}({\mathbf n})\hat{g}({\mathbf n})^{*}
=\frac{1}{(2\pi)^{m}}\int_{{\mathbb T}^{m}}
f({\mathbf t})g({\mathbf t})^{*}\,d{\mathbf t}.\]
\end{theorem}
\begin{exercise}  Prove the results from Lemma~\ref{many start}
to Theorem~\ref{many end}
\end{exercise}
\begin{exercise} The extension of Lemma~\ref{pointwise}
to many dimensions is not required for the course
but makes a nice exercise. The proof follows the
one dimensional proof but is not quite word for word.

(i) Suppose that $f$ is a  bounded
integrable function on ${\mathbb T}^{2}$ such that there
exists an $A$ with $|\hat{f}(u,v)|\leq A(u^{2}+v^{2})^{-1}$ for
all $(u,v)\neq (0,0)$. Show that,
if $f$ is continuous at $(s,t)$, then
\[\sum_{|u|,|v|\leq n}\hat{f}(u,v)\exp(i(us+vt))
\rightarrow f(s,t)\]
as $n\rightarrow\infty$

(ii) (This generalises Lemma~\ref{once is}.) 
Suppose that $f$ is a
twice differentiable function on ${\mathbb T}^{2}$
with 
${\displaystyle \frac{\partial^{2}f(x,y)}{\partial x\partial y}}$
continuous. Show that
$\sum_{(u,v)\in{\mathbb Z}^{2}}|\hat{f}(u,v)|<\infty$.

(iii) State the correct generalisations of parts (i) and
(ii) to higher dimensions. 
\end{exercise}

We immediately obtain a striking generalisation of
Weyl's theorem (Theorem~\ref{Weyl}).
\begin{theorem}\label{Weyl many} Suppose
that $\alpha_{1}$, $\alpha_{2}$, \dots, $\alpha_{M}$
are real numbers. A necessary and sufficient condition
that
\[\frac{\card\{1\leq n\leq N \mid 
(\langle n\alpha_{1}\rangle,\langle n\alpha_{2}\rangle,
\dots,\langle n\alpha_{M}\rangle)
\in \prod_{j=1}^{M}[a_{j},b_{j}]\}}{N}
\rightarrow \prod_{j=1}^{M}(b_{j}-a_{j})\]
as $N\rightarrow\infty$ whenever $0\leq a_{j}\leq b_{j}\leq 1$
is that
\begin{equation*}
\sum_{j=1}^{M} n_{j}\alpha_{j}\notin{\mathbb Z}
\ \text{for integer $n_{j}$ not all zero}. \tag*{$\bigstar$}
\end{equation*}
\end{theorem}
If $\alpha_{1}$, $\alpha_{2}$, \dots, $\alpha_{M}$
satisfy $\bigstar$ we say that they are independent.
The multidimensional version of Weyl's theorem has
an important corollary.
\begin{theorem}{\bf (Kronecker's theorem)}\label{Kronecker's theorem}
Suppose
that $\alpha_{1}$, $\alpha_{2}$, \dots, $\alpha_{M}$
are independent real numbers. Then given real
numbers $\beta_{1}$, $\beta_{2}$, \dots, $\beta_{M}$
and $\epsilon>0$ we can find integers
$N$, $m_{1}$, $m_{2}$, \dots, $m_{M}$ such that
\[|N\alpha_{j}-\beta_{j}-m_{j}|<\epsilon\]
for each $1\leq j\leq M$.

The result is false if
$\alpha_{1}$, $\alpha_{2}$, \dots, $\alpha_{M}$
are not independent.
\end{theorem} 

We use this to obtain a theorem of Kolmogorov.
\begin{theorem} There exists a Lebesgue integrable
(that is an $L^{1}$) function 
$f:{\mathbb T}\rightarrow{\mathbb C}$ such that
\[\limsup_{n\rightarrow\infty}|S_{n}(f,t)|=\infty\]
for all $t\in{\mathbb T}$.
\end{theorem}
Although this result is genuinely one of
Lebesgue integration it can be obtained
by simple (Lebesgue measure) arguments
from a result not involving Lebesgue integration.
\begin{lemma}\label{Polynomial Kolmogorov}
Given any $K>0$ we can find a
trigonometric polynomial $P$ such that

(i) ${\displaystyle \frac{1}{2\pi}\int_{\mathbb T}|P(t)|\, dt\leq 1}$,

(ii) $\max_{n\geq 0}|S_{n}(P,t)|\geq K$
for all $t\in{\mathbb T}$.
\end{lemma}

In our discussion of Kronecker's theorem
(Theorem~\ref{Kronecker's theorem}) we worked
modulo $1$. In what follows it is easier
to work modulo $2\pi$. The readier will readily
check that the definition and theorem that follow
give the appropriate restatement of Kronecker's theorem.
\begin{definition}
We work in ${\mathbb T}$.
If $\alpha_{1}$, $\alpha_{2}$, \dots, $\alpha_{M}$
satisfy 
\begin{equation*}
\sum_{j=1}^{M} n_{j}\alpha_{j}\neq 0
\ \text{for integer $n_{j}$ not all zero}. \tag*{$\bigstar$}
\end{equation*}
we say that they are independent.

\end{definition}
\begin{theorem}{\bf (Kronecker's theorem (alternative statement))}%
\label{alternative Kronecker} Suppose
that $\alpha_{1}$, $\alpha_{2}$, \dots, $\alpha_{M}$
are independent points in ${\mathbb T}$. Then, given complex
numbers $\lambda_{1}$, $\lambda_{2}$, \dots, $\lambda_{M}$
with $|\lambda_{j}|=1$ $[j=1,\ 2,\ \dots,\ M]$
and $\epsilon>0$, we can find an integer $N$
such that
\[|\exp(iN\alpha_{j})-\lambda_{j}|<\epsilon\]
for each $1\leq j\leq M$.
\end{theorem}

Our construction requires some preliminary results.
\begin{lemma} If  $x_{1}$, $x_{2}$, \dots, $x_{M}$
are independent points in ${\mathbb T}$ and $t\in{\mathbb T}$
and
\begin{align*}
\sum_{j=1}^{M} p_{j}(x_{j}-t)&=0
\ \text{for integer $p_{j}$ not all zero}\\
\sum_{j=1}^{M} q_{j}(x_{j}-t)&= 0
\ \text{for integer $q_{j}$ not all zero}
\end{align*}
then there exist 
$p,q\in{\mathbb Z}\setminus\{0\}$
such that
$pq_{j}=qp_{j}$ for $1\leq j\leq M$.
\end{lemma}
\begin{lemma} If  $x_{1}$, $x_{2}$, \dots, $x_{M}$
are independent points in ${\mathbb T}$ and $t\in{\mathbb T}$,
then one of the following must hold:-

(a) There exists a $i\neq 1$ such that
the points $x_{j}-t$ with $j\neq i$ are independent.

(b) $x_{1}-t$ is a rational multiple of $2\pi$
and the points $x_{j}-t$ with $j\neq 1$ are independent.
\end{lemma}
\begin{lemma}\label{Kronecker set up} 
(i) If $I$ is an open interval in ${\mathbb T}$
and $x_{1}$, $x_{2}$, \dots, $x_{m}$ are independent
we can find $x_{m+1}\in I$ such that
$x_{1}$, $x_{2}$, \dots, $x_{m+1}$ are independent.
 
(ii) Given an integer $M\geq 1$ we can find
independent points
$x_{1}$, $x_{2}$, \dots, $x_{M}$ in ${\mathbb T}$
such that
\[|x_{j}-2\pi j/M|\leq 10^{-3}M^{-1}\]
\end{lemma}

\begin{lemma}\label{Finite measure} If $M$ and
$x_{1}$, $x_{2}$, \dots, $x_{M}$ in ${\mathbb T}$
are as in Lemma~\ref{Kronecker set up}~(ii),
then setting
\[\mu=M^{-1}\sum_{j=1}^{M}\delta_{x_{j}}\]
we have the following two results.

(i) $\max_{n\geq 0}|S_{n}(\mu,t)|\geq 100^{-1}\log M$
for each $t\in{\mathbb T}$,

(ii) There exists an $N$ such that
$\max_{N\geq n\geq 0}|S_{n}(\mu,t)|\geq 200^{-1}\log M$
for each $t\in{\mathbb T}$.
\end{lemma}

\noindent\emph{Remark 1} If you wish you may treat 
$S_{n}(\mu,t)$ as a purely formal object. However,
it is better for later work to think what it actually is.

\noindent\emph{Remark 2} Factors like $10^{-3}$ and $100^{-1}$
in Lemmas~\ref{Kronecker set up}~(ii) and~\ref{Finite measure}
are more or less chosen at random and are not `best possible'.

A simple argument using Lemma~\ref{Finite measure} now gives
Lemma~\ref{Polynomial Kolmogorov} and we are done.
\begin{exercise} Show, by considering
Fej\'{e}r sums or otherwise, that we cannot find a continuous
function $f$ such that $S_{n}(f,t)\rightarrow\infty$
uniformly as $n\rightarrow\infty$.
\end{exercise}

\section{Some simple geometry of numbers}
We need the following
extension of Theorem~\ref{many end}.
\begin{lemma}\label{characteristic Parseval}
If $A$ is a well behaved set
and $f$ is the characteristic function
of $A$ (that is $f(x)=1$ if $x\in A$,
$f(x)=0$ otherwise), then
\[\sum_{n\in{\mathbb Z}^{m}}|\hat{f}({\mathbf n})|^{2}
=\frac{1}{(2\pi)^{m}}\int_{{\mathbb T}^{m}}
|f({\mathbf t})|^{2}\,d{\mathbf t}.\]
\end{lemma}
If you know Lebesgue measure then this is
obvious (for bounded measurable sets, say)
since a simple density argument shows that
Parseval's Theorem (Theorem~\ref{many end})
holds for every $f\in L^{1}\cap L^{2}$.
If we restrict ourselves to Riemann integration
it is obvious what sort of approximation
argument we should use but the technical details
are typically painful.
\begin{exercise} EITHER (i) Give the detailed
proof of Lemma~\ref{characteristic Parseval}
in terms of Lebesgue measure.

OR (ii) Give the detailed
proof of Lemma~\ref{characteristic Parseval}
in terms of Riemann integration in the
special case when $A$ is a sphere.
\end{exercise}

We use Parseval's Theorem (in the form of
Lemma~\ref{characteristic Parseval}) to
give Siegel's proof of Minkowski's
theorem.
\begin{theorem}[Minkowski]\label{Minkowski}
Let $\Gamma$ be an open symmetric  convex set
in ${\mathbb R}^{m}$ with volume $V$.
If $V>2^{m}$, then $\Gamma\cap {\mathbb Z}^{m}$
contains at least two points.
\end{theorem}

The reader will recall that $\Gamma$ is convex if
\[{\mathbf x},{\mathbf y}\in \Gamma
\ \text{and}\ 0\leq\lambda\leq 1
\Rightarrow
\lambda{\mathbf x}+(1-\lambda){\mathbf y}\in \Gamma\]
and that $\Gamma$ is symmetric if
\[{\mathbf x}\in \Gamma
\Rightarrow -{\mathbf x}\in \Gamma.\]
It is not entirely obvious (though it is true)
that every open convex set has a (possibly infinite)
volume in the sense of Riemann. Readers who wish
to use Riemann integration may add the
words `well behaved' to the statement of
Minkowski's theorem.
\begin{lemma} If $V\leq 2^{m}$ there exists
an open symmetric  convex set $\Gamma$
in ${\mathbb R}^{m}$ with volume $V$
such that $\Gamma\cap {\mathbb Z}^{m}=\{{\mathbf 0}\}$.
\end{lemma}

To prove Minkowski's theorem  (Theorem~\ref{Minkowski})
it suffices to prove an essentially equivalent result.
\begin{theorem}
Let $\Gamma$ be a bounded open symmetric  convex set
in ${\mathbb R}^{m}$ with volume $V$.
If $V>2^{m}(2\pi)^{m}$, then $\Gamma\cap(2\pi {\mathbb Z})^{m}$
contains at least two points.
\end{theorem}
We need the following simple but crucial result.
\begin{lemma} If $\Gamma$ is symmetric  convex set
and ${\mathbf x},{\mathbf x}-2{\mathbf y}\in \Gamma$,
then ${\mathbf y}\in \Gamma$.
\end{lemma}
By applying Parseval's theorem to 
$f({\mathbf x})=\sum_{{\mathbf k}\in {\mathbb Z}^{m}}
{\mathbb I}_{\Gamma/2}({\mathbf x}-2\pi {\mathbf k})$ we
obtain the following results.
\begin{lemma} Let $\Gamma$ be a bounded open symmetric  convex set
in ${\mathbb R}^{m}$ with volume $V$ such that
$\Gamma\cap(2\pi {\mathbb Z})^{m}$ only contains
${\mathbf 0}$. Then
\begin{equation*}
\tag*{$\bigstar$}
2^{-m}\sum_{{\mathbf k}\in {\mathbb Z}^{m}}
\left|
\int_{{\mathbb R}^{m}} 
{\mathbb I}_{\Gamma}({\mathbf x})e^{i{\mathbf k}.{\mathbf x}}
\, d{\mathbf x}
\right|^{2}
=(2\pi)^{m}V.
\end{equation*}
where ${\mathbb I}_{\Gamma}$ is the characteristic function
of $\Gamma$.
\end{lemma}
Minkowski's theorem follows at once by considering the
term with ${\mathbf k}={\mathbf 0}$ in equation
$\bigstar$.

Here is a simple application of Minkowski's theorem.
\begin{lemma} Suppose that $a$, $b$, $c$, $d$ are
real numbers with $ad-bc=1$. Given $l>0$ and $\epsilon>0$
we can find integers $m$ and $n$ such that
\[|an+bm|\leq(1+\epsilon) l,\ |cn+dm|\leq (1+\epsilon) l^{-1}.\]
\end{lemma}
Taking $c=x$, $a=1$, $b=0$, $d=-1$ and thinking carefully
we obtain in quick succession.
\begin{lemma} If $x$ is real there exist $n$ and $m$ integers
with $n\neq 0$ such that
\[\left|x-\frac{m}{n}\right|\leq\frac{1}{n^{2}}.\]
\end{lemma}
\begin{lemma} If $x$ is real there exist infinitely
many pairs of integers $n$ and $m$
with $n\neq 0$ such that
\[\left|x-\frac{m}{n}\right|\leq\frac{1}{n^{2}}.\]
\end{lemma}

Here is another simple consequence.
\begin{lemma}{\bf (Quantitative version of Dirichlet's theorem)}
If ${\mathbf x}\in{\mathbb R}^{m}$, then,
given $l>0$ and $\epsilon>0$, 
we can find
$n,\ n_{1},\ n_{2},\ \dots,\ n_{m}\in{\mathbb Z}$ such that
\[|nx_{j}-n_{j}|\leq l^{-1}\] 
for $1\leq j\leq m$ and $|n|\leq l^{m}$.
\end{lemma}

We conclude our collection of consequences with
Legendre's four squares theorem.
\begin{theorem}[Legendre]\label{Legendre} 
Every positive integer
is the sum of at most 4 squares.
\end{theorem}
\begin{lemma} We cannot reduce 4 in the statement
of Legendre's theorem (Theorem~\ref{Legendre}).
\end{lemma}

We need an observation of Euler.
\begin{lemma}\label{Euler Legendre} If $x_{0}$, $x_{1}$,
$x_{2}$, $x_{3}$, $y_{0}$, $y_{1}$,
$y_{2}$, $y_{3}$ are real, then
\begin{align*}
(x_{0}^{2}+&x_{1}^{2}+x_{2}^{2}+x_{3}^{2})
(y_{0}^{2}+y_{1}^{2}+y_{2}^{2}+y_{3}^{2})\\
=&(x_{0}y_{0}-x_{1}y_{1}-x_{2}y_{2}-x_{3}y_{3})^{2}+
(x_{0}y_{1}+x_{1}y_{0}+x_{2}y_{3}-x_{3}y_{2})^{2}+\\
&(x_{0}y_{2}-x_{1}y_{3}+x_{2}y_{0}+x_{3}y_{1})^{2}+
(x_{0}y_{3}+x_{1}y_{2}-x_{2}y_{1}+x_{3}y_{1})^{2}.
\end{align*} 
\end{lemma}
\begin{exercise} In the lectures we will use quaternions
to prove Lemma~\ref{Euler Legendre}. Prove the
equality by direct verification.
\end{exercise}
\begin{lemma} Legendre's four square theorem will follow
if we can show that every odd prime is the sum
of at most four squares.
\end{lemma}

We shall also need the volume of a $4$ dimensional
sphere. A simple argument gives the
volume of a unit sphere in any dimension.
\begin{lemma} Let $V_{n}$ be the ($n$-dimensional)
volume of an $n$ dimensional unit sphere.

(i) If $f:[0,\infty)\rightarrow{\mathbb R}$ is a continuous
function with $t^{n+2}f(t)\rightarrow 0$ as 
$t\rightarrow\infty$, then 
\[\int_{{\mathbb R}^{n}}f(\|{\mathbf x}\|)\,dV({\mathbf x})
=V_{n}\int_{0}^{\infty}f(t)nt^{n-1}\,dt.\]

(ii) $V_{2k}=\dfrac{\pi^{k}}{k!}$,
$V_{2k+1}=\dfrac{k!2^{2k+1}\pi^{k}}{(2k+1)!}$.
\end{lemma}

Finally we need the apparently more general
version of Minkowski's theorem obtained by
applying a linear map.
\begin{theorem}[Minkowski for general lattices]%
\label{Minkowski general} We work
in ${\mathbb R}^{m}$. Let $\Lambda$ 
be a lattice with fundamental region of volume $L$
and let $\Gamma$ be an open symmetric  convex set
with volume $V$.
If $V>2^{m}L$, then $\Gamma\cap\Lambda$
contains at least two points.
\end{theorem}

We now turn to the proof of the fundamental lemma.
\begin{lemma}\label{prime Legendre} Every odd prime is the sum
of at most four squares.
\end{lemma}
We begin with a simple lemma.
\begin{lemma}\label{lattice finder} Let $p$
be an odd prime.

(i)  If we work in ${\mathbb Z}_{p}$, then the set
$\{u^{2}:u\in {\mathbb Z}_{p}\}$ has at least $(p+1)/2$
elements.

(ii) We can find integers $u$ and $v$ such that
$u^{2}+v^{2}\equiv-1 \bmod p$.
\end{lemma}

We now introduce a lattice.
\begin{lemma} Let $p$, $u$ and $v$ be as in 
Lemma~\ref{lattice finder}. If 
\[\Lambda=\{(n,m,a,b)\in{\mathbb Z}^{4}
\, : \,
nu+mv\equiv a,\ mu-nv\equiv b \bmod p\}\]
then $\Lambda$ is a lattice
with fundamental region of volume $p^{2}$.

If $(n,m,a,b)\in\Lambda$, then
$n^{2}+m^{2}+a^{2}+b^{2}\equiv 0 \bmod p$.
\end{lemma}
We can now prove Lemma~\ref{prime Legendre} and with it 
Theorem~\ref{Legendre}. 

\begin{exercise} (i) Recall that if $p$
is a prime, then the
multiplicative group $({\mathbb Z}_{p}\setminus\{0\},\times)$
is cyclic. (This is the subject of Exercise~\ref{cyclic in field}.)
Deduce that if $p=4k+1$, then there is an element $u$ in 
$({\mathbb Z}_{p}\setminus\{0\},\times)$ of order $4$.
Show that $u^{2}=-1$.

(ii) If $p$ and $u$ are as in (i) show that
\[\Lambda=\{(n,m)\in{\mathbb Z}^{2}
\, : \,
m\equiv n \bmod p\}\]
is a lattice and deduce that there exist $n$, $m$ with
$n^{2}+m^{2}=p$. (This is a result of Fermat. Every
prime congruent to $1$ modulo $4$ is the sum of two
squares.)
\end{exercise}
\section{A brief look at Fourier transforms}\label{brief}
If time permits
we will look at Fourier transforms in sections~\ref{Heisenberg section}
and~\ref{Poisson section}. If $f:{\mathbb R}\rightarrow{\mathbb C}$
is integrable on each finite interval $[a,b]$
(in the Riemann or Lebesgue sense)
and $\int_{-\infty}^{\infty}|f(x)|\,dx<\infty$ 
we\footnote{The majority
of my auditors who know Lebesgue integration will
prefer the formulations `$f\in L^{1}({\mathbb R})$'
or `$f$ measurable and $\int_{-\infty}^{\infty}|f(x)|\,dx<\infty$.}
shall say that $f$ is \emph{appropriate}.
(This is non-standard notation.) 
If $f$ is appropriate define
\[\hat{f}(\lambda)
=\int_{-\infty}^{\infty}f(t)\exp(-i\lambda t)\, dt.\]

\begin{lemma} If $f$ is appropriate, 
$\hat{f}:{\mathbb R}\rightarrow{\mathbb C}$ is continuous
and bounded.
\end{lemma}
Our first problem is that even when $f$ is appropriate
$\hat{f}$ need not be.
\begin{example} If $f$ is the indicator function of
$[a,b]$ (that is, $f(x)=1$ if $x\in [a,b]$, $f(x)=0$
otherwise), then 
\[\int_{-\infty}^{\infty}|\hat{f}(\lambda)|\,d\lambda=\infty.\]
\end{example} 
This turns out not to matter very much but should be borne in mind.

When we try to imitate our treatment of Fourier series
we find that we need to interchange the order of
integration of two infinite integrals. If we use
Lebesgue integration we can use a very powerful
theorem.
\begin{theorem}[Fubini's theorem] If 
$f:{\mathbb R}^{2}\rightarrow{\mathbb C}$ is measurable
and either of the two integrals
\[\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}|f(x,y)|\,dx\,dy\ \text{and}
\ \int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}|f(x,y)|\,dy\,dx\]
exists and is finite, then they both do and the integrals 
\[\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}f(x,y)\,dx\,dy\ \text{and}
\ \int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}f(x,y)\,dy\,dx.\]
exist and are finite and equal.
\end{theorem}

If we use Riemann integration, then we have
a slogan.
\begin{pretheorem} If 
$f:{\mathbb R}^{2}\rightarrow{\mathbb C}$ is well behaved
and either of the two integrals
\[\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}|f(x,y)|\,dx\,dy\ \text{and}
\ \int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}|f(x,y)|\,dy\,dx\]
is finite, then they both are and
\[\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}f(x,y)\,dx\,dy\ \text{and}
\ \int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}f(x,y)\,dy\,dx.\]
\end{pretheorem}
In every case that we need the pretheorem can be turned
into a theorem but the proofs become more and more tedious
as we weaken the conditions on $f$.

\begin{exercise}\label{Riemann meets Fubini} In this exercise
we use Riemann integration
and derive a simple Fubini type theorem.

(i) If $I$ and $J$ are intervals on ${\mathbb R}$ (so
$I$ could have the form $[a,b]$, $[a,b)$, $(a,b)$
or $(a,b)$) and we write ${\mathbb I}_{I\times J}(x,y)=1$
if $(x,y)\in I\times J$, ${\mathbb I}_{I\times J}(x,y)=0$,
otherwise show that
\[\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}{\mathbb I}_{I\times J}(x,y)\,dx\,dy= 
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}{\mathbb I}_{I\times J}(x,y)\,dy\,dx.\]

(ii) Suppose that $I_{r}$ 
and $J_{r}$ are intervals on ${\mathbb R}$
and that $\lambda_{r}\in{\mathbb C}$ $[1\leq r\leq n]$.
If $f=\sum_{r=1}^{n}{\mathbb I}_{I_{r}\times J_{r}}$
show that
\[\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}f(x,y)\,dx\,dy=
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}f(x,y)\,dy\,dx.\]

(iii) Suppose that $f:{\mathbb R}^{2}\rightarrow{\mathbb C}$ is continuous
and that  $I$ and $J$ are intervals on ${\mathbb R}$. If
$g(x,y)={\mathbb I}_{I\times J}(x,y)f(x,y)$ show
using (ii) that
\[\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}g(x,y)\,dx\,dy=
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}g(x,y)\,dy\,dx.\]

(iv) Suppose that $f:{\mathbb R}^{2}\rightarrow{\mathbb C}$ is continuous
and that there exists a real constant $A$ such that
\begin{equation*}
|f(x,y)|\leq A(1+x^{2})^{-1}(1+y^{2})^{-1}.
\tag*{$\bigstar$}
\end{equation*}
Show, using (ii), that
\[\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}f(x,y)\,dx\,dy=
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}f(x,y)\,dy\,dx.\]
\end{exercise}
Conditions like $\bigstar$ imposing some rate of decrease
at infinity play an important role in Fourier
analysis.

In section~\ref{Heisenberg section} (if we reach it)
we shall adopt a slightly more sophisticated approach
to the Fourier transform than that given in the next
exercise but the results are sufficient for many purposes
and the exercise gives an interesting review of
earlier work. We shall need the following definition.
\begin{definition} We say that $f$ is piecewise
continuous if, for each $R>0$, $f$ is continuous
at all but finitely many points of $(-R,R)$ and
$f(t)=\lim_{h\rightarrow 0+}f(t-h)$ for all $t$.
\end{definition}
(Different authors use different definitions. They
are all the same `in spirit' but not `in logic'.) 
\begin{exercise}\label{Crude uniqueness}
If $f$ is appropriate and $R>0$ we define
\[\sigma_{R}(f,t)=\frac{1}{2\pi}\int_{-R}^{R}
\left(1-\frac{|\lambda|}{R}\right)\hat{f}(\lambda)
\exp(i\lambda t)\,d\lambda.\]
(It will become clear that this is the analogue of
the Fej\'{e}r sum.)

(i) (For users of the Lebesgue integral)
By adapting the proof
of Theorem~\ref{Fejer convergence} show that
if $f\in L^{1}$ and $f$ is continuous at $t$
then
\[\sigma_{R}(f,t)\rightarrow f(t)\]
as $R\rightarrow\infty$. Is the result necessarily true
if $f$ is not continuous at $t$? Give reasons.

(i') (For users of the Riemann integral) By adapting the proof
of Theorem~\ref{Fejer convergence} show that
if $f$ is continuous and there
there exists a real constant $A$ such that
\[|f(x)|\leq A(1+x^{2})^{-1}\]
for all $x$, show that
\[\sigma_{R}(f,t)\rightarrow f(t)\]
for all $t$.

Without going into detail, convince yourself that
the result continues to hold if we replace
the condition `$f$ continuous' by the condition
`$f$ piecewise continuous'  and the conclusion by
\[\sigma_{R}(f,t)\rightarrow f(t)\]
at all $t$ where $f$ is continuous.
(All we need is a slight extension of 
Exercise~\ref{Riemann meets Fubini}~(iv).)

(ii) (For users of the Lebesgue integral)
Suppose that $f$ and $g$
are piecewise continuous $L^{1}$
functions. Show that, if $\hat{f}(\lambda)=\hat{g}(\lambda)$
for all $\lambda$, then $f(t)=g(t)$ for all $t$.

(ii') (For users of the Riemann integral) 
Suppose $f$ and $g$ are piecewise continuous 
and there
there exists a real constant $A$ such that
\[|f(x)|,|g(x)|\leq A(1+x^{2})^{-1}\]
for all $x$. Show that, if $\hat{f}(\lambda)=\hat{g}(\lambda)$
for all $\lambda$, then $f(t)=g(t)$ for all $t$.
\end{exercise} 



\section{Infinite products} Our object in the next few
lectures will be to prove the following remarkable
theorem of Dirichlet on primes in arithmetic
progression.
\begin{theorem}[Dirichlet]\label{Dirichlet} 
If $a$ and $d$ are strictly
positive coprime integers, then there are infinitely
many primes of the form $a+nd$ with $n$ a positive integer.
\end{theorem}
(Obviously the result must fail if $a$ and $d$ are not
coprime.) 

There exist a variety of proofs of special cases
when $d$ has particular values but, so far as I know,
Dirichlet's proof of his theorem remains, essentially,
the only approachable one. In particular there is no
known reasonable\footnote{In the sense that most
reasonable people would call reasonable. Selberg produced
a (technically)  elementary proof which may be found in his
collected works.} elementary
(in the technical sense of not using analysis) 
proof.

Dirichlet's method starts from an observation of Euler.
\begin{lemma}\label{Euler prime start}
If $s$ is real with $s>1$, then
\[\prod_{\substack{\text{$p$ prime}\\p\leq N}}
\left(1-\frac{1}{p^{s}}\right)^{-1}\rightarrow
\sum_{n=1}^{\infty}\frac{1}{n^{s}}.\]
\end{lemma}
Using this result, we get a new proof of the existence
of an infinity of primes.
\begin{theorem}[Euclid] There exist an infinity of primes.
\end{theorem}

This suggests that it may be worth investigating 
infinite products a bit more.
\begin{definition} Let $a_{j}\in{\mathbb C}$. If
$\prod_{n=1}^{N}(1+a_{n})$ tends to a limit $L$
as $N\rightarrow\infty$, we say that the 
\emph{infinite product} $\prod_{n=1}^{\infty}(1+a_{n})$
\emph{converges} to a value $L$ and write
\[\prod_{n=1}^{\infty}(1+a_{n})=L.\]
If the infinite product $\prod_{n=1}^{\infty}(1+|a_{n}|)$
converges, then we say that $\prod_{n=1}^{\infty}(1+a_{n})$
is \emph{absolutely convergent}.
\end{definition}
The next result was removed from the first year of
the Tripos a couple of years before I took it.
\begin{lemma}\label{absolute convergence} 
Let $a_{j}\in{\mathbb C}$.

(i) $\prod_{n=1}^{\infty}(1+a_{n})$ is absolutely
convergent if and only if $\sum_{n=1}^{\infty}a_{n}$
is.

(ii) If $\prod_{n=1}^{\infty}(1+a_{n})$ is absolutely
convergent and $1+a_{n}\neq 0$ for each $n$, then
the infinite product converges and
\[\prod_{n=1}^{\infty}(1+a_{n})\neq 0.\]
\end{lemma}
\begin{exercise} Find $a_{j}\in{\mathbb C}$ such that
$\prod_{n=1}^{\infty}(1+a_{n})$ is not absolutely
convergent but is convergent to a non-zero value.
\end{exercise}
We shall only make use of absolute convergent infinite
products.
\begin{exercise} If $\prod_{n=1}^{\infty}(1+a_{n})$ is 
absolutely convergent and 
$\sigma:{\mathbb N}\rightarrow{\mathbb N}$
is a bijection 
(that is, $\sigma$ is a permutation of ${\mathbb N}$)
show that $\prod_{n=1}^{\infty}(1+a_{\sigma(n)})$ is 
absolutely convergent and
\[\prod_{n=1}^{\infty}(1+a_{\sigma(n)})
=\prod_{n=1}^{\infty}(1+a_{n})\]
\end{exercise}
Whilst this is a useful result to know, we shall make no essential
use of it. When we write
$\sum_{\text{$p$ prime}}$ or $\prod_{\text{$p$ prime}}$
we mean the primes $p$ to be taken in order of increasing
size.

Using Lemma~\ref{absolute convergence} we obtain
the following strengthening of Euclid's theorem.
\begin{theorem}[Euler]
${\displaystyle \sum_{\text{$p$ prime}}\frac{1}{p}=\infty.}$
\end{theorem}

Since we wish to consider infinite products of functions
it is obvious that we shall need an analogue
of the Weierstrass M-test for products, obvious what that
analogue should be and obvious how to prove it.
\begin{lemma} Suppose $U$ is an open subset of ${\mathbb C}$
and that we have a sequence of functions 
$g_{n}:U\rightarrow{\mathbb C}$ and a sequence of positive
real numbers $M_{n}$ such that $M_{n}\geq |g_{n}(z)|$
for all $z\in U$. If $\sum_{n=1}^{\infty}M_{n}$
converges, then $\prod_{n=1}^{N}(1+g_{n}(z))$ converges
uniformly on $U$.
\end{lemma}
Later we shall need to consider $\sum n^{-s}$ with $s$ complex.
To avoid ambiguity, we shall take $n^{-s}=\exp(-s\log n)$
where $\log n$ is the real logarithm of $n$.
\begin{lemma} If $\Re s>1$ we have
\[\prod_{\text{$p$ prime}}(1-p^{-s})^{-1}=\sum_{n=1}^{\infty}n^{-s}\]
both sides being absolutely convergent for each $s$
and uniformly convergent for $\Re s>1+\epsilon$ for
each fixed $\epsilon>0$.
\end{lemma}

We now detour briefly from the main argument to show
how infinite products can be used to answer a
very natural question. `Can we always find an analytic
function
with specified zeros?' (We count multiple zeros
multiply in the usual way.) Naturally we need to
take account of the following fact.
\begin{lemma} If $z_{1}$, $z_{2}$, \dots are distinct zeros
of an analytic function which is not identically
zero, then $z_{n}\rightarrow\infty$ as $n\rightarrow\infty$.
\end{lemma}

A little thought suggests the path we ought to take,
though we may not see how to reach it. A way to
reach the path is provided by the Weierstrass
primary function $E(z,m)$.
\begin{definition} If $m$ is a strictly positive integer
\[E(z,m)=(1-z)e^{z+z^{2}/2+z^{3}/3+\dots+z^{m}/m}.\]
\end{definition}
\begin{lemma} The function $E(\ ,m):\mathbb{C}\rightarrow\mathbb{C}$
is analytic with a unique zero at 1. Given $\epsilon>0$
we can find an $M$ such that
\[|1-E(z,m)|\leq \epsilon\]
for all $m\geq M$ and $|z|\leq 1/2$.
\end{lemma}


\begin{theorem}[Weierstrass] If $k$ is a positive integer
and $z_{1},z_{2},\dots$ is a sequence of non-zero complex
numbers with $z_{n}\rightarrow\infty$, then we can choose
$n(j)\rightarrow\infty$ so that
\[F(z)=z^{k}\prod_{j=1}^{\infty}E\big(z/z_{j},n(j)\big)\]
is a well defined
analytic function with a zero of order $k$ at $0$,
and zeros at the $z_{j}$ (multiple zeros counted multiply)
and no others.
\end{theorem}
\begin{lemma} If $f_{1}$ and $f_{2}$ are analytic functions
on ${\mathbb C}$ with the same zeros (multiple zeros counted
multiply), then there exists an analytic function $g$
such that
\[f_{1}(z)=e^{g(z)}f_{2}(z).\]
\end{lemma}
\begin{lemma} If $z_{1},z_{2},\dots$ and $w_{1},w_{2},\dots$
are sequences of complex numbers with 
$z_{j},w_{j}\rightarrow\infty$ as $j\rightarrow\infty$
and $z_{j}\neq w_{k}$ for all $j,k$, then there exists
a meromorphic function with zeros at the $z_{j}$ 
and poles at the $w_{k}$ (observing the usual multiplicity
conventions).
\end{lemma}
\noindent{\bf Extra exercise}
\emph{(i) Show that we can find an $A$ such that
\[|1-E(z,m)|\leq A|z|^{m+1}\]
for $|z|\leq 1/2$.}

\emph{(ii) If $k$ is a positive integer
and $z_{1},z_{2},\dots$ is a sequence of non-zero complex
numbers with $z_{n}\rightarrow\infty$, then
\[F(z)=z^{k}\prod_{j=1}^{\infty}E(z/z_{j},j)\]
is a well defined
analytic function with a zero of order $k$ at $0$,
and zeros at the $z_{j}$ (multiple zeros counted multiply)
and no others.}

\begin{exercise} (It may be helpful to attack parts
of this question non-rigorously first and then tighten up
the argument second.)

(i) If $C_{N}$ is the contour consisting of the square
with vertices 
\[\pm (N+1/2)\pm (N+1/2)i\] 
described
anti-clockwise, show that there is a constant $K$
such that
\[|\cot \pi z|\leq K\]
for all $z\in C_{N}$ and all integers $N\geq 1$.

(ii) By integrating an appropriate function round the
contour $C_{N}$, or otherwise,
show that, if $w\notin{\mathbb Z}$,
\[\sum_{n=-N}^{n=N}\frac{1}{w-n}\rightarrow  \pi\cot\pi w.\]

(iii) Is it true that, if $w\notin{\mathbb Z}$,
\[\sum_{n=-M}^{n=N}\frac{1}{w-n}\rightarrow  \pi\cot\pi w,\]
as $M,N\rightarrow\infty$? Give reasons.

(iv) Show that
\[P(z)=z\prod_{n=1}^{\infty}\left(1-\frac{z^{2}}{n^{2}}\right)\]
is a well defined analytic function and that there exists
an analytic function $g$ such that
\[\sin\pi z=e^{g(z)}P(z).\]

(v) Find a simple expression for
$P'(z)/P(z)$. 

\noindent[Hint: If $p(z)=\prod_{j=1}^{N}(z-\alpha_{j})$,
what is $p'(z)/p(z)$?]
 
Find a related expression for
$\frac{d\ }{dz}\sin \pi z/\sin \pi z$.

(vi) Show that
\[\sin\pi z=\pi z\prod_{n=1}^{\infty}\left(1-\frac{z^{2}}{n^{2}}\right).\]

(vii) Find a similar expression for $\cos\pi z$. (These
results are due to Euler.)
\end{exercise}
\begin{exercise} (This makes use of some of the
techniques of the previous exercise.) (i) Show that the infinite product
\[g(z)=\prod_{n=1}^{\infty}e^{z/n}\left(1-\frac{z}{n}\right)\]
exists and is analytic on the whole complex plane.

(ii) Show that
\[g'(z)=g(z)
\sum_{n=1}^{\infty}\left(\frac{1}{z-n}+\frac{1}{n}\right).\]
Explain why $\sum_{n=1}^{\infty}(\frac{1}{z-n}+\frac{1}{n})$
is indeed a well defined analytic function on 
${\mathbb C}\setminus{\mathbb Z}$.

(iii) By using (ii), or otherwise, show that
\begin{equation*}
g(z+1)=-Azg(z) \tag*{($*$)}
\end{equation*}
for some constant $A$.

(iv) By considering a particular value of $z$, or otherwise,
show that $A$ is real and positive and
\[\sum_{n=1}^{N}\frac{1}{n}-\log N\rightarrow \log A\]
as $N\rightarrow\infty$.
Deduce the existence of Euler's constant
\[\gamma=\lim_{N\rightarrow\infty}
\left(\sum_{n=1}^{N}n^{-1}-\log N\right)\]
and rewrite $(*)$ as
\begin{equation*}
g(z+1)=-e^{\gamma}zg(z). 
\end{equation*}

(v) Find a simple expression for $zg(z)g(-z)$. Use $(*)$
to show that $\sin \pi z$ is periodic.
\end{exercise}
\section{Fourier analysis on finite Abelian groups} One
of Dirichlet's main ideas is a clever extension of 
Fourier analysis from its classical frame. Recall
that classical Fourier analysis deals with formulae
like
\[f(t)=\sum_{n=-\infty}^{\infty}\hat{f}(n)e_{n}(t)\]
where $e_{n}(t)=\exp(int)$. The clue to further extension
lies in the following observation.
\begin{lemma} Consider the Abelian group 
${\mathbb T}={\mathbb R}/(2\pi{\mathbb Z})$ and the
subgroup $S=\{z:|z|=1\}$ of $({\mathbb C}\setminus\{0\},\times)$.
The continuous homomorphisms 
$\theta:{\mathbb T}\rightarrow S$ are precisely
the functions $e_{n}:{\mathbb T}\rightarrow S$ given
by $e_{n}(t)=\exp(int)$ with $n\in{\mathbb Z}$.
\end{lemma}
\begin{exercise} (i) Find (with proof) all the
continuous homomorphisms 
\[\theta:({\mathbb R},+)\rightarrow (S,\times).\]
What is the connection with Fourier transforms?

(ii) (Only for those who know Zorn's lemma%
\footnote{And, particularly, those who only know Zorn's lemma.}.)
Assuming Zorn's lemma show that any linearly independent
set in a vector space can be extended
to a basis. If we consider ${\mathbb R}$ as a vector
space over ${\mathbb Q}$, show that there exists
a linear map $T:{\mathbb R}\rightarrow{\mathbb R}$ such
that $T(1)=1$, $T(\surd 2)=0$. Deduce the existence
of a function $T:{\mathbb R}\rightarrow{\mathbb R}$
such that $T(x+y)=T(x)+T(y)$ for all $x,y\in {\mathbb R}$
which is not continuous (with respect to the usual metric).
Show that, if we accept Zorn's lemma, there exist
discontinuous homomorphisms
$\theta:({\mathbb R},+)\rightarrow (S,\times)$.
\end{exercise}

This suggests the following definition.
\begin{definition} If $G$ is a finite Abelian group,
we say that a homomorphism $\chi:G\rightarrow S$
is a \emph{character}. We write $\hat{G}$ for the
collection of such characters.
\end{definition}
In this section we shall accumulate a
substantial amount of information about $\hat{G}$
by a succession of small steps.

\begin{lemma} Let $G$ be a finite Abelian group.

(i) If $x\in G$ has order $m$ and $\chi\in\hat{G}$,
then $\chi(x)$ is an $m$th root of unity.

(ii) $\hat{G}$ is a finite Abelian group under
pointwise multiplication. 
\end{lemma}

To go further we consider, for each finite Abelian group
$G$, the collection $C(G)$ of functions $f:G\rightarrow{\mathbb C}$.
If $G$ has order $|G|$, then $C(G)$ is a vector space of 
dimension $|G|$ which can be made into a  complex inner
product space by means of the inner product
\[\langle f,g\rangle=\frac{1}{|G|}\sum_{x\in G}f(x)g(x)^{*}.\]
\begin{exercise} Verify the statements just made.
\end{exercise}

\begin{lemma} Let $G$ be a finite Abelian group.
The elements of $\hat{G}$ form an orthonormal
system in  $C(G)$.
\end{lemma}
Does $\hat{G}$ form an orthonormal basis of $C(G)$? The
next lemma tells us how we may hope to resolve this
question.
\begin{lemma} Let $G$ be a finite Abelian group.
The elements of $\hat{G}$  form an orthonormal basis
if and only if, given an element $x\in G$ which is
not the identity, we can find a character $\chi$
with $\chi(x)\neq 1$.
\end{lemma}

The way forward is now clear.
\begin{lemma} 
Suppose that $H$ is a subgroup of a finite Abelian
group  $G$ and that $\chi\in\hat{H}$. If $K$
is a subgroup of $G$ generated by $H$ and an element
$a\in G$, then we can find a $\tilde{\chi}\in\hat{K}$
such that $\tilde{\chi}|H=\chi$.
\end{lemma}
\begin{lemma} Let $G$ be a finite Abelian group
and $x$ an element of $G$ of order $m$. Then
we can find a $\chi\in\hat{G}$ with
$\chi(x)=\exp 2\pi i/m$.
\end{lemma}
\begin{theorem} If $G$ is a finite Abelian group,
then $\hat{G}$ has the same number of elements
as $G$ and they form an orthonormal basis
for $C(G)$.
\end{theorem}
\begin{lemma} If $G$ is a finite Abelian group
and $f\in C(G)$, then
\[f=\sum_{\chi\in\hat{G}}\hat{f}(\chi)\chi\]
where $\hat{f}(\chi)=\langle f,\chi \rangle$.
\end{lemma}
\begin{exercise} Suppose that $G$ is a finite Abelian
group. Show that if we define 
$\theta_{x}:\hat{G}\rightarrow {\mathbb C}$ by
$\theta_{x}(\chi)=\chi(x)$ for $\chi\in \hat{G}$,
$x\in G$, then the map $\Theta:G\rightarrow\Hat{\Hat{G}}$
given by $\Theta(x)=\theta_{x}$ is an isomorphism.

If we now identify $x$ with $\theta_{x}$ (and, so,
$G$ with $\Hat{\Hat{G}}$) show that
\[\Hat{\Hat{f}}(x)=|G|^{-1}f(x^{-1})\]
for all $f\in C(G)$ and $x\in G$.
\end{exercise}     

We have now done all that that is required to
understand Dirichlet's motivation. However, it
seems worthwhile to make a slight detour to
put `computational' bones on this section by
exhibiting the structure of $G$ and $\hat{G}$.
\begin{lemma} Let $(G,\times)$ be an Abelian group.

(i) Suppose that $x,y\in G$ have order $r$ and $s$
with $r$ and $s$ coprime. Then $xy$ has order $rs$.

(ii) If $G$ contains elements of order $n$ and $m$,
then $G$ contains an element of order the least
common multiple of $n$ and $m$.
\end{lemma}
\begin{lemma}\label{maximum order}
Let $(G,\times)$ be a finite Abelian group.
Then there exists an integer $N$ and an element $k$
such that $k$ has order $N$ and, whenever $x\in G$,
we have $x^{N}=e$.
\end{lemma}
\begin{exercise}\label{cyclic in field}
Let $p$ be a prime.
Use Lemma~\ref{maximum order}
together with the fact that a polynomial of degree
$k$ can have at most $k$ roots to show that the
multiplicative group $({\mathbb Z}_{p}\setminus\{0\},\times)$
is cyclic.
\end{exercise}
\begin{lemma} With the hypotheses and notation
of Lemma~\ref{maximum order}, we can write $G=K\times H$
where $K$ is the cyclic group generated by $x$
and $H$ is another subgroup of $K$.
\end{lemma}
As usual we write $C_{n}$ for the cyclic group
of order $n$.
\begin{theorem} If $G$ is a finite Abelian group, we can find
$n(1)$, $n(2)$, \dots $n(m)$ with $n(j+1)$ dividing $n(j)$
such that $G$ is isomorphic to 
\[C_{n(1)}\times C_{n(2)}\times \dots C_{n(m)}.\]
\end{theorem}
\begin{lemma} If we have two sequences
$n(1)$, $n(2)$, \dots $n(m)$ with $n(j+1)$ dividing $n(j)$
and
$n'(1)$, $n'(2)$, \dots $n'(m')$ with $n'(j+1)$ dividing $n'(j)$,
then
\[C_{n(1)}\times C_{n(2)}\times \dots C_{n(m)}
\ \text{is isomorphic to}
\ C_{n'(1)}\times C_{n'(2)}\times \dots C_{n'(m')}\]
if and only if $m=m'$ and $n(j)=n'(j)$ for each $1\leq j\leq m$.
\end{lemma}

It is easy to identify $\hat{G}$.
\begin{lemma} Suppose that
\[G=C_{n(1)}\times C_{n(2)}\times \dots C_{n(m)}\]
with $C_{n(j)}$ a cyclic group of order $n(j)$ generated
by $x_{j}$. Then the elements of $\hat{G}$ have the
form 
$\chi_{\omega_{n(1)}^{r(1)},\omega_{n(2)}^{r(2)}},\dots
_{\omega_{n(m)}^{r(m)}}$ with $\omega_{n(j)}=\exp\big(2\pi i/n(j)\big)$
and
\[\chi_{\omega_{n(1)}^{r(1)},\omega_{n(2)}^{r(2)}},\dots
\omega_{n(m)}^{r(m)}(x_{1}^{s(1)}x_{2}^{s(2)}\dots x_{m}^{s(m)})
=\omega_{n(1)}^{r(1)s(1)}\omega_{n(2)}^{r(2)s(2)}\dots
\omega_{n(m)}^{r(m)s(m)}.\]
\end{lemma}
My readers will see that $\hat{G}$ is isomorphic to $G$,
but the more sophisticated algebraists will also
see that this is \emph{not a natural isomorphism}
(whereas $G$ and $\Hat{\Hat{G}}$ are \emph{naturally isomorphic}).
Fortunately such matters are of no importance
for the present course.
\section{The Euler-Dirichlet formula} Dirichlet was interested
in a particular group. If $d$ is a positive integer consider
${\mathbb Z}/(d)$ the set of equivalence classes
\[[m]=\{r:r\equiv m \mod{d}\}\]
under the usual multiplication modulo $d$.
We set
\[G_{d}=\{[m]:\text{$m$ and $d$ coprime}\}\]
and write $\phi(d)$ for the order of $G_{d}$ ($\phi$ is
called Euler's totient function).
\begin{lemma} The set $G_{d}$ forms a finite
Abelian group under
standard multiplication.
\end{lemma}
The results of the previous section show that, if $[a]\in G_{d}$
and we define $\delta_{a}:G_{d}\rightarrow{\mathbb C}$ by
\begin{align*}
\delta_{a}([a])&=1\\
\delta_{a}([m])&=0\qquad \text{if $[m]\neq [a]$},
\end{align*}
then
\[\delta_{a}=\phi(d)^{-1}\sum_{\chi\in G_{d}}\chi([a])^{*}\chi.\]

We now take up the proof of Dirichlet's theorem in earnest.
We shall operate under the standing assumption that $a$ and $d$  
are positive coprime integers and our object is to show
that the sequence
\[a,\ a+d,\ a+2d,\ \dots, a+nd,\ \dots\]
contains infinitely many primes. Following
Euler's proof that there exist infinitely many primes
we shall seek to prove this by showing that
\[\sum_{\substack{\text{$p$ prime}\\p=a+nd\ \text{for some $n$}}}
\frac{1}{p}=\infty.\]
Henceforward, at least in the number theory part of the 
course $p$ will be a prime,  $\sum_{p}$ will mean the
sum over all primes and so on.

In order to simplify our notation it will also be convenient
to modify the definition of a character. From now on, we say
that $\chi$ is a character if $\chi$ is a map from
${\mathbb N}$ to ${\mathbb C}$ such that there exists a character
(in the old sense) $\tilde{\chi}\in \hat{G}_{d}$
with
\begin{alignat*}{2}
\chi(m)&=\tilde{\chi}([m])&&\qquad\text{if $m$ and $d$ are coprime}\\
\chi(m)&=0&&\text{otherwise}.
\end{alignat*}
We write $\sum_{\chi}$ to mean the sum over all characters
and take $\chi_{0}$ to be the character with
$\chi_{0}([m])=1$ whenever $m$ and $d$ are coprime.

\begin{lemma} (i) If $\chi$ is a character, then
$\chi(m_{1}m_{2})=\chi(m_{1})\chi(m_{2})$
for all $m_{1},m_{2}\geq 0$.

(ii) If $\chi\neq\chi_{0}$, then
$\sum_{m=k+1}^{k+d}\chi(m)=0$.

(iii) If $\delta_{a}(m)=\phi(d)^{-1}\sum_{\chi}\chi(a)^{*}\chi(m)$
then $\delta_{a}(m)=1$ when $m=a+nd$ and $\delta_{a}(m)=0$
otherwise.

(iv) $\displaystyle{\sum_{p=a+nd}p^{-s}
=\phi(d)^{-1}\sum_{\chi}\chi(a)^{*}\sum_{p}\chi(p)p^{-s}}$.
\end{lemma}
\begin{lemma} The sum $\sum_{p=a+nd}p^{-1}$ diverges if
$\sum_{p}\chi(p)p^{-s}$ remains bounded
as $s$ tends to $1$ through real values of $s>1$
for all $\chi\neq\chi_{0}$.
\end{lemma}

We now prove a new version of Euler's formula.
\begin{theorem}[Euler-Dirichlet formula]
With the notation of this section,
\[\prod_{p}(1-\chi(p)p^{-s})^{-1}=
\sum_{n=1}^{\infty}\chi(n)n^{-s},\]
both sides being absolutely convergent for $\Re s>1$.
\end{theorem}
To link $\prod_{p}(1-\chi(p)p^{-s})^{-1}$
with $\sum_{p}\chi(p)p^{-s}$ we use logarithms.
(If you go back to our discussion of infinite products,
you will see that this is not unexpected.) However,
we must, as usual, be careful when choosing our logarithm
function. For the rest of the argument, $\log$
will be the function on
\[{\mathbb C}\setminus\{x:\text{$x$ real and $x\leq 0$}\}\]
defined by $\log (re^{i\theta})=\log r+i\theta$
[$r>0$, $-\pi<\theta<\pi$].
\begin{lemma} (i) If $|z|\leq 1/2$ , then $|\log(1-z)+z|\leq |z|^{2}$.

(ii) If $\epsilon>0$, then $\sum_{p}\log(1-\chi(p)p^{-s})$
and $\sum_{p}\chi(p)p^{-s}$ converge uniformly in 
$\Re s\geq 1+\epsilon$, whilst
\[\left|\sum_{p}\log(1-\chi(p)p^{-s})+\sum_{p}\chi(p)p^{-s}\right|
\leq \sum_{n=1}^{\infty}n^{-2}.\]
\end{lemma}

We have thus shown that if $\sum_{p}\log(1-\chi(p)p^{-s})$
remains bounded as $s\rightarrow 1+$ , then $\sum_{p}\chi(p)p^{-s}$
does. Unfortunately it is not possible to equate 
$\sum_{p}\log(1-\chi(p)p^{-s})$
with $\log(\prod_{p}(1-\chi(p)p^{-s})^{-1})$.

However, we can refresh our spirits by proving Dirichlet's
theorem in some special cases.
\begin{example} There are an infinity of primes 
of the form $3n+1$ and $3n+2$.equate 
$\sum_{p}\log(1-\chi(p)p^{-s})$
with $\log(\prod_{p}(1-\chi(p)p^{-s})^{-1})$.
\end{example}
\begin{exercise} Use the same techniques to show that
there are an infinity of primes 
of the form $4n+1$ and $4n+3$.
\end{exercise} 
\section{Analytic continuation of the Dirichlet functions}
Dirichlet completed his argument withouequate 
$\sum_{p}\log(1-\chi(p)p^{-s})$
with $\log(\prod_{p}(1-\chi(p)p^{-s})^{-1})$.t having to consider
$\sum_{n=1}^{\infty}\chi(n)n^{-s}$ for anything other
than real $s$ with $s>1$. However, as we have already seen,
$\sum_{n=1}^{\infty}\chi(n)n^{-s}=L(s,\chi)$ is defined and well
behaved in $\Re s>1$. Riemann showed that it is advantageous
to extend the definition of analytic
functions like $L(s,\chi)$
to larger domains.

There are many ways of obtaining such 
\emph{analytic continuations}. Here is one.
\begin{lemma} If $f:{\mathbb R}\rightarrow{\mathbb C}$
is bounded on ${\mathbb R}$ and 
locally integrable\footnote{Riemann or Lebesgue at
the reader's choice}, then
\[F(s)=\int_{1}^{\infty}f(x)x^{-s}\,dx\]
is a well defined analytic function on the set of $s$ with
$\Re s>1$.equate 
$\sum_{p}\log(1-\chi(p)p^{-s})$
with $\log(\prod_{p}(1-\chi(p)p^{-s})^{-1})$.
\end{lemma}
\begin{lemma}\label{Extend Dirichlet 1} 
(i) If $\chi\neq \chi_{0}$ and 
$S(x)=\sum_{1\leq m\leq x}\chi(m)$, then 
$S:{\mathbb R}\rightarrow{\mathbb C}$ is bounded
and locally integrable. We have
\[\sum_{n=1}^{N}\chi(n)n^{-s}
\rightarrow s\int_{1}^{\infty}S(x)x^{-s-1}\, dx\]
as $N\rightarrow\infty$ for all $s$ with $\Re s>1$.

(ii) If $S_{0}(x)=0$ for $x\leq 0$ and
$S_{0}(x)=\sum_{1\leq m\leq x}\chi_{0}(m)$, then,
writing 
\[T_{0}(x)=S_{0}(x)-d^{-1}\phi(d)x,\]
we see that 
$T_{0}:{\mathbb R}\rightarrow{\mathbb R}$ is bounded
and locally integrable. We have
\[\sum_{n=1}^{N}\chi(n)n^{-s}\rightarrow 
s\int_{1}^{\infty}T_{0}(x)x^{-s-1}\, dx+\frac{\phi(d)s}{d(s-1)}\]
as $N\rightarrow\infty$ for all $s$ with $\Re s>1$.
\end{lemma}
\begin{lemma}\label{Extend Dirichlet 2}
(i) If $\chi\neq\chi_{0}$, there exists an 
function $L(s,\chi)$
analytic on $\{s\in{\mathbb C}:\Re s>0\}$
such that
$\sum_{n=1}^{\infty} \chi(n)n^{-s}$ converges to
$L(s,\chi)$ on
$\{s\in{\mathbb C}:\Re s>1\}$.

(ii) There exists a meromorphic function $L(s,\chi_{0})$
analytic on $\{s\in{\mathbb C}:\Re s>0\}$ except for
a simple pole, residue $\phi(d)/d$ at $1$ such that
$\sum_{n=1}^{\infty} \chi_{0}(n)n^{-s}$ converges to 
$L(s,\chi)$ for $\Re s>1$.
\end{lemma}
\begin{exercise}
(i) Explain carefully why
$L(\ ,\chi_{0})$ is defined uniquely by
the conditions given.

(ii) Show that
$\sum_{n=1}^{\infty} \chi_{0}(n)n^{-s}$ diverges
for $s$ real and $1\geq s >0$.
\end{exercise}

We now take up from where we left off at the end 
of the previous section.
\begin{lemma}  (i) If $\Re s>1$, then
$\exp(-\sum_{p}\log(1-\chi(p)p^{-s})=L(s,\chi)$.

(ii) If $\Re s>1$, then $L(s,\chi)\neq 0$.

(iii) There exists a function $\Log L(s,\chi)$ analytic
on $\{s: \Re s>1\}$ such that $\exp(\Log L(s,\chi))=L(s,\chi)$
for all $s$ with $\Re s>1$.

(iv) If $\chi\neq \chi_{0}$ and
$L(1,\chi)\neq 0$, then $\Log L(s,\chi))$ tends to
a finite limit as $s\rightarrow 1$ through real values with $s>1$.

(v) There is a fixed integer $M_{\chi}$ such that
\[\Log L(s,\chi)+\sum_{p}\log(1-\chi(p)p^{-s})=2\pi M_{\chi}\]
for all $\Re s>1$.

(vi) If $\chi\neq\chi_{0}$ and $L(1,\chi)\neq 0$, then
$\sum_{p}\chi(p)p^{-s}$ remains bounded as 
$s\rightarrow 1$ through real values with $s>1$.
\end{lemma}

We mark our progress with a theorem.
\begin{theorem} If $L(1,\chi)\neq 0$ for all $\chi\neq\chi_{0}$
then there are an infinity of primes of the form
$a+nd$.
\end{theorem}

Since it is easy to find the characters $\chi$ in any given case
and since it is then easy to compute $\sum_{n=1}^{N}\chi(n)n^{-1}$
and to estimate the error $\sum_{n=N+1}^{\infty}\chi(n)n^{-1}$
to sufficient accuracy to prove that
$L(1,\chi)=\sum_{n=1}^{\infty}\chi(n)n^{-1}\neq 0$,
it now becomes possible to prove Dirichlet's theorem
for any particular coprime  $a$ and $d$.
\begin{exercise} Choose $a$ and $d$ and carry out the
program just suggested.
\end{exercise}
However, we still need to show that the  theorem holds
in all cases.
\section{$L(1,\chi)$ is not zero} Our first steps are easy.
\begin{lemma} (i) If $s$ is real and $s>1$, then
\[\prod_{\chi}L(s,\chi)=
\exp(-\sum_{p}\sum_{\chi}\log(1-\chi(p)p^{-s}).\]

(ii) If $s$ is real and $s>1$, then $\prod_{\chi}L(s,\chi)$
is real and $\prod_{\chi}L(s,\chi)\geq 1$.

(iii) $\prod_{\chi}L(s,\chi)\nrightarrow 0$ as
$s\rightarrow 1$.
\end{lemma}
\begin{lemma} (i) There can be at most one character
$\chi$ with $L(1,\chi)=0$.

(ii) If a character $\chi$ takes non-real values
then $L(1,\chi)\neq 0$.
\end{lemma}

We have thus reduced the proof of Dirichlet's theorem
to showing that if  $\chi$ is a character
with $\chi\neq \chi_{0}$ which only
takes the values $1$, $-1$ and $0$, then $L(1,\chi)\neq 0$.
There are several approaches to this problem, but
none are short and transparent. We use a proof
of de la Vall{\'{e}}e Poussin which is quite short,
but not, I think, transparent.
\begin{lemma}\label{Smoke 1} Suppose that the
character $\chi\neq \chi_{0}$ and only
takes the values $1$, $-1$ and $0$. Set
\[\psi(s)=\frac{L(s,\chi)L(s,\chi_{0})}{L(2s,\chi_{0})}.\]

(i) The function $\psi$ is well defined and meromorphic
for $\Re s>\frac{1}{2}$. It is analytic except, possibly,
for a simple pole at $1$.

(ii) If  $L(1,\chi)=0$, then $1$ is a removable singularity
and $\psi$ is analytic everywhere on $\{s:\Re s>\frac{1}{2}\}$.

(iii) We have $\psi(s)\rightarrow 0$ as $s\rightarrow \frac{1}{2}$
through real values of $s$ with $s\geq \frac{1}{2}$.
\end{lemma}
\begin{lemma}~\label{Smoke 2}
We adopt the hypotheses and notation of
Lemma~\ref{Smoke 1}. If $\Re s>1$, then the following is true.

(i) ${\displaystyle
\psi(s)=\prod_{\chi(p)=1}\frac{1+p^{-s}}{1-p^{-s}}.}$

(ii) We can find subsets $Q_{1}$ and $Q_{2}$ of $\mathbb{Z}$
such that
\begin{align*}
\prod_{\chi(p)=1}(1+p^{-s})&=\sum_{n\in Q_{1}}n^{-s}\\
\prod_{\chi(p)=1}(1-p^{-s})^{-1}&=\sum_{n\in Q_{2}}n^{-s}.
\end{align*}

(iii) There is a sequence of real positive numbers $a_{n}$
with $a_{1}=1$ such that
\[\psi(s)=\sum_{n=1}^{\infty}a_{n}n^{-s}.\]
\end{lemma}
\begin{lemma} We adopt the hypotheses and notation of
Lemmas~\ref{Smoke 1} and~\ref{Smoke 2}.

(i) If $\Re s>1$, then
\[\psi^{(m)}(s)=\sum_{n=1}^{\infty}a_{n}(-\log n)^{m}n^{-s}.\]

(ii) If $\Re s>1$, then $(-1)^{m}\psi^{(m)}(s)>0$.

(iii) If $\psi$ has no pole at $1$, then, if $\Re s_{0}>1$
and $|s-s_{0}|<\Re s_{0}-1/2$, we have
\[\psi(s)=\sum_{m=0}^{\infty}
\frac{\psi^{(m)}(s_{0})}{m!}(s-s_{0})^{m}.\]

(iv) If $\psi$ has no pole at $1$, then
$\psi(s)\nrightarrow 0$ as $s\rightarrow \frac{1}{2}$
through real values of $s$ with $s\geq \frac{1}{2}$.
\end{lemma}

We have proved the result we set out to obtain.
\begin{lemma} If a character $\chi\neq\chi_{0}$ 
only takes real values
then $L(1,\chi)\neq 0$.
\end{lemma}
\begin{theorem} If $\chi\neq\chi_{0}$, then $L(1,\chi)\neq 0$.
\end{theorem}
We have thus proved Theorem~\ref{Dirichlet}. 
If $a$ and $d$ are strictly
positive coprime integers, then there are infinitely
many primes of the form $a+nd$ with $n$ a positive integer.
\section{Chebychev and the distribution of primes}
On the strength of numerical evidence, Gauss was
led to conjecture that the number $\pi(n)$
of primes less than $n$
was approximately $n/\log n$. The theorem which
confirms this conjecture is known as the
prime number theorem.
The first real
progress in this direction was due to 
Chebychev\footnote{His preferred transliteration
seems to have been Tchebycheff, but he has
been over-ruled.}. We give his results, not out
of historical piety, but because we shall make
use of them in our proof of the prime number
theorem. (Note the obvious conventions that
$n$ is an integer with $n\geq 1$,
$\prod_{n<p\leq 2n}$ means the product over all primes 
$p$ with $n<p\leq 2n$ and so on. It is sometimes
useful to exclude small values of $n$.)
\begin{lemma}\label{Chebychev lemma} (i) ${\displaystyle
2^{n}<\binom{2n}{n}<2^{2n}}$.

(ii) ${\displaystyle \binom{2n}{n}\ \text{divides}
\ \prod_{p<2n} p^{[(\log 2n)/(\log p)]}}$
and 
${\displaystyle \prod_{n<p\leq 2n} p\ \text{divides}
\ \binom{2n}{n}}$.

(iii) We have $\pi(2n)>(\log 2)n/(\log 2n)$.

(iv)  There exists a constant $A>0$ such that
$\pi(n)\geq An(\log n)^{-1}$.

(v) There exists a constant $B'$ such that
$\sum_{p\leq n} \log p\leq B'n$.

(vi)There exists a constant $B$ such that
$\pi(n)\leq Bn(\log n)^{-1}$.
\end{lemma}
We restate the main conclusions of Lemma~\ref{Chebychev lemma}.
\begin{theorem}[Chebychev]\label{Chebychev theorem} 
There exist constants $A$ and $B$
with $0<A\leq B$ such that
\[An(\log n)^{-1}\leq\pi(n)\leq Bn(\log n)^{-1}.\]
\end{theorem}

Riemann's approach to the prime number theorem involves
considering $\theta(n)=\sum_{p\leq n}\log p$ rather than
$\pi(n)$.
\begin{lemma} Let $Q$ be a set of positive integers
and write $\alpha(n)=\sum_{q\in Q,q\leq n} 1$ 
and $\beta(n)=\sum_{q\in Q,q\leq n} \log q$.

(i) There exist constants $A$ and $B$
with $0<A\leq B$ such that
\[An(\log n)^{-1}\leq\alpha(n)\leq Bn(\log n)^{-1}.\]
if and only if there exist constants $A'$ and $B'$
with $0<A\leq B$ such that
\[A'n\leq\beta(n)\leq B'n.\]

(ii) We have $n^{-1}(\log n)\alpha(n)\rightarrow 1$
as $n\rightarrow\infty$
if and only if $n^{-1}\beta(n)\rightarrow 1$
as $n\rightarrow\infty$.
\end{lemma}
\begin{lemma}\label{logarithm transfer}
If $n^{-1}\theta(n)\rightarrow 1$
as $n\rightarrow\infty$, then
$n^{-1}(\log n)\pi(n)\rightarrow 1$
as $n\rightarrow\infty$.
\end{lemma}
\section{The prime number theorem}\label{S;prime number} 
We start by recalling various
facts about the Laplace transform.
\begin{definition}
If $a$ is a real number,
let us write ${\mathcal E}_{a}$  for the collection
of piecewise continuous
functions $F:{\mathbb R}\rightarrow{\mathbb C}$
such that $F(t)=0$ for all $t<0$ and
$F(t)e^{-at}\rightarrow 0$ as $t\rightarrow\infty$.
If $F\in {\mathcal E}_{a}$, we define the
\emph{Laplace transform} of $F$ by
\[({\mathcal L}F)(z)=\int_{-\infty}^{\infty}F(t)\exp(-zt)\,dt\]
for $\Re z>a$.
\end{definition}
\begin{lemma} If $F\in {\mathcal E}_{a}$, then 
$({\mathcal L}F)(z)$ is well defined.
\end{lemma}


\begin{lemma}\label{Laplace}
(i) If $F\in {\mathcal E}_{a}$, then 
$({\mathcal L}F)(z)$
analytic on $\{z\in{\mathbb C}:\Re z>a\}$.

(ii) We  define the Heaviside function $H$ by
writing $H(t)=0$ for $t<0$ and $H(t)=1$ for $t\geq 0$.
If $a\in {\mathbb R}$ and $b\geq 0$ set
$H_{a,b}(t)=H(t-b)e^{at}$. Then $H_{a,b}\in {\mathcal E}_{a}$
and ${\mathcal L}H_{a,b}(z)$ can be extended
to a meromorphic function on ${\mathbb C}$ with a simple
pole at $a$. 
\end{lemma}
\begin{exercise} (Uses Exercise~\ref{Crude uniqueness}.) 
Let $F,G\in {\mathcal E}_{a}$ for some $a\in{\mathbb R}$.

(i) Suppose that there exists a $b>a$ such
that $({\mathcal L}F)(z)=({\mathcal L}G)(z)$ for
all $z$ with $\Re z=b$. Show that $F=G$.

(ii) Suppose that there exist distinct $z_{n}\in{\mathbb C}$
with $\Re z_{n}>a$ $[n\geq 0]$ such that $z_{n}\rightarrow z_{0}$
and $({\mathcal L}F)(z_{n})=({\mathcal L}G)(z_{n})$
$[n\geq 0]$. Show that $F=G$.
\end{exercise}

Engineers are convinced that the converse to 
Lemma~\ref{Laplace}~(i) holds in the sense that if
$F\in {\mathcal E}_{a}$ has a Laplace transform $f$
which can be extended to a function $\tilde{f}$
analytic on $\{z\in{\mathbb C}:\Re z>b\}$ 
[$a$, $b$ real, $a\geq b$], then $F\in {\mathcal E}_{b}$.
Unfortunately, this is not true, but it represents
a good heuristic principle to bear in mind in what follows.
Number theorists use the Mellin transform
\[{\mathcal M}F(z)=\int_{0}^{\infty}F(t)t^{z-1}\,dt\]
in preference to the Laplace transform but the
two transforms are simply related.
\begin{exercise} Give the relation explicitly.
\end{exercise}

Riemann considered the two functions
\[\Phi(s)=\sum_{p}p^{-s}\log p\]
and the \emph{Riemann zeta function}
\[\zeta(s)=\sum_{n=1}^{\infty}n^{-s}.\]
Both of these functions are defined for $\Re s>1$ 
but Riemann saw that they could be extended
to analytic functions over a larger domain.

The next lemma is essentially a repeat of 
Lemmas~\ref{Extend Dirichlet 1}~(ii)
and~\ref{Extend Dirichlet 2}~(ii).
\begin{lemma} 
(i) Let $S_{0}(x)=0$ for $x\leq 0$ and
$S_{0}(x)=\sum_{1\leq m\leq x}1$. If
\[T_{0}(x)=S_{0}(x)-x,\]
then $T_{0}$ is bounded
and locally integrable. We have
\[\sum_{n=1}^{N}n^{-s}\rightarrow 
s\int_{1}^{\infty}T_{0}(x)x^{-s-1}\, dx+\frac{s}{s-1}\]
as $N\rightarrow\infty$ for all $s$ with $\Re s>1$.

(ii) There exists a meromorphic function $\zeta$
analytic on $\{s\in{\mathbb C}:\Re s>0\}$ except for
a simple pole, residue $1$ at $1$ such that
$\sum_{n=1}^{\infty}n^{-s}$ converges to 
$\zeta(s)$ for $\Re s>1$.

(iii) If $\Re s>1$, then
\[\sum_{p\leq N}\frac{\log p}{p^{s}}
\rightarrow 
s\int_{1}^{\infty}\theta(x)x^{-s-1}\, dx\]
as $N\rightarrow\infty$.
\end{lemma}
The use of $s$ rather than $z$ goes back to Riemann.
Riemann showed that $\zeta$ can be extended to a meromorphic
function over ${\mathbb C},$ but we shall not need this.

How does this help us study $\Phi$?
\begin{lemma}\label{extend half}
(i) We have $\prod_{p<N}(1-p^{-s})^{-1}\rightarrow \zeta(s)$
uniformly for $\Re s>1+\delta$ whenever $\delta>0$.

(ii) We have
\[\frac{\zeta'(s)}{\zeta(s)}=-\sum_{p}\frac{\log p}{p^{s}-1}\]
for all $\Re s>1$.

(iii) We have
\[\Phi(s)=-\frac{\zeta'(s)}{\zeta(s)}-
\sum_{p}\frac{\log p}{(p^{s}-1)p^{s}}\]
for all $\Re s>1$.

(iv) The function $\Phi$ can be analytically extended
to a meromorphic function on
$\{s:\Re s>\frac{1}{2}\}$. It has a simple pole at 1
with residue $1$ and simple poles at the zeros of
$\zeta$ but nowhere else.
\end{lemma}


The next exercise is long and will not be used later but is,
I think, instructive.
\begin{exercise} (i) Show by grouping in pairs
that $\sum_{n=1}^{\infty}(-1)^{n-1}n^{-s}$
converges to an analytic function $g(s)$ in the
region $\{s:\Re s>0\}$.

(ii) Find $A$ and $B$ such that $g(s)=A\zeta(s)+B2^{-s}\zeta(s)$
for all $\Re s>1$. Why does this give another proof
that $\zeta$ can be extended to an analytic function 
on $\{s:\Re s>0\}$?

(iii) Show that $g(1/2)\neq 0$ and deduce that $\zeta(1/2)\neq 0$.

(iv) By imitating the arguments of Lemma~\ref{extend half},
show that we we can find an analytic function $G$ defined
on $\{s:\Re s>1/3\}$ such that
\[\Phi(s)=-\frac{\zeta'(s)}{\zeta(s)}-\Phi(2s)-G(s).\]
Deduce that $\Phi$ can be extended to a meromorphic
function on $\{s:\Re s>1/3\}$. 

(v) Show, using (iii), that $\Phi$ has a pole at $1/2$.

(vi) Show that the assumption that 
$|\sum_{p<N}\log p -N|\leq A N^{1/2-\epsilon}$ for
some $\epsilon>0$ and $A>0$ and all large enough $N$
leads to the conclusion that
$\Phi$ can be analytically extended from $\{s:\Re s>1\}$
to an everywhere analytic function
on $\{s:\Re s>1/2-\epsilon\}$.

(vii) Deduce that if $\epsilon>0$ and $A>0$
\[|\sum_{p<N}\log p -N|\geq A N^{1/2-\epsilon}
\ \text{for infinitely many values of $N$.}\]
\end{exercise}

It is well known that Riemann conjectured that $\zeta$
has no zeros in $\{s:\Re s>1/2\}$ and that his conjecture
is the most famous open problem in mathematics.
The best we can do is to follow Hadamard and
de la Vall\'{e}e Poussin and show that $\zeta$
has no zero on $\{s:\Re s=1\}$. Our proof makes
use of the slightly unconventional convention
that if $h$ and $g$ are analytic in a neighbourhood of $w$,
$g(w)\neq 0$
and $h(z)=(z-w)^{k}g(z)$, then $h$ has a zero
of order $k$ at $w$. (The mild unconventionality arises
when $k=0$.)
\begin{lemma} Suppose that $\zeta$ has a zero of order
$\mu$ at $1+i\alpha$ 
and a zero of order $\nu$ at $1+2i\alpha$
with $\alpha$ real and $\alpha>0$. Then
the following results hold.

(i) $\zeta$ has a zero of order $\mu$ at $1-i\alpha$
and a zero of order $\nu$ at $1-2i\alpha$.

(ii) As $\epsilon\rightarrow 0$ through
real positive values of $\epsilon$
\begin{align*}
\epsilon\Phi(1+\epsilon \pm i\alpha)&\rightarrow -\mu\\
\epsilon\Phi(1+\epsilon \pm 2i\alpha)&\rightarrow -\nu\\
\epsilon\Phi(1+\epsilon)&\rightarrow 1.
\end{align*}

(iii) If $s=1+\epsilon$ with $\epsilon$ real and positive,
then
\begin{align*}
0&\leq\sum_{p}p^{-s}\log p (e^{(i\alpha\log p)/2}+
e^{-(i\alpha\log p)/2})^{4}\\
&=\Phi(s+2i\alpha)+\Phi(s-2i\alpha)
+4(\Phi(s+i\alpha)+\Phi(s-i\alpha))
+6\Phi(s).
\end{align*}

(iv) We have $0\leq-2\nu-8\mu+6$.
\end{lemma}
\begin{theorem} If $\Re s=1$, then $\zeta(s)\neq 0$.
\end{theorem}
We note the following trivial consequence.
\begin{lemma}
If we write
\[T(s)=\frac{\zeta'(s)}{\zeta(s)}-(s-1)^{-1},\]
then given any $R>0$ we can find a $\delta(R)$
such that $T$ has no poles in
\[\{z:\Re z\geq 1-\delta(R),\ |\Im z|\leq R\}.\]
\end{lemma}

We shall show that the results we have obtained on 
the behaviour of $\zeta$ suffice to show that
\[\int_{1}^{X}\frac{\theta(x)-x}{x^{2}}\,dx\]
tends to a finite limit as $X\rightarrow\infty$.
The next lemma shows that this is sufficient to
give the prime number theorem.
\begin{lemma} Suppose that 
$\beta:[1,\infty)\rightarrow{\mathbb R}$
is an increasing (so integrable) function.

(i) If $\lambda>1$, $y>1$ and $y^{-1}\beta(y)>\lambda$,
then
\[\int_{y}^{\lambda y}\frac{\beta(x)-x}{x^{2}}\,dx
\geq A(\lambda)\]
where $A(\lambda)$ is a strictly positive number
depending only on $\lambda$.

(ii) If 
\[\int_{1}^{X}\frac{\beta(x)-x}{x^{2}}\,dx\]
tends to limit as $X\rightarrow\infty$, then
$x^{-1}\beta(x)\rightarrow 1$ as $x\rightarrow\infty$.
\end{lemma}

We need a couple of further preliminaries.
First we note a simple consequence of the Chebychev
estimates (Theorem~\ref{Chebychev theorem}).
\begin{lemma} There exists a constant $1>K>0$
such that
\[\frac{|\theta(x)-x|}{x}\leq K\]
for all $x$ sufficiently large.
\end{lemma}
Our second step is to translate our results into
the language of Laplace transforms. (It is 
just a matter of taste whether to work with Laplace
transforms or Mellin transforms.)
\begin{lemma} Let $f(t)=\theta(e^{t})e^{-t}-1$ for
$t\geq 0$ and $f(t)=0$ otherwise. Then
\[\mathcal{L}f(z)=\int_{-\infty}^{\infty}f(t)e^{-tz}\,dt\]
is well defined and 
\[\mathcal{L}f(z)=\frac{\Phi(z+1)}{z+1}-\frac{1}{z}\]
for all $\Re z>0$.

The statement $\int_{1}^{\infty}(\theta(x)-x)/x^{2}\,dx$
convergent is equivalent to the statement that
$\int_{-\infty}^{\infty}f(t)\,dt$ converges.
\end{lemma}

We have reduced the proof of the prime number theorem
to the proof of the following lemma.
\begin{lemma} Suppose $\Omega$ is an open set
with $\Omega\supseteq \{z:\Re z\geq 0\}$,
$F:\Omega\rightarrow{\mathbb C}$  is an analytic
function and $f:[0,\infty]\rightarrow{\mathbb R}$
is bounded locally integrable function such that
\[F(z)=\mathcal{L}f(z)=\int_{0}^{\infty}f(t)e^{-tz}\,dt\]
for $\Re z>0$. Then $\int_{0}^{\infty}f(t)\,dt$ converges.
\end{lemma}
This lemma and its use to prove the prime number theorem
are due to D.~Newman. (A version will be found in~\cite{Newman}.)
\section{The Fourier transform and Heisenberg's inequality}%
\label{Heisenberg section} In this section we return to the
Fourier transform on ${\mathbb R}$. We follow a slightly
different path to that mapped out in Section~\ref{brief}.
I shall state results using Lebesgue measure but
students using Riemann integration will find appropriate
modifications as exercises.

We pay particular attention to the Gaussian (or heat, or error)
kernel $E(x)=(2\pi)^{-1/2}\exp(-x^{2}/2)$.
\begin{lemma}\label{Fourier of Gauss}
$\hat{E}(\lambda)=(2\pi)^{1/2}E(\lambda)$.
\end{lemma}
\begin{exercise} If I prove Lemma~\ref{Fourier of Gauss}
I shall do so by setting up a differential equation.
Obtain Lemma~\ref{Fourier of Gauss}  by complex variable
techniques.
\end{exercise}

We use the following neat formula.
\begin{lemma}\label{neat} If $f,g\in L^{1}({\mathbb R})$
then the products 
$\hat{f}\times g,\ f\times\hat{g}\in L^{1}({\mathbb R})$
and
\[\int_{-\infty}^{\infty}\hat{f}(x)g(x)\,dx
=\int_{\-infty}^{\infty}f(\lambda)\hat{g}(\lambda)
\,d\lambda.\]
\end{lemma}

\begin{exercise} (For those using Riemann integration.
You will need to refer back to the exercises in
Section~\ref{brief}.) Suppose that $f$ and $g$ are continuous 
and
there exists a real constant $A$ such that
\[|f(x)|,|g(x)|\leq A(1+x^{2})^{-1}\]
for all $x$ and
\[|\hat{f}(\lambda)|,|\hat{g}(\lambda)|\leq A(1+\lambda^{2})^{-1}\]
for all $\lambda$. Show that
\[\int_{-\infty}^{\infty}\hat{f}(x)g(x)\,dx
=\int_{-\infty}^{\infty}f(\lambda)\hat{g}(\lambda)\,d\lambda.\]
Without going into detail, convince yourself that
the hypothesis `$f$ and $g$ are continuous'
can be replaced by `$f$ and $g$ are piecewise continuous'.
\end{exercise}

By taking $f=E_{h}$ where 
$E_{h}(x)=h^{-1}E(h^{-1}(x))$ $[h>0]$ in Lemma~\ref{neat}
we obtain a nice pointwise inversion result.
\begin{theorem}\label{Lebesgue point} 
If $f,\hat{f}\in L^{1}$ and $f$ is continuous
at $t$, then $\Hat{\Hat{f}}(t)=2\pi f(-t)$.
\end{theorem} 
\begin{exercise}\label{Riemann point} 
(For those using Riemann integration.)
Suppose that $f$ is piecewise continuous and
there exists a real constant $A$ such that
\[|f(x)|\leq A(1+x^{2})^{-1}\]
for all $x$ and
\[|\hat{f}(\lambda)|\leq A(1+\lambda^{2})^{-1}\]
for all $\lambda$. Show that if $f$ is continuous
at $t$, then $\Hat{\Hat{f}}(t)=2\pi f(-t)$.
\end{exercise}
\begin{exercise} Suppose that $f$ satisfies the conditions
of Theorem~\ref{Lebesgue point} (if you use Lebesgue integration)
or Exercise~\ref{Riemann point} (if you use Riemann integration).
If $t$ is a point where $f(t+)=\lim_{h\rightarrow 0+}f(t+h)$
and $f(t-)=\lim_{h\rightarrow 0+}f(t-h)$ both exist,
show that
\[\Hat{\Hat{f}}(-t)=\pi(f(t+)+f(t-)).\]
\end{exercise}
\begin{lemma} {\bf (Parseval's equality)} 
If
$f$ and $\hat{f}$ are continuous and integrable and 
$\hat{\hat{f}}(t)=2\pi f(-t)$ for all $t$
then
\[\int_{\mathbb R}|f(t)|^{2}\,dt=
\frac{1}{2\pi}\int_{\mathbb R}|\hat{f}(\lambda)|^{2}\,d\lambda.\]
\end{lemma}
(A les ad hoc version of Parseval's equality is
given in Exercise~\ref{Hilbert dual}~(v).)
\begin{exercise}\label{Hilbert dual}
(This requires Lebesgue measure.)
The present course is rather old fashioned, not least
in the way it thinks of Fourier transforms $\hat{f}$
in terms of its values $\hat{f}(\lambda)$ at points $\lambda$,
rather than an object in its own right. Here is one
of several ways in which a more general view gives a
more elegant theory.

(i) Let $S$ be the set of infinitely differentiable
functions $f$ with 
\[x^{n}f^{(m)}(x)\rightarrow 0\]
as $|x|\rightarrow\infty$ for all integers $n,m\geq 0$.
Show that, if $f\in S$, then $\hat{f}\in S$.

(ii) Let ${\mathbb I}_{[a,b]}(x)=1$ for $x\in [a,b]$,
${\mathbb I}_{[a,b]}(x)=0$ otherwise.
Show that, if $E_{h}$ is defined as above, then
\[\|{\mathbb I}_{[a,b]}-E_{h}*{\mathbb I}_{[a,b]}\|_{2}
\rightarrow 0\]
as $h\rightarrow 0+$.
Deduce, or prove otherwise, that $S$ is $L^{2}$ norm dense
in $L^{2}$.

(iii)  By taking $g=\hat{f}$ in Lemma~\ref{neat} show that
\[\|\hat{f}\|_{2}^{2}=2\pi \|f\|_{2}^{2}\]
for all $f\in S$.

(iii) Deduce that there is a unique continuous mapping 
${\mathcal F}:L^{2}\rightarrow L^{2}$ with ${\mathcal F}(f)=\hat{f}$
for all $f\in S$. (Uniqueness is easy  but you should take care
proving existence.)

(iv) Show that ${\mathcal F}:L^{2}\rightarrow L^{2}$ is linear
and that
\[\|{\mathcal F}(f)\|_{2}^{2}=2\pi \|f\|_{2}^{2}\]
for all $f\in L^{2}$.

If we define ${\mathcal J}:L^{2}\rightarrow L^{2}$ by
$({\mathcal J}f)(t)=f(-t)$ show that 
${\mathcal F}^{2}=2\pi{\mathcal J}$.

(v) If we wish to work in $L^{2}$, it makes sense to use
a different normalising factor and call 
${\mathcal G}=(2\pi)^{-1/2}{\mathcal F}$ the Fourier
transform. Show that ${\mathcal G}^{4}=I$ and that
${\mathcal G}:L^{2}\rightarrow L^{2}$ is a bijective linear
isometry.

(vi) (Parseval's equality) Show that, if we work in $L^{2}$
\[\int_{\mathbb R}
{\mathcal G}f(\lambda)({\mathcal G}g)^{*}(\lambda)
\,d\lambda=\int_{\mathbb R}f(t)g(t)^{*}\,dt.\]
\end{exercise}
 
We now come to one of the key facts about the Fourier
transform (some would say one of the key facts about
the world we live in).
\begin{theorem}[Heisenberg's inequality]\label{Heisenberg}
If $f$ is reasonably well behaved, then
\[
\frac
{\int_{-\infty}^{\infty}\lambda^{2}|\hat{f}(\lambda)|^{2}\,d\lambda} 
{\int_{-\infty}^{\infty}|\hat{f}(\lambda)|^{2}\,d\lambda}
\times
\frac
{\int_{-\infty}^{\infty}x^{2}|f(x)|^{2}\,dx} 
{\int_{-\infty}^{\infty}|f(x)|^{2}\,dx}
\geq \frac{1}{4}.
\]
If equality holds, then $f(x)=A\exp(-bx^{2})$ for some $b>0$.
\end{theorem}
\begin{exercise} Write down explicit conditions for
Theorem~\ref{Heisenberg}.
\end{exercise}               
The extension of Heisenberg's inequality to all
$f\in L^{2}$ is given in Section~2.8 of the
beautiful book~\cite{Dym} of Dym and McKean.
\section{The Poisson formula}\label{Poisson section} 
The following
remarkable observation is called Poisson's formula.
\begin{theorem}\label{Poisson}
Suppose that 
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a continuous
function such that $\sum_{m=-\infty}^{\infty}|\hat{f}(m)|$
converges and $\sum_{n=-\infty}^{\infty}|f(2\pi n+x)|$
converges uniformly on $[-\pi,\pi]$. Then
\[\sum_{m=-\infty}^{\infty}\hat{f}(m)=
2\pi\sum_{n=-\infty}^{\infty}f(2\pi n).\]
\end{theorem}

It is possible to adjust the hypotheses on $f$
in Poisson's formula in various ways,
though some hypotheses there must be. We shall simply
think of $f$ as `well behaved'. The following
rather simple lemma will suffice for our needs.
\begin{lemma}\label{simple needs}
If $f:{\mathbb R}\rightarrow{\mathbb C}$ 
is a twice continuously differentiable function such 
that $\int_{-\infty}^{\infty}|f(x)|\,dx$,
$\int_{-\infty}^{\infty}|f'(x)|\,dx$
and $\int_{-\infty}^{\infty}|f''(x)|\,dx$ 
converge whilst $f'(x)\rightarrow 0$ 
and $x^{-2}f(x)\rightarrow 0$ as $|x|\rightarrow\infty$,
then $f$ satisfies the conditions of Theorem~\ref{Poisson}.
\end{lemma}

\begin{exercise} (i) By applying Poisson's formula to the function
$f$ defined by $f(x)=\exp(-t|x|/2\pi)$, show that
\[2(1-e^{-t})^{-1}
=\sum_{n=-\infty}^{\infty}2t(t^{2}+4\pi^{2}n^{2})^{-1}.\]

(ii) By expanding $(t^{2}+4\pi n^{2})^{-1}$ and
(carefully) interchanging sums, deduce that
\[2(1-e^{-t})^{-1}=1+2t^{-1}+\sum_{m=0}^{\infty}c_{m}t^{m}\]
where $c_{2m}=0$ and
\[c_{2m+1}=a_{2m+1}\sum_{n=1}^{\infty}n^{-2m}\]
for some value of $a_{2m+1}$ to be given explicitly.

(iii) Hence obtain Euler's formula
\[\sum_{n=1}^{\infty}n^{-2m}=
(-1)^{m-1}2^{2m-1}b_{2m-1}\pi^{2m}/(2m-1)!\]
for $m\geq 1$, where the $b_{m}$ are defined by the formula
\[(e^{y}-1)^{-1}=y^{-1}-2^{-1}+\sum_{n=1}^{\infty}b_{n}y^{n}/n!\]
(The $b_{n}$ are called Bernoulli numbers.)
\end{exercise}
\begin{exercise} Suppose $f$ satisfies the conditions of
Lemma~\ref{simple needs}. Show that
\[K\sum_{m=-\infty}^{\infty}\hat{f}(Km)=
2\pi \sum_{n=-\infty}^{\infty}f(2\pi K^{-1}n)\]
for all $K>0$. What is the corresponding result when $K<0$?

By letting $K\rightarrow 0+$, deduce that
\[\Hat{\Hat{f}}(0)=2\pi f(0).\]
(There is some interest in just seeing that this is so
but it is more profitable to give a rigorous proof.)
Deduce in the usual way that
\[\Hat{\Hat{f}}(t)=2\pi f(-t)\]
for all $t$.
\end{exercise}

Poisson's formula has a particularly interesting consequence.
\begin{lemma} If $g:{\mathbb R}\rightarrow{\mathbb C}$
is twice continuously differentiable and
$g(t)=0$ for $|t|\geq \pi$, then $g$ is completely
determined by the values of $\hat{g}(m)$ for
integer $m$.
\end{lemma}
Taking $g=\hat{f}$ and remembering the inversion formula
we obtain the following result.
\begin{pretheorem}\label{before Shannon}
If $f:{\mathbb R}\rightarrow{\mathbb C}$
is a well behaved function with $\hat{f}(\lambda)=0$
for $|\lambda|\geq\pi$,
then $f$ is determined by its values at integer
points.
\end{pretheorem}
We call this a pretheorem because we have not specified
what `well behaved' should mean. 

The simplest approach is via the \emph{sinc function}
\[\sinc(x)=\frac{1}{2\pi}\int_{-\pi}^{\pi}\exp(ix\lambda)\,d\lambda.\]
We state the most immediately useful properties 
of  $\sinc$.
\begin{lemma}
(i) $\sinc(0)=1$,

(ii) $\sinc(n)=0$ if $n\in{\mathbb Z}$ but $n\neq 0$.
\end{lemma}

(We note also that although, strictly speaking,
$\widehat{\sinc}(\lambda)$ is not defined 
for us, since $\int|\sinc (x)|\,dx=\infty$,
we are strongly tempted to say that
$\widehat{\sinc}(\lambda)=1$ if $|\lambda|<\pi$
and 
$\widehat{\sinc}(\lambda)=0$ if $|\lambda|>\pi$.)

We can, at once, prove that Pretheorem~\ref{before Shannon}
is best possible.
\begin{lemma}\label{Shannon best}
If $\epsilon>0$, then we can find an
infinitely differentiable non-zero $f$ such that
$\hat{f}(\lambda)=0$ for $|\lambda|>\pi+\epsilon$,
but $f(n)=0$ for all $n\in{\mathbb Z}$.
\end{lemma}
\begin{exercise} In Lemma~\ref{Shannon best} show that
we can take $f\in S$ where $S$ is the class discussed in
Exercise~\ref{Hilbert dual}.
\end{exercise}

We can also show how to recover the function of
Pretheorem~\ref{before Shannon} from its values
at integer points.
\begin{theorem}\label{Shannon constructive}
Suppose $f:{\mathbb R}\rightarrow{\mathbb C}$
is a continuous function with 
$\int_{-\infty}^{\infty}|f(t)|\,dt<\infty$.
If $\hat{f}(\lambda)=0$
for $|\lambda|\geq\pi$,
then 
\[\sum_{n=-N}^{N}f(n)\sinc(t-n)\rightarrow f(t)\]
as uniformly as $N\rightarrow\infty$.
\end{theorem}
Thus Pretheorem~\ref{before Shannon} holds under 
very general conditions. We state it in a lightly
generalised form.
\begin{theorem}[Shannon's Theorem]~\label{Shannon}
Suppose that $f:{\mathbb R}\rightarrow{\mathbb C}$
is a continuous function with 
$\int_{-\infty}^{\infty}|f(t)|\,dt<\infty$
and that $K>0$.
If $\hat{f}(\lambda)=0$
for $|\lambda|\geq K$,
then $f$ is determined by its values at points of the form 
$n\pi K^{-1}$ with $n\in{\mathbb Z}$.
\end{theorem}

Theorem~\ref{Shannon} belongs to the same circle of ideas
as Heisenberg's inequality. It is the key to such devices
as the CD.

\section{References and further reading}
If the elegance and variety of a subject is to be judged
by the elegance and variety of the (best) texts
on that subject, Fourier Analysis must surely
stand high. On the pure side the books
of Helson~\cite{Helson} and Katznelson~\cite{Katznelson}
would be my first choice for introductions
and this course draws on both. If you wish
to think about applications, the obvious text 
is that of Dym and McKean~\cite{Dym}.
The next two recommendations are irrelevant
to Part~III but, if you go on to work in
any field involving classical analysis,
Zygmund's treatise~\cite{Zygmund} is a must
and, if you would like a first glimpse
at wavelets, (unmentioned in this course)
Babarah Hubbard's popularisation 
\emph{The World According to Wavelets}~\cite{Hubbard}
is splendid light reading.
 
  
There is an excellent treatment of Dirichlet's
theorem and much more in Davenport's
\emph{Multiplicative Number Theory}~\cite{Davenport}.
[The changes between the first and second editions
are substantial but do not affect that part which
deals with material in this course.]
If you wish to know more about the Riemann zeta-function
you can start with~\cite{Patterson}.

In preparing this course I have also used~\cite{Korner1}
and~\cite{Korner2} since I find the author sympathetic.
\begin{thebibliography}{99}
\bibitem{Newman} J.~Bak and D.~J.~Newman
\emph{Complex Analysis}
Springer, New York, 1982.
\bibitem{Davenport} H.~Davenport
\emph{Multiplicative Number Theory}
(2nd Edition), Springer, New York, 1980.
\bibitem{Dym} H.~Dym and H.~P.~McKean
\emph{Fourier Series and Integrals}
Academic Press, 1972.
\bibitem{Helson} H.~Helson
\emph{Harmonic Analysis}
Adison--Wesley, 1983.
\bibitem{Hubbard} B. Hubbard
\emph{The World According to Wavelets}
A.~K.~Peters, 1996. 
\bibitem{Katznelson} Y.~Katznelson
\emph{An Introduction to Harmonic Analysis}
Wiley, 1963. [There is a Dover reprint]
\bibitem{Korner1} T.~W.~K\"{o}rner 
\emph{Fourier Analysis}
CUP, 1988.
\bibitem{Korner2} T.~W.~K\"{o}rner 
\emph{Exercises for Fourier Analysis}
CUP, 1993.
\bibitem{Patterson} S.~J.~Patterson
\emph{An Introduction to the Theory of the Riemann zeta-function}
CUP, 1988.
\bibitem{Zygmund} A.~Zygmund
\emph{Trigonometric Series} (2 Volumes)
CUP, 1959. 
\end{thebibliography}
