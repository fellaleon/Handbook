\chapter{Analysis I}

\section{Why do we bother?} It is surprising how many people
think that analysis consists in the difficult
proofs of obvious theorems. All we need know, they say, is what
a limit is, the definition of continuity
and the definition of the derivative. All the
rest is `intuitively clear'.

If pressed they will agree that these definitions apply as
much to the rationals ${\mathbb Q}$ as to the real
numbers ${\mathbb R}$. They then have
to explain the following interesting
example.
\begin{example}\label{Rational}
If $f:{\mathbb Q}\rightarrow{\mathbb Q}$
is given by
\begin{alignat*}{2}
f(x)&=-1&&\qquad\text{if $x^{2}<2$,}\\
f(x)&=1&&\qquad\text{otherwise,}
\end{alignat*}
then

(i) $f$ is a continuous function with $f(0)=-1$, $f(2)=1$ yet
there does not exist a $c$ with $f(c)=0$,

(ii) $f$ is a differentiable function with $f'(x)=0$ for
all $x$ yet $f$ is not constant.
\end{example}

What is the difference between ${\mathbb R}$ and ${\mathbb Q}$
which makes
calculus work on one even though it fails on the other?
Both are `ordered fields', that is, both support operations
of `addition'
and `multiplication' together with a relation `greater than'
(`order') with the properties that we expect. If the
reader is interested she will find a complete list of
the appropriate axioms in texts like the altogether
excellent book of Spivak~\cite{Spivak} and its
many rather less excellent competitors,
but, interesting as
such things may be, they are irrelevant to our purpose
which is not to consider the shared properties
of ${\mathbb R}$ and ${\mathbb Q}$
but to identify  a \emph{difference} between the
two systems which will enable us to exclude the
possibility of a function like that of Example~\ref{Rational}
for functions from ${\mathbb R}$ to ${\mathbb R}$.


To state the difference we need only recall a definition
from course~C3.
\begin{definition}\label{one convergence definition}
If $a_{n}\in{\mathbb R}$ for each $n\geq 1$
and $a\in{\mathbb R}$ then we say that $a_{n}\rightarrow a$
if given $\epsilon>0$ we can find an $n_{0}(\epsilon)$
such that
\[|a_{n}-a|<\epsilon\ \text{for all $n\geq n_{0}(\epsilon)$}.\]
\end{definition}
The key property of the reals, the \emph{fundamental axiom}
which makes everything work was also stated in
the course~C3.
\begin{axiom}{\bf [The fundamental axiom of analysis]}
If $a_{n}\in{\mathbb R}$ for each $n\geq 1$, $A\in{\mathbb R}$
and $a_{1}\leq a_{2}\leq a_{3}\leq \ldots$ and
$a_{n}<A$ for each $n$ then there exists an $a\in{\mathbb R}$
such that $a_{n}\rightarrow a$ as $n\rightarrow\infty$.
\end{axiom}
Less ponderously, and just as rigorously, the fundamental axiom
for the real numbers
says \emph{every increasing sequence bounded above tends
to a limit}.

Everything which depends on the fundamental axiom is
analysis, everything else is mere algebra.

\section{The axiom of Archimedes} We start by proving 
the following results on limits, some of which
you saw proved in course~C3.
\begin{lemma}\label{one sequences}
(i) The limit is unique. That is, if $a_{n}\rightarrow a$
and $a_{n}\rightarrow b$ as $n\rightarrow\infty$
then $a=b$.

(ii) If $a_{n}\rightarrow a$ as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$a_{n(j)}\rightarrow a$ as $j\rightarrow\infty$.

(iii) If $a_{n}=c$ for all $n$ then $a_{n}\rightarrow c$
as $n\rightarrow\infty$.

(iv) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$ then
$a_{n}+b_{n}\rightarrow a+b$.

(v) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$ then
$a_{n}b_{n}\rightarrow ab$.

(vi) If $a_{n}\rightarrow a$
as $n\rightarrow\infty$, $a_{n}\neq 0$ for each $n$ and
$a\neq 0$, then $a_{n}^{-1}\rightarrow a^{-1}$.

(vii) If $a_{n}\leq A$ for each $n$ and
$a_{n}\rightarrow a$
as $n\rightarrow\infty$ then $a\leq A$.
\end{lemma}

We need the following variation on the fundamental axiom.
\begin{exercise}\label{Exercise 1.4}
A decreasing sequence of real numbers bounded below tends
to a limit.
\end{exercise}
[Hint. If $a\leq b$ then $-b\leq -a$.]

Useful as the results of Lemma~\ref{one sequences} are,
they are also true of sequences in ${\mathbb Q}$.
They are therefore mere, if important, algebra.
Our first truly `analysis' result may strike the
reader as rather odd.
\begin{theorem}{\bf [Axiom of Archimedes]}\label{Archimedes}
\[\frac{1}{n}\rightarrow 0\ \text{as $n\rightarrow\infty$}\]
\end{theorem}
Theorem~\ref{Archimedes} shows that there is no `exotic'
real number $\gimel$ say (to choose an exotic symbol)
with the property that $1/n>\gimel$ for all integers $n\geq 1$
yet $\gimel>0$ (that is, $\gimel$ is strictly positive
and yet smaller than all strictly positive rationals).
There exist number systems with such exotic numbers
(the famous `non-standard analysis' of Abraham Robinson
and the `surreal numbers' of Conway constitute two
such systems) but, just as the rationals are, in
some sense, too
small a system for the standard theorems of analysis to hold
so these non-Archimedean systems are, in some sense,
too big. Archimedes and  Eudoxus
realised the need for an axiom
to show that there is no exotic number $\daleth$ bigger
than any integer\footnote{Footnote for passing historians,
this is a course in mathematics.}
(i.e. $\daleth>n$ for all integers $n\geq 1$;
to see the connection with our form of the axiom consider
$\gimel=1/\daleth$). However, in spite of its name, what
was an axiom for Archimedes is a theorem
for us.
\begin{theorem} Given any real number $K$ we can find
an integer $n$ with $n>K$.
\end{theorem}

\section{Series and sums} There is no need to
restrict the notion of a limit to real numbers.
\begin{definition}\label{complex convergence definition}
If $a_{n}\in{\mathbb C}$ for each $n\geq 1$
and $a\in{\mathbb C}$ then we say that $a_{n}\rightarrow a$
if given $\epsilon>0$ we can find an $n_{0}(\epsilon)$
such that
\[|a_{n}-a|<\epsilon\ \text{for all $n\geq n_{0}(\epsilon)$}.\]
\end{definition}
\begin{exercise}\label{complex sequences}
We work in ${\mathbb C}$.

(i) The limit is unique. That is, if $a_{n}\rightarrow a$
and $a_{n}\rightarrow b$ as $n\rightarrow\infty$
then $a=b$.

(ii) If $a_{n}\rightarrow a$ as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$a_{n(j)}\rightarrow a$ as $j\rightarrow\infty$.

(iii) If $a_{n}=c$ for all $n$ then $a_{n}\rightarrow c$
as $n\rightarrow\infty$.

(iv) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$ then
$a_{n}+b_{n}\rightarrow a+b$.

(v) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$ then
$a_{n}b_{n}\rightarrow ab$.

(vi) If $a_{n}\rightarrow a$
as $n\rightarrow\infty$, $a_{n}\neq 0$ for each $n$ and
$a\neq 0$, then $a_{n}^{-1}\rightarrow a^{-1}$.
\end{exercise}
\begin{exercise}\label{complex series 2} Explain why there is no result in
Exercise~\ref{complex sequences} corresponding
to part (vii) of Lemma~\ref{one sequences}.
\end{exercise}

We illustrate some of the ideas introduced by studying
infinite sums.
\begin{definition}~\label{convergent sum}
We work in ${\mathbb F}$ where ${\mathbb F}={\mathbb R}$
or ${\mathbb F}={\mathbb C}$.
If $a_{j}\in{\mathbb F}$
we say that
$\sum_{j=1}^{\infty}a_{j}$ converges to $s$
if
\[\sum_{j=1}^{N}a_{j}\rightarrow s\]
as $N\rightarrow{\infty}$.
We write $\sum_{j=1}^{\infty}a_{j}=s$.

If $\sum_{j=1}^{N}a_{j}$ does not tend to a limit
as $N\rightarrow\infty$,
we say that the sum $\sum_{j=1}^{\infty}a_{j}$
diverges.
\end{definition}
\begin{lemma} We work in ${\mathbb F}$ where ${\mathbb F}={\mathbb R}$
or ${\mathbb F}={\mathbb C}$.

(i) Suppose $a_{j},\, b_{j}\in{\mathbb F}$
and $\lambda,\,\mu\in{\mathbb F}$. If
$\sum_{j=1}^{\infty}a_{j}$ and $\sum_{j=1}^{\infty}b_{j}$
converge then so does 
$\sum_{j=1}^{\infty}\lambda a_{j}+\mu b_{j}$
and
\[\sum_{j=1}^{\infty}\lambda a_{j}+\mu b_{j}
=\lambda \sum_{j=1}^{\infty}a_{j}
+\mu \sum_{j=1}^{\infty}b_{j}.\]

(ii) Suppose $a_{j},\, b_{j}\in{\mathbb F}$
and there exists an $N$ such that
$a_{j}=b_{j}$ for all $j\geq N$. Then,
either $\sum_{j=1}^{\infty}a_{j}$ and $\sum_{j=1}^{\infty}b_{j}$
both converge or they both diverge. (In other
words, initial terms do not matter.)
\end{lemma}
\begin{exercise} Any problem on sums $\sum_{j=1}^{\infty}a_{j}$
can be converted into one on sequences by considering
the sequence $s_{n}=\sum_{j=1}^{n}a_{j}$. Show conversely
that a sequence $s_{n}$ converges if and only
if, when we set $a_{1}=s_{1}$ and $a_{n}=s_{n}-s_{n-1}$
$[n\geq 2]$ we have $\sum_{j=1}^{\infty}a_{j}$
convergent. What can you say about 
$\lim_{n\rightarrow\infty}\sum_{j=1}^{n}a_{j}$
and $\lim_{n\rightarrow\infty}s_{n}$ if both exist?
\end{exercise}

The following results are fundamental to the study
of sums.
\begin{theorem}{\bf (The comparison test.)}\label{comparison test}
We work in ${\mathbb R}$.
Suppose that $0\leq b_{j}\leq a_{j}$ for all $j$.
Then,
if $\sum_{j=1}^{\infty}a_{j}$ converges, so does
$\sum_{j=1}^{\infty}b_{j}$.
\end{theorem}
\begin{theorem}\label{absolute convergence}
We work in ${\mathbb F}$ where ${\mathbb F}={\mathbb R}$
or ${\mathbb F}={\mathbb C}$.
If $\sum_{j=1}^{\infty}|a_{j}|$ converges, then so does
$\sum_{j=1}^{\infty}a_{j}$.
\end{theorem}

Theorem~\ref{absolute convergence} is often stated using
the following definition.
\begin{definition} We work in ${\mathbb F}$ where ${\mathbb F}={\mathbb R}$
or ${\mathbb F}={\mathbb C}$.
If $\sum_{j=1}^{\infty}|a_{j}|$ converges we say that the sum 
$\sum_{j=1}^{\infty}a_{j}$ is absolutely convergent.
\end{definition}
Theorem~\ref{absolute convergence} then becomes the
statement that  absolute convergence implies convergence.

Here is a trivial but useful consequence of
Theorems~\ref{comparison test} and~\ref{absolute convergence}.
\begin{lemma}{\bf (Ratio test.)}\label{ratio test}
Suppose $a_{j}\in{\mathbb C}$
and $|a_{j+1}/a_{j}|\rightarrow l$ as $j\rightarrow\infty$.
If $l<1$, then $\sum_{j=1}^{\infty}|a_{j}|$ converges.
If $l>1$, then $\sum_{j=1}^{\infty}a_{j}$ diverges.
\end{lemma}
Of course Lemma~\ref{ratio test} tells us nothing if $l=1$
or $l$ does not exist.

Sums which are not absolutely convergent are much harder
to deal with in general. It is worth
keeping in mind the following trivial observation.
\begin{lemma} We work in ${\mathbb F}$ where ${\mathbb F}={\mathbb R}$
or ${\mathbb F}={\mathbb C}$.
If $\sum_{j=1}^{\infty}a_{j}$ converges, then
$a_{j}\rightarrow 0$ as $j\rightarrow\infty$.
\end{lemma}
At a deeper level the following result is
sometimes useful.
\begin{lemma}{\bf (Alternating series test.)}\label{alternating}
We work in ${\mathbb R}$. If
we have a decreasing sequence of positive
numbers $a_{n}$ with $a_{n}\rightarrow 0$
as $n\rightarrow\infty$, then
$\sum_{j=1}^{\infty}(-1)^{j+1}a_{j}$ converges.

Further
\[\left|\sum_{j=1}^{N}(-1)^{j+1}a_{j}
-\sum_{j=1}^{\infty}(-1)^{j+1}a_{j}\right|\leq|a_{N+1}|\]
for all $N\geq 1$.
\end{lemma}
The last sentence is sometimes expressed by saying
`the error caused by replacing a convergent
infinite alternating sum of decreasing terms
by the sum of its first few terms is no greater than the
absolute value of the first term neglected'.

Later we will give another test for convergence
called the integral test (Lemma~\ref{Lemma Integral test})
from which we deduce the result known to many of
you that $\sum_{n=1}^{\infty}n^{-1}$ diverges.
I will give another proof in the next example.
\begin{example}
(i) ${\displaystyle \sum_{n=1}^{\infty}\frac{1}{n}}$ diverges.

(ii) ${\displaystyle \sum_{n=1}^{\infty}\frac{(-1)^{n}}{n}}$ is
convergent but not absolutely convergent.

(iii) If $v_{2n}=1/n$, $v_{2n-1}=-1/(2n)$ then
$\sum_{n=1}^{\infty}v_{n}$ is not convergent.
\end{example}
\section{Least upper bounds} A non-empty bounded set in ${\mathbb R}$
need not have a maximum.
\begin{example} The set $E=\{-1/n\,:\,n\geq 1\}$ is non-empty and
any $e\in E$ satisfies the inequalities $-1\leq e\leq 0$
but $E$ has no largest member.
\end{example}
However, as we shall see every non-empty bounded set in ${\mathbb R}$
has a least upper bound (or supremum).
\begin{definition}\label{Definition, sup}
Let $E$ be a non-empty set in ${\mathbb R}$.
We say that $\alpha$ is a least upper bound for $E$ if

(i) $\alpha\geq e$ for all $e\in E$ [that is, $\alpha$ is an upper bound
for E] and

(ii) If $\beta\geq e$ for all $e\in E$ then $\beta\geq \alpha$
[that is, $\alpha$ is the least such upper bound].
\end{definition}
If $E$ has a supremum $\alpha$ we write $\sup_{e\in E}e=\sup E=\alpha$.
\begin{lemma}\label{Lemma, supremum unique}
If the least upper bound exists it is unique.
\end{lemma}
The following remark is trivial but sometimes helpful.
\begin{lemma}\label{Lemma, supremum other}  
Let $E$ be a non-empty set in ${\mathbb R}$.
Then $\alpha$ is a least upper bound for $E$ if and only if
we can find $e_{n}\in E$ with $e_{n}\rightarrow \alpha$
and $b_{n}$ such that $b_{n}\geq e$ for all $e\in E$
and $b_{n}\rightarrow \alpha$ as $n\rightarrow\infty$.
\end{lemma}
Here is the promised result.
\begin{theorem}\label{theorem, supremum}
Any non-empty set in ${\mathbb R}$
with an upper bound has a least upper bound.
\end{theorem}

We observe that this result is actually equivalent
to the fundamental axiom.
\begin{theorem} Theorem~\ref{theorem, supremum}
implies the fundamental axiom.
\end{theorem}

Of course we have the notion of a greatest lower bound
or infimum.
\begin{exercise}\label{exercise infimum}  Define the greatest lower bound
in the manner of Definition~\ref{Definition, sup},
prove its uniqueness in the manner of
Lemma~\ref{Lemma, supremum unique} and state
and prove a result corresponding to Lemma~\ref{Lemma, supremum other}.
\end{exercise}
If $E$ has an infimum $\beta$ we write $\inf_{e\in E}e=\inf E=\beta$.
One way of dealing with the infimum is to
use the following observation.
\begin{lemma}\label{Lemma minus infimum}
Let $E$ be a non-empty set in ${\mathbb R}$
and write $-E=\{-e\,:\,e\in E\}$. Then $E$ has an infimum
if and only if $-E$ has a supremum. If $E$ has an infimum
$\inf E=-\sup (-E)$.
\end{lemma}
\begin{exercise}\label{exercise infimum 2} 
Use Lemma~\ref{Lemma minus infimum}
and Theorem~\ref{theorem, supremum} to show
that any non-empty set in ${\mathbb R}$
with a lower bound has a greatest lower bound. 
\end{exercise}

The notion of a supremum will play an important 
r\^{o}le in our proofs of
Theorem~\ref{Theorem, interval maximum} 
and Theorem~\ref{radius of convergence}.

The following result is also equivalent to the
fundamental axiom (that is, we can deduce it from
the fundamental axiom and conversely, if we
take it as an axiom, rather than a theorem,
then we can deduce the fundamental axiom as a theorem).
\begin{theorem}{\bf [Bolzano-Weierstrass]}\label{one Bolzano}
If $x_{n}\in{\mathbb R}$ and there exists a $K$
such that $|x_{n}|\leq K$ for all $n$, then we can find
$n(1)<n(2)<\ldots$ and $x\in{\mathbb R}$ such that
$x_{n(j)}\rightarrow x$ as $j\rightarrow\infty$.
\end{theorem}
The Bolzano-Weierstrass theorem
says that every bounded sequence of reals has a convergent
subsequence. Notice that we say nothing about uniqueness;
if $x_{n}=(-1)^{n}$ then $x_{2n}\rightarrow 1$ but
$x_{2n+1}\rightarrow -1$ as $n\rightarrow\infty$.

We shall prove the theorem of Bolzano-Weierstrass
by `lion hunting' but your supervisor may well
show you another method. We shall use the
Bolzano-Weierstrass theorem to prove that every
continuous function
on a closed bounded interval is bounded and attains its bounds
(Theorem~\ref{Theorem, interval maximum}).
The Bolzano-Weierstrass theorem
will be much used in the next analysis course
because it generalises to many dimensions.

\section{Continuity} We make the following definition.
\begin{definition}\label{Definition, Continuity 1}
A function $f:{\mathbb R}\rightarrow{\mathbb R}$
is continuous at $x$ if given $\epsilon>0$ we can find a
$\delta(\epsilon,x)>0$ [read `a delta depending on epsilon 
and $x$'] such that
\[|f(x)-f(y)|<\epsilon\]
for all $y$ with $|x-y|<\delta(\epsilon,x)$.

If $f$ is continuous at each point $x\in{\mathbb R}$ we say that
$f$ is a continuous function on ${\mathbb R}$.
\end{definition}
I shall do my best to make this seem a reasonable definition
but it is important to realise that I am really stating a rule
of the game (like a knights move in chess or the definition
of offside in football). If you wish to play the game
you must accept the rules. Results about continuous functions
must be derived from the definition and not stated as
`obvious from the notion of continuity'.

In practice we use a slightly more general definition.
\begin{definition}\label{Definition, Continuity 2}
Let $E$ be a subset of ${\mathbb R}$.
A function $f:E\rightarrow{\mathbb R}$
is continuous at $x\in E$ if given $\epsilon>0$ we can find a
$\delta(\epsilon,x)>0$ [read `a delta depending on epsilon 
and $x$'] such that
\[|f(x)-f(y)|<\epsilon\]
for all $y\in E$ with $|x-y|<\delta(\epsilon,x)$.

If $f$ is continuous at each point $x\in E$, we say that
$f$ is a continuous function on $E$.
\end{definition}
However, it will do no harm and may be positively helpful
if, whilst you are getting used to the idea of
continuity, you concentrate on the case $E={\mathbb R}$.
\begin{lemma}\label{second continuous}
Suppose that $E$ is a subset of ${\mathbb R}$,
that $x\in E$,
and that $f$ and $g$
are functions from $E$ to ${\mathbb R}$.

(i) If $f(x)=c$ for all $x\in E$, then f is continuous
on $E$.

(ii) If $f$ and $g$ are continuous at $x$,
then so is $f+g$.

(iii) Let us define $f\times g:E\rightarrow {\mathbb R}$
by $f\times g(t)=f(t)g(t)$ for all $t\in E$. Then
if $f$ and $g$ are continuous at $x$, so is $f\times g$.

(iv) Suppose that $f(t)\neq 0$ for all $t\in E$. If
$f$ is continuous at $x$ so is $1/f$.
\end{lemma}
\begin{lemma}\label{continuous chain rule} 
Let $U$ and $V$ be subsets of ${\mathbb R}$. Suppose
$f:U\rightarrow{\mathbb R}$ is such that
$f(t)\in V$ for all $t\in U$. If $f$
is continuous at $x\in U$ and $g:V\rightarrow{\mathbb R}$
is continuous at $f(x)$, then the composition
$g\circ f$ is continuous at $x$.
\end{lemma}
By repeated use of parts~(ii) and~(iii) of
Lemma~\ref{second continuous} it is easy to
show that polynomials $P(t)=\sum_{r=0}^{n}a_{r}t^{r}$
are continuous. The details are spelled out in
the next exercise.
\begin{exercise}\label{Exercise, polynomial}
 Prove the following results.

(i) Suppose that $E$ is a subset of ${\mathbb R}$
and that $f:E\rightarrow{\mathbb R}$ is continuous
at $x\in E$. If $x\in E'\subset E$ then the
restriction $f|_{E'}$ of $f$ to $E'$ is also continuous
at $x$.

(ii) If $J:{\mathbb R}\rightarrow{\mathbb R}$ is
defined by $J(x)=x$ for all $x\in{\mathbb R}$,
then $J$ is continuous on ${\mathbb R}$.

(iii) Every polynomial $P$ is continuous on ${\mathbb R}$.

(iv) Suppose that $P$ and $Q$ are polynomials
and that $Q$ is never zero on some subset $E$
of ${\mathbb R}$. Then the rational function
$P/Q$ is continuous on $E$ (or, more precisely,
the restriction of $P/Q$ to $E$ is continuous.)
\end{exercise}
The following result is little more than an observation
but will be very useful.
\begin{lemma}\label{one arrow}
Suppose that $E$ is a subset of ${\mathbb R}$,
that $x\in E$, and that $f$ is continuous at $x$.
If $x_{n}\in E$ for all $n$ and $x_{n}\rightarrow x$
as $n\rightarrow\infty$, then $f(x_{n})\rightarrow f(x)$
as $n\rightarrow\infty$.
\end{lemma}

So far in this section we have only done algebra
but the next result depends on the fundamental axiom.
It is one of the key results of analysis and although
my recommendation runs contrary to a century of
enlightened pedagogy I can see no objections
to students learning the proof as a model.
Notice that the theorem resolves 
the problem posed by Example~\ref{Rational}~(i).
\begin{theorem}{\bf (The intermediate value theorem).}%
\label{intermediate state}
If $f:[a,b]\rightarrow{\mathbb R}$ is continuous
and $f(a)\geq 0\geq f(b)$ then there exists a $c\in[a,b]$
such that $f(c)=0$.
\end{theorem}
Exercises~\ref{Intermediate 1} to~\ref{Intermediate 3}
are applications of the intermediate value
theorem.
\begin{exercise}\label{Intermediate 1} Show
that any real polynomial of odd degree has
at least one root. Is the result true for
polynomials of even degree? Give a proof or counterexample.
\end{exercise}
\begin{exercise}\label{Intermediate 2} Suppose  that
$g:[0,1]\rightarrow [0,1]$ is a continuous function.
By considering $f(x)=g(x)-x$, or otherwise,
show that there exists a $c\in [0,1]$ with $g(c)=c$.
(Thus every continuous map of $[0,1]$ into itself
has a fixed point.) Give an example of a bijective
(but, necessarily, non-continuous) function
$h:[0,1]\rightarrow [0,1]$ such that $h(x)\neq x$
for all $x\in [0,1]$.

\noindent [Hint: First find a function
$H:[0,1]\setminus\{0,\ 1,\ 1/2\}\rightarrow [0,1]\setminus\{0,\ 1,\ 1/2\}$ 
such that $H(x)\neq x$.]

\end{exercise}
\begin{exercise}\label{Intermediate 3} Every mid-summer day
at six o'clock in the morning, the youngest monk
from the monastery of Damt starts to climb the narrow
path up Mount Dipmes.
At six in the evening he reaches the small temple
at the peak where he spends the night in meditation.
At six o'clock in the morning on the following day
he starts downwards, arriving back at the monastery
at six in the evening. Of course, he does not always walk
at the same speed. Show that, none the less, there
will be some time of day when he will be at the
same place on the path on both his upward and
downward journeys.
\end{exercise}

We proved the intermediate value theorem
(Theorem~\ref{intermediate state}) by lion hunting.
We prove the next two theorems by using the
Bolzano-Weierstrass Theorem (Theorem~\ref{one Bolzano}).
Again the results are very important and I can see
no objection to learning the proofs as a model.
\begin{theorem} If $f:[a,b]\rightarrow{\mathbb R}$
is continuous then we can find an $M$ such that
$|f(x)|\leq M$ for all $x\in[a,b]$.
\end{theorem}
In other words a continuous function on a closed bounded
interval is bounded. We improve this result in the next theorem.
\begin{theorem}\label{Theorem, interval maximum}
If $f:[a,b]\rightarrow{\mathbb R}$ is continuous then we can find
$x_{1},\, x_{2}\in[a,b]$ such that
\[f(x_{1})\leq f(x)\leq f(x_{2})\]
for all $x\in[a,b]$.
\end{theorem}
In other words a continuous function on a closed bounded
interval is bounded and attains its bounds. 
\section{Differentiation} In this section it will
be useful to have another type of limit.
\begin{definition}\label{Another limit}
Let $E$ be a subset of ${\mathbb R}$,
$f$ be some function from $E$ to ${\mathbb R}$,
and $x$ some point of $E$. If $l\in{\mathbb R}$
we say that $f(y)\rightarrow l$ as $y\rightarrow x$
[or, if we wish to emphasise the restriction to $E$
that $f(y)\rightarrow l$ as $y\rightarrow x$ through
values $y\in E$]
if, given $\epsilon>0$, we can find a
$\delta(\epsilon)>0$ [read `a delta depending on epsilon']
such that
\[|f(y)-l|<\epsilon\]
for all $y\in E$ with $0<|x-y|<\delta(\epsilon)$.
\end{definition}
As before there is no real loss if the reader
initially takes $E={\mathbb R}$.

The following two exercises are easy but useful.
\begin{exercise}\label{continuous equivalent}
Let $E$ be a subset of ${\mathbb R}$.
Show that a function $f:E\rightarrow{\mathbb R}$
is continuous at $x\in E$ if and only if 
$f(y)\rightarrow f(x)$ as $y\rightarrow x$.
\end{exercise}
\begin{exercise}\label{real limits}
Let $E$ be a subset of ${\mathbb R}$,
$f$, $g$ be some functions from $E$ to ${\mathbb R}$,
and $x$ some point of $E$.
(i) The limit is unique. That is, if $f(y)\rightarrow l$
and $f(y)\rightarrow k$ as $y\rightarrow x$
then $l=k$.

(ii) If $x\in E'\subseteq E$ and $f(y)\rightarrow l$
as $y\rightarrow x$ through
values $y\in E$, then $f(y)\rightarrow l$
as $y\rightarrow x$ through
values $y\in E'$.

(iii) If $f(t)=c$ for all $t\in E$ then $f(y)\rightarrow c$
as $y\rightarrow x$.

(iv) If $f(y)\rightarrow l$ and $g(y)\rightarrow k$
as $y\rightarrow x$ then
$f(y)+g(y)\rightarrow l+k$.

(v) If $f(y)\rightarrow l$ and $g(y)\rightarrow k$
as $y\rightarrow x$ then
$f(y)g(y)\rightarrow lk$.

(vi) If $f(y)\rightarrow l$
as $y\rightarrow x$, $f(t)\neq 0$ for each $t\in E$ and
$l\neq 0$ then $f(t)^{-1}\rightarrow l^{-1}$.

(vii) If $f(t)\leq L$ for each $t\in E$ and
$f(y)\rightarrow l$ 
as $y\rightarrow x$ then $l\leq L$.
\end{exercise}
We can now define the derivative.
\begin{definition}\label{Definition, Derivative}
Let $E$ be a subset of ${\mathbb R}$.
A function $f:E\rightarrow{\mathbb R}$
is differentiable at $x\in E$ with derivative $f'(x)$
if
\[\frac{f(y)-f(x)}{y-x}\rightarrow f'(x)\]
as $y\rightarrow x$.

If $f$ is differentiable at each point $x\in E$, we say that
$f$ is a differentiable function on $E$.
\end{definition}
As usual, no harm will be done if you replace $E$
by ${\mathbb R}$.

Here are some easy consequences of the definition.
\begin{exercise} Let $E$ be a subset of ${\mathbb R}$,
$f$ some function from $E$ to ${\mathbb R}$,
and $x$ some point of $E$. Show that if
$f$ is differentiable at $x$ then
$f$ is continuous at $x$.
\end{exercise}
\begin{exercise}\label{derivatives}
Let $E$ be a subset of ${\mathbb R}$,
$f$, $g$ be some functions from $E$ to ${\mathbb R}$,
and $x$ some point of $E$. Prove the following results.

(i) If $f(t)=c$ for all $t\in E$ then $f$ is differentiable
at $x$ with $f'(x)=0$.

(ii) If $f$ and $g$ are differentiable at $x$ then so
is their sum $f+g$ and 
\[(f+g)'(x)=f'(x)+g'(x).\]

(iii) If $f$ and $g$ are differentiable at $x$ then so
is their product $f\times g$ and 
\[(f\times g)'(x)=f'(x)g(x)+f(x)g'(x).\]

(iv) If $f$ is differentiable at $x$ and $f(t)\neq 0$ for all $t\in E$
then $1/f$ is differentiable at $x$ and
\[(1/f)'(x)=-f'(x)/f(x)^{2}.\]

(v) If $f(t)=\sum_{r=0}^{N}a_{r}t^{r}$ on $E$ then 
$f$ is differentiable at $x$ and
\[f'(x)=\sum_{r=1}^{N}ra_{r}x^{r-1}\].
\end{exercise}

The next result is slightly harder to prove
than it looks. (we split the proof into two halves
depending on whether $f'(x)\neq 0$ or $f'(x)=0$.
\begin{lemma}\label{chain rule}{\bf [Chain rule]} 
Let $U$ and $V$ be subsets of ${\mathbb R}$. Suppose
$f:U\rightarrow{\mathbb R}$ is such that
$f(t)\in V$ for all $t\in U$. If $f$
is differentiable at $x\in U$ and $g:V\rightarrow{\mathbb R}$
is differentiable at $f(x)$, then the composition
$g\circ f$ is differentiable at $x$ with
\[(g\circ f)'(x)=f'(x)g'(f(x)).\]
\end{lemma}

\section{The mean value theorem} We have almost finished
our project of showing that the horrid situation
revealed by Example~\ref{Rational}
can not occur for the reals.

Our first step is to prove Rolle's
theorem.
\begin{theorem}{\bf [Rolle's theorem]}\label{Rolle}
If $g:[a,b]\rightarrow {\mathbb  R}$
is a continuous function with $g$ differentiable on $(a,b)$
and $g(a)=g(b)$, then we can find a $c\in (a,b)$ such that
$g'(c)=0$.
\end{theorem}

A simple tilt gives the famous mean value theorem.
\begin{theorem}{\bf (The mean value theorem).}\label{mean}
If $f:[a,b]\rightarrow {\mathbb  R}$
is a continuous function with $f$ differentiable on $(a,b)$,
then we can find a $c\in (a,b)$ such that
\[f(b)-f(a)=(b-a)f'(c).\]
\end{theorem}

We now have the results so long desired.
\begin{lemma}\label{easy mean}
If $f:[a,b]\rightarrow {\mathbb  R}$
is a continuous function with $f$ differentiable on $(a,b)$,
then the following results hold.

(i) If $f'(t)>0$ for all $t\in (a,b)$ then $f$ is strictly increasing
on $[a,b]$. (That is, $f(y)>f(x)$ whenever $b\geq y>x\geq a$.)

(ii) If $f'(t)\geq 0$ for all $t\in (a,b)$ then $f$ is increasing
on $[a,b]$. (That is, $f(y)\geq f(x)$ whenever $b\geq y>x\geq a$.)

(iii) {\bf [The constant value theorem]}
If $f'(t)=0$ for all $t\in (a,b)$ then $f$ is constant
on $[a,b]$. (That is, $f(y)=f(x)$ whenever $b\geq y>x\geq a$.)
\end{lemma}
Notice that since we deduce Lemma~\ref{easy mean}
from the mean value theorem we can not use it
in the proof of Rolle's theorem.

The mean value theorem has many important consequences,
some of which we look at in the remainder of the section.

We start by looking at inverse functions.
\begin{lemma} (i) Suppose $f:[a,b]\rightarrow{\mathbb R}$
is continuous. Then $f$ is injective if and only
if it is strictly increasing (that is $f(t)>f(s)$ whenever
$a\leq s<t\leq b$) or strictly decreasing.

(ii) Suppose $f:[a,b]\rightarrow{\mathbb R}$
is continuous and strictly increasing. Let $f(a)=c$
and $f(b)=d$. Then the map $f:[a,b]\rightarrow[c,d]$
is bijective and $f^{-1}$ is continuous on $[c,d]$.
\end{lemma}
\begin{lemma}{\bf [Inverse rule]} Suppose
$f:[a,b]\rightarrow{\mathbb R}$
is differentiable on $[a,b]$ and $f'(x)>0$ for
all $x\in[a,b]$. Let $f(a)=c$
and $f(b)=d$. Then the map $f:[a,b]\rightarrow[c,d]$
is bijective and $f^{-1}$ is differentiable on $[c,d]$
with
\[f^{-1}{}'(x)=\frac{1}{f'(f^{-1}(x))}.\]
\end{lemma}
In the opinion of the author the true meaning of the
inverse rule and the chain rule only becomes clear
when we consider higher dimensions in the next analysis course.

We now prove a form of Taylor's theorem.
\begin{theorem}{\bf [$n$th mean value theorem]}\label{many mean}
Suppose that $b>a$ and $f:[a,b]\rightarrow{\mathbb R}$ is $n+1$ times
differentiable. Then
\[f(b)-\sum_{j=0}^{n}\frac{f^{(j)}(a)}{j!}(b-a)^{j}=
\frac{f^{(n+1)}(c')}{(n+1)!}(b-a)^{n+1}\]
for some $c'$ with $a<c'<b$.
\end{theorem}
This gives us a global and a local Taylor's theorem.

\begin{theorem}{\bf [Global Taylor theorem]}\label{Global theta}
Suppose that $b>a$ and $f:[a,b]\rightarrow{\mathbb R}$ is $n+1$ times
differentiable. If $x,\, t\in[a,b]$
\[f(t)=\sum_{j=0}^{n}\frac{f^{(j)}(x)}{j!}(t-x)^{j}
+\frac{f^{(n+1)}(x+\theta(t-x))}{(n+1)!}(t-x)^{n+1}\]
for some $\theta\in(0,1)$.
\end{theorem}
\begin{theorem}\label{local Taylor}{\bf [Local Taylor theorem]}
Suppose that $\delta>0$ and $f:(x-\delta,x+\delta)\rightarrow{\mathbb R}$ 
is $n$ times
differentiable on $(x-\delta,x+\delta)$ and $f^{(n)}$ is
continuous at $x$. Then
\[f(t)=\sum_{j=0}^{n}\frac{f^{(j)}(x)}{j!}(t-x)^{j}+
\epsilon(t)(t-x)^{n}\]
where $\epsilon(t)\rightarrow 0$ as $t\rightarrow x$.
\end{theorem}
Notice that the local Taylor theorem always gives us some information
but that the global one is useless unless we can find
a useful bound on the $n+1$th derivative.

To reinforce this warning we consider a famous example of Cauchy.
\begin{exercise}\label{start bump} Consider the function
$F:{\mathbb R}\rightarrow{\mathbb R}$ defined by
\begin{alignat*}{2}
F(0)&=0\\
F(x)&=\exp(-1/x^{2})&&\qquad\text{otherwise}.
\end{alignat*}

(i) Prove by induction, using the standard rules of differentiation,
that $F$ is infinitely differentiable at all points $x\neq 0$
and that, at these points,
\[F^{(n)}(x)=P_{n}(1/x)\exp(-1/x^{2})\]
where $P_{n}$ is a polynomial which need not be found explicitly.

(ii) Explain why $x^{-1}P_{n}(1/x)\exp(-1/x^{2})\rightarrow 0$
as $x\rightarrow 0$.

(iii) Show by induction, using the definition of differentiation,
that $F$ is infinitely differentiable at $0$
with $F^{(n)}(0)=0$ for all $n$.
[Be careful to get this part of the argument
right.]

(iv) Show that
\[F(x)=\sum_{j=0}^{\infty}\frac{F^{(j)}(0)}{j!}x^{j}\]
if and only if $x=0$. (The reader may prefer to say
that `The Taylor expansion of $F$ is only valid at $0$'.)

(v) Why does part~(iv) not contradict the local Taylor theorem
(Theorem~\ref{local Taylor})?
\end{exercise}
Since examiners are fonder of the global Taylor theorem
than it deserves I shall go through the following example.
\begin{example} Assuming the standard properties of
the exponential function show that
\[\exp x=\sum_{j=0}^{\infty}\frac{x^{j}}{j!}\]
for all $x$.
\end{example}
Please note that in a pure mathematics question many (or even most)
of the marks in a question of this type
will depend on estimating the remainder term.
[In methods questions you may simply be asked to
`find the Taylor's series' without being asked to prove
convergence.]
\section{Complex variable} The field ${\mathbb C}$
of complex numbers resembles the field ${\mathbb R}$
of real numbers in many ways but not in all.
\begin{lemma} We can not define an order on ${\mathbb C}$
which will behave in the same way as $>$ for ${\mathbb R}$.
\end{lemma}
However there is sufficient similarity for us to define
limits, continuity and differentiability.
(We have already seen some of this in 
Definition~\ref{complex convergence definition}
and Exercise~\ref{complex sequences}.)

\begin{definition}\label{Another complex limit}
Let $E$ be a subset of ${\mathbb C}$,
$f$ be some function from $E$ to ${\mathbb C}$,
and $z$ some point of $E$. If $l\in{\mathbb C}$
we say that $f(w)\rightarrow l$ as $w\rightarrow z$
[or, if we wish to emphasise the restriction to $E$
that $f(w)\rightarrow l$ as $w\rightarrow z$ through
values $w\in E$]
if, given $\epsilon>0$, we can find a
$\delta(\epsilon)>0$ [read `a delta depending on epsilon']
such that
\[|f(w)-l|<\epsilon\]
for all $w\in E$ with $0<|w-z|<\delta(\epsilon)$.
\end{definition}
As usual there is no real loss if the reader
initially takes $E={\mathbb C}$.

\begin{definition}\label{complex continuity}
Let $E$ be a subset of ${\mathbb C}$.
We say that a function $f:E\rightarrow{\mathbb C}$
is continuous at $z\in E$ if and only if
$f(w)\rightarrow f(z)$ as $w\rightarrow z$.
\end{definition}
\begin{exercise}\label{complex limits}
Let $E$ be a subset of ${\mathbb C}$,
$f$, $g$ be some functions from $E$ to ${\mathbb C}$,
and $z$ some point of $E$.

(i) The limit is unique. That is, if $f(w)\rightarrow l$
and $f(w)\rightarrow k$ as $w\rightarrow z$
then $l=k$.

(ii) If $z\in E'\subseteq E$ and $f(w)\rightarrow l$
as $w\rightarrow z$ through
values $w\in E$, then $f(w)\rightarrow l$
as $w\rightarrow x$ through
values $w\in E'$.

(iii) If $f(u)=c$ for all $u\in E$ then $f(w)\rightarrow c$
as $w\rightarrow z$.

(iv) If $f(w)\rightarrow l$ and $g(w)\rightarrow k$
as $w\rightarrow z$ then
$f(w)+g(w)\rightarrow l+k$.

(v) If $f(w)\rightarrow l$ and $g(w)\rightarrow k$
as $w\rightarrow z$ then
$f(w)g(w)\rightarrow lk$.

(vi) If $f(w)\rightarrow l$
as $w\rightarrow z$, $f(u)\neq 0$ for each $u\in E$ and
$l\neq 0$ then $f(w)^{-1}\rightarrow l^{-1}$.
\end{exercise}
\begin{exercise}\label{first continuous complex}
Suppose that $E$ is a subset of ${\mathbb C}$,
that $z\in E$,
and that $f$ and $g$
are functions from $E$ to ${\mathbb C}$.

(i) If $f(u)=c$ for all $u\in E$, then f is continuous
on $E$.

(ii) If $f$ and $g$ are continuous at $z$,
then so is $f+g$.

(iii) Let us define $f\times g:E\rightarrow {\mathbb C}$
by $f\times g(u)=f(u)g(u)$ for all $u\in E$. Then
if $f$ and $g$ are continuous at $z$, so is $f\times g$.

(iv) Suppose that $f(u)\neq 0$ for all $u\in E$. If
$f$ is continuous at $z$ so is $1/f$.

(v) If $z\in E'\subset E$ and $f$ is continuous at $z$ then the
restriction $f|_{E'}$ of $f$ to $E'$ is also continuous
at $z$.

(vi) If $J:{\mathbb C}\rightarrow{\mathbb C}$ is
defined by $J(z)=z$ for all $z\in{\mathbb C}$,
then $J$ is continuous on ${\mathbb C}$.

(vii) Every polynomial $P$ is continuous on ${\mathbb C}$.

(viii) Suppose that $P$ and $Q$ are polynomials
and that $Q$ is never zero on some subset $E$
of ${\mathbb C}$. Then the rational function
$P/Q$ is continuous on $E$ (or, more precisely,
the restriction of $P/Q$ to $E$ is continuous.)
\end{exercise}
\begin{exercise}\label{continuous complex chain rule}
Let $U$ and $V$ be subsets of ${\mathbb C}$. Suppose
$f:U\rightarrow{\mathbb C}$ is such that
$f(z)\in V$ for all $z\in U$. If $f$
is continuous at $w\in U$ and $g:V\rightarrow{\mathbb C}$
is continuous at $f(w)$, then the composition
$g\circ f$ is continuous at $w$.
\end{exercise}

\begin{definition}\label{Definition, Complex Derivative}
Let $E$ be a subset of ${\mathbb C}$.
A function $f:E\rightarrow{\mathbb C}$
is differentiable at $z\in E$ with derivative $f'(z)$
if
\[\frac{f(w)-f(z)}{w-z}\rightarrow f'(z)\]
as $w\rightarrow z$.

If $f$ is differentiable at each point $z\in E$ we say that
$f$ is a differentiable function on $E$.
\end{definition}
\begin{exercise}\label{derivatives complex -1}
Let $E$ be a subset of ${\mathbb C}$,
$f$ some function from $E$ to ${\mathbb C}$,
and $z$ some point of $E$. Show that if
$f$ is differentiable at $z$ then
$f$ is continuous at $z$.
\end{exercise}
\begin{exercise}\label{derivatives complex}
Let $E$ be a subset of ${\mathbb C}$,
$f$, $g$ be some functions from $E$ to ${\mathbb C}$,
and $z$ some point of $E$. Prove the following results.

(i) If $f(u)=c$ for all $u\in E$ then $f$ is differentiable
at $z$ with $f'(z)=0$.

(ii) If $f$ and $g$ are differentiable at $z$ then so
is their sum $f+g$ and
\[(f+g)'(z)=f'(z)+g'(z).\]

(iii) If $f$ and $g$ are differentiable at $z$ then so
is their product $f\times g$ and
\[(f\times g)'(z)=f'(z)g(z)+f(z)g'(z).\]

(iv) If $f$ is differentiable at $z$ and $f(u)\neq 0$ for all $u\in E$
then $1/f$ is differentiable at $z$ and
\[(1/f)'(z)=-f'(z)/f(z)^{2}.\]

(v) If $f(u)=\sum_{r=0}^{N}a_{r}u^{r}$ on $E$
then $f$ is differentiable at $z$ and
\[f'(z)=\sum_{r=1}^{N}ra_{r}z^{r-1}\].
\end{exercise}
\begin{exercise}\label{complex chain rule}{\bf [Chain rule]}
Let $U$ and $V$ be subsets of ${\mathbb C}$. Suppose
$f:U\rightarrow{\mathbb C}$ is such that
$f(z)\in V$ for all $t\in U$. If $f$
is differentiable at $w\in U$ and $g:V\rightarrow{\mathbb R}$
is differentiable at $f(w)$, then the composition
$g\circ f$ is differentiable at $w$ with
\[(g\circ f)'(w)=f'(w)g'(f(w)).\]
\end{exercise}

In spite of these similarities the subject of 
complex differentiable functions is very different
from that of real differentiable functions.
It turns out that `well behaved' complex functions
need not be differentiable.
\begin{example} Consider the map $\Gamma:{\mathbb C}\rightarrow{\mathbb C}$
given by $\Gamma(z)=z^{*}$. The function $\Gamma$ is
nowhere differentiable.
\end{example}
Because complex differentiability is so much more restrictive
than real differentiability we can prove stronger theorems
about  complex differentiable functions. For example it
can be shown that such functions can be written locally 
as power series\footnote{The syllabus says that this
fact is part of this course. In this one instance
I advise you to ignore the syllabus.}
(contrast the situation in the real case revealed by
Example~\ref{start bump}). To learn more
go to the course~P3 on complex methods.
\section{Power series} In this section we work in
${\mathbb C}$ unless otherwise stated.
We start with a very
useful observation.
\begin{lemma} If $\sum_{n=0}^{\infty}a_{n}z_{0}^{n}$ converges
and $|z|<|z_{0}|$ then $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges.
\end{lemma}
This gives us the following basic theorem on
power series.
\begin{theorem}\label{radius of convergence}
Suppose that $a_{n}\in{\mathbb C}$. Then either
$\sum_{n=0}^{\infty}a_{n}z^{n}$ converges for
all $z\in{\mathbb C}$, or there exists a real number $R$
with $R\geq 0$ such that

(i) $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges if $|z|<R$,

(ii) $\sum_{n=0}^{\infty}a_{n}z^{n}$ diverges if $|z|>R$.
\end{theorem}
We call $R$ the radius of convergence of $\sum_{n=0}^{\infty}a_{n}z^{n}$.
If $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges for all $z$
we write $R=\infty$.
The following useful strengthening is left to
the reader as an exercise.
\begin{exercise}\label{New radius}
Suppose that $\sum_{n=0}^{\infty}a_{n}z^{n}$
has radius of convergence $R$.
Then the sequence $|a_{n}z^{n}|$ is unbounded if $|z|>R$
and $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges absolutely
if $|z|<R$.
\end{exercise}
Note that we say nothing about what happens on the circle of convergence.
\begin{example} (i) $\sum_{n=1}^{\infty}n^{-2}z^{n}$ has radius of convergence
$1$ and converges for all $z$ with $|z|=1$.

(ii) $\sum_{n=1}^{\infty}z^{n}$ has radius of convergence
$1$ and diverges for all $z$ with $|z|=1$.
\end{example}
A more complicated example is given in Exercise~\ref{Many pointed}.

It is a remarkable fact that we can operate with power series
in the same way as polynomials (within the radius of convergence).
In particular we shall show that we can differentiate term by term.
\begin{theorem} If $\sum_{n=0}^{\infty}a_{n}z^{n}$ has radius
of convergence $R$ and we write $f(z)= \sum_{n=0}^{\infty}a_{n}z^{n}$
then $f$ is differentiable at all points $z$ with $|z|<R$ and
\[f'(z)=\sum_{n=1}^{\infty}na_{n}z^{n-1}.\]
\end{theorem}

The proof is starred in the syllabus. We use three simple observations.
\begin{lemma} If $\sum_{n=0}^{\infty}a_{n}z^{n}$ has radius
of convergence $R$ then given any $\epsilon>0$ we can find
a $K(\epsilon)$ such that $\sum_{n=0}^{\infty}|a_{n}z^{n}|<K(\epsilon)$
for all $|z|\leq R-\epsilon$.
\end{lemma}
\begin{lemma} If $\sum_{n=0}^{\infty}a_{n}z^{n}$ has radius
of convergence $R$ then so do $\sum_{n=1}^{\infty}na_{n}z^{n-1}$
and $\sum_{n=2}^{\infty}n(n-1)a_{n}z^{n-2}$.
\end{lemma}
\begin{lemma} (i) ${\displaystyle \binom{n}{r}\leq n(n-1)\binom{n-2}{r-2}}$
for all $2\leq r\leq n$.

(ii) $|(z+h)^{n}-z^{n}-nhz^{n-1}|\leq n(n-1)(|z|+|h|)^{n-2}|h|^{2}$
for all $z,\ h\in{\mathbb C}$.
\end{lemma}

In this course we shall mainly work on the real line.
Restricting to the real line we obtain the following
result.
\begin{theorem} (i) If $a_{j}\in{\mathbb R}$ there exists a
unique $R$ (the radius of convergence) with $0\leq R\leq\infty$ such that
$\sum_{n=0}^{\infty}a_{n}x^{n}$ converges for all real
$x$ with $|x|<R$ and diverges for all real $x$ with $x>R$.

(ii) If $\sum_{n=1}^{\infty}a_{n}x^{n}$ has radius
of convergence $R$ and we write $f(x)= \sum_{n=0}^{\infty}a_{n}x^{n}$
then $f$ is differentiable at all points $x$ with $|x|<R$ and
\[f'(x)=\sum_{n=1}^{\infty}na_{n}x^{n-1}.\]
\end{theorem}
\section{The standard functions} In school you learned
all about the functions $\exp$, $\log$, $\sin$ and $\cos$
and about the behaviour of $x^{\alpha}$. Nothing that you
learned was wrong (we hope) but you might be hard
pressed to prove all the facts you know in a coherent
manner,

To get round this problem, we start from scratch
making new definitions and assuming nothing about these
various functions. One of your tasks is to make sure
that the lecturer does not slip in some unproved fact.
On the other hand you must allow your lecturer to choose
definitions which allow an easy development of the subject rather
than those that follow some `historic', `intuitive' 
or `pedagogically appropriate' path\footnote{If you
want to see a treatment along these lines see
the excellent text of Burn\cite{Burn}.}.

Let us start with the exponential function.
Throughout this section we shall restrict ourselves
to the real line.
\begin{lemma} The sum $\sum_{n=0}^{\infty}\frac{x^{n}}{n!}$
has infinite radius of convergence.
\end{lemma}
We can thus define a function $e:{\mathbb R}\rightarrow{\mathbb R}$
by
\[e(x)=\sum_{n=0}^{\infty}\frac{x^{n}}{n!}.\]
(We use $e(x)$ rather than $\exp(x)$ to help us
avoid making unjustified assumptions.)

\begin{theorem}\label{T, real exponential}
(i) The function $e:{\mathbb R}\rightarrow{\mathbb R}$
is everywhere differentiable with $e'(x)=e(x)$.

(ii) $e(x+y)=e(x)e(y)$ for all $x,\, y\in{\mathbb R}$.

(iii) $e(x)>0$ for all $x\in{\mathbb R}$.

(iv) $e$ is a strictly increasing function.

(v) $e(x)\rightarrow\infty$ as $x\rightarrow\infty$,
$e(x)\rightarrow 0$ as $x\rightarrow -\infty$.

(vi) $e:{\mathbb R}\rightarrow (0,\infty)$ is a bijection.
\end{theorem}

It is worth stating some of our results in the language of group
theory.
\begin{lemma} The mapping $e$ is an isomorphism of
the group $({\mathbb R},+)$ and the group $((0,\infty),\times)$.
\end{lemma}

Since $e:{\mathbb R}\rightarrow (0,\infty)$ is a bijection
we can consider the inverse function $l:(0,\infty)\rightarrow{\mathbb R}$.

\begin{theorem} 
(i) $l:(0,\infty)\rightarrow{\mathbb R}$ is a bijection.
We have $l(e(x))=x$ for all $x\in{\mathbb R}$
and $e(l(t))=t$ for all $t\in (0,\infty)$.

(ii) The function $l:(0,\infty)\rightarrow{\mathbb R}$
is everywhere differentiable with $l'(t)=1/t$.

(iii) $l(uv)=l(u)+l(v)$ for all $u,\, v\in(0,\infty)$.
\end{theorem}

We now write $e(x)=\exp(x)$, $l(t)=\log t$ (the use
of $\ln$ is unnecessary). If $\alpha\in{\mathbb R}$
and $x>0$.
we write $r_{\alpha}(x)=\exp(\alpha \log x)$.
\begin{theorem}\label{theorem, laws of indices} 
Suppose $x,\, y>0$ and $\alpha,\, \beta\in{\mathbb R}$.
Then

(i) $r_{\alpha}(xy)=r_{\alpha}(x)r_{\alpha}(y)$,

(ii) $r_{\alpha+\beta}(x)=r_{\alpha}(x)r_{\beta}(x)$,

(iii) $r_{\alpha}(r_{\beta}(x))=r_{\alpha\beta}(x)$

(iv) $r_{1}(x)=x$.
\end{theorem}

\begin{exercise}\label{exercise, index OK} 
Use the results of Theorem~\ref{theorem, laws of indices}
to show that if $n$ is a strictly positive integer
and $x>0$ then
\[r_{n}(x)=\underbrace{xx\dots x}_{n}.\]
\end{exercise}

Thus, if we write $x^{\alpha}=r_{\alpha}(x)$, our new notation is
\emph{consistent} with our old school notation
when $\alpha$ is rational but gives, in addition,
a valid definition when $\alpha$ is irrational.
\begin{lemma} (i) If $\alpha$ is real, 
$r_{\alpha}:(0,\infty)\rightarrow(0,\infty)$ is
everywhere differentiable and $r_{\alpha}'(x)=\alpha r_{\alpha -1}(x)$.

(ii) If $x>0$ and we define $f_{x}(\alpha)=x^{\alpha}$
then $f_{x}:{\mathbb R}\rightarrow (0,\infty)$ is
everywhere differentiable and $f_{x}'(\alpha)=\log x f_{x}(\alpha)$.
\end{lemma}

Finally we look at the trigonometric functions.
\begin{lemma} The sums $\sum_{n=0}^{\infty}\frac{(-1)^{n}x^{2n+1}}{(2n+1)!}$
and $\sum_{n=0}^{\infty}\frac{(-1)^{n}x^{2n}}{(2n)!}$
have infinite radius of convergence.
\end{lemma}
We can thus define functions $s,\, c:{\mathbb R}\rightarrow{\mathbb R}$
by
\[s(x)=\sum_{n=0}^{\infty}\frac{(-1)^{n}x^{2n+1}}{(2n+1)!}
\ \text{and}\ c(x)=\sum_{n=0}^{\infty}\frac{(-1)^{n}x^{2n}}{(2n)!}\]

\begin{lemma}\label{lemma, trigonometry}
(i) The functions $s,\, c:{\mathbb R}\rightarrow{\mathbb R}$
are everywhere differentiable with $s'(x)=c(x)$ and $c'(x)=-s(x)$.

(ii) $s(x+y)=s(x)c(y)+c(x)s(y)$, $c(x+y)=c(x)c(y)-s(x)s(y)$ and
$s(x)^{2}+c(x)^{2}=1$
for all $x,\, y\in{\mathbb R}$.

(iii) $s(-x)=-s(x)$, $c(-x)=c(x)$ for all $x\in{\mathbb R}$.
\end{lemma}

\begin{exercise}\label{Exercise, sinh}
Write down what you consider to be the
chief properties of $\sinh$ and $\cosh$. (They should convey
enough information to draw a reasonable graph of the
two functions.)

(i) Obtain those properties by starting with the definitions
\[\sinh x=\sum_{n=0}^{\infty}\frac{x^{2n+1}}{(2n+1)!}
\ \text{and}\ \cosh x=\sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!}\]
and proceeding along the lines of Lemma~\ref{lemma, trigonometry}.

(ii) Obtain those properties by starting with the definitions
\[\sinh x=\frac{\exp x-\exp (-x)}{2}
\ \text{and}\ \cosh x=\frac{\exp x+\exp (-x)}{2}\]
\end{exercise}

We have not yet proved one of the most remarkable properties
of the sine and cosine functions, their periodicity.

\begin{theorem} Let $s$ and $c$ be as in Lemma~\ref{lemma, trigonometry}.

(i) If $c(a)=0$ and $c(b)=0$ then $s(b-a)=0$.

(ii) We have $s(x)>0$ for all $0<x\leq 1$.

(iii) There exists a unique $\omega\in[0,2]$ such that $c(\omega)=0$.

(iv) $s(\omega)=1$.

(v) $s(x+4\omega)=s(x)$, $c(x+4\omega)=c(\omega)$.

(vi) The function $c$ is strictly decreasing from $1$ to $-1$
as $x$ runs from $0$ to $2\omega$,
\end{theorem}

We now {\bf define} $\pi=2\omega$. If ${\mathbf x}$ and ${\mathbf y}$
are non-zero vectors in ${\mathbb R}^{m}$ we 
know by the Cauchy-Schwarz inequality that
$|{\mathbf x}.{\mathbf y}|\leq \|{\mathbf x}\|\|{\mathbf y}\|$
and we may {\bf define} the angle between the two vectors
to be that $\theta$ with $0\leq\theta\leq \pi$ which satisfies
\[\cos\theta=\frac{{\mathbf x}.{\mathbf y}}{\|{\mathbf x}\|\|{\mathbf y}\|}.\]

We can also justify the standard use of polar coordinates. 
\begin{lemma} If $(x,y)\in{\mathbb R}^{2}$ and $(x,y)\neq(0,0)$
then there exist a unique $r>0$ and $\theta$ with $0\leq\theta<2\pi$
such that
\[x=r\cos\theta,\ y=r\sin\theta.\]
\end{lemma}

You may be unhappy with a procedure which reduces geometry to analysis.
It is possible to produce treatments which soften the blow\footnote{The
matter is more subtle than it looks. Classical Euclidean geometry
is `weaker' than the geometry required to justify analysis
and if you wished to obtain analysis from geometry you would need
to add extra axioms.} but what we can not do is to justify analytic
results by appealing to geometry and then appeal to analysis
to justify the geometry.
\section{Onwards to the complex plane} This section contains
useful background material which does not really form
part of the course. If time is short I shall omit it entirely.

The mean value theorem {\bf fails} for differentiable
functions $f:{\mathbb C}\rightarrow{\mathbb C}$.
(See Example~\ref{no complex mean}.)
However, the constant value theorem holds.
\begin{theorem}\label{T, constant complex value}
If $f:{\mathbb C}\rightarrow{\mathbb C}$
is differentiable and $f'(z)=0$ for all $z\in{\mathbb C}$
then $f$ is constant.
\end{theorem}

The proof of Theorem~\ref{T, constant complex value}
that I shall give is very ad hoc and you will meet 
better ones later.

Since the constant value theorem holds we can extend the proof
of Theorem~\ref{T, real exponential}~(ii) to this wider context
and obtain a version of the exponential function
for complex numbers. In this section we work in ${\mathbb C}$
unless otherwise stated.
\begin{lemma} The sum $\sum_{n=0}^{\infty}\frac{z^{n}}{n!}$
has infinite radius of convergence.
\end{lemma}
We can thus define a function $\exp:{\mathbb C}\rightarrow{\mathbb C}$
by
\[\exp(z)=\sum_{n=0}^{\infty}\frac{z^{n}}{n!}.\]

\begin{theorem}\label{T, complex exponential}
(i) The function $e:{\mathbb C}\rightarrow{\mathbb C}$
is everywhere differentiable with $e'(z)=e(z)$.

(ii) $e(z+w)=e(z)e(w)$ for all $z,\, w\in{\mathbb C}$.
\end{theorem}

Notice that the remaining parts of Theorem~\ref{T, real exponential}
are either meaningless or false
(compare Theorem~\ref{T, real exponential}~(vi)
with Lemma~\ref{famous}~(iii) which shows that
$\exp:{\mathbb C}\rightarrow{\mathbb C}$ is
not injective). We must be very careful 
in making the transition from real to complex.

We obtain a series of famous formulae.
\begin{lemma}\label{famous} (i) If $\theta$ is real
\[\exp i\theta=\cos\theta+i\sin\theta.\]

(ii) If $x$ and $y$ are real
\[\exp (x+iy)=(\exp x)(\cos y+i\sin y).\]

(iii) $\exp$ is periodic with period $2\pi i$, that is to say
\[\exp(z+2\pi i)=\exp z\]
for all $z\in{\mathbb C}$.

(iv) $\exp({\mathbb C})={\mathbb C}\setminus\{0\}$.
\end{lemma}
\begin{example}~\label{no complex mean} 
Observe that $\exp 0=\exp 2\pi i=1$ but
$\exp'(z)=\exp(z)\neq 0$
for all $z\in{\mathbb C}$. Thus the  mean value theorem
does not hold for differentiable
functions $f:{\mathbb C}\rightarrow{\mathbb C}$.
\end{example}

It is also possible to extend the definition of
sine and cosine to the complex plane but the
reader is warned that the behaviour of the
new functions may be somewhat unexpected.
Since these extended functions most certainly
do not form part of this course (though you will be
expected to know them after the Complex Methods course)
their study is left as an exercise.

\begin{exercise}\label{complex sine}
(i) Explain why the infinite sums
\[\sin z=\sum_{n=0}^{\infty}\frac{(-1)^{n}z^{2n+1}}{(2n+1)!}
\ \text{and}\ \cos z=\sum_{n=0}^{\infty}\frac{(-1)^{n}z^{2n}}{(2n)!}\]
converge everywhere and are differentiable everywhere
with $\sin' z=\cos z$, $\cos' z=-\sin z$.

(ii) Show that
\[\sin z=\frac{\exp iz-\exp (-iz)}{2i},\ \cos z=\frac{\exp iz+\exp (-iz))}{2}\]
and
\[\exp iz=\cos z+i\sin z\]
for all $z\in{\mathbb C}$.

(iii) Show that
\[\sin (z+w)=\sin z\cos w+\cos z\sin w, 
\ \cos(z+w)=\cos z \cos w-\sin z\sin w,\]
and
$(\sin z)^{2}+(\cos z)^{2}=1$
for all $z,\, w\in{\mathbb C}$.

(iv) $\sin(-z)=-\sin z$, $\cos (-z)=\cos z$ for all $z\in{\mathbb C}$.

(v) $\sin$ and $\cos$ are $2\pi$ periodic in the sense that
\[\sin (z+2\pi)=\sin z\ \text{and}\ \cos (z+2\pi)=\cos z\]
for all $z\in{\mathbb C}$.

(vi) If $x$ is real then $\sin ix=i\sinh x$ and $\cos ix=\cosh x$.

(vii) Recover the addition formulae for $\sinh$ and $\cosh$ by
setting $z=ix$ and $w=iy$ in part~(iii).

(ix) Show that $|\sin z|$ and $|\cos z|$ are bounded if $|\Im z|\leq K$
for some $K$ but that $|\sin z|$ and $|\cos z|$ are unbounded
on ${\mathbb C}$.
\end{exercise}

However, as you were already shown in the Algebra and Geometry
course and will be shown again in the Complex Methods course
\begin{center}
{\bf There is no logarithm defined on all of 
${\mathbb C}\setminus\{0\}$.}
\end{center}
\begin{exercise}\label{no logarithm} Suppose,
if possible, that there exists a continuous
$L:{\mathbb C}\setminus\{0\}\rightarrow{\mathbb C}$
with $\exp(L(z))=z$ for all $z\in{\mathbb C}\setminus\{0\}$.

(i) If $\theta$ is real, show that 
$L(\exp(i\theta))=i(\theta+2\pi n(\theta))$ for some
$n(\theta)\in{\mathbb Z}$.

(ii) Define $f:{\mathbb R}\rightarrow{\mathbb R}$
by
\[f(\theta)=\frac{1}{2\pi}
\left(\frac{L(\exp i\theta)-L(1)}{i}-\theta\right).\]
Show that $f$ is a well defined continuous function,
that $f(\theta)\in{\mathbb Z}$ for all $\theta\in{\mathbb R}$,
that $f(0)=0$ and that $f(2\pi)=-1$.

(iii) Show that the statements made in the last
sentence of (ii) are incompatible with the intermediate
value theorem and deduce that no function can exist
with the supposed properties of $L$.

(iv) Discuss informally what connection, if any, the
discussion above has with the existence of the
international date line.
\end{exercise}

A similar argument shows that it is not possible
to produce a continuous square root on the complex plane.
\begin{exercise}\label{no square root} Show by
modifyiing the argument of Exercise~\ref{no logarithm},
that there does not exist a continuous
$S:{\mathbb C}\rightarrow{\mathbb C}$
with $S(z)^{2}=z$ for all $z\in{\mathbb C}$.
\end{exercise}
More generally, it is not possible to define continuous non-integer
powers $z^{\alpha}$ on the complex plane. (Of course,
$z\mapsto z^{n}$ is well behaved if $n$ is an integer.)
However, in the special case when
$x$ is real and strictly positive we can define
\[x^{z}=\exp(z\log x)\] 
without problems and this enables us to write $\exp z=e^{z}$
where $e=\exp 1$.
Surprisingly, Exercise~\ref{no logarithm} is not an end but a beginning
of much important mathematics -- but that is another story.
\section{The Riemann integral}
At school we are taught that an integral is
the area under a curve. If pressed the framer of this
definition might reply that
everybody knows what area is,
but then everybody knows what honey tastes like.
But does honey taste the same to you as it
does to me? Perhaps the question is unanswerable
but, for many practical purposes, it is sufficient
that we agree on what we call honey.

In order to agree on what an integral is,
we need a definition which does not depend on
intuition. It is important that, as far as possible,
the properties of our formally defined integral
shall agree with our intuitive ideas on area
{\bf but we have to prove this agreement}
and not simply assume it.

In this section we introduce a notion of
integral due to Riemann. For the moment we only attempt to
define our integral for
bounded functions on bounded intervals.

Let $f:[a,b]\rightarrow{\mathbb R}$ be a function
such that there exists a $K$ with $|f(x)|\leq K$ for all
$x\in[a,b]$. [To see the connection with `the area
under the curve' it is helpful to suppose initially
that $0\leq f(x)\leq K$. However, all the definitions
and proofs work more generally for  $-K\leq f(x)\leq K$.]

A dissection ${\mathcal D}$ of $[a,b]$
is a finite subset of $[a,b]$ containing the end
points $a$ and $b$. By convention, we write
\[{\mathcal D}=\{x_{0},x_{1},\dots,x_{n}\}
\ \text{with}\ a=x_{0}\leq x_{1}\leq x_{2}\leq\dots\leq x_{n}=b.\]
We define the \emph{upper sum} and \emph{lower sum} 
associated with ${\mathcal D}$ by
\begin{align*}
S(f,{\mathcal D})=&\sum_{j=1}^{n}(x_{j}-x_{j-1})
\sup_{x\in [x_{j-1},x_{j}]}f(x),\\
s(f,{\mathcal D})=&\sum_{j=1}^{n}(x_{j}-x_{j-1})
\inf_{x\in [x_{j-1},x_{j}]}f(x)
\end{align*}
[Observe that, \emph{if the integral $\int_{a}^{b}f(t)\,dt$ exists,}
then the upper sum ought to provide an upper bound and
the lower sum a lower bound for that integral.]

The next lemma is obvious but useful.
\begin{lemma}\label{Lemma, more dissections}
If ${\mathcal D}$ and ${\mathcal D}'$ are
dissections with ${\mathcal D}'\supseteq{\mathcal D}$ then
\[S(f,{\mathcal D})\geq S(f,{\mathcal D}')
\geq
s(f,{\mathcal D}')\geq s(f,{\mathcal D}).\]
\end{lemma}

The next lemma is again hardly more than an observation
but it is the key to the proper treatment of the integral.
\begin{lemma}{\bf [Key integration property]}%
\label{Lemma, Key integration property} If
$f:[a,b]\rightarrow{\mathbb R}$
is bounded and
${\mathcal D}_{1}$ and ${\mathcal D}_{2}$ are two dissections,
then
\begin{equation*}
S(f,{\mathcal D}_{1})\geq S(f,{\mathcal D}_{1}\cup{\mathcal D}_{2})
\geq
s(f,{\mathcal D}_{1}\cup{\mathcal D}_{2})\geq s(f,{\mathcal D}_{2}).
\tag*{$\bigstar$}
\end{equation*}
\end{lemma}
The inequalities $\bigstar$ tell us that, whatever dissection
you pick and whatever dissection I pick, your lower
sum cannot exceed my upper sum. There is no way we
can put a quart in a pint pot\footnote{Or put a litre
in a half litre bottle.}.

Since $S(f,{\mathcal D})\geq -(b-a)K$
for all dissections ${\mathcal D}$ 
we can define the \emph{upper integral} as
$I^{*}(f)=\inf_{{\mathcal D}}S(f,{\mathcal D})$.
We define the \emph{lower integral} similarly
as $I_{*}(f)=\sup_{{\mathcal D}}s(f,{\mathcal D})$.
The inequalities $\bigstar$ tell us that these
concepts behave well.
\begin{lemma} If $f:[a,b]\rightarrow{\mathbb R}$
is bounded, then $I^{*}(f)\geq I_{*}(f)$.
\end{lemma}
[Observe that, \emph{if the integral $\int_{a}^{b}f(t)\,dt$ exists,}
then the upper integral ought to provide an upper bound and
the lower integral a lower bound for that integral.]

If $I^{*}(f)=I_{*}(f)$, we say that $f$ is Riemann
integrable and we write
\[\int_{a}^{b}f(x)\,dx=I^{*}(f).\]

The following lemma provides a convenient criterion
for Riemann integrability.
\begin{lemma}\label{Lemma, Riemann criterion}
(i) A bounded function
$f:[a,b]\rightarrow{\mathbb R}$ is Riemann integrable
if and only if, given any $\epsilon>0$, we can find
a dissection ${\mathcal D}$ with
\[S(f,{\mathcal D})-s(f,{\mathcal D})<\epsilon.\]

(ii) A bounded function
$f:[a,b]\rightarrow{\mathbb R}$ is Riemann integrable
with integral $I$
if and only if, given any $\epsilon>0$, we can find
a dissection ${\mathcal D}$ with
\[S(f,{\mathcal D})-s(f,{\mathcal D})<\epsilon
\ \text{and}\ |S(f,{\mathcal D})-I|\leq\epsilon.\]
\end{lemma}

Many students are tempted to use
Lemma~\ref{Lemma, Riemann criterion}~(ii) as the
\emph{definition} of the Riemann integral.
The reader should reflect that, 
without the inequality $\bigstar$, it is not even clear
that such a definition gives a unique value for $I$.
(This is only the first of a series of nasty problems
that arise if we attempt to develop the theory without
first proving $\bigstar$, so I strongly advise the reader
not to take this path.)

We now prove a series of standard results on the integral.

\begin{lemma}
(i) The function $J:[a,b]\rightarrow{\mathbb R}$ given by
$J(t)=1$ is integrable and
\[\int_{a}^{b} 1\,dx=b-a.\] 

(ii) If $f,g:[a,b]\rightarrow{\mathbb R}$ are
Riemann integrable, then so is $f+g$
and
\[\int_{a}^{b} f(x)+g(x)\,dx
=\int_{a}^{b}f(x)\,dx+ \int_{a}^{b}g(x)\,dx.\]

(iii) If $f:[a,b]\rightarrow{\mathbb R}$ is
Riemann integrable, then $-f$ is and
\[\int_{a}^{b} (-f(x))\,dx
=-\int_{a}^{b}f(x)\,dx.\]

(iv) If $\lambda\in{\mathbb R}$
and $f:[a,b]\rightarrow{\mathbb R}$ is
Riemann integrable, then $\lambda f$ is Riemann
integrable and 
\[\int_{a}^{b} \lambda f(x)\,dx
=\lambda\int_{a}^{b}f(x)\,dx.\]

(v) If $f,g:[a,b]\rightarrow{\mathbb R}$ are
Riemann integrable functions with $f(t)\geq g(t)$ for
all $t\in[a,b]$, then
\[\int_{a}^{b}f(x)\,dx\geq \int_{a}^{b}g(x)\,dx.\]
\end{lemma}
\begin{lemma} (i) If $f:[a,b]\rightarrow{\mathbb R}$ is 
Riemann integrable then so is $f^{2}$.

(ii) If $f,g:[a,b]\rightarrow{\mathbb R}$ are
Riemann integrable, then so is the product $fg$.
\end{lemma}
\begin{lemma} (i) If $f:[a,b]\rightarrow{\mathbb R}$ is
Riemann integrable then so is $f_{+}(t)=\max(f(t),0)$.

(ii) If $f:[a,b]\rightarrow{\mathbb R}$ is
Riemann integrable, then $|f|$ is Riemann
integrable and
\[\int_{a}^{b} |f(x)|\,dx
\geq\left|\int_{a}^{b}f(x)\,dx\right|.\]
\end{lemma}
Notice that we often only need the much weaker
inequality
\[\left|\int_{a}^{b}f(x)\,dx\right|\leq \sup_{t\in[a,b]}|f(t)|(b-a)\]
usually stated as
\[|\text{integral}|\leq\text{length}\times\text{sup}.\]

The next lemma is also routine in its proof
but continues our programme of showing that
the integral has all the properties we expect.
\begin{lemma}  Suppose that $a\leq c\leq b$ and that
$f:[a,b]\rightarrow{\mathbb R}$ is a bounded function. 
Then, $f$ is Riemann integrable on $[a,b]$ if and only
if $f|_{[a,c]}$ is Riemann integrable on $[a,c]$
and $f|_{[c,b]}$ is Riemann integrable on $[c,b]$. Further,
if $f$ is Riemann integrable on $[a,b]$, then
\[\int_{a}^{b}f(x)\,dx=\int_{a}^{c}f|_{[a,c]}(x)\,dx
+\int_{c}^{b}f|_{[c,b]}(x)\,dx.\]
\end{lemma}
In a very slightly less precise and very much more usual notation
we write
\[\int_{a}^{b}f(x)\,dx=\int_{a}^{c}f(x)\,dx
+\int_{c}^{b}f(x)\,dx.\]

There is a standard convention which
says that, if $b\geq a$ and $f$ is Riemann
integrable on $[a,b]$, we define
\[\int_{b}^{a}f(x)\,dx=-\int_{a}^{b}f(x)\,dx.\]
It is, however, a convention that requires care in use.
\begin{exercise}\label{social convention} 
Suppose that $b\geq a$, $\lambda,\mu\in{\mathbb R}$, 
and $f$ and $g$ are Riemann integrable. Which of the
following statements are always true and which are not?
Give a proof or counter-example. If the statement
is not always true, find an appropriate correction
and prove it.

(i) ${\displaystyle \int_{b}^{a}\lambda f(x)+\mu g(x)\,dx
=\lambda\int_{b}^{a}f(x)\,dx+\mu \int_{b}^{a}g(x)\,dx}$.

(ii) If $f(x)\geq g(x)$ for all $x\in[a,b]$, then
${\displaystyle \int_{b}^{a}f(x)\,dx\geq\int_{b}^{a}g(x)\,dx}$.
\end{exercise}

\section{Some properties of the integral}
Not all bounded functions are Riemann integrable
\begin{lemma}
If $f:[0,1]\rightarrow{\mathbb R}$ is
given by
\begin{align*}
f(x)=1&\qquad\text{when $x$ is rational,}\\
f(x)=0&\qquad\text{when $x$ is irrational,}
\end{align*}
then $f$ is not Riemann integrable
\end{lemma}

This does not worry us unduly, but makes
it more important to show that the functions we
wish to be integrable actually are.

Our first result goes back to Riemann
(indeed, essentially, to Newton and Leibniz).
\begin{lemma}\label{Lemma, Riemann Bounded variation}
(i) If $f:[a,b]\rightarrow{\mathbb R}$ is increasing.
then $f$ is Riemann integrable.
 
(ii)If $f:[a,b]\rightarrow{\mathbb R}$ can be written
as $f=f_{1}-f_{2}$ with $f_{1},f_{2}:[a,b]\rightarrow{\mathbb R}$
increasing, then $f$ is Riemann integrable.

(iii) If $f:[a,b]\rightarrow{\mathbb R}$ 
is piecewise monotonic,
then $f$ is Riemann integrable.
\end{lemma}

It should be noted that the results of 
Lemma~\ref{Lemma, Riemann Bounded variation}
do not require $f$ to be continuous.
(For example, the Heaviside function,
given by $H(t)=0$ for $t<0$, $H(t)=1$
for $t\geq 0$ is increasing but not continuous.)
It is quite hard to find a continuous function
which is not the difference of two increasing functions
but 
an example is given in
Exercise~\ref{exercise, bounded variation}.

The proof of the next result is starred. Next
year you will see a simpler proof based on a different
idea (that of uniform continuity).
\begin{theorem}
If $f:[a,b]\rightarrow{\mathbb R}$ is continuous
then $f$ is Riemann integrable.
\end{theorem}

We complete the discussion of integration and this
course with a series of results which apply
only to \emph{continuous} functions.


Our first result is an isolated, but useful, one.
\begin{lemma}\label{Lemma, integral positive}
If $f:[a,b]\rightarrow{\mathbb R}$
is continuous, $f(t)\geq 0$ for all $t\in[a,b]$
and
\[\int_{a}^{b}f(t)\,dt=0,\]
it follows that $f(t)=0$ for all $t\in[a,b]$.
\end{lemma} 

\begin{exercise}\label{Exercise, integral positive zero} 
Let $a\leq c\leq b$.
Give an example of a
Riemann integrable function $f:[a,b]\rightarrow{\mathbb R}$
such that $f(t)\geq 0$ for all $t\in[a,b]$
and
\[\int_{a}^{b}f(t)\,dt=0,\]
but $f(c)\neq 0$.
\end{exercise}

We now come to the justly named fundamental theorem of the calculus.
\begin{theorem}{\bf [The fundamental theorem of the calculus]}%
\label{Theorem, fundamental calculus}
Suppose that $f:(a,b)\rightarrow{\mathbb R}$ is a
continuous function and that $u\in(a,b)$. If we set
\[F(t)=\int_{u}^{t}f(x)\,dx,\]
then $F$ is differentiable on $(a,b)$ and $F'(t)=f(t)$
for all $t\in (a,b)$.
\end{theorem}

\begin{exercise}\label{not fundamental}
(i) Let $H$ be the Heaviside function
$H:{\mathbb R}\rightarrow{\mathbb R}$ given by
$H(x)=0$ for $x<0$, $H(x)=1$ for $x\geq 0$. Calculate
$F(t)=\int_{0}^{t}H(x)\,dx$ and show that $F$ is
not differentiable at $0$. Where does our
proof of Theorem~\ref{Theorem, fundamental calculus}
break down?

(ii) Let $f(0)=1$, $f(t)=0$ otherwise. Calculate
$F(t)=\int_{0}^{t}f(x)\,dx$ and show that $F$ is
differentiable at $0$ but $F'(0)\neq f(0)$. Where does our
proof of Theorem~\ref{Theorem, fundamental calculus}
break down?
\end{exercise}

Sometimes we think of the fundamental theorem
in a slightly different way.
\begin{theorem}\label{Theorem, fundamental as equation} 
Suppose that $f:(a,b)\rightarrow{\mathbb R}$ is
continuous, that $u\in(a,b)$ and $c\in{\mathbb R}$.
Then there is a unique solution
to the differential equation $g'(t)=f(t)$ $[t\in(a,b)]$
such that $g(u)=c$.
\end{theorem}
We call the solutions of $g'(t)=f(t)$ \emph{indefinite integrals}
(or, simply, \emph{integrals}) of $f$.

Yet another version of the fundamental
theorem is given by the next theorem.
\begin{theorem}\label{Theorem, integral derivative}
Suppose that
$g:[a,b]\rightarrow{\mathbb R}$ has
continuous derivative.
Then
\[\int_{a}^{b}g'(t)\,dt=g(b)-g(a).\]
\end{theorem}

Theorems~\ref{Theorem, fundamental calculus}
and~\ref{Theorem, integral derivative} show that
(under appropriate circumstances) integration
and differentiation are inverse operations and the
the theories of differentiation and integration
are subsumed in the greater theory of the calculus.
 
We use the fundamental theorem of the calculus
to prove the formulae for integration by substitution
and integration by parts.
\begin{theorem}{\bf [Change of variables for integrals]}
Suppose that
$f:[a,b]\rightarrow{\mathbb R}$ is continuous
and $g:[\gamma,\delta]\rightarrow{\mathbb R}$
is differentiable with continuous derivative.
Suppose further that $g([\gamma,\delta])\subseteq [a,b]$.
Then, if $c$, $d\in[\gamma,\delta]$, we have
\[\int_{g(c)}^{g(d)}f(s)\,ds=\int_{c}^{d}f(g(x))g'(x)\,dx.\]
\end{theorem}
\begin{exercise}\label{No substitutes} 
The following exercise is traditional.

(i) Show that integration by substitution, using $x=1/t$,
gives
\[\int_{a}^{b}\frac{dx}{1+x^{2}}
=
\int_{1/b}^{1/a}\frac{dt}{1+t^{2}}\]
when $b>a>0$.

(ii) If we set $a=-1$, $b=1$ in the formula of~(i),
we obtain
\[\int_{-1}^{1}\frac{dx}{1+x^{2}}
\overset{?}{=}
-\int_{-1}^{1}\frac{dt}{1+t^{2}}\]
Explain
this apparent failure 
of the method of integration by substitution.

(iii) Write the result of (i) in terms of $\tan^{-1}$
and prove it using standard trigonometric identities.
\end{exercise}
\begin{lemma}\label{Lemma, Integration by parts} 
Suppose that
$f:[a,b]\rightarrow{\mathbb R}$ has
continuous derivative and
$g:[a,b]\rightarrow{\mathbb R}$
is continuous. Let $G:[a,b]\rightarrow{\mathbb R}$
be an indefinite integral of $g$.
Then, we have
\[\int_{a}^{b}f(x)g(x)\,dx=\left[f(x)G(x)\right]_{a}^{b}
-\int_{a}^{b}f'(x)G(x)\,dx.\]
\end{lemma}

We obtain the following version of Taylor's theorem 
by repeated integration by parts.
\begin{theorem}{\bf [A global Taylor's theorem
with integral remainder]}\label{Integral Taylor}
If $n\geq 1$ and $f:(-a,a)\rightarrow{\mathbb R}$
is $n$ times differentiable with continuous $n$th
derivative, then
\[f(t)=\sum_{j=0}^{n-1}\frac{f^{(j)}(0)}{j!}t^{j}
+R_{n}(f,t)\]
for all $|t|<a$, where
\[R_{n}(f,t)=\frac{1}{(n-1)!}
\int_{0}^{t}(t-x)^{n-1}f^{(n)}(x)\,dx.\]
\end{theorem}
In the opinion of the lecturer this form is
powerful enough for most purposes
and is a form that is easily remembered 
and proved for examination.
\section{Infinite integrals} The reader may already
be familiar with definitions of the following type.

\begin{definition}\label{infinite integral up} 
If $f:[a,b]\rightarrow{\mathbb R}$
and $M,P\geq 0$ let us write
\begin{equation*}
f_{M,P}(x)=
\begin{cases}
f(x)&\text{if $-P\leq f(x)\leq M$}\\
M&\text{if $f(x)>M$}\\
-P&\text{if $f(x)<-P$.}
\end{cases}
\end{equation*}
If $f_{M,P}$ is Riemann integrable for each $M,\, P\geq 0$
and
\[\int_{a}^{b}f_{M,P}(x)\,dx\rightarrow L\]
as $M,P\rightarrow\infty$ then we say that $f$
is Riemann integrable and
\[\int_{a}^{b}f(x)\,dx=L\]
\end{definition}
\begin{definition}\label{infinite integral along}
If $f:[a,\infty)\rightarrow{\mathbb R}$
is such that $f|_{[a,X]}$ is Riemann integrable for each $X>a$
and $\int_{a}^{X}f(x)\,dx\rightarrow L$ as $X\rightarrow\infty$,
then we say that $\int_{a}^{\infty}f(x)\,dx$ exists
with value $L$.
\end{definition}

It must be said that neither Definition~\ref{infinite integral up}
nor Definition~\ref{infinite integral along} are more
than ad hoc. 

In the rest of this section we look at 
Definition~\ref{infinite integral along}.
\begin{lemma}\label{Lemma, increasing integral} 
Suppose $f:[a,\infty)\rightarrow{\mathbb R}$
is such that $f|_{[a,X]}$ is Riemann integrable on
$[a,X]$ for each $X>a$.
If $f(x)\geq 0$ for all $x$, then
$\int_{a}^{\infty}f(x)\,dx$ exists if and only if
there exists a $K$ such that
$\int_{a}^{X}f(x)\,dx\leq K$ for all $X$.
\end{lemma}

We use Lemma~\ref{Lemma, increasing integral} to
prove the integral comparison test.
\begin{lemma}\label{Lemma Integral test} If
$f:[1,\infty)\rightarrow{\mathbb R}$ is a decreasing
function with $f(x)\rightarrow 0$ as $x\rightarrow\infty$
then
\[\sum_{n=1}^{\infty}f(n)\ \text{and}
\ \int_{1}^{\infty}f(x)\,dx\]
either both diverge or both converge.
\end{lemma}
\begin{example}\label{Slow powers}
If $\alpha>1$ then $\sum_{n=1}^{\infty}n^{-\alpha}$
converges. If $\alpha\leq 1$ 
then $\sum_{n=1}^{\infty}n^{-\alpha}$
diverges.
\end{example}

This is really as far as we need to go, but
I will just add one further remark.
\begin{lemma}\label{Lemma, absolute integal}
Suppose $f:[a,\infty)\rightarrow{\mathbb R}$
is such that $f|_{[a,X]}$ is Riemann integrable on $[a,X]$ for each $X>a$.
If
$\int_{a}^{\infty}|f(x)|\,dx$ exists, then
$\int_{a}^{\infty}f(x)\,dx$ exists.
\end{lemma}
It is natural to state Lemma~\ref{Lemma, absolute integal}
in the form `absolute convergence of the integral implies
convergence'. 

Speaking broadly, infinite integrals $\int_{a}^{\infty}f(x)\,dx$
work well when they are absolutely
convergent, that is to say,
$\int_{a}^{\infty}|f(x)|\,dx<\infty$, but are full of traps for
the unwary otherwise. This is not a weakness of the
Riemann integral but inherent in any mathematical situation
where an object only exists `by virtue of the cancellation
of two infinite objects'. (Question~\ref{Dirichlet kernel}
gives an example of an integral which is convergent but not absolutely
convergent.)

\section{Further reading} The two
excellent books
Spivak's \emph{Calculus}~\cite{Spivak} and
J.~C.~Burkill's
\emph{A First Course in Mathematical Analysis}~\cite{Burkill}
both cover the course completely and should be in your
college library\footnote{A quieter version of the JCR
with shelves of books replacing the bar.}. Burkill's
book is more condensed and Spivak's more leisurely.
A completely unsuitable but interesting version of the
standard analysis course is given by Berlinski's
\emph{A Tour of the Calculus}~\cite{Berlinski}
--- Spivak rewritten by Sterne with additional purple
passages by the Ankh-Morpork tourist board.
I have written \emph{A Companion to Analysis}~\cite{Korner}
which covers this course at a higher level
together with the next analysis course. It is available
off the web but is unlikely to be as suitable for beginners
as Spivak and Burkill. If you do download it,
remember that you are under a moral obligation to
send me an e-mail about any mistakes you find.
\begin{thebibliography}{9}
\bibitem{Berlinski} D.~Berlinski
\emph{A Tour of the Calculus}
Mandarin Paperbacks 1997.
\bibitem{Burkill} J.~C.~Burkill
\emph{A First Course in Mathematical Analysis}
CUP, 1962.
\bibitem{Burn} R.~P.~Burn
\emph{Numbers and Functions}
CUP, 1992.
\bibitem{Korner} T.~W.~K\"{o}rner
\emph{A Companion to Analysis} for the moment
available via my home page {\sf http://www.dpmms.cam.ac.uk/\textasciitilde twk/}\ .
\bibitem{Spivak} M.~Spivak,
\emph{Calculus}
Addison-Wesley/Benjamin-Cummings, 1967.
\end{thebibliography}
\section{First example sheet} Students vary in how much work
they are prepared to do. On the whole, exercises from
the main text are reasonably easy and provide good
practice in the ideas. Questions and parts of questions
marked with $^{\star}$ are not intended to be hard
but cover topics less central to the present course.
 
\begin{question}\label{Q1}
Let $a_{n}\in{\mathbb R}$. We say that
$a_{n}\rightarrow\infty$ as $n\rightarrow\infty$
if, given $K>0$, we can find
an $n_{0}(K)$ such that $a_{n}\geq K$ for all $n\geq n_{0}(K)$.

(i) Write down a similar definition for $a_{n}\rightarrow-\infty$.

(ii) Show that $a_{n}\rightarrow-\infty$ if and only if
$-a_{n}\rightarrow\infty$.

(iii) If $a_{n}\neq 0$ for all $n$, show that, if
$a_{n}\rightarrow\infty$, it follows that $1/a_{n}\rightarrow 0$
as $n\rightarrow\infty$.

(iv) Is it true that, if $a_{n}\neq 0$ for all $n$
and $1/a_{n}\rightarrow 0$, then $a_{n}\rightarrow\infty$
as $n\rightarrow\infty$? Give a proof or a counter example.
\end{question}
\begin{question}\label{Q2} Prove that, if
\[a_{1}>b_{1}>0\ \text{and}\ a_{n+1}=\frac{a_{n}+b_{n}}{2}
,\ \ b_{n+1}=\frac{2a_{n}b_{n}}{a_{n}+b_{n}},\]
then $a_{n}>a_{n+1}>b_{n+1}>b_{n}$. Prove that, as
$n\rightarrow\infty$, $a_{n}$ and $b_{n}$ both tend to
the limit $\surd(a_{1}b_{1})$.

Use this result to give an example of an increasing 
sequence of rational numbers
tending to a limit $l$ which is not rational.
\end{question}
\begin{question} (Exercise~\ref{Exercise 1.4}.)\label{Q3}
Show that
a decreasing sequence of real numbers bounded below tends
to a limit.

\noindent [Hint. If $a\leq b$ then $-b\leq -a$.]
\end{question}
\begin{question}\label{Q4}
(i) By using the binomial theorem, or otherwise, show that,
if $\eta>0$ and $n$ is a positive integer, then
\[(1+\eta)^{n}\geq \eta n.\]
Deduce that $(1+\eta)^{n}\rightarrow\infty$ as $n\rightarrow\infty$.

(ii) By using the binomial theorem, or otherwise, show that,
if $\eta>0$, then
\[(1+\eta)^{n}\geq \eta^{2} \frac{n(n-1)}{2}.\]
Deduce that $n^{-1}(1+\eta)^{n}\rightarrow\infty$ as $n\rightarrow\infty$.
                   
(iii) Show that, if $k$ is any positive integer
and $a>1$, then $n^{-k}a^{n}\rightarrow\infty$ as $n\rightarrow\infty$.
[Thus `powers beat polynomial growth'.]

(iv) Show that if $k$ is any positive integer
and $1>a\geq 0$, then $n^{k}a^{n}\rightarrow 0$ as $n\rightarrow\infty$.
\end{question}
\begin{question}\label{Q5} If $a\in{\mathbb R}$ and $a\neq -1$
describe the behaviour of 
\[\frac{a^{n}-1}{a^{n}+1}\]
as $n\rightarrow\infty$. (That is, for each value of $a$
say whether the sequence converges or not, and, if it
converges, say what it converges to. For certain values of $a$
you may find it useful to divide top and bottom by $a^{n}$.)
\end{question} 
\begin{question} (Exercises~\ref{complex sequences}\label{Q6}
and~\ref{complex series 2}.)

We work in ${\mathbb C}$.

(i) The limit is unique. That is, if $a_{n}\rightarrow a$
and $a_{n}\rightarrow b$ as $n\rightarrow\infty$,
then $a=b$.

(ii) If $a_{n}\rightarrow a$ as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$, then
$a_{n(j)}\rightarrow a$ as $j\rightarrow\infty$.

(iii) If $a_{n}=c$ for all $n$, then $a_{n}\rightarrow c$
as $n\rightarrow\infty$.

(iv) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$, then
$a_{n}+b_{n}\rightarrow a+b$.

(v) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$, then
$a_{n}b_{n}\rightarrow ab$.

(vi) If $a_{n}\rightarrow a$
as $n\rightarrow\infty$ and $a_{n}\neq 0$ for each $n$ and
$a\neq 0$, then $a_{n}^{-1}\rightarrow a^{-1}$.

(vii) Explain why there is no result in
Exercise~\ref{complex sequences} corresponding
to part (vii) of Lemma~\ref{one sequences}.
\end{question}

\begin{question}$\negthickspace^{\star}${\bf [Ces\'{a}ro summation]}
If\label{Q7}
$a_{j}\in{\mathbb R}$ we write
\[\sigma_{n}=\frac{a_{1}+a_{2}+\dots+a_{n}}{n}.\]

(i) Show that, if $a_{n}\rightarrow 0$, then $\sigma_{n}\rightarrow 0$.
[Hint: Given $\epsilon>0$ we can find an $j_{0}(\epsilon)$
such that $|a_{j}|<\epsilon/2$ for all $j\geq j_{0}(\epsilon)$.]

(ii) Show that, if $a_{n}\rightarrow a$, then $\sigma_{n}\rightarrow a$.

(iii) If $a_{n}=(-1)^{n}$ show that $a_{n}$ does not converge
but $\sigma_{n}$ does.

(iv) If $a_{2^{m}+r}=(-1)^{m}$ for $0\leq r\leq 2^{m}-1$, $m\geq 0$
show that $\sigma_{n}$ does not converge.

(v) If $\sigma_{n}$ converges show that $n^{-1}a_{n}\rightarrow 0$
as $n\rightarrow\infty$.
\end{question}
\begin{question}\label{Q8} In this question you may assume the
standard properties of the exponential function
including the relation $\exp x\geq 1+x$ for $x\geq 0$.

(i) Suppose that $a_{n}\geq 0$. Show that
\[\sum_{j=1}^{n}a_{j}\leq\prod_{j=1}^{n}(1+a_{j})\leq
\exp\left(\sum_{j=1}^{n}a_{j}\right).\]
Deduce, carefully, that $\prod_{j=1}^{n}(1+a_{j})$ tends
to a limit as $n\rightarrow\infty$ if and only if
$\sum_{j=1}^{n}a_{j}$ does.

(ii)$^{\star}$ Euler made use of these ideas as follows.
Let $p_{k}$ be the $k$th prime.  By observing that
\[\frac{1}{1-p_{k}^{-1}}\geq \sum_{r=0}^{m}p_{k}^{-r}\]
show that
\[\prod_{k=1}^{n}\frac{1}{1-p_{k}^{-1}}\geq \sum_{u\in S(N,n)}\frac{1}{u}\]
where $S(N,n)$ is the set of all integers of the form
\[u=\prod_{k=1}^{n}p_{k}^{m_{k}}\]
with $0\leq m_{k}\leq N$.

By letting $N\rightarrow\infty$ show that
\[\prod_{k=1}^{n}\frac{1}{1-p_{k}^{-1}}\geq \sum_{u\in S(n)}\frac{1}{u}\]
where $S(n)$ is the set of all integers whose only prime factors
are $p_{1}$, $p_{2}$, \dots, $p_{n}$. Deduce that
there are infinitely many primes (what can could you
say about $S(n)$ if there were only $n$ primes?)
and show that
\[\prod_{k=1}^{n}\frac{1}{1-p_{k}^{-1}}\rightarrow\infty\]
as $n\rightarrow\infty$.

Conclude that
\[\sum_{k=1}^{\infty}\left(\frac{1}{1-p_{k}^{-1}}-1\right)
\ \text{diverges}.\]
Show that $|1-(1-p_{k}^{-1})^{-1}|\leq 2p_{k}^{-1}$ and deduce
that
\[\sum_{k=1}^{\infty}\frac{1}{p_{k}}\ \text{diverges}.\]
\end{question}
\begin{question}\label{Q9} (i) If $a_{j}$ is an integer 
with $0\leq a_{j}\leq 9$ show \emph{from the fundamental axiom}
that
\[\sum_{j=1}^{\infty}a_{j}10^{-j}\]
exists. Show that $0\leq \sum_{j=1}^{\infty}a_{j}10^{-j}\leq 1$,
carefully quoting any theorems that you use.

(ii)$^{\star}$ If $0\leq x\leq 1$, show that we can find integers
$x_{j}$ with $0\leq x_{j}\leq 9$ such that
\[x=\sum_{j=1}^{\infty}x_{j}10^{-j}.\]

(iii)$^{\star}$ If $a_{j}$ and $b_{j}$ are integers 
with $0\leq a_{j},b_{j}\leq 9$ and
$a_{j}=b_{j}$ for $j<N$, $a_{N}>b_{N}$ show that
\[\sum_{j=1}^{\infty}a_{j}10^{-j}\geq \sum_{j=1}^{\infty}b_{j}10^{-j}.\]
Give the precise necessary and sufficient condition
for equality and prove it.

\end{question}
\begin{question}$\negthickspace^{\star}$\label{1.1}\label{Q10}
(We work with the same ideas as
in Example~\ref{Rational}.)

(i) Find a differentiable function
$f:{\mathbb Q}\rightarrow{\mathbb Q}$ such that
$f'(x)=1$ for all $x\in{\mathbb Q}$ but $f(0)>f(1)$.

(ii) If we define $g:{\mathbb Q}\rightarrow{\mathbb Q}$
by $g(x)=(x^{2}-2)^{-2}$ show that $g$ is continuous
but unbounded on the set of $x$ with $|x|\leq 4$.
\end{question}

\begin{question}\label{Q11} 
In each of the following cases determine an integer $N$
(not necessarily the least such integer) with the property
that if $m\geq N$ the $m$th partial sum of the series
$\sum_{n=1}^{\infty}a_{n}$ differs from the sum of the series
by less than 0.01:

(i) $a_{n}=1/n(n+1)$;

(ii) $a_{n}=2^{-n}$;

(iii)$^{\star}$ $a_{n}=n2^{-n}$;

(iv)$^{\star}$ $a_{n}=n^{-n}$.
\end{question}
\begin{question}\label{Q12}
 For what values of real $\beta$ is
\[\sum_{n=1}^{\infty}\frac{n^{\beta}}{n^{2\beta}-n^{\beta}+1}\]
convergent and for which divergent. Prove your answer.
(You may assume that $\sum_{n=1}^{\infty}n^{-\beta}$
converges if $\beta>1$ and diverges otherwise.)
\end{question}
\begin{question}$\negthickspace^{\star}$ (i) Let us write
\[S_{n}=\sum_{r=0}^{n}\frac{1}{r!}.\]
Show by induction, or otherwise, that $1/r!\leq 2^{-r+1}$ 
for $r\geq 1$ and deduce that $S_{n}\leq 3$.\label{Q13}
Show, from first principles, that $S_{n}$ converges
to a limit (which, with the benefit of extra knowledge,
we call $e$).

(ii) Show that, if $n\geq 2$  and $r\geq 0$ then
\[\frac{n!}{(n+r)!}\leq \frac{1}{3^{r}}.\]
Deduce carefully that, if $m\geq n\geq 2$,
\[0\leq n!(S_{m}-S_{n})\leq \frac{1}{2}.\] 
and that
\[0<n!(e-S_{n})\leq \frac{1}{2}.\]
Deduce that $n!e$ is not an integer for any $n$  and
conclude that $e$ is irrational.

(iii) Show similarly that 
${\displaystyle\sum_{r=0}^{\infty}\frac{1}{(2r)!}}$
is irrational.

\end{question}
\section{Second example sheet}  Students vary in how much work
they are prepared to do. On the whole, exercises from
the main text are reasonably easy and provide good
practice in the ideas. Questions and parts of questions
marked with $^{\star}$ are not intended to be hard
but cover topics less central to the present course.

\begin{question} (Exercises~\ref{exercise infimum}
and ~\ref{exercise infimum 2}.)\label{Q14}

(i)Define the greatest lower bound
in the manner of Definition~\ref{Definition, sup},
prove its uniqueness in the manner of
Lemma~\ref{Lemma, supremum unique} and state
and prove a result corresponding to Lemma~\ref{Lemma, supremum other}.

(ii)  Use Lemma~\ref{Lemma minus infimum}
and Theorem~\ref{theorem, supremum} to show
that any non-empty set in ${\mathbb R}$
with a lower bound has a greatest lower bound.
\end{question}
\begin{question}\label{Q15}
Suppose that $A$ and $B$ are non-empty
bounded subsets of ${\mathbb R}$. Show that
\[\sup\{a+b:a\in A,\ b\in B\}=\sup A+\sup B.\]
The last formula is more frequently written
\[\sup_{a\in A,\ b\in B}a+b=\sup_{a\in A}a+\sup_{b\in B}b.\]

Suppose, further that $a_{n}$ and $b_{n}$ are bounded
sequences of real numbers. For each of the following
statements either give a proof that it is always true
or an example to show that it is sometimes false.

(i) $\sup_{n}(a_{n}+b_{n})=\sup_{n}a_{n}+\sup_{n}b_{n}$.

(ii) $\sup_{n}(a_{n}+b_{n})\leq\sup_{n}a_{n}+\sup_{n}b_{n}$.

(iii) $\sup_{n}(a_{n}+b_{n})\geq\sup_{n}a_{n}+\inf_{n}b_{n}$.

(iv) $\sup_{a\in A,\ b\in B}ab=(\sup_{a\in A}a)(\sup_{b\in B}b)$.

(v) $\inf_{a\in A,\ b\in B}a+b=\inf_{a\in A}a+\inf_{b\in B}b$.

\end{question}
\begin{question}(Exercise~\ref{Exercise, polynomial})\label{Q16}
Prove the following results.

(i) Suppose that $E$ is a subset of ${\mathbb R}$
and that $f:E\rightarrow{\mathbb R}$ is continuous
at $x\in E$. If $x\in E'\subset E$ then the
restriction $f|_{E'}$ of $f$ to $E'$ is also continuous
at $x$.

(ii) If $J:{\mathbb R}\rightarrow{\mathbb R}$ is
defined by $J(x)=x$ for all $x\in{\mathbb R}$,
then $J$ is continuous on ${\mathbb R}$.

(iii) Every polynomial $P$ is continuous on ${\mathbb R}$.

(iv) Suppose that $P$ and $Q$ are polynomials
and that $Q$ is never zero on some subset $E$
of ${\mathbb R}$. Then the rational function
$P/Q$ is continuous on $E$ (or, more precisely,
the restriction of $P/Q$ to $E$ is continuous.)
\end{question}
\begin{question} (Exercises~\ref{Intermediate 1} to~\ref{Intermediate 3}.)

(i) Show\label{Q17}
that any real polynomial of odd degree has
at least one root. Is the result true for
polynomials of even degree? Give a proof or counterexample.

(ii)$^{\star}$ Suppose  that
$g:[0,1]\rightarrow [0,1]$ is a continuous function.
By considering $f(x)=g(x)-x$, or otherwise,
show that there exists a $c\in [0,1]$ with $g(c)=c$.
(Thus every continuous map of $[0,1]$ into itself
has a fixed point.) Give an example of a bijective
(but, necessarily, non-continuous) function
$h:[0,1]\rightarrow [0,1]$ such that $h(x)\neq x$
for all $x\in [0,1]$.

\noindent [Hint: First find a function
$H:[0,1]\setminus\{0,\ 1,\ 1/2\}\rightarrow [0,1]\setminus\{0,\ 1,\ 1/2\}$
such that $H(x)\neq x$.]

(iii)$^{\star}$ Every mid-summer day
at six o'clock in the morning, the youngest monk
from the monastery of Damt starts to climb the narrow
path up Mount Dipmes.
At six in the evening he reaches the small temple
at the peak where he spends the night in meditation.
At six o'clock in the morning on the following day
he starts downwards, arriving back at the monastery
at six in the evening. Of course, he does not always walk
at the same speed. Show that, none the less, there
will be some time of day when he will be at the
same place on the path on both his upward and
downward journeys.
\end{question}
\begin{question} (Exercise~\ref{continuous equivalent}.)\label{Q18}

Let $E$ be a subset of ${\mathbb R}$.
Show that function $f:E\rightarrow{\mathbb R}$
is continuous at $x\in E$ if and only if
$f(y)\rightarrow f(x)$ as $y\rightarrow x$.
\end{question}
\begin{question}(Exercise~\ref{real limits}.)\label{Q19}

Let $E$ be a subset of ${\mathbb R}$,
$f$, $g$ be some functions from $E$ to ${\mathbb R}$,
and $x$ some point of $E$.
(i) The limit is unique. That is, if $f(y)\rightarrow l$
and $f(y)\rightarrow k$ as $y\rightarrow x$
then $l=k$.

(ii) If $x\in E'\subseteq E$ and $f(y)\rightarrow l$
as $y\rightarrow x$ through
values $y\in E$, then $f(y)\rightarrow l$
as $y\rightarrow x$ through
values $y\in E'$.

(iii) If $f(t)=c$ for all $t\in E$ then $f(y)\rightarrow c$
as $y\rightarrow x$.

(iv) If $f(y)\rightarrow l$ and $g(y)\rightarrow k$
as $y\rightarrow x$ then
$f(y)+g(y)\rightarrow l+k$.

(v) If $f(y)\rightarrow l$ and $g(y)\rightarrow k$
as $y\rightarrow x$ then
$f(y)g(y)\rightarrow lk$.

(vi) If $f(y)\rightarrow l$
as $y\rightarrow x$, $f(t)\neq 0$ for each $t\in E$ and
$l\neq 0$ then $f(t)^{-1}\rightarrow l^{-1}$.

(vii) If $f(t)\leq L$ for each $t\in E$ and
$f(y)\rightarrow l$
as $y\rightarrow x$ then $l\leq L$.
\end{question}
\begin{question}(Exercise~\ref{derivatives}.)\label{Q20}

Let $E$ be a subset of ${\mathbb R}$,
$f$, $g$ be some functions from $E$ to ${\mathbb R}$,
and $x$ some point of $E$. Prove the following results.

(i) If $f(t)=c$ for all $t\in E$ then $f$ is differentiable
at $x$ with $f'(x)=0$.

(ii) If $f$ and $g$ are differentiable at $x$ then so
is their sum $f+g$ and 
\[(f+g)'(x)=f'(x)+g'(x).\]

(iii) If $f$ and $g$ are differentiable at $x$ then so
is their product $f\times g$ and 
\[(f\times g)'(x)=f'(x)g(x)+f(x)g'(x).\]

(iv) If $f$ is differentiable at $x$ and $f(t)\neq 0$ for all $t\in E$
then $1/f$ is differentiable at $x$ and
\[(1/f)'(x)=-f'(x)/f(x)^{2}.\]

(v) If $f(t)=\sum_{r=0}^{N}a_{r}t^{r}$ on $E$ then
then $f$ is differentiable at $x$ and
\[f'(x)=\sum_{r=1}^{N}ra_{r}x^{r-1}\].
\end{question}

\begin{question} We work in the real numbers.\label{Q21}
Are the following true or false?
Give a proof or counterexample as appropriate.

(i) If $\sum_{n=1}^{\infty}a_{n}^{4}$ converges then
$\sum_{n=1}^{\infty}a_{n}^{5}$ converges.

(ii) If $\sum_{n=1}^{\infty}a_{n}^{5}$ converges
then $\sum_{n=1}^{\infty}a_{n}^{4}$ does.

(iii) If $a_{n}\geq 0$ for all $n$ and $\sum_{n=1}^{\infty} a_{n}$
converges then $na_{n}\rightarrow 0$ as $n\rightarrow\infty$. 

(iv)$^{\star}$ If $a_{n}\geq 0$ for all $n$ and $\sum_{n=1}^{\infty} a_{n}$
converges then $n(a_{n}-a_{n-1})\rightarrow 0$ as $n\rightarrow\infty$. 

(v)$^{\star}$ If $a_{n}$ is a decreasing sequence of positive numbers
and $\sum_{n=1}^{\infty} a_{n}$
converges then $na_{n}\rightarrow 0$ as $n\rightarrow\infty$.

(vi)$^{\star}$ If $a_{n}$ is a decreasing sequence of positive numbers
and $na_{n}\rightarrow 0$ as $n\rightarrow\infty$
then $\sum_{n=1}^{\infty} a_{n}$ converges.

\noindent[Hint. If necessary, look at 
Lemmas~\ref{Lemma Integral test} and~\ref{Slow powers}.]

(vii)$^{\star}$ If
$\sum_{n=1}^{\infty} a_{n}^{2}$ converges then 
$\sum_{n=1}^{\infty} n^{-1}a_{n}$ converges.
 
\noindent[Hint: Cauchy-Schwarz]

(viii)$^{\star}$ If $\sum_{n=1}^{\infty} a_{n}$ converges then 
$\sum_{n=1}^{\infty} n^{-1}|a_{n}|$ converges.
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Q22}
{\bf [General principle of convergence]}
We say that a sequence $x_{n}$ of points in ${\mathbb R}$
form a Cauchy sequence if given any $\epsilon>0$
we can find an $n_{0}(\epsilon)$ such that
\[|x_{n}-x_{m}|<\epsilon\ \text{for all $n,\, m\geq n_{0}(\epsilon)$}.\]

(i) Show that any convergent sequence is a Cauchy sequence.

(ii) Suppose that the points  $x_{n}$ form a Cauchy sequence.
Show that, if we can find $n(j)\rightarrow\infty$ such
that $x_{n(j)}\rightarrow x$, it follows that
$x_{n}\rightarrow x$. (Thus, if any subsequence of a Cauchy
sequence converges, so does the sequence.)

(iii) Show that if the points  $x_{n}$ form a Cauchy sequence
the set $\{x_{n}\,:\,n\geq 1\}$ is bounded.

(iv) Use the theorem of Bolzano-Weierstrass to show that
any Cauchy sequence converges.

[We have thus shown that a sequence converges if and only
if it is a Cauchy sequence. This is the famous Cauchy
general principle of convergence. Its generalisation will play
an important r\^{o}le in next year's analysis course~C9.]
\end{question} 
\begin{question}$\negthickspace^{\star}$\label{Q23}
{\bf [A method of Abel]}

(i) Suppose that
$a_{j}$ and $b_{j}$ are sequences of complex numbers
and that $S_{n}=\sum_{j=1}^{n}a_{j}$ for $n\geq 1$
and $S_{0}=0$. Show that, if $1\leq u\leq v$ then
\[\sum_{j=u}^{v}a_{j}b_{j}=\sum_{j=u}^{v}S_{j}(b_{j}-b_{j+1})
-S_{u-1}b_{u}+S_{v}b_{v+1}.\]
(This is known as partial summation, for obvious reasons.)

(ii) Suppose now that, in addition, the $b_{j}$
form a decreasing sequence of positive terms
and that $|S_{n}|\leq K$ for all $n$.
Show that
\[\left|\sum_{j=u}^{v}a_{j}b_{j}\right|
\leq 3Kb_{u}.\]
(You can replace $3Kb_{u}$ by $2Kb_{u}$ if you are careful
but there is no advantage in this.)
Deduce that if $b_{j}\rightarrow 0$ as $j\rightarrow\infty$
then $\sum_{j=1}^{\infty}a_{j}b_{j}$ converges.

Deduce the alternating series test.

(iii) If $b_{j}$ is a decreasing sequence of positive terms
with $b_{j}\rightarrow 0$ as $j\rightarrow\infty$ show
that  $\sum_{j=1}^{\infty}b_{j}z^{j}$ converges
in the region given by $|z|\leq 1$ and $z\neq 1$.
Show by example that we must have the condition $z\neq 1$.
Show by example that we must have the condition $|z|\leq 1$.
\end{question}

\begin{question}$\negthickspace^{\star}$\label{Q24}
Enter any number on your calculator.
Press the $\sin$ button repeatedly. What appears to happen?
Prove your conjecture using any properties of $\sin$ that
you need.
\end{question}
\section{Third example sheet}  Students vary in how much work
they are prepared to do. On the whole, exercises from
the main text are reasonably easy and provide good
practice in the ideas. Questions and parts of questions
marked with $^{\star}$ are not intended to be hard
but cover topics less central to the present course.
I know that there is a general belief amongst students,
directors of studies and the Faculty Board
that there is a magic set of questions which is suitable
for everybody. If there is one, I will be happy to circulate it.
For the moment I remark that the unstarred questions 
on this example sheet represent
a good week's work for someone who finds the course hard
and the whole sheet represents
a good week's work for someone who finds the unstarred
questions boring.

\begin{question}{\bf [Very traditional]}\label{Exercise, non Darboux}
(i) Consider the function $f:{\mathbb R}\rightarrow{\mathbb R}$
given by $f(t)=t^{2}\sin 1/t$ for $t\neq 0$, $f(0)=0$.\label{Q25}
Show that $f$ is everywhere differentiable and find its derivative.
Show that $f'$ is not continuous.

\noindent
[Deal quickly with the easy part and then
go back to the definition to deal with $t=0$.
There are wide selection of counter-examples
obtained by looking at $t^{\beta}\sin t^{\alpha}$
for various values of $\alpha$ and $\beta$.]

(ii) Find an infinitely differentiable
function $g:(1,\infty)\rightarrow{\mathbb R}$
such that $g(t)\rightarrow 0$ but $g'(t)\nrightarrow 0$
as $t\rightarrow\infty$.
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Q26}
Question~\ref{Exercise, non Darboux} shows that the
derivative of a differentiable function need not be continuous.
In spite of this the derivative still obeys Darboux's
theorem:- If $f:[a,b]\rightarrow{\mathbb R}$ is differentiable
and $k$ lies between $f'(a)$ and $f'(b)$ then there is
a $c\in[a,b]$ such that $f'(c)=k$. In this question
we prove the result.

Explain why there is no loss of generality in supposing
that $f'(a)>k>f'(b)$. Set $g(x)=f(x)-kx$. By looking
at $g'(a)$ and $g'(b)$ show that $g$ cannot have a maximum
at $a$ or $b$. Use the method of proof of Rolle's
theorem to show that there exists a $c\in(a,b)$ with $g'(c)=0$
and deduce Darboux's theorem.

Give an example of a function $f:{\mathbb R}\rightarrow{\mathbb R}$
such that there does not exist a differentiable function
$F:{\mathbb R}\rightarrow{\mathbb R}$ with $F'=f$.
\end{question}
\begin{question}\label{Q27}
(i) {\bf [Cauchy's mean value theorem]}
Suppose that $f,\, g:[a,b]\rightarrow{\mathbb R}$
are continuous and that $f$ and $g$ are differentiable
on $(a,b)$. Suppose further that $g'(x)\neq 0$ for
all $x\in (a,b)$. Explain why $g(a)\neq g(b)$.
By applying Rolle's theorem to $F$ where
\[F(x)=(g(b)-g(a))(f(x)-f(a))-(g(x)-g(a))(f(b)-f(a)),\]
show that there is a $\zeta\in (a,b)$ such that
\[\frac{f'(\zeta)}{g'(\zeta)}=\frac{f(b)-f(a)}{g(b)-g(a)}.\]

(ii) {\bf [L'H\^{o}pital's rule]} 
Suppose that $f,\, g:[a,b]\rightarrow{\mathbb R}$
are continuous and that $f$ and $g$ are differentiable
on $(a,b)$. Suppose that $f(a)=g(a)=0$, that $g'(t)$
does not vanish near $a$ and
$f'(t)/g'(t)\rightarrow l$ as $t\rightarrow a$
through values of $t>a$. Show that
$f(t)/g(t)\rightarrow l$ as $t\rightarrow a$
through values of $t>a$.
\end{question}
\begin{question}(Exercise~\ref{start bump}.)\label{Q28}

Consider the function
$F:{\mathbb R}\rightarrow{\mathbb R}$ defined by
\begin{alignat*}{2}
F(0)&=0\\
F(x)&=\exp(-1/x^{2})&&\qquad\text{otherwise}.
\end{alignat*}

(i) Prove by induction, using the standard rules of differentiation,
that $F$ is infinitely differentiable at all points $x\neq 0$
and that, at these points,
\[F^{(n)}(x)=P_{n}(1/x)\exp(-1/x^{2})\]
where $P_{n}$ is a polynomial which need not be found explicitly.

(ii) Explain why $x^{-1}P_{n}(1/x)\exp(-1/x^{2})\rightarrow 0$
as $x\rightarrow 0$.

(iii) Show by induction, using the definition of differentiation,
that $F$ is infinitely differentiable at $0$
with $F^{(n)}(0)=0$ for all $n$.
[Be careful to get this part of the argument
right.]

(iv) Show that
\[F(x)=\sum_{j=0}^{\infty}\frac{F^{(j)}(0)}{j!}x^{j}\]
if and only if $x=0$. (The reader may prefer to say
that `The Taylor expansion of $F$ is only valid at $0$'.)

(v) Why does part~(iv) not contradict the local Taylor theorem
(Theorem~\ref{local Taylor})?
\end{question}

\begin{question} In this question you may assume the
standard properties of $\sin$ and $\cos$ but not their
power series expansion.\label{Q29}

(i) By considering the sign of $f'_{1}(x)$, when
$f_{1}(t)=t-\sin t$, show that
\[t\geq \sin t\]
for all $t\geq 0$.

(ii) By considering the sign of $f'_{2}(x)$, when
$f_{2}(t)=\cos t-1+t^{2}/2!$, show that
\[\cos t\geq 1-\frac{t^{2}}{2!}\]
for all $t\geq 0$.

(iii) By considering the sign of $f'_{3}(x)$, when
$f_{3}(t)=\sin t -t+t^{3}/3!$, show that
\[\sin t\geq t-\frac{t^{3}}{3!}\]
for all $t\geq 0$.

(iv) State general results suggested by parts~(i) to~(iii)
and prove them by induction. State and prove
corresponding results for $t<0$.

(v) Using~(iv), show that
\[\sum_{n=0}^{N}\frac{(-1)^{n}t^{2n+1}}{(2n+1)!}\rightarrow\sin t\]
as $N\rightarrow\infty$ for all $t\in{\mathbb R}$. State and prove
a corresponding result for $\cos$.

\noindent[This question could be usefully illustrated by computer
graphics.]
\end{question}
\begin{question} (i) Suppose $f:[0,1]\rightarrow{\mathbb R}$
is twice differentiable with $f''(t)\geq 0$ for all $t\in[0,1]$.
If $f'(0)>0$ and $f(0)=0$\label{Q30}
explain why $f(t)>0$ for $t>0$. If $f'(0)\geq 0$
and $f(0)=f(1)=0$ what can you say about $f$ and why?
If $f'(1)\leq 0$
and $f(0)=f(1)=0$ what can you say about $f$ and why?

(ii) Suppose $f:[0,1]\rightarrow{\mathbb R}$
is twice differentiable with $f''(t)\geq 0$ for all $t\in[0,1]$
and $f(0)=f(1)=0$. Show that $f(t)\leq 0$ for all $t\in[0,1]$.
[Hint: Consider the sign of $f'$.]

(iii) Suppose $g:[a,b]\rightarrow{\mathbb R}$
is twice differentiable with $g''(t)\geq 0$ for all $t\in[a,b]$.
By considering the function $f:[0,1]\rightarrow{\mathbb R}$
defined by
\[f(t)=g\big((1-t)a+tb\big)-(1-t)g(a)-tg(b)\]
show that
\[g\big((1-t)a+tb\big)\leq (1-t)g(a)+tg(b)\]
for all $t$ with $1\geq t\geq 0$.

[In other words a twice differentiable function with everywhere
positive second derivative is \emph{convex}. Convex functions
are considered in the last part of the probability course
where you prove the very elegant Jensen's inequality.
You should note, however, that not all convex functions are twice
differentiable (look at $x\mapsto |x|$).]
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Q31}
The results of this question are also useful
in the probability course when you study extinction probabilities.
Notice that the point of this question is to obtain rigorous
proofs of the results stated.

Suppose $a_{0}>0$, $a_{1},\,a_{2},\,\dots,\,a_{n}\geq 0$
and $\sum_{j=0}^{n}a_{j}=1$. We set $P(t)=\sum_{j=0}^{n}a_{j}t^{j}$.

(i) Find $P(0)$, $P(1)$ and $P'(1)$. Show that $P'(t)\geq 0$
and $P''(t)\geq 0$ for all $t\geq 0$.

(ii) Show that the equation $P(t)=t$ has no solution
with $0\leq t<1$ if $\sum_{j=1}^{n}ja_{j}\leq 1$
and exactly one such solution if  $\sum_{j=1}^{n}ja_{j}>1$.
We write $\alpha$ for the smallest solution of $P(t)=t$
with $0\leq t\leq 1$.

(iii) If we set $x_{0}=0$ and $x_{n}=P(x_{n-1})$ for $n\geq 1$
show, by induction, that $x_{n-1}\leq x_{n}\leq\alpha$.

(iv) Deduce, giving your reasons explicitly, that $x_{n}$
must converge to a limit $\beta$. Show that $0\leq\beta\leq\alpha$
and $P(\beta)=\beta$. Deduce that $\beta=\alpha$ and so
\[x_{n}\rightarrow\alpha\ \text{as $n\rightarrow\infty$}.\]
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Q32}
The first proof that transcendental
numbers existed is due to Liouville. This question
gives a version of his proof. 

Let $P(t)=\sum_{j=0}^{N}a_{j}t^{j}$ with the $a_{j}$
integers and $a_{N}\neq 0$. Let $\alpha$ be a root
of $P$. 

(i) Why can we choose a $\delta>0$ such that $P(t)\neq 0$
for all $t$ with $t\in[\alpha-\delta,\alpha+\delta]$
and $t\neq\alpha$. 

(ii) Let $\delta$ be as in (i). Explain why there
exists a $K$ such that $|P(t)-P(s)|\leq K|t-s|$
for all $t,\, s\in[\alpha-\delta,\alpha+\delta]$.

(iii) If $p$ and $q$ are integers such that $q\geq 1$
and $P(p/q)\neq 0$
show that $|P(p/q)|\geq q^{-N}$. [Hint: Remember that the
coefficients of $P$ are integers.]

(iv) If $p$ and $q$ are integers such that $q\geq 1$,
$P(p/q)\neq 0$ and $p/q\in[\alpha-\delta,\alpha+\delta]$
use~(ii) and~(iii) to show that
\[\left|\alpha-\frac{p}{q}\right|\geq K^{-1}q^{-N}.\]

(v) Show that, if
$p$ and $q$ are integers such that $q\geq 1$ 
and $\alpha\neq p/q$, then
\[\left|\alpha-\frac{p}{q}\right|\geq \min(\delta,K^{-1}q^{-N}).\]

(vi) Show that if $\alpha$ is a root of
a polynomial with integer coefficients then there exists a
$K$ and an $N$ (depending on $\alpha$) such that
\[\left|\alpha-\frac{p}{q}\right|\geq K^{-1}q^{-N}\]
whenever $p$ and $q$ are integers with $q\geq 1$ and $\alpha\neq p/q$.

(vii) Explain why $\sum_{n=0}^{\infty}10^{-n!}$ converges.
Call the limit $L$. Show that
\[\left|L-\sum_{n=0}^{m}10^{-n!}\right|\leq 2((10)^{m!})^{-m-1}.\]
By taking $q=(10)^{m!}$ and looking at~(vi), show that $L$
can not be the root of
a polynomial with integer coefficients.

(viii) By looking at
\[\sum_{n=0}^{\infty}\zeta_{n}10^{-n!}\]
with $\zeta_{n}=\pm 1$ show that there are uncountably many
transcendentals.
\end{question}
\begin{question}\label{Q33} In this question you may assume standard
results on the power function $t\mapsto t^{\alpha}$.

Use the form of Taylor's
theorem given in  Theorem~\ref{Global theta} to show that
\[(1+x)^{\alpha}=1+\alpha x+ \frac{\alpha(\alpha-1)}{2!}x^{2}
+\dots+\frac{\alpha(\alpha-1)\dots(\alpha-n+1)}{n!}x^{n}+\dots\]
for all $0\leq x<1$. Note that the main part of your task
is to estimate the remainder term.

What happens if $\alpha$ is a positive integer?

Why does your argument fail for $-1<x<0$? (The result is true
but our form of the remainder does not give it. This is one
of the reasons why there are so many forms of Taylor's
with different remainders.)
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Use Rolle}\label{Q34}
(i) Suppose that $x_{0}$, $x_{1}$,
\dots $x_{n}$  are distinct. Set
\[e_{j}(x)=\prod_{k\neq j}\frac{x-x_{k}}{x_{j}-x_{k}}.\]
What is the value of $e_{j}(x_{k})$ if $j\neq k$ and
if $j=k$? Show that given real  numbers $\alpha_{j}$ there is
a polynomial of degree $n$ with $f(x_{j})=\alpha_{j}$.

(ii) Suppose $f:[a,b]\rightarrow{\mathbb R}$ is $n+1$ times
differentiable, $a<x_{0}<x_{1}<\dots<x_{n}<b$
and $P$ is a polynomial of degree $n$ with $P(x_{j})=f(x_{j})$
for $0\leq j\leq n$. We are interested in the error
\[e(x)=f(x)-P(x)\]
at a point $x\in[a,b]$ with $x\neq x_{j}$ for all $j$.

Set
\[F(t)=f(t)-P(t)-e(x)\prod_{k=0}^{n}\frac{t-x_{k}}{x-x_{k}}.\]
Explain why $F$ vanishes at least $n+2$ times on $[a,b]$.
Explain why $F'$ vanishes at least $n+1$ times on $[a,b]$.
By repeating the argument show that $F^{(n+1)}$ must vanish
at least once (at $\zeta$, say) on $[a,b]$. Show that
\[e(x)=\frac{1}{(n+1)!}f^{(n+1)}(\zeta)\prod_{k=0}^{n}(x-x_{k}).\]

Deduce that if $|f^{(n+1)}(t)|\leq M$ for all $t\in[a,b]$
then
\[|f(t)-P(t)|\leq \frac{M}{(n+1)!}|Q(t)|\]
where $Q(t)=\prod_{k=0}^{n}(t-x_{k})$.

[The argument just given should remind you of the
proof of Theorem~\ref{many mean}.]
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Q35}
Recall from last term (or earlier)
that
\[(\cos\theta+i\sin\theta)^{n}=\cos n\theta+i\sin n\theta\]
for all real $\theta$ and all integers $n\geq 0$.

By taking real parts, show that there is a real
polynomial $T_{n}$ of degree $n$ such that
\[T_{n}(\cos\theta)=\cos n\theta\]
for all real $\theta$.

Write down $T_{0}(t)$, $T_{1}(t)$, $T_{2}(t)$ and $T_{3}(t)$
explicitly.

Show that, if $n\geq 1$ then

(a) $|T_{n}(t)|\leq 1$ for all $t$ with $|t|\leq 1$,

(b) $T_{n+1}(t)=2tT_{n}(t)-T_{n-1}(t)$, (why does this result hold for all $t$?)

(c) the coefficient of $t^{n}$ in $T_{n}(t)$ is $2^{n-1}$.


Explain why $T_{n+1}$ has $n+1$ zeros $x_{0}$, $x_{1}$,
\dots, $x_{n}$ lying in $(-1,1)$. Use~(a), (c) and the
final result of question~\ref{Use Rolle} to show
that if $[a,b]=[-1,1]$ and $f$ and $P$ obey the
hypotheses of question~\ref{Use Rolle}
(so that, in particular, 
$|f^{(n+1)}(t)|\leq M$ for all $t\in[-1,1]$)
then
\[|f(t)-P(t)|\leq \frac{M}{(n+1)!}|2^{-n}T_{n+1}(t)|
\leq \frac{M}{2^{n}(n+1)!}\]

The rest of the question just asks for another
of useful property of the Tchebychev
polynomials $T_{n}$. (The modern view is that
Tchebychev should have called himself Chebychev.
He seems to have preferred Tchebycheff.)
                   
(d) If $n,\ m\geq 0$ then
\begin{equation*}
\int_{-1}^{1}\frac{T_{n}(x)T_{m}(x)}{(1-x^{2})^{1/2}}\,dx=
\begin{cases}
0&\text{if $n\neq m$}\\
\frac{\pi}{2}&\text{if $n=m\neq 0$}
\end{cases}
\end{equation*}
What happens if $n=m=0$?
\end{question}
\begin{question}(Exercise~\ref{complex limits}.)\label{Q36}

Let $E$ be a subset of ${\mathbb C}$,
$f$, $g$ be some functions from $E$ to ${\mathbb C}$,
and $z$ some point of $E$.

(i) The limit is unique. That is, if $f(w)\rightarrow l$
and $f(w)\rightarrow k$ as $w\rightarrow z$
then $l=k$.

(ii) If $z\in E'\subseteq E$ and $f(w)\rightarrow l$
as $w\rightarrow z$ through
values $w\in E$, then $f(w)\rightarrow l$
as $w\rightarrow z$ through
values $w\in E'$.

(iii) If $f(u)=c$ for all $u\in E$ then $f(w)\rightarrow c$
as $w\rightarrow z$.

(iv) If $f(w)\rightarrow l$ and $g(w)\rightarrow k$
as $w\rightarrow z$ then
$f(w)+g(w)\rightarrow l+k$.

(v) If $f(w)\rightarrow l$ and $g(w)\rightarrow k$
as $w\rightarrow z$ then
$f(w)g(w)\rightarrow lk$.

(vi) If $f(w)\rightarrow l$
as $w\rightarrow z$, $f(u)\neq 0$ for each $u\in E$ and
$l\neq 0$ then $f(w)^{-1}\rightarrow l^{-1}$.
\end{question}
\begin{question}(Exercise~\ref{first continuous complex} %
and~\ref{continuous complex chain rule}.)\label{Q37}
Suppose that $E$ is a subset of ${\mathbb C}$,
that $z\in E$,
and that $f$ and $g$
are functions from $E$ to ${\mathbb C}$.

(i) If $f(u)=c$ for all $u\in E$, then f is continuous
on $E$.

(ii) If $f$ and $g$ are continuous at $z$,
then so is $f+g$.

(iii) Let us define $f\times g:E\rightarrow {\mathbb C}$
by $f\times g(u)=f(u)g(u)$ for all $u\in E$. Then
if $f$ and $g$ are continuous at $z$, so is $f\times g$.

(iv) Suppose that $f(u)\neq 0$ for all $u\in E$. If
$f$ is continuous at $z$ so is $1/f$.

(v) If $z\in E'\subset E$ and $f$ is continuous at $z$ then the
restriction $f|_{E'}$ of $f$ to $E'$ is also continuous
at $z$.

(vi) If $J:{\mathbb C}\rightarrow{\mathbb C}$ is
defined by $J(z)=z$ for all $z\in{\mathbb C}$,
then $J$ is continuous on ${\mathbb C}$.

(vii) Every polynomial $P$ is continuous on ${\mathbb C}$.

(viii) Suppose that $P$ and $Q$ are polynomials
and that $Q$ is never zero on some subset $E$
of ${\mathbb C}$. Then the rational function
$P/Q$ is continuous on $E$ (or, more precisely,
the restriction of $P/Q$ to $E$ is continuous.)

(ix) Let $U$ and $V$ be subsets of ${\mathbb C}$. Suppose
$f:U\rightarrow{\mathbb C}$ is such that
$f(z)\in V$ for all $z\in U$. If $f$
is continuous at $w\in U$ and $g:V\rightarrow{\mathbb C}$
is continuous at $f(w)$, then the composition
$g\circ f$ is continuous at $w$.

\end{question}
\begin{question}(Exercises~\ref{derivatives complex -1}\label{Q38}
\ref{derivatives complex} and~\ref{complex chain rule}.)

Let $E$ be a subset of ${\mathbb C}$,
$f$, $g$ be some functions from $E$ to ${\mathbb C}$,
and $z$ some point of $E$. 

Show that if
$f$ is differentiable at $z$ then
$f$ is continuous at $z$.

Prove the following results.

(i) If $f(u)=c$ for all $u\in E$ then $f$ is differentiable
at $z$ with $f'(z)=0$.

(ii) If $f$ and $g$ are differentiable at $z$ then so
is their sum $f+g$ and
\[(f+g)'(z)=f'(z)+g'(z).\]

(iii) If $f$ and $g$ are differentiable at $z$ then so
is their product $f\times g$ and
\[(f\times g)'(z)=f'(z)g(z)+f(z)g'(z).\]

(iv) If $f$ is differentiable at $z$ and $f(u)\neq 0$ for all $u\in E$
then $1/f$ is differentiable at $z$ and
\[(1/f)'(z)=-f'(z)/f(z)^{2}.\]

(v) If $f(u)=\sum_{r=0}^{N}a_{r}u^{r}$ on $E$ then
then $f$ is differentiable at $z$ and
\[f'(z)=\sum_{r=1}^{N}ra_{r}z^{r-1}\].

(vi) Let $U$ and $V$ be subsets of ${\mathbb C}$. Suppose
$f:U\rightarrow{\mathbb C}$ is such that
$f(z)\in V$ for all $t\in U$. If $f$
is differentiable at $w\in U$ and $g:V\rightarrow{\mathbb R}$
is differentiable at $f(w)$, then the composition
$g\circ f$ is differentiable at $w$ with
\[(g\circ f)'(w)=f'(w)g'(f(w)).\]

\end{question}
\begin{question}(Exercise~\ref{New radius}.)\label{Q39}
Suppose that $\sum_{n=0}^{\infty}a_{n}z^{n}$
has radius of convergence $R$.
Then the sequence $|a_{n}z^{n}|$ is unbounded if $|z|>R$
and $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges absolutely
if $|z|<R$.
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Many pointed}\label{Q40}
This question requires Abel's test from Exercise~\ref{Q23}
which is also starred.

(i) Show that $\sum_{n=1}^{\infty}z^{n}/n$ has radius of
convergence $1$, converges for all $z$ with $|z|=1$
and $z\neq 1$ but diverges if $z=1$.

(ii) Let $|z_{1}|=|z_{2}|=\dots=|z_{m}|=1$.
Find a power series $\sum_{n=0}^{\infty}a_{n}z^{n}$ 
which has radius of convergence
$1$ and converges for all $z$ with $|z|=1$
and $z\neq z_{1},\ z_{2},\ \dots,\ z_{m}$
but diverges if $z=z_{j}$ for some $1\leq j\leq m$.
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Add power series}
(i) Show that there exist power series\label{Q41}
of all radii of convergence (including $0$ and $\infty$).

(ii) Suppose the power series $\sum_{n=0}^{\infty}a_{n}z^{n}$
has radius of convergence $R$ and
the power series $\sum_{n=0}^{\infty}b_{n}z^{n}$
has radius of convergence $S$. Show that, if $R\neq S$
then  $\sum_{n=0}^{\infty}(a_{n}+b_{n})z^{n}$
has radius of convergence $\min(R,S)$.

(iii) Suppose that the conditions of~(ii) hold except that $R=S$.
Show that the radius of convergence $T$  of
$\sum_{n=0}^{\infty}(a_{n}+b_{n})z^{n}$ satisfies the
condition $T\geq R$. Show by means of examples that
$T$ can take any value with $T\geq R$. [Hint: Start by
thinking of a simple relation between $a_{n}$ and $b_{n}$
which will give $T=\infty$.]
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Q42}
(i) Suppose that $x_{n}$ is a bounded sequence
of real numbers. Show carefully that $y_{n}=\sup_{m\geq n}x_{m}$
is a bounded decreasing sequence and deduce that $y_{n}$
tends to a limit $y$ say. We write
\[\limsup_{n\rightarrow\infty}x_{n}=y.\]

(ii) Suppose that $x_{n}$ is a bounded sequence
of real numbers. Prove the following two results.

\ \ (A) Given any $\epsilon>0$, we can find an $M(\epsilon)$
such that
\[x_{n}\leq \limsup_{m\rightarrow\infty}x_{m}+\epsilon\]
for all $n\geq M(\epsilon)$.

\ \ (B) Given any $\epsilon>0$ and any $N$ we can find
an integer $P(N,\epsilon)\geq N$ such that
\[x_{P(N,\epsilon)}\geq \limsup_{m\rightarrow\infty}x_{m}-\epsilon.\]

(iii) Using (ii) or otherwise show that if $a_{n}\in{\mathbb C}$
and the sequence $|a_{n}|^{1/n}$ is bounded and
$\limsup_{n\rightarrow\infty}|a_{n}|^{1/n}\neq 0$
then $\sum_{n=0}^{\infty}a_{n}z^{n}$
has radius of convergence $(\limsup_{n\rightarrow\infty}|a_{n}|^{1/n})^{-1}$.

Show also that, if the sequence $|a_{n}|^{1/n}$ is unbounded,
$\sum_{n=0}^{\infty}a_{n}z^{n}$
has radius of convergence $0$, and, if
$\limsup_{n\rightarrow\infty}|a_{n}|^{1/n}=0$,
$\sum_{n=0}^{\infty}a_{n}z^{n}$ has infinite radius of
convergence.

[This result has considerable theoretical significance
but I strongly advise using the definition directly
rather than relying on the formula.]

(iv) Use the formula of this question to obtain
the results of Question~\ref{Add power series}.
\end{question}
\begin{question}\label{Q43} (i) Starting from the observation that
\[1+x+x^{2}+\dots+x^{n}=\frac{1-x^{n+1}}{1-x}\]
show that
\[\frac{1}{1-x}=\sum_{n=0}^{\infty}x^{n}\]
for all $|x|<1$. Show that the result is false or meaningless
if $|x|\geq 1$.

(ii) Use term by term differentiation to obtain power series
expansions for $(1-x)^{-n}$ for all integer $n$ with $n\geq 1$.

(iii) Find the radius of convergence of $\sum_{n=1}^{\infty}x^{n}/n$.
Use term by term differentiation and the constant
value theorem to show that
\[\log (1-x)=-\sum_{n=1}^{\infty}\frac{x^{n}}{n}.\]

(iv) Use similar ideas to obtain a power series expansion
for $\tan^{-1}x$ in a range to  be stated.
\end{question}

\begin{question}(Exercise~\ref{Exercise, sinh}.)\label{Q44}
 
Write down what you consider to be the
chief properties of $\sinh$ and $\cosh$. (They should convey
enough information to draw a reasonable graph of the
two functions.)

(i) Obtain those properties by starting with the definitions
\[\sinh x=\sum_{n=0}^{\infty}\frac{x^{2n+1}}{(2n+1)!}
\ \text{and}\ \cosh x=\sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!}\]
and proceeding along the lines of Lemma~\ref{lemma, trigonometry}.

(ii) Obtain those properties by starting with the definitions
\[\sinh x=\frac{\exp x-\exp (-x)}{2}
\ \text{and}\ \cosh x=\frac{\exp x+\exp (-x)}{2}\]
\end{question}
\begin{question}$\negthickspace^{\star}$(Exercise~\ref{complex sine}.)
 
(i) Explain why the infinite sums\label{Q45}
\[\sin z=\sum_{n=0}^{\infty}\frac{(-1)^{n}z^{2n+1}}{(2n+1)!}
\ \text{and}\ \cos z=\sum_{n=0}^{\infty}\frac{(-1)^{n}z^{2n}}{(2n)!}\]
converge everywhere and are differentiable everywhere
with $\sin' z=\cos z$, $\cos' z=-\sin z$.

(ii) Show that
\[\sin z=\frac{\exp iz-\exp (-iz)}{2i},\ \cos z=\frac{\exp iz+\exp (-iz)}{2}\]
and
\[\exp iz=\cos z+i\sin z\]
for all $z\in{\mathbb C}$.

(iii) Show that
\[\sin (z+w)=\sin z\cos w+\cos z\sin w,
\ \cos(z+w)=\cos z \cos w-\sin z\sin w,\]
and
$(\sin z)^{2}+(\cos z)^{2}=1$
for all $z,\, w\in{\mathbb C}$.

(iv) $\sin(-z)=-\sin z$, $\cos (-z)=\cos z$ for all $z\in{\mathbb C}$.

(v) $\sin$ and $\cos$ are $2\pi$ periodic in the sense that
\[\sin (z+2\pi)=\sin z\ \text{and}\ \cos (z+2\pi)=\cos z\]
for all $z\in{\mathbb C}$.

(vi) If $x$ is real then $\sin ix=i\sinh x$ and $\cos ix=\cosh x$.

(vii) Recover the addition formulae for $\sinh$ and $\cosh$ by
setting $z=ix$ and $w=iy$ in part~(iii).

(viii) Show that $|\sin z|$ and $|\cos z|$ are bounded if $|\Im z|\leq K$
for some $K$ but that $|\sin z|$ and $|\cos z|$ are unbounded
on ${\mathbb C}$.
\end{question}
\begin{question}$\negthickspace^{\star}$(Exercise~\ref{no logarithm}.)
\label{Q46}

Suppose,
if possible, that there exists a continuous
$L:{\mathbb C}\setminus\{0\}\rightarrow{\mathbb C}$
with $\exp(L(z))=z$ for all $z\in{\mathbb C}\setminus\{0\}$.

(i) If $\theta$ is real, show that
$L(\exp(i\theta))=i(\theta+2\pi n(\theta))$ for some
$n(\theta)\in{\mathbb Z}$.

(ii) Define $f:{\mathbb R}\rightarrow{\mathbb R}$
by
\[f(\theta)=\frac{1}{2\pi}
\left(\frac{L(\exp i\theta)-L(1)}{i}-\theta\right).\]
Show that $f$ is a well defined continuous function,
that $f(\theta)\in{\mathbb Z}$ for all $\theta\in{\mathbb R}$,
that $f(0)=0$ and that $f(2\pi)=-1$.

(iii) Show that the statements made in the last
sentence of (ii) are incompatible with the intermediate
value theorem and deduce that no function can exist
with the supposed properties of $L$.

(iv) Discuss informally what connection, if any, the
discussion above has with the existence of the
international date line.
\end{question}
\begin{question}$\negthickspace^{\star}$(Exercise~\ref{no square root}.)
\label{Q46 A}
Show by
modifying the argument of Exercise~\ref{no logarithm},
that there does not exist a continuous
$S:{\mathbb C}\rightarrow{\mathbb C}$
with $S(z)^{2}=z$ for all $z\in{\mathbb C}$.
\end{question}

\section{Fourth example sheet}  Students vary in how much work
they are prepared to do. On the whole, exercises from
the main text are reasonably easy and provide good
practice in the ideas. Questions and parts of questions
marked with $^{\star}$ are not intended to be hard
but cover topics less central to the present course.


\begin{question}(Exercise~\ref{social convention}.)\label{Q47} 
Suppose that $b\geq a$, $\lambda,\mu\in{\mathbb R}$,
and $f$ and $g$ are Riemann integrable. Which of the
following statements are always true and which are not?
Give a proof or counter-example. If the statement
is not always true, find an appropriate correction
and prove it.

(i) ${\displaystyle \int_{b}^{a}\lambda f(x)+\mu g(x)\,dx
=\lambda\int_{b}^{a}f(x)\,dx+\mu \int_{b}^{a}g(x)\,dx}$.

(ii) If $f(x)\geq g(x)$ for all $x\in[a,b]$, then
${\displaystyle \int_{b}^{a}f(x)\,dx\geq\int_{b}^{a}g(x)\,dx}$.
\end{question}
\begin{question}(Exercise~\ref{Exercise, integral positive zero}.)
Let $a\leq c\leq b$.\label{Q48}
Give an example of a
Riemann integrable function $f:[a,b]\rightarrow{\mathbb R}$
such that $f(t)\geq 0$ for all $t\in[a,b]$
and
\[\int_{a}^{b}f(t)\,dt=0,\]
but $f(c)\neq 0$.
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Q49}
Define $f:[0,1]\rightarrow{\mathbb R}$ by
$f(p/q)=1/q$ when $p$ and $q$ are coprime integers
with $1\leq p <q$ and $f(x)=0$ otherwise.

(i) Show that $f$ is Riemann integrable and
find $\int_{0}^{1}f(x)\,dx$.

(ii) At which points is $f$ continuous? Prove
your answer.
\end{question}

\begin{question}(Exercise~\ref{not fundamental}).\label{Q50}

(i) Let $H$ be the Heaviside function
$H:{\mathbb R}\rightarrow{\mathbb R}$ given by
$H(x)=0$ for $x<0$, $H(x)=1$ for $x\geq 0$. Calculate
$F(t)=\int_{0}^{t}H(x)\,dx$ and show that $F$ is
not differentiable at $0$. Where does our
proof of Theorem~\ref{Theorem, fundamental calculus}
break down?

(ii) Let $f(0)=1$, $f(t)=0$ otherwise. Calculate
$F(t)=\int_{0}^{t}f(x)\,dx$ and show that $F$ is
differentiable at $0$ but $F'(0)\neq f(0)$. Where does our
proof of Theorem~\ref{Theorem, fundamental calculus}
break down?
\end{question}
\begin{question}(Exercise~\ref{No substitutes}.)\label{Q51}

The following exercise is traditional.

(i) Show that integration by substitution, using $x=1/t$,
gives
\[\int_{a}^{b}\frac{dx}{1+x^{2}}
=
\int_{1/b}^{1/a}\frac{dt}{1+t^{2}}\]
when $b>a>0$.

(ii) If we set $a=-1$, $b=1$ in the formula of~(i),
we obtain
\[\int_{-1}^{1}\frac{dx}{1+x^{2}}
\overset{?}{=}
-\int_{-1}^{1}\frac{dt}{1+t^{2}}\]
Explain
this apparent failure
of the method of integration by substitution.

(iii) Write the result of (i) in terms of $\tan^{-1}$
and prove it using standard trigonometric identities.
\end{question}

\begin{question}\label{Question, define logarithm}\label{Q52}
In this question we give an alternative
treatment of the logarithm so no properties of the
exponential or logarithmic function should be used.
You should quote all the theorems that you use,
paying particular attention to those on integration.

We set
\[l(x)=\int_{1}^{x}\frac{1}{t}\,dt.\]

(i) Explain why $l:(0,\infty)\rightarrow{\mathbb R}$
is a well defined function.

(ii) Use the change of variable theorem for integrals to
show that
\[\int_{x}^{xy}\frac{1}{t}\,dt=l(y)\]
for all $x,\, y>0$. Deduce that $l(xy)=l(x)+l(y)$.

(iii) Show that $l$ is everywhere differentiable with 
$l'(x)=1/x$.

(iv) Show that $l$ is a strictly increasing function.

(v) Show that $l(x)\rightarrow\infty$ as $x\rightarrow\infty$.
\end{question}
\begin{question}\label{Q53} In the lectures we deduced the properties
of the logarithm from those of the exponential.
Reverse this by making a list of the properties
of the exponential, define $\exp$ as the inverse function of $\log$
(explaining carefully why you can do this) and using
the properties of $\log$ found in the previous question
(Question~\ref{Question, define logarithm}).
\end{question}

\begin{question}$\negthickspace^{\star}$\label{Q54}
{\bf [The first mean value theorem for integrals]}
Suppose $g:[a,b]\rightarrow{\mathbb R}$ is a continuous
function such that
$g(x)\geq 0$ for all $x\in[a,b]$. If $f:[a,b]\rightarrow{\mathbb R}$
is continuous explain why we can find $k_{1}$ and $k_{2}$
in $[a,b]$ such that
\[f(k_{1})\leq f(x)\leq f(k_{2})\]
for all $x\in[a,b]$. Deduce carefully that
\[f(k_{1})\int_{a}^{b}g(x)\,dx\leq \int_{a}^{b}f(x)g(x)\,dx
\leq f(k_{2})\int_{a}^{b}g(x)\,dx\]
and show, stating carefully any theorem that you need, that
there exists a $c\in[a,b]$ such that
\[\int_{a}^{b}f(x)g(x)\,dx=f(c)\int_{a}^{b}g(x)\,dx.\]

Does this result remain true if $g(x)\leq 0$ for all $x\in[a,b]$?
Does this result remain true if we place no restrictions on
$g$ (apart from continuity). In each case give a proof or
a counter example.
(The first mean value theorem for integrals is used in
the numerical analysis course.)
\end{question}

\begin{question} Use the integral form of the remainder\label{Q55}
in Taylor's theorem (i.e. Theorem~\ref{Integral Taylor}) to obtain
the power series expansion for $\sin$.
\end{question}
\begin{question}{\bf [The binomial theorem]}\label{Q56}
If $1>x\geq t\geq 0$ show that
\[\frac{x-t}{1+t}\leq x.\]
If $-1<x\leq t\leq 0$ show that
\[\frac{t-x}{1+t}\leq -x.\]
Use the integral form of the
Taylor theorem to show that, if $|x|<1$, then
\[(1+x)^{\alpha}=1+\alpha x+ \frac{\alpha(\alpha-1)}{2!}x^{2}
+\dots+\frac{\alpha(\alpha-1)\dots(\alpha-n+1)}{n!}ix^{n}+\dots.\]
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Q57}
{\bf [Cauchy-Schwarz for integrals]}
Write $C([a,b])$ for the set of continuous functions
$f:[a,b]\rightarrow{\mathbb R}$. 

(i) If you are doing course~P1 verify that $C([a,b])$
is a vector space over ${\mathbb R}$. Is it finite dimensional?
Prove your answer.

(ii) If $f,\, g\in C([a,b])$
we write
\[\langle f,g\rangle=\int_{a}^{b}f(t)g(t)\,dt.\]
Show that if $f,\, g,\, h\in C([a,b])$ and 
$\lambda,\, \mu\in{\mathbb R}$ then

\ \ \ (a) $\langle f,f\rangle\geq 0$,

\ \ \ (b) if $\langle f,f\rangle=0$ then $f=0$.

\ \ \ (c) $\langle f,g\rangle=\langle g,f\rangle$,

\ \ \ (d) $\langle \lambda f+\mu g,h\rangle
=\lambda \langle f,h\rangle+\mu \langle g,h\rangle$.

(iii) By imitating the proof of the Cauchy-Schwarz
inequality for ${\mathbb R}^{n}$ in the course C1/C2
show that
\[\langle f,g\rangle^{2}\leq \langle f,f\rangle\langle g,g\rangle.\]
In other words
\[\left(\int_{a}^{b}f(t)g(t)\,dt\right)^{2}
\leq \int_{a}^{b}f(t)^{2}\,dt\int_{a}^{b}g(t)^{2}\,dt.\]

(iv) When do we have equality in the inequality proved
in~(iii).
\end{question}
\begin{question}$\negthickspace^{\star}$ Explain why\label{Q58}
\[\frac{1}{n+1}\leq\int_{n}^{n+1}\frac{1}{x}\,dx\leq\frac{1}{n}.\]
Hence or otherwise show that if we write
\[T_{n}=\sum_{r=1}^{n}\frac{1}{r}-\log n\]
we have $T_{n+1}\leq T_{n}$ for all $n\geq 1$.
Show also that $1\geq T_{n}\geq 0$. Deduce
that $T_{n}$ tends to a limit $\gamma$ (Euler's constant)
with $1\geq \gamma\geq 0$. [It is an indication of how little
we know about specific real numbers that, after three
centuries, we still do not know whether $\gamma$
is irrational. G.~H.~Hardy is said to have offered
his chair to anyone who could prove that $\gamma$
was transcendental.]

(ii) By considering $T_{2n}-T_{n}$, show that
\[1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+
\frac{1}{5}-\frac{1}{6}+\frac{1}{7}-\frac{1}{8}\dots=\log 2\]

(iii) By considering $T_{4n}-\frac{1}{2}T_{2n}-\frac{1}{2}T_{n}$,
show that
\[1+\frac{1}{3}-\frac{1}{2}+\frac{1}{5}+
\frac{1}{7}-\frac{1}{4}\dots=\frac{3}{2}\log 2\]

The famous example is due to Dirichlet. It gives a specific 
example where rearranging a non-absolutely convergent
sum changes its value.
\end{question}
\begin{question}
{\bf [Simple versions of Stirling's formula]\label{Q59}}
(The first part of this question
is in the probability course.)

(i) Prove that
\[\int_{1}^{n}\log x\, dx\leq \sum_{r=1}^{n}\log r\leq
\int_{1}^{n}\log x\, dx+\log n.\]
Compute $\int_{1}^{n}\log x\, dx$
and hence show that
\[\frac{1}{n}(n!)^{1/n}\rightarrow \frac{1}{e}\]
as $n\rightarrow\infty$.

(ii)$^{\star}$ Show that
\[\int_{n-1/2}^{n+1/2}\log x\, dx-\log n=
\int_{0}^{1/2}\log\left(1+\frac{t}{n}\right)
+\log\left(1-\frac{t}{n}\right)\,dt.\]
By using the mean value theorem, or otherwise, deduce that
\[\left|\int_{n-1/2}^{n+1/2}\log x\, dx-\log n\right|\leq
\frac{4}{3n^{2}}.\]
(You may replace $4/(3n^{2})$ by $An^{-2}$ with $A$ another constant,
if you wish.)
Deduce that
$\int_{1/2}^{N+1/2}\log x\, dx -\log N!$ converges to a limit.
Conclude that
\[\frac{n!}{(n+1/2)^{(n+1/2)}}e^{n}
\rightarrow C\]
as $n\rightarrow\infty$ for some constant $C$.

(iii)$^{\star}$ Find a function $f(n)$ not involving factorials
such that
\[f(n)\binom{2n}{n}\]
tends to a limit as $n\rightarrow\infty$. You should try for an
$f$ in as simple a form as possible. (But, of course, different people
may have different views on what simple means.)

(iv)$^{\star}$ Show that the result of (ii) implies the result of (i).
Give an example of a sequence $a_{n}$ such that
$a_{n}^{1/n}$ converges but $a_{n}$ does not.
\end{question}
\begin{question}\label{Q60}
(i) Show that 
$\sum_{n=27}^\infty \frac{1}{n\log n\log\log n}$
diverges. Find a value of $N$ such that
$\sum_{n=27}^{N} \frac{1}{n\log n\log\log n}\geq 3$.
Try and find its numerical value on your calculator.

(ii)$^{*}$ Given $a_{n}>0$ such that $\sum_{n=1}^{\infty}a_{n}$
diverges show that we can find $b_{n}>0$ such that
$b_{n}/a_{n}\rightarrow 0$ but $\sum_{n=1}^{\infty}b_{n}$
diverges. 

Given $a_{n}>0$ such that $\sum_{n=1}^{\infty}a_{n}$
converges show that we can find $b_{n}>0$ such that
$b_{n}/a_{n}\rightarrow \infty$ but $\sum_{n=1}^{\infty}b_{n}$
converges.

[These two results show that it is futile to look for
some sort of `supercharged ratio test' to decide the
convergence of all possible series.]
\end{question}
\begin{question}\label{Q61}

(i) By writing \[
r(r+1)\dots (r+m-1)=A_{m}\big(r(r+1)\dots (r+m)-(r-1)r\dots (r+m-1)\big),\]
where $A_{m}$ is to found explicitly, compute 
$\sum_{r=1}^{N}r(r+1)\dots (r+m-1)$. Deduce that
\[N^{-m-1}\sum_{r=1}^{N}r(r+1)\dots (r+m-1)\rightarrow\frac{1}{m+1}.\]

(ii) Show that
\[r(r+1)\dots (r+m-1)-r^{m}=P(r)\]
where $P$ is a polynomial of degree less than $m$.
Show using (i) and induction, or otherwise, that
\[N^{-m-1}\sum_{r=1}^{N}r^{m}\rightarrow\frac{1}{m+1}.\]

(iii) Use dissections of the form
\[{\mathcal D}=\{0,\ a/n,\ 2a/n,\ \dots,\ a\}\]
to compute
\[\int_{0}^{a}x^{m}\,dx\]
directly from the definition.

(iv) Use dissections of the form
\[{\mathcal D}=\{br^{n},\ br^{n-1},\ br^{n-2},\ \dots,\ b\}\]
with $0<r$ and $r^{n}=a/b$ to compute
\[\int_{a}^{b}x^{m}\,dx\]
directly from the definition.
\end{question}
\begin{question}$\negthickspace^{\star}$\label{Q62} 
If $A_{n}$ and $G_{n}$ are the arithmetic and geometric means of the
$n$ positive integers
\[n+1,\ n+2,\ldots,\ n+n\]
show that, as $n\rightarrow\infty$,
\[\frac{A_{n}}{n}\rightarrow\frac{3}{2}
\ \ {\rm and}\ \ \frac{G_{n}}{n}\rightarrow\frac{4}{e}.\]
Deduce that $e\geq 8/3$.
\end{question}

\begin{question}$\negthickspace^{\star}$\label{Dirichlet kernel} 
(i) Let\label{Q63}
\[v_{n}=\int_{n\pi}^{(n+1)\pi}\frac{\sin x}{x}\,dx.\]
By writing both integrals as integrals from $0$ to $\pi$,
or otherwise, show that $|v_{n}|\geq |v_{n+1}|$.
By using a theorem on the convergence of sums (to be
stated) show that the sequence
\[\int_{0}^{n\pi}\frac{\sin x}{x}\,dx\rightarrow L\]
as $n\rightarrow\infty$ \emph{through integer values of} $n$
where $L$ is a strictly positive
real number.

(ii) Deduce carefully that
\[\int_{0}^{X}\frac{\sin x}{x}\,dx\rightarrow L\]
as $X\rightarrow\infty$ \emph{through real values of} $X$.
Thus $\int_{0}^{\infty}\frac{\sin x}{x}\,dx$ exists
with value $L$.

(iii)$^{*}$ Let
\[G(\lambda)=\int_{0}^{\infty}\frac{\sin \lambda x}{x}\,dx.\]
Show carefully (we have not actually proved a change
of variables theorem for infinite integrals)
that $G(\lambda)$ exists for all real $\lambda$ and
\begin{equation*}
G(\lambda)=
\begin{cases}
L&\text{if $\lambda>0$}\\
0&\text{if $\lambda=0$}\\
-L&\text{if $\lambda<0$}
\end{cases}
\end{equation*}
\noindent [Note that $G$ is not continuous at $0$. This is an
indication of the unintuitive behaviour which infinite integrals
can exhibit.]
\end{question}
\begin{question}$\negthickspace^{\star}$\label{exercise, bounded variation}
(i) Suppose $f_{1},\, f_{2}:[a,b]\rightarrow{\mathbb R}$\label{Q64}
are increasing and $g=f_{1}-f_{2}$. Show that there
exists a $K$ such that, whenever 
\[a=x_{0}\leq x_{1}\leq x_{2}\leq\dots\leq x_{n}=b\]
we have
\[\sum_{j=1}^{n}|g(x_{j})-g(x_{j-1})|\leq K.\]

(ii) Let $g:[-1,1]\rightarrow{\mathbb R}$ be given by
$g(x)=x^{2}\sin x^{-4}$ for $x\neq 0$, $g(0)=0$.
Show that $g$ is once differentiable everywhere 
but that $g$ is not the difference of two increasing
functions.
\end{question}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% analysis II

\section{Why do we bother?} It is surprising how many people
think that that analysis consists in the difficult
proofs of obvious theorems. All we need know, they say, is what
a limit is, the definition of continuity
and the definition of the derivative. All the
rest is `intuitively clear'.

If pressed they will agree that these definitions apply as
much to the rationals ${\mathbb Q}$ as to the real
numbers ${\mathbb R}$. They then have
to explain the following interesting
example.
\begin{example}\label{Rational} 
If $f:{\mathbb Q}\rightarrow{\mathbb Q}$
is given by
\begin{alignat*}{2}
f(x)&=-1&&\qquad\text{if $x^{2}<2$,}\\
f(x)&=1&&\qquad\text{otherwise,}
\end{alignat*}
then 

(i) $f$ is continuous function with $f(0)=-1$, $f(2)=1$ yet
there does not exist a $c$ with $f(c)=0$,

(ii) $f$ is a differentiable function with $f'(x)=0$ for
all $x$ yet $f$ is not constant.
\end{example}

What is the difference between ${\mathbb R}$ and ${\mathbb Q}$
which makes  
calculus work on one even though it fails on the other.
Both are `ordered fields', that is, both support operations
of `addition'
and `multiplication' together with a relation `greater than'
(`order') with the properties that we expect. If the
reader is interested she will find a complete list of 
the appropriate axioms in texts like the altogether
excellent book of Spivak~\cite{Spivak} and its
many rather less excellent competitors,
but, interesting as
such things may be, they are irrelevant to our purpose
which is not to consider the shared properties
of ${\mathbb R}$ and ${\mathbb Q}$ 
but to identify  a \emph{difference} between the
two systems which will enable us to exclude the
possibility of a function like that of Example~\ref{Rational}
for functions from ${\mathbb R}$ to ${\mathbb R}$.

To state the difference we need only recall a definition
from the beginning of the course~C5/6.
\begin{definition}\label{one convergence definition}
If $a_{n}\in{\mathbb R}$ for each $n\geq 1$
and $a\in{\mathbb R}$ then we say that $a_{n}\rightarrow a$
if given $\epsilon>0$ we can find an $n_{0}(\epsilon)$
such that
\[|a_{n}-a|<\epsilon\ \text{for all $n\geq n_{0}(\epsilon)$}.\]
\end{definition}
The key property of the reals, the \emph{fundamental axiom}
which makes everything work was also stated in 
the course~C5/6.
\begin{axiom}[The fundamental axiom of analysis]
If $a_{n}\in{\mathbb R}$ for each $n\geq 1$, $A\in{\mathbb R}$
and $a_{1}\leq a_{2}\leq a_{3}\leq \ldots$ and
$a_{n}<A$ for each $n$ then there exists an $a\in{\mathbb R}$
such that $a_{n}\rightarrow a$ as $n\rightarrow\infty$.
\end{axiom}
Less ponderously, and just as rigorously, the fundamental axiom
for the real numbers
says \emph{every increasing sequence bounded above tends
to a limit}.

Everything which depends on the fundamental axiom is
analysis, everything else is mere algebra.

\section{The axiom of Archimedes} In the course~C5/6 you
proved the following results on the limits of sequences.
\begin{lemma}\label{one sequences} 
(i) The limit is unique. That is, if $a_{n}\rightarrow a$
and $a_{n}\rightarrow b$ as $n\rightarrow\infty$
then $a=b$.

(ii) If $a_{n}\rightarrow a$ as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$a_{n(j)}\rightarrow a$ as $j\rightarrow\infty$.

(iii) If $a_{n}=c$ for all $n$ then $a_{n}\rightarrow c$
as $n\rightarrow\infty$.

(iv) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$ then
$a_{n}+b_{n}\rightarrow a+b$.

(v) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$ then
$a_{n}b_{n}\rightarrow ab$.

(vi) If $a_{n}\rightarrow a$
as $n\rightarrow\infty$ and $a_{n}\neq 0$ for each $n$,
$a\neq 0$ then $a_{n}^{-1}\rightarrow a^{-1}$.

(vii) If $a_{n}\leq A$ for each $n$ and
$a_{n}\rightarrow a$
as $n\rightarrow\infty$ then $a\leq A$.
\end{lemma}

Since these results were proved in the earlier course I shall
not prove them. However, it is \emph{essential} for
further progress that the reader should be able to prove
them for herself. If you can not prove these results
\emph{consult your supervisor at once}. You should do the
same if you cannot prove the following variation
on the fundamental axiom.
\begin{lemma}\label{Exercise 1.4} 
A decreasing sequence of real numbers bounded below tends
to a limit.
\end{lemma}
[Hint. If $a\leq b$ then $-b\leq -a$.]


Useful as the results of Lemma~\ref{one sequences} are,
they are also true of sequences in ${\mathbb Q}$.
They are therefore mere, if important, algebra.
Our first truly `analysis' result may strike the
reader as rather odd.
\begin{theorem}[Axiom of Archimedes]\label{Archimedes}
\[\frac{1}{n}\rightarrow 0\ \text{as $n\rightarrow\infty$}\]
\end{theorem}
Theorem~\ref{Archimedes} shows that there is no `exotic'
real number $\gimel$ say (to choose an exotic symbol)
with the property that $1/n>\gimel$ for all integers $n\geq 1$
yet $\gimel>0$ (that is, $\gimel$ is strictly positive
and yet smaller than all strictly positive rationals). 
There exist number systems with such exotic numbers
(the famous `non-standard analysis' of Abraham Robinson
and the `surreal numbers' of Conway constitute two
such systems) but, just as the rationals are, in
some sense, too
small a system for the standard theorems of analysis to hold
so these non-Archimedean systems are, in some sense,
too big. Archimedes and  Eudoxus
realised the need for an axiom
to show that there is no exotic number $\daleth$ bigger
than any integer\footnote{Footnote for passing historians,
this is a course in mathematics.}
(i.e. $\daleth>n$ for all integers $n\geq 1$;
to see the connection with our form of the axiom consider
$\gimel=1/\daleth$). However, in spite of its name, what
was an axiom for Archimedes is a theorem
for us. 
\begin{theorem} Given any real number $K$ we can find
an integer $n$ with $n>K$.
\end{theorem}
\section{The Bolzano--Weierstrass theorem} It is all very well
knowing that the fundamental axiom is the foundation
of mathematics but how can we find a proof technique
which will allow us to apply it.  One technique uses
the  Bolzano--Weierstrass theorem.
\begin{theorem}[Bolzano--Weierstrass]\label{one Bolzano}
If $x_{n}\in{\mathbb R}$ and there exists a $K$
such that $|x_{n}|\leq K$ for all $n$ then we can find
$n(1)<n(2)<\ldots$ and $x\in{\mathbb R}$ such that
$x_{n(j)}\rightarrow x$ as $j\rightarrow\infty$.
\end{theorem}
Recall from C5/6
that we say that a sequence converges if it tends to 
a limit.
The Bolzano--Weierstrass theorem thus
says that every bounded sequence of reals has a convergent
subsequence. Notice that we say nothing about uniqueness,
if $x_{n}=(-1)^{n}$ then $x_{2n}\rightarrow 1$ but
$x_{2n+1}\rightarrow -1$ as $n\rightarrow\infty$.

There is a proof of the Bolzano--Weierstrass theorem 
by repeated dissection along the lines of the proof
of the intermediate value theorem given in course~C5/6.
We shall give another proof based on the following
elegant combinatorial lemma.
\begin{lemma} If $x_{n}\in{\mathbb R}$ then at least
one of the following two statements must be true.

(A) There exist $n(1)<n(2)<\ldots$ such that
$x_{n(j)}\leq x_{n(j+1)}$ for each $j\geq 1$.

(B) There exist $m(1)<m(2)<\ldots$ such that
$x_{m(j)}\geq x_{m(j+1)}$ for each $j\geq 1$.
\end{lemma}

There are a variety of techniques for using the fundamental axiom:-
successive dissection, using Heine--Borel, looking for
the supremum and so on. When the reader is confident
in her control of the material in this course (and when she
knows the various techniques enumerated in
the last sentence) she should test her skill by
proving the main results of this course by using each
technique in turn. For the moment it seems sensible
to concentrate on one proof technique until
it is fully mastered. The technique chosen for this
exposition is that of Bolzano--Weierstrass.
We conclude this section with two important examples,
Theorem~\ref{supremum} on the existence of suprema
and Theorem~\ref{Cauchy} on the general principle of convergence.

It is an unfortunate fact of life that many, otherwise
perfectly well behaved sets of numbers do not have
greatest members (maxima). A simple example is
given by $\{x\in{\mathbb R}:0<x<1\}$. However,
we shall see in Theorem~\ref{supremum}
any non-empty bounded set of real numbers does have 
a least upper bound (supremum).
\begin{definition} Consider a non-empty set $A$ of real 
numbers. We say that $\alpha$ is a \emph{least upper bound}
(or \emph{supremum})
for $A$ if the following two conditions hold.

(i) $\alpha\geq a$ for all $a\in A$ (that is, $\alpha$ is
an upper bound).

(ii) If $\beta\geq a$ for all $a\in A$ then $\beta\geq\alpha$
(that is, $\alpha$ is no greater than any possible upper bound).
\end{definition}
\begin{lemma} The supremum is unique if it exists.
\end{lemma}

It is convenient to have the following alternative characterisation
of the supremum.
\begin{lemma} Consider a non-empty set $A$ of real 
numbers; $\alpha$ is a \emph{least upper bound}
for $A$ if and only if
the following two conditions hold.

(i) $\alpha\geq a$ for all $a\in A$.

(ii) Given $\epsilon>0$ there exists an $a\in A$ such that
$a+\epsilon\geq \alpha$.
\end{lemma}
We write $\sup A$ or $\sup_{a\in A}a$ for the supremum 
of $A$ if it exists.

Here is the promised theorem.
\begin{theorem}[Existence of the supremum]\label{supremum}
If $A$ is a non empty set of real numbers which is
bounded above (that is, there exists a $K$ such that
$a\leq K$ for all $a\in A$) then $A$ has a supremum.
\end{theorem}
That this is a genuine theorem of analysis is shown
by the fact that, if we work in $\mathbb Q$ the set
$\{x\in\mathbb Q:x^{2}<2\}$ has no least upper bound.
(See Question~\ref{square supremum} for a detailed
discussion.)

We leave it to the reader to define the \emph{greatest
lower bound} also called the \emph{infimum}
of a set $A$ and written $\inf A$ or $\inf_{a\in A}a$.
She should prove that
\[\inf_{a\in A}a=-\sup_{a\in A}(-a)\]
provided that either side of the equation exists.

As an example of its use, recall the following lemma
from course C5/6.
\begin{lemma} We work in ${\mathbb C}$.
If $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges
for $z=z_{0}$ then it converges for all $z$ with
$|z|<|z_{0}|$.
\end{lemma}
The use of Theorem~\ref{supremum} gives us a rather
transparent proof of the existence of the radius of convergence.
\begin{theorem} We work in ${\mathbb C}$. Consider
the sum $\sum_{n=0}^{\infty}a_{n}z^{n}$. Either
$\sum_{n=0}^{\infty}a_{n}z^{n}$ converges for all
$z$ (in this case we say that the series has infinite radius of
convergence) or there exists a real number $R\geq 0$
such that

(i) $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges for all $|z|<R$,

(ii) $\sum_{n=0}^{\infty}a_{n}z^{n}$ diverges for all $|z|>R$,

\noindent (in this case we say that the series has radius of
convergence R).
\end{theorem}

The nature of the supremum is illustrated by the
following example which we leave to the reader as an
exercise in the methods of the course C5/6. (As usual
contact your supervisor if you can not do it.)
\begin{example}\label{on and off}
(i) The sum $\sum_{n=1}^{\infty}n^{-2}z^{n}$
has radius of convergence $1$ and converges for all $|z|=1$.

(ii) The sum $\sum_{n=0}^{\infty}z^{n}$
has radius of convergence $1$ and diverges for all $|z|=1$.

(iii) The sum $\sum_{n=1}^{\infty}n^{-1}z^{n}$
has radius of convergence $1$, diverges for $z=1$
and converges for $z=-1$.
\end{example}
[When revising this part of the course you should also bear in
mind Theorem~\ref{Radius and uniform} discussed later.]

We turn now to the general principle of convergence.
\begin{definition}\label{one Cauchy sequence} 
We say that a sequence of real numbers
$x_{n}$ is a \emph{Cauchy sequence} if given any
$\epsilon>0$ we can find $n_{0}(\epsilon)$ such that
$|x_{p}-x_{q}|<\epsilon$ whenever $p,q\geq n_{0}(\epsilon)$.
\end{definition}
Our first lemma is merely algebraic. 
\begin{lemma} Any convergent sequence forms a Cauchy sequence.
\end{lemma}
The converse is a powerful theorem of analysis.
\begin{theorem} Any Cauchy sequence of real numbers
converges.
\end{theorem}
Combining the two results we get the general principle
of convergence.
\begin{theorem}[General principle of convergence]\label{Cauchy}
A sequence of real numbers converges if and only if
it is a Cauchy sequence.
\end{theorem}
The general principle of convergence is usually
too general to
be used in the same way as
the convergence tests of Course~C5/6
but has great theoretical importance as we shall see.

For those who can not wait here is a proof of the
uncountability of ${\mathbb R}$. The argument is
much closer to Cantor's original proof than the
standard one given in course C1. I present
it as a heavily starred exercise.
\begin{exercise}[Uncountability of the reals]%
\label{Cantor}
Let $y_{1}$, $y_{2}$, \dots be any
sequence of points in ${\mathbb R}$. Let $x_{0}=0$,
$\delta_{0}=1$.

(i) Show that you can construct inductively
a sequence of real numbers $x_{1}$, $x_{2}$, \dots
and positive numbers $\delta_{j}$ such that

\ \ (a) $|x_{n}-x_{n-1}|<\delta_{n-1}/4$,

\ \ (b) $x_{n}\neq y_{n}$,

\ \ (c) $0<\delta_{n}<|x_{n}-y_{n}|$,

\ \ (d) $\delta_{n}<\delta_{n-1}/4$.

(ii) Show that $\delta_{n+m}<4^{-m}\delta_{n}$ for $m,n\geq 0$
and deduce that the $x_{n}$ form a Cauchy sequence.
Conclude that $x_{n}$ tends to a limit $x$.

(iii) Show that $|x_{n+m}-x_{n}|<\delta_{n}/3$ for all 
$m,n\geq 0$. Deduce that $|x-x_{n}|\leq \delta_{n}/3$
for all $n\geq 0$. Why does this show that $y_{n}\neq x$
for all $n$?

(iv) Deduce that the real numbers are uncountable.
\end{exercise}

\section{Higher dimensions} In 1908, G.~H.~Hardy wrote
a textbook to introduce the new rigorous
analysis (or `continental analysis' as it was known
in a Cambridge even more insular than today)
to `first year students at the Universities
whose abilities approach something like what is
usually described as ``scholarship standard''\,'.
Apart from the fact that even the most hardened
analyst would now hesitate to call an introduction
to analysis  \emph{A Course of Pure Mathematics}
it is striking how close the book is in both
content and feel to a modern first course in
analysis. (And where there are changes it is
often not clear that the modern course\footnote{Indeed,
anyone who works through the exercises in Hardy
as a first course and the exercises in Whittaker
and Watson's even older 
\emph{A Course of Modern Analysis}~\cite{Whittaker}
as a second will have had a splendid education in analysis.}  
has the advantage.)
The only major difference is that Hardy only
studies the real line but later advances in mathematics
mean that we must now study analysis in ${\mathbb R}^{m}$
as well.

In the course C1/2 you saw that ${\mathbb R}^{m}$  has
a  Euclidean norm
\[\|{\mathbf x}\|=\left(\sum_{i=1}^{m}x_{j}^{2}\right)^{1/2}\]
(taking the positive square root) with the
following properties.
\begin{lemma}\label{Euclidean norm}
If $\|\ \|$ is the Euclidean norm on 
${\mathbb R}^{m}$  then

(i) $\|{\mathbf x}\|\geq 0$ for all ${\mathbf x}\in{\mathbb R}^{m}$,

(ii) If  $\|{\mathbf x}\|= 0$ then ${\mathbf x}={\mathbf 0}$,

(iii) If $\lambda\in{\mathbb R}$ and ${\mathbf x}\in{\mathbb R}^{m}$
then $\|\lambda {\mathbf x}\|=|\lambda|\|{\mathbf x}\|$.

(iv) (The triangle inequality)
If  ${\mathbf x},{\mathbf y}\in{\mathbb R}^{n}$
then $\|{\mathbf x}+{\mathbf y}\|
\leq\|{\mathbf x}\|+\|{\mathbf y}\|$.
\end{lemma}
The only non-trivial part of this lemma was the triangle inequality
which you proved using the Cauchy-Schwarz inequality.

In the course C5/6 you studied the notion of the
limit for sequences in ${\mathbb R}^{m}$.
\begin{definition}\label{many convergence definition}
If $\mathbf{a}_{n}\in{\mathbb R}^{m}$ for each $n\geq 1$
and $\mathbf{a}\in{\mathbb R}^{m}$ then we say that 
$\mathbf{a}_{n}\rightarrow \mathbf{a}$
if given $\epsilon>0$ we can find an $n_{0}(\epsilon)$
such that
\[\|\mathbf{a}_{n}-\mathbf{a}\|<\epsilon
\ \text{for all $n\geq n_{0}(\epsilon)$}.\]
\end{definition}
Notice that this shows that Definition~\ref{one convergence definition}
was about the \emph{distance} between two points and not
the absolute value of the difference of two numbers.

You proved the following results on sequences
in ${\mathbb R}^{m}$  in exactly the same way as
you proved the corresponding results in ${\mathbb R}$.
\begin{lemma}\label{many sequences}
(i) The limit is unique. That is, if 
$\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\mathbf{a}_{n}\rightarrow \mathbf{b}$
as $n\rightarrow\infty$
then $\mathbf{a}=\mathbf{b}$.

(ii) If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$\mathbf{a}_{n(j)}\rightarrow \mathbf{a}$ 
as $j\rightarrow\infty$.

(iii) If $\mathbf{a}_{n}=\mathbf{c}$ for all $n$ 
then $\mathbf{a}_{n}\rightarrow \mathbf{c}$
as $n\rightarrow\infty$.

(iv) If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\mathbf{b}_{n}\rightarrow\mathbf{b}$
as $n\rightarrow\infty$ then
$\mathbf{a}_{n}+\mathbf{b}_{n}
\rightarrow \mathbf{a}+\mathbf{b}$.

(v) Suppose $\mathbf{a}_{n}\in {\mathbb R}^{m}$,
$\mathbf{a}\in {\mathbb R}^{m}$,
$\lambda_{n}\in {\mathbb R}$
and $\lambda\in {\mathbb R}$. 
If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\lambda_{n}\rightarrow\lambda$ then
$\lambda_{n}\mathbf{a}_{n}\rightarrow 
\lambda\mathbf{a}$.


\end{lemma}
Once again I leave it to the reader to check that
she can indeed prove these results and to
consult her supervisor if she can not. She should also
make sure that she understands why certain results
in Lemma~\ref{one sequences} which dealt with
an ordered field do not appear in Lemma~\ref{many sequences}
which deals with a vector space.

Lemma~\ref{many sequences} is, of course, merely algebra
and applies to ${\mathbb Q}^{m}$ as much as to 
${\mathbb R}^{m}$. In order to do analysis we need
a more powerful tool  and, in keeping with the spirit
of this course, we extend the Bolzano--Weierstrass
theorem to ${\mathbb R}^{m}$. The extension is easy
since the real work has already been done in the one dimensional
case.
\begin{theorem}[Bolzano--Weierstrass]\label{many Bolzano}
If $\mathbf{x}_{n}\in{\mathbb R}^{m}$ and there exists a $K$
such that $\|\mathbf{x}_{n}\|\leq K$ for all $n$ then we can find
$n(1)<n(2)<\ldots$ and $\mathbf{x}\in{\mathbb R}$ such that
$\mathbf{x}_{n(j)}\rightarrow \mathbf{x}$ as $j\rightarrow\infty$.
\end{theorem}
Once again `any bounded sequence has a convergent subsequence'.

The proof of Theorem~\ref{many Bolzano} involves
extending a one dimensional result to several dimensions.
This is more or less inevitable because we stated the
fundamental axiom of analysis in a one dimensional form.
However the Bolzano--Weierstrass theorem itself 
contains no reference as to whether we are working in ${\mathbb R}$
or ${\mathbb R}^{m}$.  Thus \emph{exactly the same proofs}
as we used in the one dimensional case
give us a general principle of convergence in ${\mathbb R}^{m}$.
(Compare Definition~\ref{one Cauchy sequence} and
Theorem~\ref{Cauchy} with what follows.)
\begin{definition} We say that a sequence of points
$\mathbf{x}_{n}$ in ${\mathbb R}^{m}$ is a \emph{Cauchy sequence} 
if given any
$\epsilon>0$ we can find $n_{0}(\epsilon)$ such that
$\|\mathbf{x}_{p}-\mathbf{x}_{q}\|<\epsilon$ 
for all $p,q\geq n_{0}(\epsilon)$.
\end{definition}
\begin{theorem}[General Principle of Convergence]\label{many Cauchy}
A sequence in ${\mathbb R}^{m}$ converges if and only if
it is a Cauchy sequence.
\end{theorem}

Here is an example of the use of the general principle of convergence.
(Following the conventions of course C5/6 we say that 
$\sum_{j=1}^{\infty}\mathbf{a}_{j}$ converges if
the sequence of partial sums
$\sum_{j=1}^{N}\mathbf{a}_{j}$ tends to a limit  ${\mathbf s}$
say. We write $\sum_{j=1}^{\infty}\mathbf{a}_{j}={\mathbf s}$.)
\begin{theorem} Let $\mathbf{a}_{n}\in{\mathbb R}^{m}$ for each $n$.
If $\sum_{j=1}^{\infty}\|\mathbf{a}_{j}\|$ converges then so does
$\sum_{j=1}^{\infty}\mathbf{a}_{j}$.
\end{theorem}
This result substantially generalises the result in course C5/6
which says that an absolutely convergent series converges.
\section{Open and closed sets}\label{open and closed}
When we work in ${\mathbb R}$
the intervals are, in some sense, the `natural' sets
to consider. One of the problems that we face when we
try to do analysis in many dimensions is that the
types of sets with which we have to deal are much more
diverse. It turns out that the so called closed
and open sets are both sufficiently diverse
and sufficiently well behaved to be useful.
This short section is devoted to deriving some of their
simpler properties. Novices frequently find the
topic hard but eventually the reader will appreciate
that this section is a rather trivial interlude
in a deeper discussion.

The definition of a closed set is a natural one.
\begin{definition} A set $F\subseteq{\mathbb R}^{m}$
is closed if whenever $\mathbf{x}_{n}\in F$ for each
$n$ and $\mathbf{x}_{n}\rightarrow \mathbf{x}$
as $n\rightarrow\infty$ then $\mathbf{x}\in F$.
\end{definition}
Thus a set is closed in the sense of this course if
it is `closed under the operation of taking limits'.
An indication of why this is good definition is given
by the following version of the Bolzano--Weierstrass
theorem.
\begin{theorem} If $K$ is closed bounded set in
${\mathbb R}^{m}$ then every sequence in $K$ has
a subsequence converging to a point of $K$.
\end{theorem}
When working in ${\mathbb R}^{m}$ the words
`closed and bounded' should always elicit
the response `Bolzano--Weierstrass'. We shall
see important examples of this slogan in action
in the next section (Theorem~\ref{compact image}
and Theorem~\ref{uniform continuity}).

We turn now to the definition of an open set.
\begin{definition} A set $U\subseteq{\mathbb R}^{m}$
is open if whenever $\mathbf{x}\in U$ there exists
an $\epsilon({\mathbf x})>0$ such that whenever
$\|\mathbf{x}-\mathbf{y}\|<\epsilon({\mathbf x})$ we
have $\mathbf{y}\in U$.
\end{definition}
Thus every point of an open set lies `well inside the set'.
\begin{example} Consider sets in ${\mathbb R}$.
The interval $[a,b]=\{x\in{\mathbb R}:a\leq x \leq b\}$ is closed, 
the interval $(a,b)=\{x\in{\mathbb R}:a< x <b\}$ is open,
the interval $[a,b)=\{x\in{\mathbb R}:a\leq x <b\}$ is 
neither open nor closed, ${\mathbb R}$ is both open and
closed.
\end{example}
\begin{example} Consider sets in ${\mathbb R}^{m}$. Let
${\mathbf x}\in{\mathbb R}^{m}$ and $r>0$. 

(i) The set $B({\mathbf x},r)
=\{\mathbf{y}\in{\mathbb R}^{m}:
\|\mathbf{x}-\mathbf{y}\|<r\}$ is open.

(ii) The set $\bar{B}({\mathbf x},r)
=\{\mathbf{y}\in{\mathbb R}^{m}:
\|\mathbf{x}-\mathbf{y}\|\leq r\}$ is closed.
\end{example}
We call $B({\mathbf x},r)$ the open ball of radius $r$
and centre ${\mathbf x}$.
We call $\bar{B}({\mathbf x},r)$ the closed ball of radius $r$
and centre ${\mathbf x}$. Observe that the closed and and open
balls of ${\mathbb R}$ are precisely the closed
and open intervals.

The following restatement of the
definition helps us picture an open set.
\begin{lemma} A subset $A$ of ${\mathbb R}^{m}$ is open
if and only if each point of $A$ is the centre of
an open ball lying entirely within $A$.
\end{lemma}
Thus every point of an open set is surrounded by a ball
consisting only of points of the set.

The topics of this section are often treated
using the idea of \emph{neighbourhoods}. We shall
not use neighbourhoods very much but 
they come in useful from time to time.
\begin{definition} The set $N$ is a neighbourhood
of the point ${\mathbf x}$ if we can find an $r({\mathbf x})>0$
such that $B({\mathbf x},r({\mathbf x}))\subseteq N$.
\end{definition}
Thus a set is open if and only if it is neighbourhood
of every point that it contains.

Returning to the main theme we note the following
remarkable fact.
\begin{lemma}\label{complement open}
A subset $A$ of ${\mathbb R}^{m}$ is open
if and only if its complement ${\mathbb R}^{m}\setminus A$
is closed.
\end{lemma}
We observe the following basic results on open and closed sets.

\begin{lemma}\label{open familly} 
Consider the collection $\tau$ of open sets
in ${\mathbb R}^{m}$.

(i) $\emptyset\in\tau$, ${\mathbb R}^{m}\in \tau$.

(ii)  If $U_{\alpha}\in\tau$ for all $\alpha\in A$ then
$\bigcup_{\alpha\in A} U_{\alpha}\in\tau$.

(iii) If $U_{1},U_{2},\dots,U_{n}\in\tau$ then
$\bigcap_{j=1}^{n}U_{j}\in\tau$.
\end{lemma}
\begin{lemma}\label{closed familly}
Consider the collection $\mathcal{F}$ of closed sets
in ${\mathbb R}^{m}$.

(i) $\emptyset\in\mathcal{F}$, ${\mathbb R}^{m}\in \mathcal{F}$.

(ii)  If $F_{\alpha}\in\mathcal{F}$ for all $\alpha\in A$ then
$\bigcap_{\alpha\in A} F_{\alpha}\in\mathcal{F}$.

(iii) If $F_{1},F_{2},\dots,F_{n}\in\mathcal{F}$ then
$\bigcup_{j=1}^{n}F_{j}\in\mathcal{F}$.
\end{lemma}
In this context you should note the following examples.
\begin{example} (i) $\bigcap_{j=1}^{\infty}(-2-j^{-1},2+j^{-1})
=[-2,2]$, $\bigcap_{j=1}^{\infty}(-2+j^{-1},2-j^{-1})
=(-1,1)$, $\bigcap_{j=1}^{\infty}(-2+j^{-1},2+j^{-1})=(-1,2]$.

(ii) $\bigcup_{j=1}^{\infty}[-1-j^{-1},1+j^{-1}]
=[-2,2]$, $\bigcup_{j=1}^{\infty}[-1+j^{-1},1-j^{-1}]
=(-1,1)$, $\bigcup_{j=1}^{\infty}[-1+j^{-1},1+j^{-1}]=(-1,2]$.
\end{example}
Thus the restriction to finite collections
in part~(iii) of Lemmas~\ref{open familly}
and~\ref{closed familly} can not be removed.

So far in this course we have only dealt with
subsets and sequences. But analysis deals
with functions, or rather with `well behaved
functions'. The simplest such class was defined
in course C5/6.
\begin{definition}\label{definition continuity}
Let $E\subseteq{\mathbb R}^{m}$
we say that a function $f:E\rightarrow{\mathbb R}^{p}$
is continuous at some point ${\mathbf x}\in E$
if given $\epsilon>0$ we can find a 
$\delta(\epsilon,{\mathbf x})>0$ such that
whenever ${\mathbf y}\in E$ and
$\|{\mathbf x}-{\mathbf y}\|<\delta(\epsilon,{\mathbf x})$
we have 
\[\|f({\mathbf x})-f({\mathbf y})\|<\epsilon.\]
If $f$ is continuous at every point ${\mathbf x}\in E$
we say that $f$ is a continuous function on $E$.
\end{definition}
In other words $f$ is continuous at ${\mathbf x}$ if
$f({\mathbf y})$ can be made as close as we wish
to $f({\mathbf x})$ simply by taking ${\mathbf y}$ sufficiently
close to ${\mathbf x}$. In other words
$f$ is locally approximately constant. In less precise
terms `small changes produce small effects'.

\begin{exercise}\label{art of continuity} 
After looking at parts~(iii) to~(v)
of Lemma~\ref{many sequences} state the corresponding
results for continuous functions. (Thus part~(v) 
corresponds to the statement that if 
$\lambda:E\rightarrow{\mathbb R}$ and 
$f:E\rightarrow{\mathbb R}^{p}$ are continuous
at ${\mathbf x}\in E$ then so is $\lambda f$.)
Prove your statements directly from 
Definition~\ref{definition continuity}.
Consult your supervisor if you have any problems.
\end{exercise}

In this course we shall mainly use the following
simple consequence of the definition.
\begin{lemma} Let $E\subseteq{\mathbb R}^{m}$
and suppose that the function $f:E\rightarrow{\mathbb R}^{p}$
is continuous at ${\mathbf x}\in E$. Then if 
${\mathbf x}_{n}\in E$ and ${\mathbf x}_{n}\rightarrow{\mathbf x}$
it follows that $f({\mathbf x}_{n})\rightarrow f({\mathbf x})$.
\end{lemma}

However, more advanced work uses generalisations of the
following lemma. (Remember that if $f:X\rightarrow Y$ is a function
and $A\subseteq Y$
we write
\[f^{-1}(A)=\{x\in X\, :\, f(x)\in A\}\]
and this notation does not require $f$ to be invertible.)
\begin{lemma}\label{continuous open}
The function 
$f:{\mathbb R}^{m}\rightarrow {\mathbb R}^{p}$
is continuous if and only if $f^{-1}(O)$ is open
whenever $O$ is open.
\end{lemma}
As a simple example of how Lemma~\ref{continuous open}
can be used contrast the `$\epsilon$, $\delta$ proof'
of the next lemma
using Definition~\ref{definition continuity} with
a proof using  Lemma~\ref{continuous open}.
\begin{lemma}\label{composition} 
If $f:{\mathbb R}^{m}\rightarrow {\mathbb R}^{p}$
and $g:{\mathbb R}^{p}\rightarrow {\mathbb R}^{g}$
are continuous the so is their composition 
$g\circ f$.
\end{lemma}
(Recall that we write $g\circ f({\mathbf x})=g(f({\mathbf x}))$.)

Although we shall not make much use of it, the following
is an important consequence of the Bolzano--Weierstrass theorem.
\begin{theorem}\label{nested}
Suppose that $F_{1}$, $F_{2}$, \dots are 
non-empty bounded
closed sets in ${\mathbb R}^{m}$ such that 
$F_{1}\supseteq F_{2}\supseteq\dots$. 
Then $\bigcap_{j=1}^{\infty}F_{j}\neq\emptyset.$
\end{theorem}
That is, the intersection of bounded, closed, nested non-empty
sets is itself non-empty.
The example $F_{j}=[j,\infty)$ shows that `bounded'
can not be dropped from the hypothesis. 
The example $F_{j}=(0,j^{-1})$ shows that `closed'
can not be dropped from the hypothesis.

Finally, it is worth emphasising that openess is not an
intrinsic property of a set but depends on the space in which
the set finds itself.
\begin{example}\label{now you}  The set 
$\{(x,0)\in{\mathbb R}^{2}:0<x<1\}$ is not open
in ${\mathbb R}^{2}$.
\end{example}
(If you have a sufficiently logical mind Example~\ref{now you}
is otiose. Few of us have that kind of mind.)
\section{The central theorems of analysis} This section contains the core
of a first course in analysis. For completeness I shall include
some theorems already proved in course C5/6.
The first such theorem is the intermediate value theorem
proved in C5/6 by successive bisection. That proof is
entirely satisfactory but, as an exercise, we shall prove
the result using the method of Bolzano--Weierstrass.
\begin{theorem}[Intermediate value theorem]  If 
$f:[a,b]\rightarrow{\mathbb R}$ is continuous and
$f(a)\leq 0\leq f(b)$ then there exists a $c\in[a,b]$
such that $f(c)=0$.
\end{theorem}
You should compare this result with part~(i)
of Example~\ref{Rational}. Note also that
the result depends on $f$ taking values in ${\mathbb R}$
rather than ${\mathbb R}^{2}$ or ${\mathbb C}$.
\begin{example} If $f:[0,1]\rightarrow {\mathbb C}$ 
is given by $f(t)=\exp \pi it$ then $f$
is continuous,  $f(0)=1$, $f(1)=-1$ but $f(t)\neq 0$ for
all $t$.
\end{example}

Our next result looks a little abstract at first.
\begin{theorem}\label{compact image}
Let $K$ be a closed bounded subset of
${\mathbb  R}^{m}$ and $f:K\rightarrow {\mathbb  R}^{p}$
a continuous function. Then  $f(K)$ is closed and bounded.
\end{theorem}
Thus the continuous image of a closed bounded set is
closed and bounded. (The example $m=2$, $p=1$,
\[K=\{(x,y)\, : \, y\geq x^{-1}\ ,x>0\}\]
$f(x,y)=x$ shows that the continuous image of a closed
set need not be closed.
 
The example The example $m=1$, $p=1$, $K$ the open
interval $(0,1)$, $f(x)=x^{-1}$,
shows that the continuous image of a bounded
set need not be bounded.)

We derive a much more concrete corollary.
\begin{theorem}\label{maximum}
Let $K$ be a non-empty closed bounded subset of
${\mathbb  R}^{m}$ and $f:K\rightarrow {\mathbb  R}$
a continuous function. Then we can find ${\mathbf k}_{1}$
and ${\mathbf k}_{2}$ in $K$ such that
\[f({\mathbf k}_{1})\leq f({\mathbf k})\leq f({\mathbf k}_{2})\]
for all ${\mathbf k}\in K$.
\end{theorem}
In other words a real valued continuous function on a closed
bounded set is bounded and attains its bounds. Less usefully
we may say that, in this case $f$ actually has a maximum
and a minimum. Notice that there is no analogous result
for vector valued functions.  Much of economics consists
in an attempt to disguise this fact (there is unlikely to
be a state of the economy in which \emph{everybody}  is best
off\footnote{Of course economists know this but few of their
public pronouncements seem to mention it.}).

Theorem~\ref{maximum} has an even more concrete consequence.
\begin{theorem}\label{interval maximum}
Let $f:[a,b]\rightarrow {\mathbb  R}$
be a continuous function. Then we can find $k_{1},k_{2}\in [a,b]$ 
such that
\[f(k_{1})\leq f(k)\leq f(k_{2})\]
for all $k\in [a,b]$
\end{theorem}
In course C5/6 you used this result to prove  Rolle's
theorem.
\begin{theorem}[Rolle's theorem] If $f:[a,b]\rightarrow {\mathbb  R}$
is a continuous function with $f$ differentiable on $(a,b)$
and $f(a)=f(b)$ then we can find a $c\in (a,b)$ such that
$f'(c)=0$.
\end{theorem}
By `tilting Rolle's theorem' we obtain the mean value
(in one dimension).
\begin{theorem}[The one dimensional
mean value theorem]~\label{mean}
If $f:[a,b]\rightarrow {\mathbb  R}$
is a continuous function with $f$ differentiable on $(a,b)$
then we can find a $c\in (a,b)$ such that
\[f(b)-f(a)=(b-a)f'(c).\]
\end{theorem}
Historically, mathematicians were slow to grasp the
importance of the mean value theorem and the subtlety 
of its proof. Fallacious proofs still appeared in textbooks
(British textbooks, admittedly) in the early years of the 20th
century. (The proofs were fallacious in the strictest sense.
They `proved' the impossibility of the result given
in part~(ii) of Example~\ref{Rational}.)

As you saw in course C5/6, the mean value theorem  
Theorem~\ref{mean} has the following key corollaries.
\begin{theorem}\label{one growth} Suppose that
$f:(\alpha,\beta)\rightarrow {\mathbb  R}$
is a differentiable function.

(i) If $f'(x)\geq 0$ for all $x\in (\alpha,\beta)$
then $f$ is an increasing function 
on $(\alpha,\beta)$ (that is $f(x)\leq f(y)$ whenever
$\alpha<x<y<\beta$).

(ii) If $f'(x)> 0$ for all $x\in (\alpha,\beta)$
then $f$ is a strictly increasing function 
on $(\alpha,\beta)$ (that is $f(x)<f(y)$ whenever
$\alpha<x<y<\beta$).

(iii) If $f'(x)=0$ for all $x\in (\alpha,\beta)$
then $f$ is constant.

(iv) If $|f'(x)|\leq K$ for all $x\in (\alpha,\beta)$
then $|f(u)-f(v)|\leq K|u-v|$ for all $u,v\in (\alpha,\beta)$
\end{theorem}

Part~(iii) of of Theorem~\ref{one growth} is the result
that tells us that the solutions (if any)
of $f'(x)=g(x)$ on $(\alpha,\beta)$ differ only by
the addition of a constant (the so called
`constant of integration'). 

Part~(iv) of of Theorem~\ref{one growth} is an excellent
example of the way that the theorems of this section
(and many other theorems of analysis) convert \emph{local}
information into \emph{global} information. We know
that $|f'(x)|\leq K$ so that \emph{locally} the rate
of change of $f$ is no greater than $K$. We deduce that
$|f(u)-f(v)|\leq K|x-y|$ so that \emph{globally} the rate
of change of $f$ is no greater than $K$.

The final theorem of this section (Theorem~\ref{uniform continuity}
on uniform continuity) is another excellent example of the
conversion of local information to global. We need
a couple of definitions and examples. Recall first
from Definition~\ref{definition continuity} what
it means for a function to be continuous on a set
\begin{definition}\label{pointwise continuity}
Let $E\subseteq{\mathbb R}^{m}$
we say that a function $f:E\rightarrow{\mathbb R}^{p}$
is \emph{continuous} on $E$ if given any point ${\mathbf x}\in E$
and any $\epsilon>0$ we can find a 
$\delta(\epsilon,{\mathbf x})>0$ such that
whenever ${\mathbf y}\in E$ and
$\|{\mathbf x}-{\mathbf y}\|<\delta(\epsilon,{\mathbf x})$
we have 
\[\|f({\mathbf x})-f({\mathbf y})\|<\epsilon.\]
\end{definition}
Now compare that definition with our definition of
\emph{uniform} continuity.
\begin{definition}\label{definition uniform continuity}
Let $E\subseteq{\mathbb R}^{m}$
we say that a function $f:E\rightarrow{\mathbb R}^{p}$
is \emph{uniformly continuous} on $E$ 
if given any 
$\epsilon>0$ we can find a 
$\delta(\epsilon)>0$ such that
whenever ${\mathbf x},{\mathbf y}\in E$ and
$\|{\mathbf x}-{\mathbf y}\|<\delta(\epsilon)$
we have 
\[\|f({\mathbf x})-f({\mathbf y})\|<\epsilon.\]
\end{definition}

\begin{example} The following three functions are continuous
but not uniformly continuous.

(i) $f_{1}:{\mathbb R}\rightarrow{\mathbb R}$ given by
$f_{1}(x)=x^{2}$.

(ii) $f_{2}:(0,1)\rightarrow{\mathbb R}$ given by
$f_{2}(x)=x^{-1}$.

(iii) $f_{3}:(0,1)\rightarrow{\mathbb R}$ given by
$f_{2}(x)=\sin(x^{-1})$.
\end{example}
\begin{theorem}\label{uniform continuity}
Let $K$ be a closed bounded subset of
${\mathbb  R}^{m}$. If $f:K\rightarrow {\mathbb  R}^{p}$
is continuous on $K$ then $f$ is uniformly continuous
on K.
\end{theorem}

The results of this section are the foundations of all
analysis. It took 200 years to found analysis on the
same axiomatic basis as Greek geometry and the successful
completion of the project was a triumph of the human intellect.
If you can trace a compete path from the fundamental axiom
to the one dimensional mean value theorem you will
have mastered the essential point of this course.
\section{Differentiation}  A function is continuous if
it is locally approximately constant. A function is
differentiable if it is locally approximately linear.
More precisely, a function is continuous at a point ${\mathbf x}$
if it is locally approximately constant  with an error
which decreases to zero as we approach ${\mathbf x}$.
A function is
differentiable at a point ${\mathbf x}$
if it is locally approximately linear with an error
which decreases to zero \emph{faster than linearly}
as we approach  ${\mathbf x}$.
\begin{definition}\label{differentiation}
Suppose that $E$ is a subset of
of ${\mathbb R}^{m}$ and ${\mathbf x}$ a point such that
there exists a $\delta>0$ with 
$B({\mathbf x},\delta)\subseteq E$.
We say that $f:E\rightarrow  {\mathbb R}^{p}$,
is differentiable at ${\mathbf x}$ if we can find a linear
map $\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
such that, when $\|{\mathbf h}\|<\delta$
\begin{equation*}
f({\mathbf x}+{\mathbf h})=f({\mathbf x})+\alpha{\mathbf h}
+{\boldsymbol\epsilon}({\mathbf x},{\mathbf h})\|{\mathbf h}\|
\tag*{$\bigstar$}
\end{equation*}
where 
$\|{\boldsymbol\epsilon}({\mathbf x},{\mathbf h})\|\rightarrow 0$
as $\|{\mathbf h\|}\rightarrow 0$.
We write $\alpha=Df({\mathbf x})$
or $\alpha=f'({\mathbf x})$.

If $E$ is open and $f$ is differentiable at each point of
$E$ we say that $f$ is differentiable on $E$.
\end{definition}
Needless to say, the centre of the definition is
the formula $\bigstar$ and the reader should concentrate
on understanding the r\^{o}le of each term in that
formula. The rest of the definition is just supporting
waffle. Formula $\bigstar$ is sometimes rewritten
\[\frac{f({\mathbf x}+{\mathbf h})-f({\mathbf x})
-\alpha{\mathbf h}}{\|{\mathbf h}\|}\rightarrow 0\]
as $\|{\mathbf h}\|\rightarrow 0$.

You have already studied differentiation in
course C5/6 and you regularly use partial differentiation
in applied courses and elsewhere. I shall therefore
concentrate on recalling some of the key points.
\begin{lemma} Let $f$ be as in Definition~\ref{differentiation}.
If we use standard coordinates then
if $f$ is differentiable at ${\mathbf x}$ its
partial derivatives 
${\displaystyle  f_{i,j}({\mathbf x})=
\frac{\partial f_{i}}{\partial x_{j}}}$ exist and the
matrix of the derivative $Df({\mathbf x})$ is the 
Jacobian matrix $(f_{i,j}({\mathbf x}))$ of partial 
derivatives.
\end{lemma}
It is customary to point out that the existence
of the partial derivatives does not imply the
differentiability of the function (see Example~\ref{Jacob not}
below) but the main objections to over-reliance on
partial derivatives are that this makes formulae
cumbersome and stifles geometric intuition.
Let your motto be
{\bf `coordinates and matrices for calculation,
vectors and linear maps for understanding'.}

One key property of differentiation is the chain
rule. You have already seen this discussed and proved
in course C5/6.  Our discussion will run along the same lines
but is slightly neater.
\begin{lemma} If
$\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$ 
is a linear map we can find a constant $K$
such that $|\alpha{\mathbf x}|\leq K\|{\mathbf x}\|$
for all ${\mathbf x}\in {\mathbb R}^{m}$.
\end{lemma}
We can now make the following definition.
\begin{definition}\label{Definition operator norm}
If
$\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
is a linear map then
\[\|\alpha\|=\sup_{\|{\mathbf x}\|\leq 1}\|\alpha{\mathbf x}\|.\]
\end{definition}
The following easy exercise provides an equivalent definition.
\begin{exercise} Show that if
$\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
is a linear map then 
\[\|\alpha\|=\sup_{\|{\mathbf x}\|= 1}\|\alpha{\mathbf x}\|.\]
\end{exercise}
This `operator norm' has many pleasant properties.
\begin{lemma}\label{Operator norm}
Let  $\alpha,\beta:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
be linear maps.

(i) If ${\mathbf x}\in{\mathbb R}^{m}$ then
$\|\alpha{\mathbf x}\|\leq\|\alpha\|\,\|{\mathbf x}\|$.

(ii) $\|\alpha\|\geq 0$,

(iii) If  $\|\alpha\|= 0$ then $\alpha=0$,

(iv) If $\lambda\in{\mathbb R}$
then $\|\lambda \alpha\|=|\lambda|\|\alpha\|$.

(v) (The triangle inequality)
$\|\alpha+\beta\|\leq\|\alpha\|+\|\beta\|$.

(vi) If $\gamma:{\mathbb R}^{p}\rightarrow{\mathbb R}^{q}$
then $\|\gamma\alpha\|\leq\|\gamma\|\,\|\alpha\|$.
\end{lemma}

\begin{lemma}[The chain rule] Let $U$ be a neighbourhood
of ${\mathbf x}$ in ${\mathbb R}^{m}$, and
$V$ a neighbourhood of ${\mathbf x}$ in ${\mathbb R}^{p}$.
Suppose that $f:U\rightarrow V$ is differentiable
at ${\mathbf x}$ with derivative $\alpha$, that
$f:V\rightarrow {\mathbb R}^{q}$ is differentiable
at ${\mathbf y}$ with derivative $\beta$ and that
$f({\mathbf x})={\mathbf y}$. Then $g\circ f$ is
differentiable at ${\mathbf x}$ with derivative
$\beta\alpha$.
\end{lemma}
In more condensed notation
\begin{equation*}
D(g\circ f)({\mathbf x})=Dg(f({\mathbf x}))Df({\mathbf x}).
\tag*{$\bigstar\bigstar$}
\end{equation*}
It is important to see that formula $\bigstar\bigstar$
is exactly what we should expect and that its proof
is simple book-keeping.

\noindent{\bf Remark} One of the most troublesome
culture clashes between pure mathematics and applied
is that to an applied mathematician variables
like $x$ and $t$ have meanings such
as position and
time whereas to a pure mathematician 
all variables are `dummy variables' or `place-holders'
to be interchanged at will. To a pure mathematician
$v$ is an arbitrary function defined by its
effect on a variable so that $v(t)=At^{3}$ means
precisely the same thing as $v(x)=Ax^{3}$
whereas to an applied mathematician who thinks of
$v$ as a velocity the statements $v(t)=At^{3}$ and
$v(x)=Ax^{3}$ mean very different (indeed incompatible)
things. The applied mathematician writes
\[\frac{dv}{dt}=\frac{dv}{dx}\frac{dx}{dt}\]
but the nearest the pure mathematician can get
to this statement is
\[\frac{d\ }{dt}v(x(t))=v'(x(t))x'(t).\]
The same problems occur in still more confusing form
when we deal with partial derivatives. 
In particular the formula
\[\frac{\partial f}{\partial x_{i}}
=\sum_{k=1}^{n}\frac{\partial f}{\partial y_{k}}
\frac{\partial y_{k}}{\partial x_{i}}\]
makes no sense without using further
\emph{unspoken} conventions.
The only
remedy that I can suggest is `think like a pure mathematician
when doing pure mathematics and like an applied
mathematician when doing applied mathematics'.

The following simple result has a simple 
direct proof but
it is instructive to prove it by the chain rule.
\begin{lemma} Let $U$ be a neighbourhood
of ${\mathbf x}$ in ${\mathbb R}^{m}$.
Suppose that $f,g:U\rightarrow{\mathbb R}^{p}$
are differentiable at ${\mathbf x}$. Then
$f+g$ is differentiable at ${\mathbf x}$
and $D(f+g)({\mathbf x})=Df({\mathbf x})+Dg({\mathbf x})$. 
\end{lemma}

So far our study of differentiation in higher dimensions
has remained on the level of mere algebra. (The definition
of the operator norm used the supremum and so lay
deeper but we could have avoided the use of the
operator norm as was done in course C5/6.) The
next result is a true theorem of analysis.
\begin{theorem}[The mean value inequality] Suppose that
$U$ is an open set in ${\mathbb R}^{m}$ and that
$f:U\rightarrow{\mathbb R}^{p}$ is differentiable.
Consider the straight line segment 
\[L=\{(1-t){\mathbf a}+t{\mathbf b}:0\leq t\leq 1\}\]
joining ${\mathbf a}$ and ${\mathbf b}$. If $L\subseteq U$
(i.e. $L$ lies entirely within $U$) and
$\|Df({\mathbf x})\|\leq K$ for all 
${\mathbf x}\in L$ then
\[\|f({\mathbf a})-f({\mathbf b})\|
\leq K\|{\mathbf a}-{\mathbf b}\|.\]
\end{theorem}
The reader may be disappointed that a theorem involving
an equality in one dimension should be replaced by
one involving an inequality in many dimensions.
However, inspection of the use of the mean value
theorem in one dimension shows that its essential content
is the statement that `local growth bounds global
growth' and this statement is precisely the
mean value inequality. 

In any case the following example shows that we can
not hope for an exact analogue of the one dimensional
theorem.
\begin{example} Let $f:{\mathbb R}\rightarrow{\mathbb R}^{2}$
be given by $f(t)=(\cos t,\sin t)^{T}$. Then
$f(0)=f(2\pi)$ but $Df(t)\neq 0$ for all $t$.
\end{example}
It is also clear (though we shall not prove it,
and, indeed can not state it without using concepts
which we have not formally defined) that the
correct generalisation when $L$ is not a straight
line will run as follows.
`If  $L$ is a well behaved path lying entirely
within $U$  and
$\|Df({\mathbf x})\|\leq K$ for all
${\mathbf x}\in L$ then
$\|f({\mathbf a})-f({\mathbf b})\|
\leq K\times\operatorname{length}L$'.
\begin{example} Let
\[U=\{{\mathbf x}\in{\mathbb R}^{2}:\|{\mathbf x}\|>1\}
\setminus\{(x,0)^{T}:x\leq 0\}\}\]
If we take $\theta({\mathbf x})$ to be the unique solution 
of
\[\cos(\theta({\mathbf x}))=x,\ \sin(\theta({\mathbf x}))=y,
\ -\pi<\theta(\mathbf x)<\pi\]
for ${\mathbf x}=(x,y)^{T}\in U$ then
$\theta:U\rightarrow{\mathbb R}$ is everywhere differentiable
with $\|D\theta({\mathbf x})\|<1$. However
if $\mathbf{a}=(-1,10^{-1})^{T}$, $\mathbf{b}=(-1,-10^{-1})^{T}$
then 
\[|\theta(\mathbf{a})-\theta(\mathbf{b})|>                  
\|\mathbf{a}-\mathbf{b}\|.\]
\end{example}
\section{Local Taylor theorems} In this section we exploit
the mean value inequality to obtain information  about
the behaviour of reasonably well behaved functions
$f:{\mathbb R}^{m}\rightarrow{\mathbb R}$ in the neighbourhood
of a point ${\mathbf x}$. (Since we can always translate 
we shall sometimes put ${\mathbf x}={\mathbf 0}$ without
comment.) It may be worth alerting the reader to the
fact that we shall be using the one-dimensional
mean value inequality (see
Theorem~\ref{one growth}~(iv)) in forms like the following.
`If $|f_{,1}(x,c)|\leq K$  for some fixed $c$ and
all $a\leq x\leq b$ then $|f(a,c)-f(b,c)|\leq K|b-a|$.'
In this section we shall use row rather than column 
vectors.

Here is our first example.
\begin{theorem}[Continuity of partial derivatives implies
differentiability]\label{continuous first} Suppose $\delta>0$,
${\mathbf x}=(x,y)\in{\mathbb R}^{2}$,
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{2}$
and that $f:E\rightarrow{\mathbb R}$. If  the partial
derivatives $f_{,1}$ and $f_{,2}$ exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ then writing
\[f((x+h,y+k))=f(x,y)+f_{,1}(x,y)h+f_{,2}(x,y)k+
\epsilon(h,k)(h^{2}+k^{2})^{1/2}\]
we have $\epsilon(h,k)\rightarrow 0$ as
$(h^{2}+k^{2})^{1/2}\rightarrow 0$.  (In other words
$f$ is differentiable at ${\mathbf x}$.)
\end{theorem}
Although this is not one of the great theorems
of all time (or indeed of this course) it does provide
a useful short cut for proving functions differentiable.
The following easy extension is left to the reader.
\begin{theorem}
(i) Suppose $\delta>0$,
${\mathbf x}\in{\mathbb R}^{m}$
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{m}$
and that $f:E\rightarrow{\mathbb R}$. If  the partial
derivatives $f_{,1}$, $f_{,2}$, \dots $f_{,m}$ 
exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ then 
$f$ is differentiable at ${\mathbf x}$.

(ii)  Suppose $\delta>0$,
${\mathbf x}\in{\mathbb R}^{m}$
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{m}$
and that $f:E\rightarrow{\mathbb R}^{p}$. If  the partial
derivatives $f_{i,j}$ 
exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ 
$[1\leq i\leq p,\ 1\leq j\leq m]$ then 
$f$ is differentiable at ${\mathbf x}$.
\end{theorem}

The next result, although along similar lines, is
much more interesting. We write
\[f_{,ij}({\mathbf x})=(f_{,j})_{,i}({\mathbf x}),\]
or, in more familiar notation
\[f_{,ij}=\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}.\]
\begin{theorem}[Second order Taylor series]\label{2 Taylor}
Suppose $\delta>0$,
${\mathbf x}=(x,y)\in{\mathbb R}^{2}$
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{2}$
and that $f:E\rightarrow{\mathbb R}$. If  the partial
derivatives $f_{,1}$, $f_{,2}$, $f_{,11}$, $f_{,12}$,
$f_{,22}$ exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ then writing
\begin{align*}
f((x+h,y+k))=&f(x,y)+f_{,1}(x,y)h+f_{,2}(x,y)k\\
&+\tfrac{1}{2}(f_{,11}(x,y)h^{2}+2f_{,12}(x,y)hk
+f_{,22}(x,y)k^{2})+\epsilon(h,k)(h^{2}+k^{2})
\end{align*}
we have $\epsilon(h,k)\rightarrow 0$ as
$(h^{2}+k^{2})^{1/2}\rightarrow 0$.
\end{theorem}
We have the following important corollary.
\begin{theorem}[Symmetry of second partial derivative]%
\label{Symmetry derivative}
Suppose $\delta>0$,
${\mathbf x}=(x,y)\in{\mathbb R}^{2}$
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{2}$
and that $f:E\rightarrow{\mathbb R}$. If  the partial
derivatives $f_{,1}$, $f_{,2}$, $f_{,11}$, $f_{,12}$, $f_{,21}$
$f_{,22}$ exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ then 
$f_{,12}({\mathbf x})=f_{,21}({\mathbf x})$.
\end{theorem}
Stripped of all the fine (and for practical purposes
irrelevant) detail this tells us that we can  interchange
the order of partial differentiation for well behaved functions.

It is possible to produce plausibility arguments
for the symmetry of second partial derivatives.
Here are a couple.

(1) If $f$ is a multinomial, i.e.
$f(x,y)=\sum_{p=0}^{P}\sum_{q=0}^{Q}a_{p,q}x^{p}y^{q}$,
then $f_{,12}=f_{,21}$. But smooth functions are
very close to being polynomial so we would expect
the result to be true in general.

(2) Although we can not interchange limits in general
it is plausible that if $f$ is well behaved then
\begin{align*}
f_{,12}(x,y)&=\lim_{h\rightarrow 0}\lim_{k\rightarrow 0}
h^{-1}k^{-1}(f(x+h,y+k)-f(x+h,y)-f(x,y+k)+f(x,y))\\
&=\lim_{k\rightarrow 0}\lim_{h\rightarrow 0}
h^{-1}k^{-1}(f(x+h,y+k)-f(x+h,y)-f(x,y+k)+f(x,y))\\
&=f_{,21}(x,y).
\end{align*}

\noindent However, these are merely plausible arguments.
They do not make clear the role of the continuity
of the second derivative (in Example~\ref{symmetry fail}
we shall see that the result
may fail for discontinuous second partial derivatives).
More fundamentally they are \emph{algebraic} arguments
and, as the use of the mean value theorem indicates,
the result is one of \emph{analysis}. 

Theorems~\ref{continuous first} and~\ref{Symmetry derivative}
are inextricably linked in many examiners' minds with the
following two counter-examples. The present lecturer
would rather you forgot all the course than that
you forgot how to prove the theorems and remembered
the counter-examples but will not let his personal
feelings get in the way of his duty.

Suppose $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
has $f({\mathbf 0})=0$. We write 
$g(r,\theta)=f(r\cos\theta,r\sin\theta)$ 
(i.e. use polar coordinates). 
If $f$ is once differentiable then, to first order in $r$,

(i) $g(r,\theta)$ grows like $r$,

(ii) the contours $f(x,y)=c$ are parallel lines.

\noindent If we seek counter-examples it is reasonable
to look for an $f$ such that (i) is true but (ii) is not.
We might therefore be led to consider
$g(r,\theta)=r\sin 2\theta$. (Finding counter-examples
is more like tinkering with a meccano kit than anything else.)
\begin{example}\label{Jacob not} If
\begin{align*}
f(x,y)&=\frac{xy}{(x^{2}+y^{2})^{1/2}}\qquad\text{for $(x,y)\neq (0,0)$},\\
f(0,0)&=0,
\end{align*}
then $f$ has first partial derivatives everywhere,
$f$ is differentiable except at $(0,0)$ where
it is not.
\end{example}

Emboldened by our success we could well guess immediately
a suitable function to look for in the context of 
Theorem~\ref{Symmetry derivative}. If not
let us consider $f$ and $g$ as before. Suppose that
\[f({\mathbf 0})=f_{,1}({\mathbf 0})=f_{,2}({\mathbf 0})=0.\]
If $f$ obeys the conclusions of Theorem~\ref{2 Taylor}
(i.e. has a second order Taylor series) then,
to second order in $r$, 

(i) $g(r,\theta)$ grows like $r^{2}$,

(ii) the contours $f(x,y)=c$ are conics.

\noindent Once again
we look for an $f$ such that (i) is true but (ii) is not.
We might, for example, consider
$g(r,\theta)=r^{2}\sin 4\theta$.
\begin{example}\label{symmetry fail} If
\begin{align*}
f(x,y)&=\frac{xy(x^{2}-y^{2})}{(x^{2}+y^{2})}
\qquad\text{for $(x,y)\neq (0,0)$},\\
f(0,0)&=0,
\end{align*}
then $f$ has first and second partial derivatives everywhere
but $f_{,12}(0,0)\neq f_{,21}(0,0)$.
\end{example}     

It is profoundly unfortunate that the last contact
many mathematicians have with this topic is a couple
of complicated counter-examples. Multi-dimensional
calculus leads towards differential geometry and
infinite dimensional calculus (functional analysis).
Both subjects depend on \emph{understanding}
objects which we know to be well behaved but
which our limited geometric intuition makes
it hard for us to comprehend. Counter-examples
such as the ones just produced are simply irrelevant


What happens if a function is smooth (has partial
derivatives of all orders)? The following theorem
completes the discussion of this section. It is not
on the syllabus and left to the reader as an exercise
(but a very instructive one).
\begin{theorem}[The local Taylor's theorem]\label{local}
Suppose $\delta>0$,
${\mathbf x}\in{\mathbb R}^{m}$
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{m}$
and that $f:E\rightarrow{\mathbb R}^{p}$. If the
all partial
derivatives $f_{i,j}$, $f_{i,jk}$, $f_{i,jkl}$, \dots
exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ then writing
\begin{align*}
f_{i}({\mathbf x}+{\mathbf h})=f_{i}({\mathbf x})&
+\sum_{j=1}^{m}f_{i,j}({\mathbf x})h_{j}+
\sum_{j=1}^{m}\sum_{k=1}^{m}f_{i,jk}({\mathbf x})h_{j}h_{k}\\
&+\dots+\text{sum up to $n$th powers}+
\epsilon_{i}(\mathbf{h})\|\mathbf{h}\|^{n}
\end{align*}
we have $\|{\boldsymbol\epsilon}(\mathbf{h})\|\rightarrow 0$ as
$\|\mathbf{h}\|\rightarrow 0$.
\end{theorem}

The wide awake reader may observe that I used (gasp,
horror!) coordinates in the statement of Theorem~\ref{local}.
However, the main formula
can be stated in a coordinate free way
as
\[
f({\mathbf x}+{\mathbf h})=f({\mathbf x})
+\alpha_{1}({\mathbf h})+\alpha_{2}({\mathbf h},{\mathbf h})+
+\dots+\alpha_{n}({\mathbf h},{\mathbf h},\dots,{\mathbf h})
+{\boldsymbol\epsilon}(\mathbf{h})\|\mathbf{h}\|^{n},
\]
where $\alpha_{k}:{\mathbb R}^{m}\times{\mathbb R}^{m}
\dots\times{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
is linear in each variable (i.e. a $k$-linear function).
For more details consult section~12
of chapter VIII of Dieudonn\'{e}'s
\emph{Foundations of Modern Analysis}~\cite{Dieudonn}
where the  higher derivatives are dealt with in a coordinate
free way. Like Hardy's book~\cite{Hardy}, Dieudonn\'{e}'s
is a masterpiece but in very different tradition.

Anyone who feels that the higher derivatives are best
studied using coordinates should reflect
that if $f:{\mathbb R}^{3}\rightarrow{\mathbb R}^{3}$
is well behaved then the `third derivative behaviour'
at of $f$ at a single point is apparently
given by the
$3\times3\times3\times3=81$ numbers
$f_{i,jkl}(\mathbf{x})$. By symmetry (see
Theorem~\ref{Symmetry derivative}) only $30$
of the numbers are distinct but these
$30$ numbers are independent  (consider
polynomials in three variables  for which
the total degree of each term is $3$).
How can we understand the information carried
by an array of $30$ real numbers?
\section{Riemann integration}\label{Riemann integration}
(Much of this section was sketched in the course C5/6
but this treatment will supply some important
extra detail.) Everybody knows what area is,
but then everybody knows what honey tastes like.
But does honey taste the same to you as it
does to me? Perhaps the question is unanswerable
but for many practical purposes it is sufficient
that we agree on what we call honey.
In the same way it is important that when
two mathematicians talk about area that
(1) they should agree on which sets $E$ actually have area,
and that
(2) when a set $E$ has area they should agree as
to what the area is.

At one time some mathematicians may well have
hoped that every bounded set in ${\mathbb R}^{2}$
could be given an area,
every bounded set in ${\mathbb R}^{3}$ could be given a volume,
and so on.
However Banach and Tarski showed that a ball
in ${\mathbb R}^{3}$ could be split into seven
parts which could be reassembled after rotation
and translation into a ball of smaller radius.
(The proof is non-trivial but there is a good discussion
in Wagon's book \emph{The Banach--Tarski Paradox}~\cite{Wagon}.)
If each of these seven parts had a volume then
repeated Banach Tarski decompositions would enable
us to get a quart into a pint pot.
Warned by this we shall not attempt to assign
area to every subset of ${\mathbb R}^{2}$ or
to integrate every function.

Turning specifically to the Riemann integral we
consider bounded functions $f:[a,b]\rightarrow{\mathbb R}$.
(Thus there exists a $K$ with $|f(x)|\leq K$ for all
$x\in[a,b]$). A dissection $\mathcal{D}$ of $[a,b]$
is a finite subset of $[a,b]$ containing the end
points $a$ and $b$. By convention we write
\[\mathcal{D}=\{x_{0},x_{1},\dots,x_{n}\}
\ \text{with}\ a=x_{0}\leq x_{1}\leq x_{2}\leq\dots\leq x_{n}=b.\]
We define the \emph{upper sum} and \emph{lower sum}
of $f$ associated with $\mathcal{D}$ by
\begin{align*}
S(f,\mathcal{D})=&\sum_{j=1}^{n}(x_{j}-x_{j-1})
\sup_{x\in [x_{j-1},x_{j}]}f(x),\\
s(f,\mathcal{D})=&\sum_{j=1}^{n}(x_{j}-x_{j-1})
\inf_{x\in [x_{j-1},x_{j}]}f(x)
\end{align*}

The next lemma is hardly more than an observation
but it is the key to the proper treatment of the integral.
\begin{lemma}[Key integration property] If
$f:[a,b]\rightarrow{\mathbb R}$
is bounded and
$\mathcal{D}_{1}$ and $\mathcal{D}_{2}$ are two dissections
then
\begin{equation*}
S(f,\mathcal{D}_{1})\geq S(f,\mathcal{D}_{1}\cup\mathcal{D}_{2})
\geq
s(f,\mathcal{D}_{1}\cup\mathcal{D}_{2})\geq s(f,\mathcal{D}_{2}).
\tag*{$\bigstar$}
\end{equation*}
\end{lemma}
The inequality $\bigstar$ tells us that, whatever dissection
you pick and whatever dissection I pick, your lower
sum can not exceed my upper sum. There is no way we
can put a quart in a pint pot and the Banach-Tarski
phenomenon is avoided.


Since $S(f,\mathcal{D})\geq -(b-a)K$
for all dissections $\mathcal{D}$
we can define the \emph{upper integral}
$I^{*}(f)=\inf_{\mathcal{D}}S(f,\mathcal{D})$.
We define the \emph{lower integral} similarly
as $I_{*}(f)=\sup_{\mathcal{D}}s(f,\mathcal{D})$.
The inequality $\bigstar$ tells us that these
concepts behave well.
\begin{lemma} If $f:[a,b]\rightarrow{\mathbb R}$
is bounded then $I^{*}(f)\geq I_{*}(f)$.
\end{lemma}
If $I^{*}(f)=I_{*}(f)$ we say that $f$ is Riemann
integrable and we write
\[\int_{a}^{b}f(x)\,dx=I^{*}(f).\]
We write $\mathcal{R}[a,b]$ or sometimes
just $\mathcal{R}$ for the set of Riemann integrable
functions on $[a,b]$.

The following lemma provides a convenient criterion
for Riemann integrability.
\begin{lemma}\label{Riemann criterion}
(i) A bounded function
$f:[a,b]\rightarrow{\mathbb R}$ is Riemann integrable
if and only if given any $\epsilon>0$ we can find
a dissection ${\mathcal D}$ with
\[S(f,{\mathcal D})-s(f,{\mathcal D})<\epsilon.\]

(ii) A bounded function
$f:[a,b]\rightarrow{\mathbb R}$ is Riemann integrable
with integral $I$
if and only if given any $\epsilon>0$ we can find
a dissection ${\mathcal D}$ with
\[S(f,{\mathcal D})-s(f,{\mathcal D})<\epsilon,
\ \text{and}\ S(f,{\mathcal D})\geq I\geq s(f,{\mathcal D}).\]
\end{lemma}
The reader who is tempted to start her treatment of
the Riemann integral with Lemma~\ref{Riemann criterion}~(ii)
should reflect that without the inequality $\bigstar$
she will find it hard to prove the uniqueness of $I$.

Even though we admit that we can not expect all functions
to be integrable, any satisfactory theory of integration
must have a large collection of integrable functions.
\begin{theorem}\label{Riemann class}
(i) Any monotonic (that is any increasing
or decreasing) function $f:[a,b]\rightarrow{\mathbb R}$
is Riemann integrable.

(ii) Any continuous function $f:[a,b]\rightarrow{\mathbb R}$
is Riemann integrable.
\end{theorem}
Part~(i) of Theorem~\ref{Riemann class} looks a little
odd. However most of the functions $f$ met with in `every day
life' including many discontinuous ones
are the difference of two increasing functions
(that is $f=f_{1}-f_{2}$ with $f_{1}$ and $f_{2}$ 
increasing). Such functions are called `functions of
bounded variation' and have a very pretty,
if old fashioned, theory of their own.
Lemma~\ref{add  Riemann} tells us that every
function of bounded variation is  Riemann 
integrable. It is worth noting that
Riemann did not have the theorem that every
continuous function on a closed bounded interval
is uniformly continuous and so was unable to
prove that every continuous function
is Riemann integrable.

The reader will recall from course C5/6 the standard
example of bounded function which is not Riemann
integrable.
\begin{example} If $f:[0,1]\rightarrow{\mathbb R}$ is
given by
\begin{align*} 
f(x)=0&\qquad\text{when $x$ is rational,}\\
f(x)=1&\qquad\text{when $x$ is irrational,}
\end{align*}
then $f$ is not Riemann integrable.
\end{example}

The following results are easy to prove.
\begin{lemma} Suppose that $a\leq c\leq b$ and
$f:[a,b]\rightarrow{\mathbb R}$. 
If $f|_{[a,c]}\in{\mathcal R}[a,c]$ and
$f|_{[c,b]}\in{\mathcal R}[c,b]$ then
$f\in{\mathcal R}[a,b]$ and
\[\int_{a}^{b}f(x)\,dx=\int_{a}^{c}f(x)\,dx
+\int_{c}^{b}f(x)\,dx .\]
\end{lemma}
If we adopt the convention that 
$\int_{b}^{a}f(x)\,dx=-\int_{a}^{b}f(x)\,dx$
then the formula
\[\int_{a}^{b}f(x)\,dx=\int_{a}^{c}f(x)\,dx
+\int_{c}^{b}f(x)\,dx \]
holds independently of the order of $a$, $b$ and $c$.
\begin{lemma}\label{add Riemann} 
If $\lambda,\mu\in{\mathbb R}$ and
$f,g:[a,b]\rightarrow{\mathbb R}$ are
Riemann integrable then so is $\lambda f+\mu g$
and 
\[\int_{a}^{b}\lambda f(x)+\mu g(x)\,dx
=\lambda\int_{a}^{b}f(x)\,dx+\mu \int_{a}^{b}g(x)\,dx.\]
\end{lemma}
In the language of linear algebra ${\mathcal R}[a,b]$
is a vector space and the integral is a linear functional
(i.e. a linear map from ${\mathcal R}[a,b]$ to
${\mathbb R}$.). 
\begin{lemma} If $f,g:[a,b]\rightarrow{\mathbb R}$ are
Riemann integrable and $f(x)\geq g(x)$ for all $x\in [a,b]$
then 
\[\int_{a}^{b}f(x)\geq \int_{a}^{b}g(x)\,dx.\]
\end{lemma}
\begin{lemma} If $f:[a,b]\rightarrow{\mathbb R}$ is
Riemann integrable then so is $|f|$.
\end{lemma}
The next result requires
a small idea.
\begin{lemma} (i) If $f:[a,b]\rightarrow{\mathbb R}$ is
Riemann integrable then so is $f^{2}$.

(ii) If $f,g:[a,b]\rightarrow{\mathbb R}$ are
Riemann integrable then so is $fg$.
\end{lemma}  

We are now in a position to prove the fundamental
theorem of the calculus.
\begin{theorem}[Fundamental theorem of the calculus]
Suppose that $u\in (a,b)$ and
$f:(a,b)\rightarrow{\mathbb R}$ is a
Riemann integrable function function which is
continuous at some $t\in(a,b)$. If we set
\[F(s)=\int_{u}^{s}f(x)\,dx\]
then $F$ is differentiable at $t$ and $F'(t)=f(t)$.
\end{theorem}

Sometimes we think of the fundamental theorem
in a slightly different way.
\begin{theorem} Suppose that $f:(a,b)\rightarrow{\mathbb R}$ is
continuous, that $u\in(a,b)$ and $c\in{\mathbb R}$.
Then there is a unique solution
to the differential equation $g'(t)=f(t)$ $[t\in(a,b)]$
such that $g(u)=c$.
\end{theorem}
Thus (under appropriate circumstances) integration
and differentiation are inverse operations and the
the theories of differentiation and integration
are subsumed in the greater theory of the calculus.
Under appropriate circumstances, if the graph of $F$
has tangent with slope $f(x)$ at $x$
\begin{align*}
\text{area under}&\ \text{the graph of slope of tangent of $F$}\\
&=\text{area under the graph of $f$}\\
&=\int_{a}^{b}f(x)\,dx=\int_{a}^{b}F'(x)\,dx=F(b)-F(a).
\end{align*}

Although it is not explicitly on the syllabus
the examiners seem deeply attached to the
next theorem. The result was stated in
course~C4 and the proof makes use of quite 
a lot of the methods developed in the present course.
\begin{theorem}[Differentiation under the integral]%
\label{under integral} 
Suppose $g:[a,b]\times[c,d]$ is
continuous and that the partial derivative $g_{2}$
exists and is continuous. Then writing 
$G(y)=\int_{a}^{b}g(x,y)\,dx$
we have $G$ differentiable on $(c,d)$ with
\[G'(y)=\int_{a}^{b}g_{,2}(x,y)\,dx.\]
\end{theorem}

In the last few years the examiners
have shown an excessive interest in the next lemma
probably because the proof
neatly combines several of our previous results.
\begin{lemma}~\label{double}
Suppose that $g:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is continuous and has continuous partial derivative
$g_{,2}$.

(i) If we set $G(x,y)=\int_{0}^{x}g(u,y)\,du$ then
$G$ is differentiable with partial derivatives
\[G_{,1}(x,y)=g(x,y)\ \text{and}
\ G_{,2}(x,y)=\int_{0}^{x}g_{,2}(u,y)\,du.\]

(ii) If we set $F(t)=\int_{0}^{t}g(u,t)\,du$ then
$F$ is differentiable with derivative
\[F'(t)=g(t,t)+\int_{0}^{t}g_{,2}(u,t)\,du.\]
\end{lemma}

\section{Further remarks on integration} We have
defined Riemann integration for bounded functions
on bounded intervals. However, the reader will already
have evaluated, as a matter of routine, integrals 
in the following manner
\[\int_{0}^{1}x^{-1/2}\,dx=\lim_{\epsilon\rightarrow 0+}
\int_{\epsilon}^{1}x^{-1/2}\,dx=
\lim_{\epsilon\rightarrow 0+}[2x^{1/2}]_{\epsilon}^{1}=2,\]
and
\[\int_{1}^{\infty}x^{-2}\,dx=\lim_{R\rightarrow \infty}
\int_{1}^{R}x^{-2}\,dx=
\lim_{R\rightarrow \infty}[-x^{-1}]_{1}^{R}=1.\]

A full theoretical treatment of such integrals with
the tools at our disposal is apt to lead into
a howling wilderness of `infinite integrals of the first kind',
`Cauchy principal values' and so on. Instead I shall give
a few typical theorems, definitions and counter-examples
from which the reader should be able to reconstruct
any parts of the theory that she needs.
\begin{definition} If $f:[a,\infty)\rightarrow{\mathbb R}$
is such that $f|_{[a,X]}\in{\mathcal R}[a,X]$ for each $X>a$
and $\int_{a}^{X}f(x)\,dx\rightarrow L$ as $X\rightarrow\infty$
then we say that $\int_{a}^{\infty}f(x)\,dx$ exists
with value $L$.
\end{definition}
\begin{lemma} Suppose $f:[a,\infty)\rightarrow{\mathbb R}$
is such that $f|_{[a,X]}\in{\mathcal R}[a,X]$ for each $X>a$.
If $f(x)\geq 0$ for all $x$ then
$\int_{a}^{\infty}f(x)\,dx$ exists if and only if
there exists a $K$ such that
$\int_{a}^{X}f(x)\,dx\leq K$ for all $X$.
\end{lemma}
\begin{lemma}\label{conditional}
Suppose $f:[a,\infty)\rightarrow{\mathbb R}$
is such that $f|_{[a,X]}\in{\mathcal R}[a,X]$ for each $X>a$.
If
$\int_{a}^{\infty}|f(x)|\,dx$ exists then
$\int_{a}^{\infty}f(x)\,dx$ exists.
\end{lemma}
It is natural to state Lemma~\ref{conditional} as saying
that `absolute convergence of the integral implies
conditional convergence'.

Additional problems arise when there are two limits
involved.
\begin{example} If $\lambda,\mu>0$ then
\[\int_{-\mu R}^{\lambda R}\frac{x}{1+x^{2}}\,dx
\rightarrow\log\frac{\lambda}{\mu}\]
as $R\rightarrow\infty$.
\end{example}
A pure mathematician gets round this problem
by making a definition along these lines.
\begin{definition} If $f:{\mathbb R}\rightarrow{\mathbb R}$
is such that $f|_{[-X,Y]}\in{\mathcal R}[X,Y]$ for each $X,Y>0$
$\int_{-\infty}^{\infty}f(x)\,dx$ exists
with value $L$ if and only if the following condition holds.
Given $\epsilon>0$ we can find an $X_{0}(\epsilon)>0$
such that
\[\left|\int_{-X}^{Y}f(x)\,dx-L\right|<\epsilon.\]
for all $X,Y>X_{0}(\epsilon)$.
\end{definition}
The physicist gets round the problem by ignoring it.
If she is a \emph{real physicist} with correct
physical intuition this works 
splendidly\footnote{In~\cite{Boas}
Boas reports the story of
a friend visiting the Princeton common room
`\dots where Einstein was talking to another man,
who would shake his head and stop him; Einstein
then thought for a while, then started talking
again; was stopped again; and so on. After
a while, \dots my friend was introduced to
Einstein. He asked Einstein who the other
man was. ``Oh,'' said Einstein, ``that's my mathematician.''\,'}
but if not, not.

Speaking broadly, infinite integrals $\int_{E}f(x)\,dx$
work well when they are absolutely convergent, that is
$\int_{E}|f(x)|\,dx<\infty$, but are full of traps for
the unwary otherwise.

So far we have dealt only with the integration
of functions $f:E\rightarrow{\mathbb R}$ with
$E$ a `well behaved' subset of ${\mathbb R}$.
It is not difficult to extend our definitions to
the full multidimensional case but the main users
of multidimensional integrals are either
applied mathematicians (in the broadest sense),
differential geometers and analysts (including probabilists).
The applied mathematicians are happy to take the
details on trust and both the differential geometers
and the analysts require more sophisticated approaches
(though of rather different kinds).

However the particular case of a function
$f:[a,b]\rightarrow{\mathbb R}^{m}$ is needed
in the complex variable courses (with $m=2$)
and elsewhere when line integrals are used.
The definition is simple.
\begin{definition} If $f:[a,b]\rightarrow{\mathbb R}^{m}$
is such that $f_{j}:[a,b]\rightarrow{\mathbb R}$
is Riemann integrable for each $j$ then 
$\int_{a}^{b}f(x)\,dx=\mathbf{y}$ where
${\mathbf y}\in{\mathbb R}^{m}$ and
\[y_{j}=\int_{a}^{b}f_{j}(x)\,dx.\]
\end{definition}
It is easy to obtain the properties of this
integral directly from its definition. Here
is an example.
\begin{lemma} If 
$\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
is linear and $f:[a,b]\rightarrow{\mathbb R}^{m}$
is Riemann integrable then so is $\alpha\circ f$
and
\[\int_{a}^{b}\alpha(f(x))\,dx=\alpha\left(
\int_{a}^{b}f(x)\,dx\right).\]
\end{lemma}
Taking $\alpha$ to be any orthogonal transformation
of ${\mathbb R}^{m}$ to itself we see that that
our definition is, in fact, coordinate independent.
Choosing a particular orthogonal transformation
we obtain the following nice result.
\begin{theorem} If $f:[a,b]\rightarrow{\mathbb R}^{m}$
is Riemann integrable then
\[\left\|\int_{a}^{b}f(x)\,dx\right\|
\leq (b-a)\sup_{x\in [a,b]}\|f(x)\|.\]
\end{theorem}
This result and its extensions are often quoted in the form
\[\text{integral}\leq\text{length}\times\text{$\sup$}.\] 
\begin{exercise} Show that the collection ${\mathcal R}$ of
Riemann integrable functions 
$f:[a,b]\rightarrow{\mathbb R}^{m}$ form
a real vector space. If we write
\[Tf=\int_{a}^{b}f(x)\,dx\]
show that $T:{\mathcal R}\rightarrow{\mathbb R}$
is a linear map and $|Tf|\leq (b-a)\|f\|$.
\end{exercise}

The Riemann integral is simple to define and easy to
use. So long as we only need to integrate continuous
functions (or functions continuous except at a few
simple discontinuities) it is perfectly satisfactory.
The great majority of mathematicians need nothing
else. However, the modern theory of probability
and many parts of modern analysis need the
more powerful integral of Lebesgue
(a hint as to why this might be is given
in Exercise~\ref{integral not complete}) and
its relatives. This is just as easy to use but
substantially harder to set up\footnote{I know
of two universities which teach undergraduates
Lebesgue theory in their
first year but of no university where
undergraduates learn Lebesgue theory in their
first year.}.
\section{Metric spaces}\label{section metric}
One of the unifying ideas
of modern analysis is that of distance.
We start with a definition modelled on those
properties of Euclidean distance which do not
refer to vector space structures. (Thus you
should both \emph{compare} and \emph{contrast}
Lemma~\ref{Euclidean norm}.)
\begin{definition}\label{metric}
We say that $(X,d)$ is a metric space if $X$ is
a set and $d:X^{2}\rightarrow\mathbb{R}$ is a function with the
following properties:-

(i) $d(x,y)\geq 0$ for all $x,y\in X$.

(ii) $d(x,y)=0$ if and only if $x=y$.

(iii) $d(x,y)=d(y,x)$ for all $x,y\in X$.

(iv) (The triangle inequality)
$d(x,z)\leq d(x,y)+d(y,z)$
\end{definition}
As ought to be the case, we have the following result.
\begin{lemma}~\label{natural Euclid}
If $d({\mathbf x},{\mathbf y})=\|{\mathbf x}-{\mathbf y}\|$
then $({\mathbb R}^{m},d)$ is a metric space.
\end{lemma}
I shall give later some examples of metric spaces which are
both useful and novel (in the context of this course).
The metric of Lemma~\ref{natural Euclid} is useful
but hardly novel. In the interest of balance 
let me give a metric which will be novel to most
of my readers even if it is not useful.
\begin{example}\label{Railway 1}
If 
$d({\mathbf x},{\mathbf y})=\|{\mathbf x}\|+\|{\mathbf y}\|$
when $\mathbf{x}\neq \mathbf{y}$ and 
$d({\mathbf x},{\mathbf x})=0$ then 
$({\mathbb R}^{m},d)$ is a metric space.
\end{example}
(This metric is called the British Railway metric.)
Here is an even odder railway metric.
\begin{example}\label{Railway 2}
If
${\mathbf x}$ and ${\mathbf y}$ are linearly dependent
(in more geometrical language) if 
${\mathbf x}$, ${\mathbf y}$ and ${\mathbf 0}$
are on the same straight line) then set
\[d({\mathbf x},{\mathbf y})=\|{\mathbf x}-{\mathbf y}\|.\]
Otherwise, set
\[d({\mathbf x},{\mathbf y})=\|{\mathbf x}\|+\|{\mathbf y}\|.\]
With this definition,
$({\mathbb R}^{m},d)$ is a metric space.
\end{example}

Surprising as it may seem the following rather trivial
metric is a rich source of intuition for communication
theory. (However it is of no analytic interest.)
\begin{lemma}[Hamming metric] Let  $M$ be the space
of sequences
\[{\mathbf x}=(x_{1},x_{2},\dots,x_{n})\]
with $x_{j}=0$ or $x_{j}=1$.  We call $M$ the space
of messages of length $n$. If  ${\mathbf x}$ and
${\mathbf y}$ are messages we define
\[d({\mathbf x},{\mathbf y})=\sum_{j=1}^{n}|x_{j}-y_{j}|\]
to be the \emph{Hamming distance} between the two messages.
$(M,d)$ is a metric space.
\end{lemma}

Much of the `mere algebra' which we did for the
Euclidean metric carries over with hardly any
change to the general metric case. Compare
the following definition with 
Definition~\ref{many convergence definition}.)
\begin{definition}\label{metric convergence definition}
Let $(X,d)$ be a metric space.
If $a_{n}\in X$ for each $n\geq 1$
and $a\in X$ then we say that 
$a_{n}\rightarrow a$
if given $\epsilon>0$ we can find an $n_{0}(\epsilon)$
such that
\[d(a_{n},a)<\epsilon
\ \text{for all $n\geq n_{0}(\epsilon)$}.\]
\end{definition}
However, we are not now dealing with a `norm 
on a vector space' so some results do not carry over.
The result corresponding to Lemma~\ref{many sequences}
has far fewer parts.
\begin{lemma}\label{metric sequences}
Let $(X,d)$ be a metric space.

(i) The limit is unique. That is, if 
$a_{n}\rightarrow a$
and $a_{n}\rightarrow b$
as $n\rightarrow\infty$
then $a=b$.

(ii) If $a_{n}\rightarrow a$
as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$a_{n(j)}\rightarrow a$ 
as $j\rightarrow\infty$.

(iii) If $a_{n}=c$ for all $n$ 
then $a_{n}\rightarrow c$
as $n\rightarrow\infty$.
\end{lemma}

The material on open and closed sets from 
Section~\ref{open and closed} goes through 
essentially unchanged.
\begin{definition} Let $(X,d)$ be metric space.
A set $F\subseteq X$
is closed if whenever $x_{n}\in F$ for each
$n$ and $x_{n}\rightarrow x$
as $n\rightarrow\infty$ then $x\in F$.
\end{definition}
\begin{definition} Let $(X,d)$ be metric space
A set $U\subseteq X$
is open if whenever $x\in U$ there exists
an $\epsilon>0$ such that whenever
$d(x,y)<\epsilon$ we
have $y\in U$.
\end{definition}
\begin{example} Let $(X,d)$ be metric space.
Let $x\in X$ and $r>0$. 

(i) The set $B(x,r)
=\{y\in X:d(x,y)<r\}$ is open.

(ii) The set $\bar{B}(x,r)
=\{y\in X:d(x,y)\leq r\}$ is closed.
\end{example}
We call $B(x,r)$ the open ball of radius $r$
and centre $x$.
We call $\bar{B}(x,r)$ the closed ball of radius $r$
and centre $x$.
\begin{lemma} Let $(X,d)$ be a metric space.
A subset $A$ of $X$ is open
if and only if each point of $A$ is the centre of
an open ball lying entirely within $A$.
\end{lemma}
\begin{definition} The set $N$ is a neighbourhood
of the point $x$ if we can find an $r>0$
such that $B(x,r)\subseteq N$.
\end{definition}
\begin{lemma}\label{complement open metric}
Let $(X,d)$ be a metric space.
A subset $A$ of $X$ is open
if and only if its complement $X\setminus A$
is closed.
\end{lemma}
\begin{lemma}\label{open familly metric}
Let $(X,d)$ be a metric space. 
Consider the collection $\tau$ of open sets
in $X$.

(i) $\emptyset\in\tau$, $X\in \tau$.

(ii)  If $U_{\alpha}\in\tau$ for all $\alpha\in A$ then
$\bigcup_{\alpha\in A} U_{\alpha}\in\tau$.

(iii) If $U_{1},U_{2},\dots,U_{n}\in\tau$ then
$\bigcap_{j=1}^{n}U_{j}\in\tau$.
\end{lemma}
\begin{lemma}\label{closed familly metric}
Let $(X,d)$ be a metric space.
Consider the collection $\mathcal{F}$ of closed sets
in $X$.

(i) $\emptyset\in\mathcal{F}$, $X\in \mathcal{F}$.

(ii)  If $F_{\alpha}\in\mathcal{F}$ for all $\alpha\in A$ then
$\bigcap_{\alpha\in A} F_{\alpha}\in\mathcal{F}$.

(iii) If $F_{1},F_{2},\dots,F_{n}\in\mathcal{F}$ then
$\bigcup_{j=1}^{n}F_{j}\in\mathcal{F}$.
\end{lemma}
 
The new definition of continuity only breaks the chain
of translations slightly because it involves 
\emph{two} metric spaces.
\begin{definition}\label{definition continuity metric}
Let $(X,d)$ and $(Z,\rho)$ be metric spaces.
We say that a function $f:X \rightarrow Z$
is continuous at some point $x\in X$
if given $\epsilon>0$ we can find a 
$\delta(\epsilon,x)>0$ such that
if $y\in X$ and
$d(x,y)<\delta(\epsilon,x)$
we have 
\[\rho[(f(x),f(y))<\epsilon.\]
If $f$ is continuous at every point $x\in X$
we say that $f$ is a continuous function on $X$.
\end{definition}
The reader may feel that 
Definition~\ref{definition continuity} is more general 
in than Definition~\ref{definition continuity metric}
because it involves a set $E$.

The following remark shows that this is not so.
\begin{lemma}\label{restriction metric}
Let $(X,d)$ be a metric space and $E\subseteq X$.
Let $d_{E}:E^{2}\rightarrow{\mathbb R}$ be given
by $d_{E}(u,v)=d(u,v)$ whenever $u,v\in E$. Then
$(E,d_{E})$ is a metric space.
\end{lemma}

We conclude this section with more results
taken directly from Section~\ref{open and closed}
\begin{lemma} Let $(X,d)$ and $(Z,\rho)$
be metric spaces
and suppose that the function $f:X\rightarrow Z$
is continuous at $x\in X$. Then if 
$x_{n}\in X$ and $x_{n}\rightarrow x$
it follows that $f(x_{n})\rightarrow f(x)$.
\end{lemma}
\begin{lemma}\label{continuous open metric}
Let $(X,d)$ and $(Z,\rho)$
be metric spaces. 
The function $f:X\rightarrow Z$
is continuous if and only if $f^{-1}(O)$ is open
whenever $O$ is open.
\end{lemma}
\begin{lemma} Let $(X,d)$ $(Y,\theta)$
and $(Z,\rho)$ be metric spaces.
If $f:X\rightarrow Y$
and $g:Y\rightarrow Z$
are continuous then so is their composition 
$g\circ f$.
\end{lemma}

Question~\ref{Odd railway} which asks you to look at
open balls and open sets in the  two railway metrics
given in
Examples~\ref{Railway 1} and~\ref{Railway 2}
emphasises the fact that open sets in different metrics
may be very different.

\section{A look to the future} In this short starred
section I look at a very important class of metrics.
The discussion will be informal, as indicated by the
replacement of the word `Theorem' by `Statement'.

Suppose we wish to build a road joining two points of the
plane. The cost of a building will depend on the
nature of the terrain. If the cost of building 
a short stretch of length $\delta s$ near a point $(x,y)$
is (to first order) $g(x,y)\delta s$ then
subject to everything being well behaved the cost
of  road  $\Gamma$ will be 
\[\int_{\Gamma}g(x,y)ds.\]
`But' cries the reader `you have not defined line
integrals yet!' To which I reply `Mathematical discourse
takes place on many levels and we must learn to move
freely between them'\footnote{`Shut up!' he explained.}.
It is tempting to define the distance $d(A,B)$ between
two points $A$ and $B$ in the plane as
\[d(A,B)=\inf\left\{\int_{\Gamma}g(x,y)ds:
\text{$\Gamma$ joining $A$ and $B$}\right\}.\]

If we include amongst our conditions that
$g$ is continuous and $g(x,y)>0$ everywhere
we see that $d$ is a metric.
\begin{statement}\label{geodesic distance}
(i) $d(A,B)\geq 0$ for all $A$ and $B$.

(ii) $d(A,B)=0$ if and only if $A=B$.

(iii) $d(A,B)=d(B,A)$ for all $A$ and $B$.

(iv) $d(A,C)\leq d(A,B)+d(B,C)$ for all $A$, $B$ and $C$.
\end{statement}

I have rather carefully kept to `$\inf$' rather than to `$\min$'.
One problem is that I have not specified the kind of
$\Gamma$ that is permissible. (Notice that the argument
we used to prove Statement~\ref{geodesic distance}
means that we could not just use `smooth'.)
However, it can be shown that if we choose a suitable
class for the possible $\Gamma$ and a suitably
well behaved $g$ then the minimum is attained.
If $\Gamma_{0}$ is a path from $A$ to $B$
such that
\[\int_{\Gamma_{0}}g(x,y)ds=d(A,B)\]
we call $\Gamma_{0}$ a geodesic. (The geodesic need
not be unique, consider road building for
two towns at diametrically
opposite points of a circular marsh.)
If any two points are joined by a geodesic then
$d(A,B)$ is the `length of the geodesic path joining
$A$ and $B$' where length refers to the metric $d$
and not to the Euclidean metric.

Let us try to use these ideas to find a metric
on the upper half-plane 
\[H=\{z\in{\mathbb C}:\Im z>0\}\]
which is invariant under M\"{o}bius transformation.
(See course C1/2 for background). More precisely
we want a metric $d$ such that if $T$ is a 
M\"{o}bius map mapping $H$ to itself bijectively,
then $d(Tz_{1},Tz_{2})=d(z_{1},z_{2})$ for all
$z_{1},z_{2}\in H$. Of course no such map may
exist but we shall see where the question leads.

\begin{statement} The set $\mathcal{H}$
of M\"{o}bius maps $T$ such that $T|_{H}:H\rightarrow H$
is bijective
is a subgroup of the group $\mathcal{M}$
of all M\"{o}bius transformations.
The subgroup $\mathcal{H}$ is generated by
the transformations $T_{a}$ with $a\in{\mathbb R}$,
$D_{\lambda}$ with $\lambda>0$ and $J$ where
\begin{align*}
T_{a}=&a+z\\
D_{\lambda}(z)=&\lambda z\\
J(z)=&-z^{-1}
\end{align*}
\end{statement}
By looking at the effect of elements of $\mathcal{H}$
on a small arc, we are led to the following.
(Throughout we write write $z=x+iy$ and identify
the plane ${\mathbb R}^{2}$ with ${\mathbb C}$ in
the usual manner.)
\begin{statement} Suppose $g:H\rightarrow {\mathbb R}$ is well 
behaved. Then
\[\int_{T(\Gamma)}g(x,y)ds=\int_{\Gamma}g(x,y)ds\]
for all nice paths $\Gamma$ in $H$ and all $T\in{\mathcal H}$
if and only if $g(x,y)=Ay^{-1}$ for some  $A>0$.
\end{statement}

Let us now fix $g(x,y)=y^{-1}$  and let $d$ be the
derived metric. 
\begin{statement} The metric just
described is invariant under M\"{o}bius transformation.
\end{statement}
We can use the calculus of
variations (course C10, Mathematical Methods) to find geodesics.
\begin{statement} The geodesics for the metric just
described are arcs of circles with centres
on the real axis and straight lines
which cut the real axis at right angles.
\end{statement}
Those who are interested in the axiom of parallels
for Euclidean geometry will note that, for $(H,d)$,
if we call circles with centres
on the real axis and straight lines
which cut the real axis at right angles
geodesic lines for $(H,d)$,  then given
any geodesic line $\Gamma$ and any point $A$ in $H$
not on $\Gamma$ we can find many  geodesic
lines through $A$ which do not intersect $\Gamma$.
More details are given in course  D1 (Geometry).

Of course the methods of C10 are indicative rather
than conclusive. (We have used a \emph{necessary}
condition and not a \emph{sufficient} one.)
However, in this particular case, it is not hard,
once the answer is known, to state and prove everything
rigorously. If we want general underpinning theorems
they can be found in Differential Geometry
but require a lot of hard work.

There is an interesting analogue of this kind
of metric for groups.
\begin{lemma} Let $G$ be a group and $A$ a subset
of $G$ generating $G$.  Write $A^{-1}=\{a^{-1}:a\in A\}$.
If we set
\[d(x,y)=\min\{n:
\text{we can find $a_{1},a_{2},\dots,a_{n}\in A\cup A^{-1}$
with $a_{1}a_{2}\dots a_{n}=xy^{-1}$}\},\]
then $d$ is a metric on $G$ with $d(xz,yz)=d(x,y)$
for all $x,y,z\in G$. (Thus $d$ is a right translation
invariant metric.)
\end{lemma}
If $d(x,y)$ is bounded we call $\max d(x,y)$ the
\emph{diameter} of the group. The notion of diameter
has an obvious relevance to the problem
`How many shuffles do we need to produce a well shuffled
pack?'.


\section{Completeness} If we examine the arguments
of Section~\ref{section metric}  we see that they
are all mere algebra. What do we need to
introduce to do genuine analysis on metric spaces.
We can not use a variant of the fundamental axiom
because there is no order on our spaces\footnote{There
is an appropriate theory  for objects with order
(lattices) but we shall not pursue it here.}. 
The correct variant of the Bolzano--Weierstrass
method is, it is generally agreed, the notion of compactness
which will be studied in the context of topological
spaces (a concept more general than metric spaces)
in course C~12. Instead we use a generalisation of
the general principle of convergence.
\begin{definition} If $(X,d)$ is a metric space
we say that a sequence of points $x_{n}\in X$
is Cauchy if given any $\epsilon>0$ we
can find $n_{0}(\epsilon)$ such that
$d(x_{p},x_{q})<\epsilon$ for all $p,q\geq n_{0}(\epsilon)$.
\end{definition}
\begin{definition} A metric space $(X,d)$ is complete
if every Cauchy sequence converges.
\end{definition}
Of course, ${\mathbb R}^{n}$ with the Euclidean metric
is complete.

To show that the notion of completeness is distinct
from `Bolzano--Weierstrass notions' we introduce a dull but
useful metric space.
\begin{lemma} Let $X$ be any set. If we define
$d:X^{2}\rightarrow\mathbb{R}$ by
\begin{align*}
d(x,y)=&1\qquad\text{if $x\neq y$}\\
d(x,x)=&0
\end{align*}
then  $(X,d)$ is a metric space.
\end{lemma}
We call the metric $d$ of the previous lemma the
\emph{discrete metric}.
\begin{lemma} Let $(X,d)$ be a space with the discrete
metric. Then $(X,d)$ is complete.

Suppose $X$ is infinite. Then although $X$ is bounded
(if $x_{0}\in X$ then $\bar{B}(x_{0},1)=X$)
we can find a sequence $x_{n}$ such that no subsequence converges.
\end{lemma}
Here is another property of the discrete metric.
\begin{lemma} If $(X,d)$ is a space with the discrete
metric then every subset of $X$ is both open and closed.
\end{lemma}

The contraction mapping theorem (Theorem~\ref{contraction})
and its applications will provide a striking example
of the utility of the concept of completeness. However this section
and the next are devoted more to examples of spaces
which are and are not complete. Students
who want to see completeness in action immediately
should do the next, starred, example.

\begin{exercise}
We say that a metric space $(X,d)$ has no isolated points
if given $y\in X$ and $\epsilon>0$ we can find
an $x\in X$ such that $0<d(x,y)<\epsilon$. Show
by the methods of Exercise~\ref{Cantor} that
a non-empty metric space with no isolated points
is uncountable.

Give an example of an infinite countable metric space.
Give an example of an uncountable metric space
all of whose points are isolated.
\end{exercise}

The next lemma gives a good supply of metric spaces
which are complete and of metric spaces
which are not complete. 
\begin{lemma}\label{closed complete}
Let $(X,d)$ be a complete metric space.
If $E$ is a subset of $X$ and we define 
$d_{E}:E^{2}\rightarrow{\mathbb R}$
by $d_{E}(u,v)=d(u,v)$ whenever $u,v\in E$
then $(E,d_{E})$ is complete if and only if
$E$ is closed in $(X,d)$.
\end{lemma}
Thus, for example, the closed interval $[a,b]$
is complete for the usual metric but the
open interval $(a,b)$ is not.

Here is a more interesting example of a metric which 
is not complete.
\begin{example}\label{one norm} 
Consider $C([-1,1])$ the set of continuous
functions $f:[-1,1]\rightarrow{\mathbb R}$. If we set
\[d(f,g)=\|f-g\|_{1}=\int_{-1}^{1}|f(x)-g(x)|\,dx\]
then $d$ is a metric. Let
\begin{alignat*}{2}
f_{n}(x)&=-1&&\qquad\text{for $-1\leq x\leq -1/n$}\\
f_{n}(x)&=nx&&\qquad\text{for $-1/n\leq x\leq 1/n$}\\
f_{n}(x)&=1&&\qquad\text{for $1/n\leq x\leq 1$}.
\end{alignat*}
The sequence $f_{n}$ is Cauchy but has no limit.
Thus $(C([-1,1]),d)$ is not complete.
\end{example}

The example just given leaves open the possibility
that the same type of metric might be be complete
when applied to Riemann integrable functions.
The following example which is long, heavily starred,
and only for the ambitious shows that this is not so.
\begin{exercise}\label{integral not complete}
Consider ${\mathcal R}([-1,1])$ the set of
Riemann integrable functions 
functions $f:[-1,1]\rightarrow{\mathbb R}$. If we set
\[d(f,g)=\|f-g\|_{1}=\int_{-1}^{1}|f(x)-g(x)|\,dx\]
show that $d$ satisfies conditions~(i), (iii) and
(iv) of the definition of a metric (see Definition~\ref{metric})
but give an example to show that condition~(ii)
may fail.  We could now stop since $d$ is not a metric
but this would be to miss the interesting part.

In many ways, condition~(ii) is the least important
part of the definition of a metric. We call something
satisfying all the other conditions a \emph{pseudo-metric}.
We can define limits for a pseudo-metrics in the same way
as for metrics. 

If $(X,\rho)$ is a pseudo-metric space 
show that if $x_{n}\rightarrow x$
then $x_{n}\rightarrow y$ if and only if $\rho(x,y)=0$.

In the same way we can define Cauchy sequences. A
pseudo-metric space is called complete if every
Cauchy sequence converges. Our object is to show
that $({\mathcal R}([-1,1]),d)$ is not complete.

(i) Set
\begin{alignat*}{2} 
\Delta_{n}(x)=&0&&\qquad\text{if $2^{-3n}\leq |x|$}\\
\Delta_{n}(x)=&1-2^{3n}|x|&&\qquad\text{if $|x|\leq 2^{-3n}$.}
\end{alignat*}
and $g_{n}(t)=\sum_{r=-2^{n}}^{2^{n}}\Delta_{n}(t-r2^{-n})$
for all $-1\leq t\leq 1$. Sketch $g_{n}$ and show that
$\|g_{n}\|_{1}\leq 2^{-2n+1}$.

(ii) If $f_{n}(x)=\max\{f_{1}(x),f_{2}(x),\dots,f_{n}(x)\}$
explain briefly why $f_{n}$ is a continuous function
on $[-1,1]$ with $0\leq f_{n}(x)\leq 1$. Show that
$\|f_{n+1}-f_{n}\|\leq 2^{-2n+3}$ and deduce that
$f_{n}$ is a Cauchy sequence in $({\mathcal R}([-1,1]),d)$.

(iii) Suppose if possible that there exists an 
$f\in {\mathcal R}([-1,1])$ such that $\|f_{n}-f\|\rightarrow 0$
as $n\rightarrow\infty$. Show that $\|f\|\leq 2/3$.

(iv) We now use the notation of Section~\ref{Riemann integration}.
Explain why there must exist a dissection 
$\mathcal{D}$ of $[-1,1]$ such that $S(f,\mathcal{D})\leq 1$.
Deduce that we can find $a$ and $b$ with $-1\leq a <b\leq 1$
such that $f(x)\leq 1/2$ for all $x\in [a,b]$. 

(v) Show that $\int_{a}^{b}|f(x)-f_{n}(x)|\,dx\nrightarrow 0$
and deduce that $\|f-f_{n}\|_{1}\nrightarrow 0$. This
contradiction shows that the sequence $f_{n}$ does not converge
and $({\mathcal R}([-1,1]),d)$ is not complete.

The reader who has got this far can feel well pleased
with herself but by working still harder
we can extract a little more juice from this example
and show that $f_{n}$ converges pointwise to a function
which is not Riemann integrable.

(v) Show that, for each $x$, $f_{n}(x)$ is a bounded increasing
function. Deduce that $f_{n}(x)$ tends to limit $F(x)$,
say, for each $x$. Show that $I^{*}(F)=2$.

(vi)  Let $Z_{n}=f_{n}^{-1}(0)$. Explain why $Z_{n}$
is closed and $Z_{1}\supseteq Z_{2}\supseteq \dots$.
If $\mathcal{D}$ is a fixed dissection of $[-1,1]$
show that for each $n$ we can find a collection
of  $\mathcal{I}_{n}$ of intervals $I=[x_{j},x_{j-1}]$
with end points successive points of the dissection
and of total length greater than $1$ such that
$I\cap Z_{n}\neq\emptyset$.

(vii) Let $\mathcal{I}=\bigcap_{n=1}^{\infty}\mathcal{I}_{n}$.
Show that $\mathcal{I}$ is a collection
of intervals $I=[x_{j},x_{j-1}]$
with end points successive points of the dissection
$\mathcal{D}$
and with total length greater than $1$ such that
$I\cap Z_{n}\neq\emptyset$ for each $n$.

(viii) Use Theorem~\ref{nested} to
show that $I\cap\bigcap_{n=1}^{\infty}Z_{n}\neq\emptyset$
whenever $I\in mathcal{I}$. Deduce that
$s(F,\mathcal{D})\leq 1$.

(ix) Conclude that $F$ is not Riemann integrable.
\end{exercise}

If we seek to embed $({\mathcal R}([-1,1]),d)$ in
some larger space of functions which is complete
we are led to Lebesgue theory.
\section{The uniform metric} This section is devoted
to one of the most important metrics on functions.
We shall write $\mathbb{F}$ to mean either
$\mathbb{R}$ or $\mathbb{C}$.
\begin{definition} If $E$ is a non-empty set
we write $\mathcal{B}(E)$ for the set of bounded
functions $f:E\rightarrow{\mathbb F}$.
The uniform norm $\|\ \|_{\infty}$ on $\mathcal{B}$
is defined by $\|f\|_{\infty}=\sup_{x\in E}|f(x)|$.
\end{definition}
\begin{lemma} If we use the standard operations
$\mathcal{B}(E)$  is a vector space
over $\mathbb{F}$. If $f,g,h\in\mathcal{B}(E)$ the following
results are true.

(i) $\|f\|_{\infty}\geq 0$,

(ii) If  $\|f\|_{\infty}= 0$ then $f=0$,

(iii) If $\lambda\in{\mathbb F}$
then $\|\lambda f\|_{\infty}=|\lambda|\|f\|_{\infty}$.

(iv) (The triangle inequality)
$\|f+g\|_{\infty}\leq\|f\|_{\infty}+\|g\|_{\infty}$.

(v) The pointwise product $fg\in \mathcal{B}(E)$
(where $fg(x)=f(x)g(x)$) and 
$\|fg\|_{\infty}\leq\|f\|_{\infty}\|g\|_{\infty}$.
\end{lemma}
We call the metric $d$ given by $d(f,g)=\|f-g\|_{\infty}$
the uniform metric.
\begin{theorem}\label{uniform complete}
The uniform metric on
$\mathcal{B}(E)$ is complete.
\end{theorem}

The space $\mathcal{B}(E)$ is not very interesting
in itself but if $E$ is a metric space it has 
a very interesting subspace.
\begin{definition} If $(E,d)$ is a 
non-empty metric space
we write $\mathcal{C}(E)$ for the set of bounded
continuous functions $f:E\rightarrow{\mathbb F}$.
\end{definition}
The next remark merely restates what we already know.
\begin{lemma}
If $(E,d)$ is a 
non-empty metric space
then $\mathcal{C}(E)$ is a vector subspace of 
$\mathcal{B}(E)$. Further if $f,g\in \mathcal{C}(E)$
the pointwise product $fg\in \mathcal{C}(E)$.
\end{lemma}

However the next result is new and crucial.
\begin{theorem}\label{uniform closed}
If $(E,d)$ is a 
non-empty metric space
then $\mathcal{C}(E)$ is a
closed subset of $\mathcal{B}(E)$
under the uniform metric.
\end{theorem}
This is the famous `$\epsilon/3$ Theorem'\footnote{Or
according to a rival school of thought the
`$3\epsilon$ Theorem'.}. Traditionally, it
has been presented in a different but
essentially equivalent manner.
\begin{definition}[Uniform convergence]
If $E$ is a non-empty set and
$f_{n}:E\rightarrow{\mathbb F}$  
and $f:E\rightarrow{\mathbb F}$ 
are functions
we say that $f_{n}$ \emph{converges uniformly}
to $f$ as $n\rightarrow\infty$
if, given any $\epsilon>0$
we can find an $n_{0}(\epsilon)$ such that
$|f_{n}(x)-f(x)|<\epsilon$ for all $x\in E$
and all $n\geq n_{0}(\epsilon)$.
\end{definition}
Theorem~\ref{uniform closed} now takes the following
form.
\begin{theorem} If $(E,d)$ is a non-empty metric space
and $f_{n}:E\rightarrow{\mathbb F}$  form
a sequence of continuous functions tending
uniformly to $f$ then $f$ is continuous.
\end{theorem}
More briefly, the uniform limit of continuous
functions is continuous. (To see the exact
equivalence observe that if $f_{n}\rightarrow f$
uniformly then we can find an $N$ such that
$f_{n}-f_{N}$ is bounded for all $n\geq N$.)

Since $\mathcal{C}(E)$ is a closed subset
of $\mathcal{B}(E)$, Theorem~\ref{uniform complete}
and Lemma~\ref{closed complete}
gives us another important theorem. 
\begin{theorem} If $(E,d)$ is a 
non-empty metric space
then the the uniform metric on $\mathcal{C}(E)$ is 
complete.
\end{theorem}
This result, also, has a more traditional form.
\begin{theorem}[General principle of uniform convergence]
Suppose that $(E,d)$ is a non-empty metric space
and $f_{n}:E\rightarrow{\mathbb F}$ is a continuous
function $[n\geq 1]$.
The sequence $f_{n}$ converge uniformly to
a continuous function $f$ if and only if
given any $\epsilon>0$ we can find an $n_{0}(\epsilon)$
such that $|f_{n}(x)-f_{m}(x)|<\epsilon$ for all
$n,m\geq n_{0}(\epsilon)$.
\end{theorem}
This theorem is also known as the GPUC by those who do
not object to theorems which sound as though they were
a branch of the secret police.

If $E$ is a closed bounded subset of ${\mathbb R}^{m}$
with the Euclidean metric then by Theorem~\ref{maximum}
all continuous functions $f:E\rightarrow{\mathbb F}$
are bounded. In these circumstances we shall
write $C(E)=\mathcal{C}$. 
For the rest of the course we will only
consider this special case. We start with a couple of
important examples.
\begin{example}[The witch's hat] Define 
$f_{n}:[0,2]\rightarrow{\mathbb R}$ by
\begin{alignat*}{2}
f_{n}(x)=&1-n|x-n^{-1}|&&\qquad\text{for $|x-n^{-1}|\leq n^{-1}$,}\\
f_{n}(x)=&0&&\qquad\text{otherwise.}
\end{alignat*}
Then the $f_{n}$ are continuous functions such that
$f_{n}(x)\rightarrow 0$ as $n\rightarrow \infty$
for each $x$ but $f_{n}\nrightarrow 0$ uniformly.
\end{example}
More briefly, pointwise convergence does not imply
uniform convergence.
\begin{example} Define $f_{n}:[0,1]\rightarrow{\mathbb R}$
by $f_{n}(x)=x^{n}$. Then $f_{n}(x)\rightarrow f(x)$
as $n\rightarrow\infty$ where $f(x)=0$ for $0\leq x<1$
but $f(1)=1$.
\end{example}
Thus the pointwise limit of continuous functions need
not be continuous.

Uniform convergence is a very useful tool when dealing
with integration.
\begin{theorem}\label{limit of integral}
Let $f_{n}\in C([a,b])$. 
If $f_{n}\rightarrow f$ uniformly then $f\in C([a,b])$
and
\[\int_{a}^{b}f_{n}(x)\,dx\rightarrow \int_{a}^{b}f(x)\,dx.\]
\end{theorem}
Students often miss the full force of this theorem
because the formula is so easy to prove. We also
need to prove that the second integral actually
exists.
The second half of Example~\ref{integral not complete} 
shows that the pointwise limit
of continuous functions need not be integrable.

Theorem~\ref{limit of integral} should be considered
in the context of the following two examples.
\begin{example}[The tall witch's hat] Define 
$f_{n}:[0,2]\rightarrow{\mathbb R}$ by
\begin{alignat*}{2}
f_{n}(x)=&n(1-n|x-n^{-1}|)&&\qquad
\text{for $|x-n^{-1}|\leq n^{-1}$,}\\
f_{n}(x)=&0&&\qquad\text{otherwise.}
\end{alignat*}
Then the $f_{n}$ are continuous functions such that
$f_{n}(x)\rightarrow 0$ as $n\rightarrow \infty$
but
\[\int_{0}^{2}f_{n}(x)\,dx\nrightarrow  0\]
as $n\rightarrow\infty$.
\end{example}
\begin{example}[Escape to infinity]\label{escape}
Define 
$f_{n}:{\mathbb R}\rightarrow{\mathbb R}$ by
\begin{alignat*}{2}
f_{n}(x)=&1-n^{-1}|x|&&\qquad
\text{for $|x|\leq n$,}\\
f_{n}(x)=&0&&\qquad\text{otherwise.}
\end{alignat*}
Then the $f_{n}$ are continuous functions such that
$f_{n}(x)\rightarrow 0$ uniformly as $n\rightarrow \infty$
but
\[\int_{-\infty}^{\infty}f_{n}(x)\,dx\nrightarrow  0\]
as $n\rightarrow\infty$.
\end{example}
Example~\ref{escape} must be well understood since
it represents a very common phenomenon which we
have to learn to live with.

Traditionally, Theorem~\ref{limit of integral}
is always paired with the following result
which is really a disguised theorem on integration!
\begin{theorem}\label{limit of derivative}
Suppose that 
$f_{n}:[a,b]\rightarrow{\mathbb R}$ is differentiable
on $[a,b]$ with continuous derivative $f_{n}'$
(we take the one-sided derivative at end points 
if necessary). Suppose that $f_{n}(x)\rightarrow f(x)$
as $n\rightarrow\infty$ for each $x\in [a,b]$
and suppose that $f_{n}'$ converges uniformly
to a limit $F$ on $[a,b]$. Then $f$ is differentiable
with derivative $F$.
\end{theorem}

The reader knows how to turn results on the limits of
sequences into results on infinite sums and vice versa.
Applied to Theorems~\ref{limit of integral} 
and~\ref{limit of derivative} the techniques produces
the following results.
\begin{theorem}[Term by term integration] Let 
$g_{j}:[a,b]\rightarrow{\mathbb R}$ be continuous.
If $\sum_{j=1}^{n}g_{j}$ converges uniformly as 
$n\rightarrow\infty$ then
\[\int_{a}^{b}\sum_{j=1}^{\infty}g_{j}(x)\,dx
=\sum_{j=1}^{\infty}\int_{a}^{b}g_{j}(x)\,dx.\]
\end{theorem}
\begin{theorem}[Term by term differentiation]
Let $g_{j}:[a,b]\rightarrow{\mathbb R}$ be differentiable
with continuous derivative. If $\sum_{j=1}^{n}g_{j}(x)$
converges for each $x$ and 
$\sum_{j=1}^{n}g_{j}'$ converges uniformly as 
$n\rightarrow\infty$ then $\sum_{j=1}^{\infty}g_{j}$ is
differentiable and
\[\frac{d\ }{dx}\left(\sum_{j=1}^{\infty}g_{j}(x)\right)
=\sum_{j=1}^{\infty}g_{j}(x).\]
\end{theorem}

Here is a starred application of 
Theorem~\ref{limit of derivative}.
The reader may, of course omit it but I include
it partly because it is often more useful than the
theorem (Theorem~\ref{under integral}) that it extends and
partly because it provides an excellent example
of how Theorem~\ref{limit of derivative} is used in practice.
\begin{theorem}[Differentiation under an infinite integral]%
\label{under infinite} 
Suppose $g:[0,\infty]\times[c,d]$ is
continuous and that the partial derivative $g_{,2}$
exists and is continuous.  Suppose further that
that there exists a continuous function 
$h:[0,\infty]\times[c,d]$ with $|g_{,2}(x,y)|\leq h(x)$
for all $(x,y)$ and such that $\int_{0}^{\infty}h(x)\,dx$
exists and is finite. Then, if 
$G(y)=\int_{0}^{\infty}g(x,y)\,dx$ exists for all $y\in (c,d)$
we have $G$ differentiable on $(c,d)$ with
\[G'(y)=\int_{0}^{\infty}g_{,2}(x,y)\,dx.\]
\end{theorem}

We conclude this section with a remark which may
appear rather isolated but will become more important
in course C12 and later work on complex variable.
\begin{theorem}\label{Radius and uniform}
We work in ${\mathbb C}$. Let
$\sum_{n=0}^{\infty}a_{n}z^{n}$ be a power series
with radius of convergence $R$. If $0\leq r< R$
then  $\sum_{n=0}^{N}a_{n}z^{n}$ converges uniformly
on $\bar{B}(0,r)$ as $N\rightarrow\infty$. However,
it need not be true that $\sum_{n=0}^{N}a_{n}z^{n}$ 
converges uniformly on $B(0,R)$.
\end{theorem}
\section{The contraction mapping theorem}
The next result is a beautiful example of the
power of abstraction. In it Banach transformed
a `folk-technique' into a theorem.
\begin{theorem}[The contraction mapping theorem]%
\label{contraction}
Let $(X,d)$ be a non-empty complete metric space
and $T:X\rightarrow X$ a mapping such that
there exists a $K<1$ with $d(Tx,Ty)\leq K d(x,y)$
for all $x,y\in X$. Then there exists a unique
$x_{0}\in X$ such that $Tx_{0}=x_{0}$.
\end{theorem}
More briefly, a contraction mapping on a complete
metric space has a unique fixed point.

Wide though the conditions are the reader should exercise
caution before attempting to widen them further.
\begin{example}\label{no fixed}
(i) If $X$ is the unit circle centre ${\mathbf 0}$
in the plane, $d$ Euclidean distance and $T$ a 
non-trivial rotation then $(X,d)$ is a complete
metric space and 
$d(T{\mathbf x},T{\mathbf y})=d({\mathbf x},{\mathbf y})$
for all ${\mathbf x},{\mathbf y}\in X$ but $T$ has
no fixed point.

(ii) If $X={\mathbb R}$, $d$ is Euclidean distance
and
\[Tx=1+x+g(x)\]
for some suitably chosen $g$ then $(X,d)$ is a complete
metric space and $d(Tx,Ty)< d(x,y)$
for all $x,y\in X$ but $T$ has
no fixed point.
\end{example}

We use the contraction mapping theorem to show
that a wide class of differential equations
actually have a solution. We shall be looking
at equations of the form
\[y'=f(x,y).\]
Our first, simple but important, result is that
this problem on differential equations can be turned into
a problem on integral equations.
\begin{lemma} If $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is continuous, $x_{0},y_{0}\in{\mathbb R}$ and $\delta>0$
then the following two statements
are equivalent.

(A) The function 
$y:(x_{0}-\delta,x_{0}+\delta)\rightarrow{\mathbb R}$
is differentiable and satisfies the equation $y'(x)=f(x,y(x))$
for all $x\in (x_{0}-\delta,x_{0}+\delta)$ together
with the boundary condition $y(x_{0})=y_{0}$.

(B) The function 
$y:(x_{0}-\delta,x_{0}+\delta)\rightarrow{\mathbb R}$
is continuous and satisfies the condition
\[y(x)=y_{0}+\int_{x_{0}}^{x}f(u,y(u))\,du\]
for all $x\in (x_{0}-\delta,x_{0}+\delta)$.
\end{lemma}

\begin{theorem}~\label{integral solution}
Suppose $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is continuous, $x_{0},y_{0}\in{\mathbb R}$ and $\delta>0$.
Suppose further that there exists a $K>0$ such that
$K\delta<1$ and
\begin{equation*}
|f(x,u)-f(x,v)|\leq K|u-v| \tag*{$\bigstar$}
\end{equation*}
for all $u,v\in (x_{0}-\delta,x_{0}+\delta)$.
Then there exists a unique
$y:(x_{0}-\delta,x_{0}+\delta)\rightarrow{\mathbb R}$
is continuous and satisfies the condition
\[y(x)=y_{0}+\int_{x_{0}}^{x}f(t,y(t))\,dt\]
for all $x\in (x_{0}-\delta,x_{0}+\delta)$.
\end{theorem}
Condition $\bigstar$ is called a Lipschitz condition.
It closely related to differentiability (consider
the mean value theorem) but does not imply differentiability
(consider $f(x,y)=|y|$). In the absence of such
a condition differential equations can have unexpected
properties.
\begin{example} There are an infinity of different
solutions to the equation
\[y'=3y^{2/3}\]
with $y(0)=0$.
\end{example}

The lecturer can not bear to let the topic go at 
this point but the rest of this section is heavily
starred.
As it stands Theorem~\ref{integral solution} is not
quite powerful enough for practical purposes. 
Careful thought, without the need for new ideas
shows that it can be extended to the following                              
result. 
\begin{theorem}\label{local solution}
Suppose $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is a continuous function satisfying the following condition.
There exists a $K:[0,\infty)\rightarrow{\mathbb R}$
such that 
\[|f(x,u)-f(x,v)|\leq K(R) |u-v| \]
whenever $|x|,|u|,|v|\leq R$.
Then given any $(x_{0},y_{0})\in{\mathbb R}^{2}$
we can find a $\delta(x_{0},y_{0})>0$ such that there
exists a unique differentiable function
$y:(x_{0}-\delta,x_{0}+\delta)\rightarrow{\mathbb R}$
which satisfies the equation $y'(x)=f(x,y(x))$
for all $x\in (x_{0}-\delta,x_{0}+\delta)$ together
with the boundary condition $y(x_{0})=y_{0}$.
\end{theorem}
Theorem~\ref{local solution} tells us that the
under very wide conditions the differential
equation has a \emph{local solution} through each
$(x_{0},y_{0})$. Does it have a \emph{global solution}, 
that is, can we find a solution for
the equation $y'(x)=f(x,y(x))$ which is defined for all 
$x\in{\mathbb R}$?

We finish our discussion by showing that the question
is non-trivial.
\begin{theorem}\label{nice global}
Suppose $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is a continuous function satisfying the following condition.
There exists a $K:[0,\infty)\rightarrow{\mathbb R}$
such that
\[|f(x,u)-f(x,v)|\leq K(R) |u-v| \]
for all $|x|\leq R$ and all $u$ and $v$.
Then given any $(x_{0},y_{0})\in{\mathbb R}^{2}$
we can find 
a unique $y:{\mathbb R}\rightarrow{\mathbb R}$
which is
is differentiable and satisfies the equation $y'(x)=f(x,y(x))$
for all $x\in {\mathbb R}$ together
with the boundary condition $y(x_{0})=y_{0}$
\end{theorem}

\begin{example} If $f(x,y)=1+y^{2}$ then
\[|f(x,u)-f(x,v)|\leq 2R |u-v| \]
whenever $|u|,|v|\leq R$. However, there does not exist
a differentiable function $y:{\mathbb R}\rightarrow{\mathbb R}$
with $y'=f(x,y)$ for all $x$.
\end{example}
\section{Green's function} (The contents of this section are
all starred)

In course C12 you saw how to solve the differential
equation
\begin{equation*}
y''+a(x)y'+b(x)y=f(x) \tag*{$(*)$}
\end{equation*}
subject to the conditions $y(0)=y(1)=0$ by using the
Green's function. There is no doubt in the present
author's mind that the `physicist's approach'
using impulses, delta functions and hand-waving
is the most important for the student to master.
However, it adds to our confidence in the hand-waving
approach to see that its results can be confirmed
rigourously.

We start by looking at solutions of
\begin{equation*}
y''+a(x)y'+b(x)y=0 \tag*{$(**)$}
\end{equation*}
Observe first that the results of the last section
on the existence of solutions of differential
equations can be extended to the vectorial case
with only the most minor variations in the proof.
Thus, for example, Theorem~\ref{nice global}
extends as follows.
\begin{theorem}\label{nice global many}
Suppose $f:{\mathbb R}\times{\mathbb R}^{m}
\rightarrow{\mathbb R}^{m}$
is a continuous function satisfying the following condition.
There exists a $K:[0,\infty)\rightarrow{\mathbb R}$
such that
\[\|f(x,{\mathbf w})-f(x,{\mathbf z})\|
\leq K(R) \|{\mathbf w}-{\mathbf z}\| \]
for all $|x|\leq R$
and all $\mathbf{w}$ and $\mathbf{z}$.
Then given any $x_{0}\in{\mathbb R}$
and ${\mathbf y}_{0}\in{\mathbb R}^{m}$
we can
find a unique
$\mathbf{y}:{\mathbb R}\rightarrow{\mathbb R}^{m}$
which is
is differentiable and satisfies the equation 
\[{\mathbf y}'(x)=f(x,{\mathbf y})\]
for all $x\in {\mathbb R}$ together
with the boundary condition $\mathbf{y}(x_{0})=\mathbf{y}_{0}$.
\end{theorem}

As the the reader knows from course C4 (Differential Equations)
it is easy to convert equation $(**)$ into a form appropriate
for the application of Theorem~\ref{nice global many}.
If we set
\[{\mathbf y}=\begin{pmatrix}y\\y'\end{pmatrix}
\ \text{and}\ f(x,{\mathbf w})=
\begin{pmatrix}0&1\\-a(x)&-b(x)\end{pmatrix}{\mathbf w}
=\begin{pmatrix}w_{2}\\-b(x)w_{1}-a(x)w_{2}\end{pmatrix},\]
then
\[{\mathbf y}'(x)=f(x,{\mathbf y}).\]
Now
\begin{align*}
\|f(x,{\mathbf w})-f(x,{\mathbf z})\|
&=\left\|\begin{pmatrix}
w_{2}-z_{2}\\-b(x)(w_{1}-z_{1})-a(x)(w_{2}-z_{2})
\end{pmatrix}\right\|\\
&\leq (1+|a(x)|+|b(x)|)\|{\mathbf w}-{\mathbf z}\|
\end{align*}
so that writing $K(R)=\sup_{t\in[-R,R]}(1+|a(t)|+|b(t)|)$
we have
\[\|f(x,{\mathbf w})-f(x,{\mathbf z})\|\leq K(R)
\|{\mathbf w}-{\mathbf z}\|\]
whenever $\|x|\leq K(R)$.
Thus there is one and only one solution $y_{[\lambda]}$, say,
of $(**)$ with $y(0)=0$ and $y'(0)=\lambda$. By linearity,
$y_{[\lambda]}=\lambda y_{[1]}$.

Let us write $y_{1}$ for the solution of $(**)$ with
$y_{1}(0)=0$, $y_{1}'(0)=1$. We make the following
\[\text{\bf key assumption}\qquad y_{1}(1)\neq 0.\]
We write $y_{2}$ for the solution of $(**)$ with
$y_{2}(0)=0$, $y_{2}'(0)=1$. The next lemmas are
familiar from courses~C4 and C10. We define
\[W(x)=y_{1}(x)y_{2}'(x)-y_{2}(x)y_{1}'(x).\]
The function $W$ is called the Wronskian.
\begin{lemma} (i) The Wronskian is differentiable
with
\[W'(x)=-a(x)W(x).\]

(ii) The Wronskian is never zero.
\end{lemma}

We can now define the Green's function 
$G:{\mathbb R}^{2}\rightarrow{\mathbb R}$ by
\begin{alignat*}{2}
G(s,t)=&y_{1}(s)y_{2}(t)W(t)^{-1}&&\qquad\text{for $s<t$}\\
G(s,t)=&y_{2}(s)y_{1}(t)W(t)^{-1}&&\qquad\text{for $t\leq s$.}
\end{alignat*}
(Why should we do any such thing? Because the intuitive
arguments of C10 tell us to.) Using Lemma~\ref{double}
it is easy to verify our main result.
\begin{theorem} If $f:{\mathbb R}\rightarrow{\mathbb R}$
is continuous then
\[y(x)=\int_{0}^{1}G(x,w)f(w)\,dw\]
is twice differentiable and satisfies
\begin{equation*}
y''+a(x)y'+b(x)y=f(x) \tag*{$(*)$}
\end{equation*}
together with the conditions $y(0)=y(1)=0$.
\end{theorem}
(Our proof is very direct and involves neither differentiation
under the integral nor Lemma~\ref{double}. However,
it is not hard to see that such ideas would
be relevant in more complicated situations.)

From the point of view of more advanced work
this shows how differential operators like
\[y\mapsto Sy=y''+a(x)y+b(x)y\]
can be linked with better behaved integral
operators like
\[f\mapsto Tf \ \text{with}\ Tf(x)=\int_{0}^{1}G(x,v)f(v)\,dv.\]
Note that we have shown $STf=f$ for $f$ continuous,
but note also
that, if $f$ is merely continuous, $Sf$ need not
be defined. The Green's function $G$ is an example
of an integral kernel\footnote{Most of the new words
in this last paragraph are well worth dropping.
\begin{verse}
You must lie among the daisies and discourse in novel
phrases of your complicated state of mind,\\
The meaning doesn't matter if it's only idle chatter
of a transcendental kind.\\
And everyone will say,\\
As you walk your mystic way,\\
`If this young man expresses himself in terms too deep for
me\\
Why, what a very singularly deep young man this deep
young man must be!'
\end{verse}}. More formally, if we
write
\[Tf(x)=\int_{0}^{1}K(x,v)f(v)\,dv,\]
then $T$ is called an integral operator with kernel $K$.
(So far as I know, there is no connection with the
`kernel' that you meet in the linear mathematics course~P1.)



We end with an example to show that things really
do go awry if our {\bf key assumption} fails.
\begin{example} The equation
\[y''+\pi^{2}y=x\]
has no solution satisfying $y(0)=y(1)=0$.
\end{example}
\section{Further reading} Since new introductions to
analysis pour off the printing presses in an unending
torrent, it may well be my fault that I can not
recommend one book to cover the whole course. 
Spivak's \emph{Calculus}~\cite{Spivak} and
J.~C.~Burkill's
\emph{A First Course in Mathematical Analysis}~\cite{Burkill 1}
are both excellent introductions to analysis
but only deal with the real line and not with ${\mathbb R}^{m}$.
J.~C.~Burkill and H.~Burkill's
\emph{A Second Course in Mathematical Analysis}~\cite{Burkill 2}
is also good but rather old fashioned. (Of course, this
has nothing to do with the authors' knowledge but a great
deal to do with what was then thought the appropriate
content for a second course.) Dieudonn\'{e}'s
\emph{Foundations of Modern Analysis}~\cite{Dieudonn}
is superb
but written by someone who neither knew nor cared
what others thought was the appropriate
content for a second course\footnote{Boas notes that
`There
is a test for identifying some of the future professional
mathematicians at an early age. These are students
who instantly comprehend a sentence beginning
``Let $X$ be an ordered quintuple 
$(a,T,\pi,\sigma,{\mathcal B})$ where \dots'. They are 
even more promising if they add, `I never really understood
it before.''\,' (\cite{Boas} page231.)}. 
A  suitable compromise is reached in Marsden and Hoffman's
\emph{Elementary Classical Analysis}~\cite{Marsden}.
This covers the material on multidimensional calculus
very clearly and is probably the book that most students
will find most useful. Make sure that your college
library has a copy of the second edition.

The material on metric spaces is covered
in Sutherland's \emph{Introduction to Metric and Topological
Spaces}~\cite{Sutherland} which is workmanlike though
a bit dull. (Once analytic topology is detached from its 
applications it takes a very gifted writer to 
make it exciting.)

I would be glad to hear of other suggestions for suitable books.
A completely unsuitable but interesting version of the
standard analysis course is given by Berlinski's
\emph{A Tour of the Calculus}~\cite{Berlinski}
--- Spivak rewritten by Sterne with additional purple
passages by the Ankh-Morpork tourist board.
\begin{thebibliography}{10}
\bibitem{Berlinski} D.~Berlinski
\emph{A Tour of the Calculus}
Mandarin Paperbacks 1997.
\bibitem{Boas} R.~P.~Boas 
\emph{Lion Hunting and Other Mathematical Pursuits}
(Editors G.~L.~Alexander and D.~H.~Mugler),
Dolciani Expositions, Vol 15, MAA, 1995.
\bibitem{Burkill 1} J.~C.~Burkill
\emph{A First Course in Mathematical Analysis}
CUP, 1962.
\bibitem{Burkill 2} J.~C.~Burkill and H.~Burkill
\emph{A Second Course in Mathematical Analysis}
CUP, 1970.
\bibitem{Dieudonn} J.~Dieudonn\'{e}
\emph{Foundations of Modern Analysis},
Academic Press, 1960.
\bibitem{Hardy} G.~H.~Hardy
\emph{A Course of Pure Mathematics}, CUP, 1908. 
(Still available in its 10th edition.)
\bibitem{Marsden} J.~E.~Marsden and M.~J.~Hoffman
\emph{Elementary Classical Analysis} (2nd Edition),
W.~H.~Freeman,
New York, 1993.
\bibitem{Spivak} M.~Spivak,
\emph{Calculus}
Addison-Wesley/Benjamin-Cummings, 1967.
\bibitem{Sutherland} W.~A.~Sutherland
\emph{Introduction to Metric and Topological Spaces},
OUP, 1975.
\bibitem{Wagon}S.~Wagon \emph{The Banach--Tarski Paradox},
CUP, 1993.
\bibitem{Whittaker} E.~T.~Whittaker and G.~N.~Watson
\emph{A Course of Modern Analysis} CUP, 1902
(Still available in its 4th edition.)
\end{thebibliography}
\newpage
\section{First Sheet of Exercises} 
I have tried to 
produce 12 rather routine questions for each sheet.
Any further questions are for interest
only. Ambitious
students and their supervisors should look at Tripos 
questions to supplement this meagre fare.
\vspace{1\baselineskip}
                                        
\begin{question}\label{1.1}
(We work with the same ideas as
in Example~\ref{Rational}.) 
(i) Find a differentiable function 
$f:{\mathbb Q}\rightarrow{\mathbb Q}$ such that
$f'(x)=1$ for all $x\in{\mathbb Q}$ but $f(0)>f(1)$.

(ii) If we define $g:{\mathbb Q}\rightarrow{\mathbb Q}$
by $g(x)=(x^{2}-2)^{-2}$ show that $g$ is continuous
but unbounded on $\{x\in{\mathbb Q}:|x|\leq 2\}$.
\end{question}
\begin{question}
In Question~\ref{1.3} below you are asked to
prove Lemma~\ref{one sequences}. Write a paragraph
on each of the following subjects.

(i) Would it have been better for the lecturer to prove
Lemma~\ref{one sequences} rather than leave it to you.
Why?

(ii) What benefit, if any, do you expect to derive from
doing Lemma~\ref{one sequences}? Give reasons.

(iii) What is the best way of approaching Question~\ref{1.1}.
For example should you look at your notes from last year
before attacking the question? Should you look at your notes 
from last year after attacking the question?
\end{question}
\begin{question}~\label{1.3}
Prove Lemma~\ref{one sequences}
(restated below for your convenience)

(i) The limit is unique. That is, if $a_{n}\rightarrow a$
and $a_{n}\rightarrow b$ as $n\rightarrow\infty$
then $a=b$.

(ii) If $a_{n}\rightarrow a$ as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$a_{n(j)}\rightarrow a$ as $j\rightarrow\infty$.

(iii) If $a_{n}=c$ for all $n$ then $a_{n}\rightarrow c$
as $n\rightarrow\infty$.

(iv) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$ then
$a_{n}+b_{n}\rightarrow a+b$.

(v) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$ 
as $n\rightarrow\infty$ then
$a_{n}b_{n}\rightarrow ab$.

(vi) If $a_{n}\rightarrow a$
as $n\rightarrow\infty$ and $a_{n}\neq 0$ for each $n$,
$a\neq 0$ then $a_{n}^{-1}\rightarrow a^{-1}$.

(vii) If $a_{n}\leq A$ for each $n$ and
$a_{n}\rightarrow a$
as $n\rightarrow\infty$ then $a\leq A$.


\end{question}
\begin{question}
Prove
Lemma~\ref{Exercise 1.4} which states that 
a decreasing sequence of real numbers bounded below tends
to a limit.

\end{question}
\begin{question} (i) If $a_{j}$ is a integer 
with $0\leq a_{j}\leq 9$ show \emph{from the fundamental axiom}
that
\[\sum_{j=1}^{\infty}a_{j}10^{-j}\]
exists. Show that $0\leq \sum_{j=1}^{\infty}a_{j}10^{-j}\leq 1$
carefully quoting any theorems that you use.

(ii) If $0\leq x\leq 1$ show that we can find integers
$x_{j}$ with $0\leq x_{j}\leq 9$ such that
\[x=\sum_{j=1}^{\infty}x_{j}10^{-j}.\]
What important deep result do you use?

(iii) If $a_{j}$ and $b_{j}$ are integers 
with $0\leq a_{j},b_{j}\leq 9$ and
$a_{j}=b_{j}$ for $j<N$, $a_{N}>b_{N}$ show that
\[\sum_{j=1}^{\infty}a_{j}10^{-j}\geq \sum_{j=1}^{\infty}b_{j}10^{-j}.\]
Give the precise necessary and sufficient condition
for equality and prove it.

\end{question}
\begin{question} (i) Let us write
\[S_{n}=\sum_{r=0}^{n}\frac{1}{r!}.\]
Show, from first principles, that $S_{n}$ converges
to a limit (which, with the benefit of extra knowledge,
we call $e$).

(ii) Show that, if $n\geq 2$  and $r\geq 0$ then
\[\frac{n!}{(n+r)!}\leq \frac{1}{3^{r}}.\]
Deduce carefully that, if $m\geq n\geq 2$,
\[0\leq n!(S_{m}-S_{n})\leq \frac{1}{2}.\] 
and that
\[0<n!(e-S_{n})\leq \frac{1}{2}.\]
Deduce that $n!e$ is not an integer for any $n$  and
conclude that $e$ is irrational.

(iii) Show similarly that 
${\displaystyle\sum_{r=0}^{\infty}\frac{1}{(2r)!}}$
is irrational.

\end{question}
\begin{question}
In this question you should feel free
to use any results you know (provided you quote them 
correctly).

(i) By using the mean value theorem, show that 
$(1+x^{-1})^{x}$  is an increasing function of
$x$ for $x\geq 0$.

(ii) Show that $(1+n^{-1})^{n}$ tends to limit $L$
as $n\rightarrow\infty$.

(iii) Show that, if $n\geq N$,
\[e\geq \left(1+\frac{1}{n}\right)^{n}\geq 
\sum_{r=0}^{N}\binom{n}{r}n^{-r}.\]
Deduce that
\[e\geq L\geq \sum_{r=0}^{N}\frac{1}{r!},\]
and conclude that
\[\left(1+\frac{1}{n}\right)^{n}\rightarrow e\]
as $n\rightarrow\infty$.

(iv) Assuming the Taylor expansion for $e^{t}$ show
that
\[\left(1+\frac{t}{n}\right)^{n}\rightarrow e^{t}\]
as $n\rightarrow\infty$ for all $t>0$.

(v) Show that
\[1-\left(1+\frac{t^{2}}{n^{2}}\right)^{n}\rightarrow 0\]
and deduce that
\[1-\left(1-\frac{t^{2}}{n^{2}}\right)^{n}\rightarrow 0\]
as $n\rightarrow\infty$.
Hence deduce that
\[\left(1-\frac{t}{n}\right)^{n}\rightarrow e^{-t}\]
as $n\rightarrow\infty$ for all $t>0$.

(vi) Conclude that 
\[\left(1+\frac{t}{n}\right)^{n}\rightarrow e^{t}\]
as $n\rightarrow\infty$ for all real $t$.

\end{question}
\begin{question} (a) Prove the statements made in
Example~\ref{on and off}.

(i) The sum $\sum_{n=1}^{\infty}n^{-2}z^{n}$
has radius of convergence $1$ and converges for all $|z|=1$.

(ii) The sum $\sum_{n=0}^{\infty}z^{n}$
has radius of convergence $1$ and diverges for all $|z|=1$.

(iii) The sum $\sum_{n=1}^{\infty}n^{-1}z^{n}$
has radius of convergence $1$, diverges for $z=1$
and converges for $z=-1$.

(b) Suppose
that $\sum_{j=0}^{\infty}a_{j}z^{j}$ has radius of convergence
$R$ and $\sum_{j=0}^{\infty}b_{j}z^{j}$ has radius of
convergence $S$.

(i) Show that if $R\neq S$ then
$\sum_{j=0}^{\infty}(a_{j}+b_{j})z^{j}$ has radius of convergence
$\min(R,S)$.

(ii) Show that if $R=S$ then
$\sum_{j=0}^{\infty}(a_{j}+b_{j})z^{j}$ has radius of convergence
at least $R$.

(iii) If $T\geq 1=R=S$ give an example where
$\sum_{j=0}^{\infty}(a_{j}+b_{j})z^{j}$ has radius of convergence
$T$.
\end{question}
\begin{question} (i) Suppose that  $x_{n}$ is a bounded
sequence of real
numbers. Show that $y_{n}=\sup_{m\geq n}x_{m}$
is a well defined bounded decreasing sequence
and so $y_{n}$ tends to a limit $y$ say. We
write
\[\limsup_{n\rightarrow\infty}x_{n}=y.\]

(ii) Let $a_{n}$ be a sequence of complex numbers.
Show that if the sequence $|a_{n}|^{1/n}$ is unbounded, then
$\sum_{j=0}^{\infty}a_{j}z^{j}$ has radius of convergence $0$
and if the the sequence $|a_{n}|^{1/n}$ is bounded and non-zero, then
$\sum_{j=0}^{\infty}a_{j}z^{j}$ has radius of convergence 
$(\limsup_{n\rightarrow\infty}|a_{n}|^{1/n})^{-1}$.
State, with reasons, what happens when 
$\limsup_{n\rightarrow\infty}|a_{n}|^{1/n}=0$

\end{question}
\begin{question}
Suppose that $A$ and $B$ are non-empty
bounded subsets of ${\mathbb R}$. Show that
\[\sup\{a+b:a\in A,\ b\in B\}=\sup A+\sup B.\]
The last formula is more frequently written
\[\sup_{a\in A,\ b\in B}a+b=\sup_{a\in A}a+\sup_{b\in B}b.\]

Suppose, further that $a_{n}$ and $b_{n}$ are bounded
sequences of real numbers. For each of the following
statements either give a proof that it is always true
or an example to show that it is sometimes false.

(i) $\sup_{n}(a_{n}+b_{n})=\sup_{n}a_{n}+\sup_{n}b_{n}$.

(ii) $\sup_{a\in A,\ b\in B}ab=(\sup_{a\in A}a)(\sup_{b\in B}b)$.

(iii) $\inf_{a\in A,\ b\in B}a+b=\inf_{a\in A}a+\inf_{b\in B}b$.

\end{question}
\begin{question}
Prove all the statements made
in Lemma~\ref{many sequences}, viz:

(i) The limit is unique. That is, if 
$\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\mathbf{a}_{n}\rightarrow \mathbf{b}$
as $n\rightarrow\infty$
then $\mathbf{a}=\mathbf{b}$.

(ii) If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$\mathbf{a}_{n(j)}\rightarrow \mathbf{a}$ 
as $j\rightarrow\infty$.

(iii) If $\mathbf{a}_{n}=\mathbf{c}$ for all $n$ 
then $\mathbf{a}_{n}\rightarrow \mathbf{c}$
as $n\rightarrow\infty$.

(iv) If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\mathbf{b}_{n}\rightarrow\mathbf{b}$
as $n\rightarrow\infty$ then
$\mathbf{a}_{n}+\mathbf{b}_{n}
\rightarrow \mathbf{a}+\mathbf{b}$.

(v) Suppose $\mathbf{a}_{n}\in {\mathbb R}^{m}$,
$\mathbf{a}\in {\mathbb R}^{m}$
$\lambda_{n}\in {\mathbb R}$,
and $\lambda\in {\mathbb R}$. 
If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\lambda_{n}\rightarrow\lambda$ then
$\lambda_{n}\mathbf{a}_{n}\rightarrow 
\lambda\mathbf{a}$.

\end{question}
\begin{question}
Use the Bolzano--Weierstrass theorem to prove 
the general principle of convergence in ${\mathbb R}^{m}$
(Theorem~\ref{many Cauchy}).
\end{question}

\vspace{1\baselineskip}

The remaining questions are included for general interest
and not for relevance to the syllabus or to passing Tripos exams.

\vspace{1\baselineskip}

\begin{question}\label{square supremum} By comparing
\[\frac{a}{b}\ \text{with}\ \frac{3a+4b}{2a+3b}\]
show that given any $x\in{\mathbb Q}$ with $x^{2}\leq 2$
we can find a $y\in{\mathbb Q}$ with $y>x$ and $y^{2}\leq 2$.
Deduce that
the set $\{x\in{\mathbb Q}:x^{2}<2\}$ has no supremum
in ${\mathbb Q}$. (The idea of comparing $a/b$ with
$(3a+4b)/(2a+3b)$ goes back as far as Euclid.)
\end{question}


\vspace{1\baselineskip}

\begin{question}
If $a<c<b$ and $E$ is a subset of ${\mathbb R}$
explain why if $E\cap [a,c]$ and $E\cap [c,b]$ are finite
then so is $E\cap [a,b]$. Use this to give a proof by bisection
of Bolzano's theorem.

\end{question}
\begin{question}
Do Exercise~\ref{Cantor}
showing the uncountability of the reals.

Let $y_{1}$, $y_{2}$, \dots be any
sequence of points in ${\mathbb R}$. Let $x_{0}=0$,
$\delta_{0}=1$.

(i) Show that you can construct inductively
a sequence of real numbers $x_{1}$, $x_{2}$, \dots
and positive numbers $\delta_{j}$ such that

\ \ (a) $|x_{n}-x_{n-1}|<\delta_{n-1}/4$,

\ \ (b) $x_{n}\neq y_{n}$,

\ \ (c) $0<\delta_{n}<|x_{n}-y_{n}|$,

\ \ (d) $\delta_{n}<\delta_{n-1}/4$.

(ii) Show that $\delta_{n+m}<4^{-m}\delta_{n}$ for $m,n\geq 0$
and deduce that the $x_{n}$ form a Cauchy sequence.
Conclude that $x_{n}$ tends to a limit $x$.

(iii) Show that $|x_{n+m}-x_{n}|<\delta_{n}/3$ for all 
$m,n\geq 0$. Deduce that $|x-x_{n}|\leq \delta_{n}/3$
for all $n\geq 0$. Why does this show that $y_{n}\neq x$
for all $n$.

(iv) Prove that the real numbers are uncountable.

\end{question}
\begin{question}
Suppose we consider the collection $\mathcal Q$
of rational functions
\[f(X)=\frac{a_{0}+a_{1}X+\dots+a_{n}X^{n}}
{b_{0}+b_{1}X+\dots+b_{m}X^{m}}\]
(where the $a_{j}$ and $b_{j}$ are real
and where $b_{m}\neq 0$ and $a_{n}\neq 0$ if $n\geq 1$)
with the usual rules for addition and multiplication.
Suppose that we say that $f(X)>0$ if
$a_{n}b_{m}>0$ and that $f>g$ if $f-g>0$.
Convince yourself that  $\mathcal Q$
has all the standard properties of arithmetic
and order we expect. (Some students will only be able
to convince themselves by looking up the axioms for
an ordered field in, for example,~\cite{Spivak}
and then verifying each axiom in turn. Good luck
to them, I say.)

Show that $1/n>1/X>0$ for all integers $n$ and
so Archimedes' Axiom is false for $\mathcal Q$.
\end{question}

\newpage
\section{Second Sheet of Exercises}
I have tried to
produce 12 rather routine questions for each sheet.
Any further questions are for interest
only. Ambitious
students and their supervisors should look at Tripos
questions to supplement this meagre fare.

\vspace{1\baselineskip}

\begin{question}
Here is an alternative proof of Theorem~6.8

(i) Let $K\geq 0$ and $\epsilon>0$.
If $f:[a,b]\rightarrow{\mathbb R}$  has the property
that
\[f(b)-f(a)\geq (K+\epsilon)(b-a)\]
and $N$ is strictly
positive integer show that there exists an integer $r$
with $1\leq r\leq N$ and
\[f(a+r(b-a)/N)-f(a+(r-1)(b-a)/N)\geq (K+\epsilon)(b-a)/N\]

(ii) Under the conditions of part (i) deduce that there
exists a sequence of closed intervals $[x_{n},y_{n}]$
with
\[[a,b]=[x_{0},y_{0}]\supseteq [x_{1},y_{1}]\supseteq
[x_{2},y_{2}]\supseteq \dots\]
such that $f(y_{n})-f(x_{n})\geq (K+\epsilon)(y_{n}-x_{n})$
and $y_{n}-x_{n}\rightarrow 0$ as $n\rightarrow\infty$.

(iii) Continuing with the same notation, show that
$x_{n}\rightarrow c$ for some $c\in [a,b]$. Show
that $c\in [x_{n},y_{n}]$ for all $n$ and that,
if $c\neq x_{n},y_{n}$ then
\[\max\left(
\frac{f(y_{n})-f(c)}{y_{n}-c},
\frac{f(c)-f(x_{n})}{c-x_{n}}
\right)\geq K+\epsilon.\]
Conclude that if $f$ is differentiable at $c$ then
$f'(c)\geq K+\epsilon$.

(iv) Deduce all the results of Theorem 6.8.

\end{question}
\begin{question} (i) If $E$ is a set show that the set
\[E^{\circ}=\bigcup\{U:\text{$U$ is open and $U\subseteq E$}\}\]
has the following properties.

\ \ (a) $E^{\circ}$ is open.

\ \ (b) $E^{\circ}\subseteq E$.

\ \ (c) If $A$ is open and $A\subseteq E$
then $A\subseteq E^{\circ}$.

(ii) Suppose that $E'$ is such that

\ \ (a) $E'$ is open.

\ \ (b) $E'\subseteq E$.

\ \ (c) If $A$ is open and $A\subseteq E$
then $A\subseteq E'$.

\noindent
Show that $E'=E^{\circ}$.

(iii) Explain why (i) and (ii) make it possible to define
the \emph{interior} of $E$ as the largest open set
contained in $E$.

(iv) Show why it is possible to define the
\emph{closure} of $E$ as the smallest closed set
containing $E$.

(v) Find the interior and closure of $(a,b)$, $[a,b)$,
$(a,b]$ and $[a,b]$ where $a<b$.

\end{question}
\begin{question}\label{2.3}
Do the Exercise~\ref{art of continuity}
restated below.
 
After looking at parts~(iii) to~(v)
of Lemma~\ref{many sequences} state the corresponding
results for continuous functions. (Thus part~(v) 
corresponds to the statement that if 
$\lambda:E\rightarrow{\mathbb R}$ and 
$f:E\rightarrow{\mathbb R}^{p}$ are continuous
at ${\mathbf x}\in E$ then so is $\lambda f$.)
Prove your statements directly from 
Definition~\ref{definition continuity}.

\end{question}
\begin{question}
If $f,g:{\mathbb R}^{p}\rightarrow{\mathbb R}^{m}$
are continuous
show that 
\[\begin{pmatrix}
f\\g
\end{pmatrix}
:{\mathbb R}^{p}\rightarrow{\mathbb R}^{2m}
={\mathbb R}^{m}\times{\mathbb R}^{m}\]
defined by
\[\begin{pmatrix}
f\\g
\end{pmatrix}({\mathbf x})=
\begin{pmatrix}
f({\mathbf x})\\g({\mathbf x})
\end{pmatrix}\]
is continuous.

Show that the map $A:{\mathbb R}^{m}\times{\mathbb R}^{m}
\rightarrow {\mathbb R}^{m}$ defined by
\[A\begin{pmatrix}
\mathbf{x}\\ \mathbf{y}
\end{pmatrix}=\mathbf{x}+\mathbf{y}\]
whenever $\mathbf{x},\mathbf{y}\in {\mathbb R}^{m}$
is continuous. By applying the continuity of compositions
of continuous functions (Lemma~\ref{composition}) to 
${\displaystyle A\circ
\begin{pmatrix}
f\\g
\end{pmatrix}}$
deduce that $f+g$ is continuous.

Obtain the other results of Question~\ref{2.3} similarly. 

\end{question}
\begin{question}
Let $f:{\mathbb R}\rightarrow{\mathbb R}$.
State which of the following statements are always
true and which may be false giving a proof or a counter
example as appropriate.

(i) If $f^{-1}(E)$ is closed whenever $E$ is closed
then $f$ is continuous.

(ii) If $f$ is continuous then $f(U)$ is open whenever
$U$ is open.

(iii) If $f$ is continuous then $f(E)$ is bounded whenever
$E$ is bounded.

(iv) If $f(E)$ is bounded whenever
$E$ is bounded then $f$ is continuous.

\end{question}
\begin{question}
Suppose that $E$ is a subset of ${\mathbb R}^{n}$.
Show that $E$ is closed and bounded if and only if
every continuous function $f:E\rightarrow{\mathbb R}^{n}$
is bounded.                      

\end{question}
\begin{question}
If $A$ and $B$ are subsets of ${\mathbb R}^{m}$
we write
\[A+B=\{{\mathbf a}+{\mathbf b}:
{\mathbf a}\in A,\ {\mathbf b}\in B\}\]

By considering the case $m=1$, 
\[A=\{-n:n\in{\mathbb N}\}\ \text{and}
\ B=\{n+1/n:n\in{\mathbb N},\ n\geq 2\}\]
show that if $A$ and $B$ are closed it does not follow
that $A+B$ is closed.

Show however that if $A$ is closed and bounded and
$B$ is closed then $A+B$ is closed.

If $A$ and $B$ are open, does it follow that $A+B$
is open? Give reasons.  

\end{question}
\begin{question}
Consider the map 
$\Omega:{\mathbb R}^{6}\rightarrow{\mathbb R}^{3}$
given by
\[\Omega\begin{pmatrix}{\mathbf x}\\{\mathbf y}
\end{pmatrix}=
{\mathbf x}\wedge{\mathbf y}\]
for all ${\mathbf x},{\mathbf y}\in{\mathbb R}^{3}$.
(Here $\wedge$ is the `vector product' of mathematical
methods.)
Show directly from
the definition that $\Omega$ is differentiable.

Having done this, find the Jacobian matrix of $\Omega$.

\end{question}
\begin{question}[Very traditional] Consider the map
$f:{\mathbb R}^{2}\rightarrow{\mathbb R}$ given 
by
\[f(x,y)=xy|x-y|.\]
At which points is $f$ differentiable? Prove
your statements.

\noindent
[Deal quickly with the easy part. Do not be surprised
if the hard part reveals a special case that must
be dealt with separately.]

\end{question}
\begin{question}[Also very traditional]
(i) Consider the function $f:{\mathbb R}\rightarrow{\mathbb R}$
given by $f(t)=t^{2}\sin 1/t$ for $t\neq 0$, $f(0)=0$.
Show that $f$ is everywhere differentiable and find its derivative.
Show that $f'$ is not continuous.
\noindent
[Deal quickly with the easy part and then
go back to the definition to deal with $t=0$.
There are wide selection of counter-examples
obtained by looking at $t^{\beta}\sin t^{\alpha}$
for various values of $\alpha$ and $\beta$.]

(ii) Find an infinitely differentiable
function $g:(1,\infty)\rightarrow{\mathbb R}$
such that $g(t)\rightarrow 0$ but $g'(t)\nrightarrow 0$
as $t\rightarrow\infty$.

(iii) Consider the function 
$F:{\mathbb R}^{2}\rightarrow{\mathbb R}$
given by $F(s,t)=f(s)+f(t)$ where $f$ is the function
defined in (i). Show that $F$ is everywhere differentiable
but $F_{,1}$ and $F_{,2}$ are not continuous at
$(0,0)$.

\end{question}
\begin{question}
Let $\mathbf{u}$ and $\mathbf{v}$ be linearly
independent vectors in ${\mathbb R}^{2}$.
Suppose that $F:{\mathbb R}^{2}\rightarrow{\mathbb R}$
has the property that
\[\frac{F({\mathbf x}+h{\mathbf u})-F({\mathbf x})}{h}
\rightarrow a({\mathbf x}),\ \text{and}
\ \frac{F({\mathbf x}+h{\mathbf v})-F({\mathbf x})}{h}
\rightarrow b({\mathbf x})\]
as $h\rightarrow 0$ for all ${\mathbf x}\in{\mathbb R}^{2}$.
Suppose further that $a:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is continuous. Show that $F$ is everywhere differentiable.

\end{question}
\begin{question}
Show that Definition~\ref{Definition operator norm}
can be modified to give the definition of an operator
norm $\|\alpha\|$ for a linear map 
$\alpha:{\mathbb C}^{m}\rightarrow{\mathbb C}^{p}$
and that the results of Lemma~\ref{Operator norm}
continue
hold \emph{mutatis mutandis}\footnote{Changing those
things that need to be changed.}.

If $\alpha$ has matrix $(a_{ij})$ with respect to
the standard bases show that
\[\max_{i,j}|a_{ij}|\leq \|\alpha\|\leq mp\max_{i,j}|a_{ij}|.\]

For the rest of this question we take $m=p$ (so we
are dealing with \emph{endomorphisms} of the
vector space ${\mathbb C}^{m}$).

(i) If $\alpha$ has a lower triangular matrix $(a_{ij})$
with respect to the standard basis show that the
set of its eigenvalues is precisely the set
of the diagonal entries $a_{ii}$. (Pure algebra.)

(ii) If $\alpha$ has a lower triangular matrix $(a_{ij})$
and $\epsilon>0$ show that we can find $\beta$ such that
$\beta$ has $m$ distinct non-zero eigenvalues and
$\|\beta-\alpha\|<\epsilon$.

(iii) In algebra you show that given any endomorphism
$\alpha$ we can find an invertible endomorphism
$\theta$ such that $\theta\alpha\theta^{-1}$
has a lower triangular matrix
with respect to the standard basis.
(N.B. This is true for complex vector spaces but not
for real ones.) Use this fact to show that
given any endomorphism
$\alpha$ and any $\eta>0$ we can an endomorphism $\gamma$
such that $\|\gamma-\alpha\|<\eta$ and 
$\gamma$ has $m$ distinct non-zero eigenvalues.

(iv) Write $P_{\alpha}(t)=\det(t\iota-\alpha)$
(where $\iota$ is the identity endomorphism). 
Show that if $\alpha$ has $m$ distinct eigenvalues
then $P_{\alpha}(\alpha)=0$. (Pure algebra.)

(v) Let $\alpha$ be any endomorphism. By considering
a sequence $\alpha_{n}$ of endomorphisms with
$m$ distinct eigenvalues such that 
$\|\alpha_{n}-\alpha\|\rightarrow 0$ as $n\rightarrow \infty$
show that $P_{\alpha}(\alpha)=0$.

\noindent
[This procedure is substantially longer than that used to
prove the Cayley-Hamilton theorem in your algebra
course but it is an instructive way of looking at things.]  
\end{question}    


\section{Third Sheet of Exercises}
I have tried to
produce 12 rather routine questions for each sheet.
Any remaining questions are for interest
only. Ambitious
students and their supervisors should look at Tripos
questions to supplement this meagre fare.

\vspace{1\baselineskip}

\begin{question}
At the beginning of the 19th century it was
hoped that the calculus could be reduced to 
\emph{mere algebra} by using Taylor's theorem.
The following example due to Cauchy shows why
such hopes were misplaced.

Let $E:{\mathbb R}\rightarrow{\mathbb R}$ be defined
by $E(t)=\exp(-1/t^{2})$ for $t\neq 0$ and $E(0)=0$.
Use induction to show that $E$ is infinitely differentiable
with
\begin{align*}
E^{(n)}(t)&=Q_{n}(1/t)E(t)\qquad\text{for $t\neq 0$}\\
E^{(n)}(0)&=0.
\end{align*}
where $Q_{n}$ is a polynomial.
For which values of $t$ is it true that
\[E(t)=\sum_{n=0}^{\infty}\frac{E^{n}(0)t^{n}}{n!}?\] 

Explain why the behaviour of $E$ is consistent
with Lemma~\ref{local} (taking $m=1$).

\end{question}
\begin{question}
In the first year you saw how the find
stationary points for a well behaved
function $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
and how to decide (except in
certain specified cases) whether they
are maxima, minima or saddle-points.
Use Theorem~\ref{2 Taylor} to state and
prove your results rigorously.

In the course on quadratic mathematics you
will see that given any real symmetric $m\times m$
matrix $A$ you can find a special orthogonal
matrix $P$ such that $PAP^{T}$ is diagonal.
Explain the relevance of this result to
the problem of deciding (except in certain
\emph{specified} cases) when a stationary
point of a well behaved function
$f:{\mathbb R}^{m}\rightarrow{\mathbb R}$
is a maximum, a minimum or a saddle point.

\end{question}
\begin{question}
Define $f:[0,1]\rightarrow{\mathbb R}$ by
$f(p/q)=1/q$ when $p$ and $q$ are coprime integers
with $1\leq p <q$ and $f(x)=0$ otherwise.

(i) Show that $f$ is Riemann integrable and
find $\int_{0}^{1}f(x)\,dx$.

(ii) At which points is $f$ continuous? Prove
your answer.

\end{question}
\begin{question}
We say that a function $f:[0,1]\rightarrow{\mathbb R}$
is of bounded variation if there exists a constant $K$
such that whenever
$0\leq x_{0}\leq x_{1}\leq x_{2}\dots\leq x_{n}\leq 1$
we have
\[\sum_{j=1}^{n}|f(x_{j-1})-f(x_{j})|\leq K.\]

(i) Show that if $g:[0,1]\rightarrow{\mathbb R}$ is
increasing then $g$ is of bounded variation. Show that
if $h_{1},h_{2}:[0,1]\rightarrow{\mathbb R}$ are
increasing then $h=h_{1}-h_{2}$ is of bounded variation.

(ii) Suppose that $f$ is of bounded variation. Show
that
\[f_{1}(x)=\sup\{
{\textstyle \sum_{j=1}^{n}f(x_{j})-f(y_{j})}
:0\leq y_{1}\leq x_{1}\leq y_{2}\leq x_{2}\leq
\dots\leq y_{n}\leq x_{n}\leq x\}\]
is a well defined increasing function. Show
that $f_{1}-f$ is also an increasing function
and so $f$ is the difference of two increasing functions.

(iii) Show that if we set $f(x)=x\sin x^{-1}$ for
$x\neq 0$ and $f(0)=0$ then $f:[0,1]\rightarrow{\mathbb R}$
is a continuous function which is not of bounded
variation.

\noindent
[Remember diagrams may not be rigorous but they certainly help
you understand what you are talking about.]

\end{question}
\begin{question}
If $f:[0,1]\rightarrow{\mathbb R}$ is increasing
show that if $x_{n}$ is an increasing sequence
then $f(x_{n})$ tends to limit as $n\rightarrow\infty$.
Deduce carefully that if $0<y\leq 1$ then
$f(x)$ tends to a limit $f(y-)$ as $x\rightarrow y$
through values of $x$ with $x<y$. (We shall set $f(0-)=f(0)$.)
Define $f(y+)$ similarly.

Let us call $f(y+)-f(y-)$ the jump $J(y)$ at $y$. Show
that if $y_{1}$, $y_{2}$, \dots $y_{n}$ are distinct
points of $(0,1)$ then
\[\sum_{j=1}^{n}J(y_{j})\leq f(1)-f(0).\]
Show that $f$ can have only finitely many jumps of
size $1/n$ or more. Deduce that $f$ has only
countably many jumps
and that $f$ must be continuous at some point.

\end{question}
\begin{question}
Let ${\mathcal K}$ be the set of functions
$f:[0,1]\rightarrow{\mathbb R}$ such that $f$ is continuous
on $(0,1]$ and $f$ has an improper Riemann integral
\[{\mathcal I}f=\int_{0}^{1}f(x)\,dx.\]

Show that if $f,g\in \mathcal K$ and $\lambda,\mu\in{\mathbb R}$
then $\lambda f+\mu g\in \mathcal K$ and
\[{\mathcal I}(\lambda f+\mu g)=\lambda {\mathcal I}f
+\mu {\mathcal I}g.\]

By giving a proof or counter-example establish whether
it is always true that if $f,g\in {\mathcal K}$
then $fg\in {\mathcal K}$.

\end{question}
\begin{question}
Suppose that $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is continuous. By using results on the differentiation
of integrals which should be quoted exactly show that
\[\frac{d\ }{dt}\int_{a}^{t}
\left(\int_{c}^{d}f(u,v)\,dv\right)\,du
=\frac{d\ }{dt}\int_{c}^{d}
\left(\int_{a}^{t}f(u,v)\,du\right)\,dv.\]
Deduce that
\[\int_{a}^{b}
\left(\int_{c}^{d}f(u,v)\,dv\right)\,du
=\int_{c}^{d}
\left(\int_{a}^{b}f(u,v)\,du\right)\,dv,\]
or, more informally
\[\int_{a}^{b}
\int_{c}^{d}f(u,v)\,dv\,du
=\int_{c}^{d}
\int_{a}^{b}f(u,v)\,du\,dv.\]

\end{question}
\begin{question}\label{First whinge}
Prove all the results of Section~\ref{section metric}.
(A long but easy and useful exercise. In order to keep the
noise level from whinging students down to tolerable
levels this counts as 2 questions.)
\end{question}
\begin{question}
See Question~\ref{First whinge}.
\end{question}
\begin{question}\label{3.10}
Consider ${\mathcal E}$ the space of linear
maps $\alpha:{\mathbb R}^{u}\rightarrow{\mathbb R}^{u}$.
We wish to show that $d(\alpha,\beta)=\|\alpha -\beta\|$
(with $\|\alpha\|$ the usual `operator norm') defines
a complete metric. 

To this end consider a Cauchy sequence $\alpha_{n}$ in
${\mathcal E}$. Show that if $\mathbf{x}\in{\mathbb R}^{u}$
then $\alpha_{n}(\mathbf{x})$ tends to limit, call it
$\alpha(\mathbf{x})$. Now show that 
$\alpha:{\mathbb R}^{u}\rightarrow{\mathbb R}^{u}$
is linear. By using the inequality
\begin{align*}
\|\alpha(\mathbf{x})-\alpha_{n}(\mathbf{x})\|&
\leq\|\alpha_{m}(\mathbf{x})-\alpha_{n}(\mathbf{x})\|
+\|\alpha_{m}(\mathbf{x})-\alpha(\mathbf{x})\|\\
&\leq\sup_{p,q\geq n}\|\alpha_{p}(\mathbf{x})-\alpha_{q}(\mathbf{x})\|
+\|\alpha_{m}(\mathbf{x})-\alpha(\mathbf{x})\|,
\end{align*}
and allowing $m\rightarrow\infty$ show that
\[\|\alpha-\alpha_{n}\|\leq\sup_{p,q\geq n}\|\alpha_{p}-\alpha_{q}\|,\]
and deduce that $\|\alpha-\alpha_{n}\|\rightarrow\infty$
as $n\rightarrow\infty$.

\end{question}
\begin{question}\label{3.11}
In Question~\ref{3.10}
we showed that $\mathcal{E}$ with
the appropriate metric is complete. In this question
we make use of the result.

(i) Suppose $\alpha\in\mathcal{E}$ and $\|\alpha\|<1$.
Let 
\[\sigma_{n}=\iota+\alpha+\alpha^{2}+\dots+\alpha^{n}.\]
Show that $\sigma_{n}$ converges to a limit $\sigma$,
say.

Compute $(\iota-\alpha)\sigma_{n}$ and show, carefully
that $(\iota-\alpha)\sigma=\iota$.

(iii) Deduce that if $\|\iota-\gamma\|<1$ then $\gamma$
is invertible.

(iv) Show that if $\beta\in\mathcal{E}$ is invertible
and $\|\beta-\tau\|<\|\beta^{-1}\|^{-1}$ then $\tau$
is invertible.

(v) Conclude that the invertible elements of ${\mathcal E}$
form an open set.
\end{question}
\begin{question}  
Consider $C([-1,1])$ the set of continuous
functions $f:[-1,1]\rightarrow{\mathbb R}$. Show that,
if we set
\[d(f,g)=\|f-g\|_{2}=
\left(\int_{-1}^{1}|f(x)-g(x)|^{2}\,dx\right)^{1/2}\]
then $d$ is a metric but $d$ is not complete.

Show that if $\|f-g\|_{1}$ is defined as in
Example~\ref{one norm} then $2^{1/2}\|f\|_{2}\geq \|f\|_{1}$
for all $f\in C([-1,1])$ but that, given any
constant $K>0$ we can find a $g\in C([-1,1])$
such that $\|g\|_{2}\geq K\|g\|_{1}$.
\end{question}

\vspace{1\baselineskip}

The remaining question is included for general interest
and not for relevance to the syllabus or to passing Tripos exams.
\vspace{1\baselineskip}

\begin{question}
We use the notation of Questions~\ref{3.10} and~\ref{3.11}.

(i) Show that if $\alpha\in\mathcal{E}$ we can find
$e^{\alpha}\in\mathcal{E}$ such that
\[\left\|\sum_{r=0}^{n}\frac{\alpha^{r}}{r!}
-e^{\alpha}\right\|\rightarrow 0\]
as $n\rightarrow\infty$.

(ii) Show carefully that if $\alpha$ and $\beta$ commute
\[e^{\alpha}e^{\beta}=e^{\alpha+\beta}.\]

(iii) Show that if $\alpha$ and $\beta$ are general
(not necessarily commuting) elements of ${\mathcal E}$
then
\[\left\|h^{-2}(e^{h\alpha}e^{h\beta}-e^{h\beta}e^{h\alpha})
-(\alpha\beta-\beta\alpha)\right\|\rightarrow 0\]
as the real number $h\rightarrow 0$.

Conclude that, in general, $e^{\alpha}e^{\beta}$
and $e^{\alpha+\beta}$ need not be equal.
\end{question}
\newpage
\section{Fourth Sheet of Exercises}
I have tried to
produce 12 rather routine questions for each sheet.
The remaining questions are for interest
only. Ambitious
students and their supervisors should look at Tripos
questions to supplement this meagre fare.

\vspace{1\baselineskip}

\begin{question}\label{Odd railway}
Consider the two railway metrics
$d_{1}$ and $d_{2}$ on ${\mathbb R}^{2}$ given in
Examples~\ref{Railway 1} and~\ref{Railway 2}.
For each of the two metrics give a reasonably
simple description of the open balls
of radius $r$ and centre ${\mathbf x}$
when  ${\mathbf x}={\mathbf 0}$
and when  ${\mathbf x}\neq{\mathbf 0}$
and (a)~$r\leq\|{\mathbf x}\|$, (b)~$r<\|{\mathbf x}\|\leq 2r$,
(c)~$2r<\|{\mathbf x}\|$.

Explain the statement `when finding open sets only
balls of small radius are important so cases
(b) and (c) are irrelevant'.

Give the simplest description you can find
of open sets in the two metrics.
\end{question}

\begin{question}
Let $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
be differentiable  and let $g(x)=f(x,c-x)$ where $c$
constant. Show that $g:{\mathbb R}\rightarrow{\mathbb R}$
is differentiable and find its derivative

(i) directly from the definition of differentiability

\noindent
and also

(ii) by using the chain  rule.

\noindent
Deduce that if $f_{,1}=f_{,2}$ throughout ${\mathbb R}$
then $f(x,y)=h(x+y)$ for some differentiable
function $h$.

\end{question}
\begin{question}
[Traditional] Consider the functions
$f_{n}:[0,1]\rightarrow{\mathbb R}$ defined by
$f_{n}(x)=n^{p}x\exp(-n^{q}x)$ where $p,q>0$.

(i) Show that $f_{n}$ converges pointwise on $[0,1]$.

(ii) Show that if $p<q$ then $f_{n}$ converges uniformly
on $[0,1]$.

(iii) Show that if $p\geq q$ then $f_{n}$ does not converge
uniformly on $[0,1]$. Does $f_{n}$ converge
uniformly on $[0,1-\epsilon]$? 
Does $f_{n}$ converge
uniformly on $[\epsilon,1]$?
(Here $0<\epsilon<1$, you should justify your answers.)

\end{question}
\begin{question}
Let $(X,d)$ be a metric space. We say
that $E$ is dense in $X$ if whenever $x\in X$
we can find $e_{n}\in E$ with $e_{n}\rightarrow x$.

(i) Show that the rationals are dense in the reals
(i.e. $\mathbb Q$ is dense in $\mathbb R$ if we give
$\mathbb R$ the usual metric). Show that the
irrationals are dense in the reals.

\noindent
[You should make explicit any use you make of the axiom
of Archimedes.]

(ii) Consider the space $C([0,1])$ of continuous
functions with the uniform norm. Show that the
piecewise linear functions are dense in $C([0,1])$.

\end{question}
\begin{question}\label{4.3}
Recall from your courses
in mathematical methods that, if 
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a 
continuous periodic function with period $2\pi$
then we  write
\[\hat{f}(n)=\frac{1}{2\pi}
\int_{-\pi}^{\pi}f(t)\exp(-int)\,dt.\]

Show, quoting carefully any results that you use,
that, if $a_{n}\in{\mathbb C}$ and 
$\sum_{n=-N}^{N} |a_{n}|$ converges,
then $\sum_{n=-N}^{N} a_{n}\exp(int)$ converges
uniformly to a continuous periodic function
$f(t)$, say, with period $2\pi$.
Show that $\hat{f}(n)=a_{n}$. for each $n$.

\noindent
[This chain of thought is continued in Question~\ref{4.13}.]
\end{question}
\begin{question}
Recall that $\|\ \|$ is said to be a norm
on a real vector space $V$ if the following conditions
hold.

(1) $\|\mathbf{x}\|\geq 0$ for all $\mathbf{x}\in V$.

(2) $\|\mathbf{x}\|=0$ implies $\mathbf{x}=\mathbf{0}$.
     
(3) $\|\lambda\mathbf{x}\|=|\lambda|\|\mathbf{x}\|$
for all $\lambda\in{\mathbb R}$ and all $\mathbf{x}\in V$.

(4) $\|\mathbf{x}+\mathbf{y}\|\leq \|\mathbf{x}\|+
\|\mathbf{y}\|$ for all $\mathbf{x},\mathbf{y}\in{\mathbb R}$.

(i) Let $\|\ \|_{2}$ be the usual Euclidean norm
on ${\mathbb R}^{m}$ and let $\|\ \|$ be another
norm on ${\mathbb R}^{m}$. Let 
$I:({\mathbb R}^{m},\|\ \|_{2})\rightarrow ({\mathbb R}^{m},\|\ \|)$
be the identity map, given by $I\mathbf{x}=\mathbf{x}$.
By considering basis vectors or 
otherwise that there exists a constant $K$ 
such that $\|\mathbf{x}\|\leq K\|\mathbf{x}\|_{2}$
for all $\|\mathbf{x}\|\in {\mathbb R}^{m}$. Deduce
that $I$ is continuous. 

By applying a theorem on continuous functions on closed
bounded sets show that there exists a $k>0$ such that
$\|\mathbf{x}\|\geq k$ for all $\mathbf{x}$ with
$\|\mathbf{x}\|_{2}=1$. Conclude that  
\[k\|\mathbf{x}\|_{2}\leq
\|\mathbf{x}\|\leq K\|\mathbf{x}\|_{2}\]
for all $\|\mathbf{x}\|\in {\mathbb R}^{m}$.

\noindent
[Thus all norms on a finite dimensional space are
essentially the same.]


(ii) Consider the real vector space $\mathbb{R}^{\mathbb{N}}$
of all real sequences $\mathbf{x}=(x_{1},x_{2},\dots)$
with the usual vector addition and multiplication
by scalars.
Let $V$ be the set of all sequences 
$\mathbf{x}=(x_{1},x_{2},\dots)$ such that $x_{j}\neq 0$ for
only finitely many $j$. Show that $V$ is a subspace of
$\mathbb{R}^{\mathbb{N}}$ and so a vector space in its own right.
By considering norms of the form 
$\|\mathbf{x}\|=\max_{j}\kappa_{j}|x_{j}|$, or otherwise,
find two norms $\|\ \|_{A}$ and $\|\ \|_{B}$ such that
\[\sup_{\|\mathbf{x}\|_{A}=1}\|\mathbf{x}\|_{B}
=\sup_{\|\mathbf{x}\|_{B}=1}\|\mathbf{x}\|_{A}=\infty.\]

\end{question}
\begin{question} (i) Suppose that $f:{\mathbb R}\rightarrow{\mathbb R}$
is a twice differentiable function such that
\[\left|\frac{f(x)f''(x)}{f'(x)^{2}}\right|
\leq\lambda\]
for all $x$ and some $|\lambda|<1$
Show that the mapping
\[Tx=x-\frac{f(x)}{f'(x)}\]
is a contraction mapping and deduce that $f$ has
a unique root $y$.

(ii) Suppose that $F:{\mathbb R}\rightarrow{\mathbb R}$
is a twice differentiable function such that
\[\left|\frac{F(x)F''(x)}{F'(x)^{2}}\right|
\leq\lambda\]
for all $|x|\leq a$ and some $|\lambda|<1$
and that $F(0)=0$. 
Consider the mapping
\[Tx=x-\frac{F(x)}{F'(x)}.\]
Show that $T^{n}x\rightarrow 0$.

Suppose that
\[\frac{\sup_{|t|\leq a}|f'(t)|\sup_{|t|\leq a}|f''(t)|}
{\inf_{|t|\leq a}|f'(t)|^{2}}=M.\]
By using the mean value theorem twice,
show that if $|x|\leq a$ then
\[|Tx|\leq Mx^{2}.\]

(iii) If you know what the Newton--Raphson method is,
comment on the relevance of the results of (i) and (ii)
to that method.


\end{question}
\begin{question}
We work in ${\mathbb R}^{m}$
with the usual `Euclidean norm'.
The following line of thought is
extremely important in later work. Suppose
we want the solution ${\mathbf x}_{0}$  of
\[{\mathbf x}+{\boldsymbol\epsilon}({\mathbf x})=
{\mathbf y}\]
where ${\boldsymbol\epsilon}({\mathbf x})$ is 
`a small error term' or `of first order compared
to ${\mathbf x}$. The following method
of solution is traditional (for the excellent
reason that it usually works like a Spanish charm).
Start by ignoring the small ${\boldsymbol\epsilon}$
term and guess ${\mathbf x}_{0}\approx
{\mathbf x}_{1}={\mathbf y}$.
Since this initial guess is good we estimate
the small ${\boldsymbol\epsilon}$
term as
${\boldsymbol\epsilon}({\mathbf x}_{1})$.
Feeding this estimate back into the equation
we now guess ${\mathbf x}_{0}\approx
{\mathbf x}_{2}={\mathbf y}-{\boldsymbol\epsilon}({\mathbf x}_{1})$.
Repeating this process gives us the rule for
successive approximations
\[\mathbf{x}_{n+1}=\mathbf{y}-{\boldsymbol\epsilon}(\mathbf{x}_{n}).\]
We now try to justify this in certain cases.

(i) Suppose that ${\boldsymbol\epsilon}({\mathbf 0})={\mathbf 0}$
and 
\[\|{\boldsymbol\epsilon}({\mathbf a})-
{\boldsymbol\epsilon}({\mathbf b})\|
<\|{\mathbf a}-{\mathbf b}\|/2\]
for all $\|\mathbf{a}\|,\|\mathbf{b}\|\leq\delta$ where $\delta>0$.
(Note that these are rather stronger conditions
than might have been expected. However, this is not
to say that the general idea might not work under
weaker conditions.)
We shall show that the method works for $\|{\mathbf y}\|\leq\delta/2$.

Consider the closed ball 
$B=\{{\mathbf x}:\|{\mathbf x}\|\leq \delta\}$.
Show that the equation
\[T({\mathbf x})=\mathbf{y}-{\boldsymbol\epsilon}({\mathbf x})\]
defines a map $T:B\rightarrow B$. Now use the contraction
mapping theorem to show that
\[{\mathbf x}+{\boldsymbol\epsilon}({\mathbf x})=
{\mathbf y}\]
has a unique solution ${\mathbf x}(\mathbf{y})$ with
$\|{\mathbf x}(\mathbf{y})\|\leq\delta$ and that
$T^{n}({\mathbf y})\rightarrow {\mathbf x}(\mathbf{y})$
as $n\rightarrow\infty$.

(ii) Suppose that $f:{\mathbb R}^{m}\rightarrow{\mathbb R}^{m}$
is a differentiable function with derivative continuous
at ${\mathbf 0}$, $f({\mathbf 0})={\mathbf 0}$ 
and $Df({\mathbf 0})=I$. Show that
we can find a $\delta>0$ such that whenever
$\|{\mathbf y}\|\leq\delta/2$ the equation
\[f({\mathbf x})={\mathbf y}\]
has a unique solution ${\mathbf x}(\mathbf{y})$ with
$\|{\mathbf x}(\mathbf{y})\|\leq\delta$.

(iii) Suppose that $f:{\mathbb R}^{m}\rightarrow{\mathbb R}^{m}$
is a differentiable function
with derivative continuous
at ${\mathbf 0}$, $f({\mathbf 0})={\mathbf 0}$ 
and $Df({\mathbf 0})$ invertible. Show that
we can find a $\delta_{1},\delta_{2}>0$ such that whenever
$\|{\mathbf y}\|\leq\delta_{1}$ the equation
\[f({\mathbf x})={\mathbf y}\]
has a unique solution ${\mathbf x}(\mathbf{y})$ with
$\|{\mathbf x}(\mathbf{y})\|\leq\delta_{2}$.


\end{question}
\begin{question} (A method of Abel) (i) Suppose that
$a_{j}$ and $b_{j}$ are sequences of complex numbers
and that $S_{n}=\sum_{j=1}^{n}a_{j}$ for $n\geq 1$
and $S_{0}=0$. Show that, if $1\leq u\leq v$ then
\[\sum_{j=u}^{v}a_{j}b_{j}=\sum_{j=u}^{v}S_{j}(b_{j}-b_{j+1})
-S_{u-1}b_{u}+S_{v}b_{v+1}.\]
(This is known as partial summation, for obvious reasons.)

(ii) Suppose now that, in addition, the $b_{j}$
form a decreasing sequence of positive terms
and that $|S_{n}|\leq K$ for all $n$.
Show that
\[\left|\sum_{j=u}^{v}a_{j}b_{j}\right|
\leq 2Kb_{u}.\]
Deduce that if $b_{j}\rightarrow 0$ as $j\rightarrow\infty$
then $\sum_{j=1}^{\infty}a_{j}b_{j}$ converges.

Deduce the alternating series test.

(iii) If $b_{j}$ is a decreasing sequence of positive terms
with $b_{j}\rightarrow 0$ as $j\rightarrow\infty$ show
that  $\sum_{j=1}^{\infty}b_{j}z^{j}$ converges uniformly
in the region given by $|z|\leq 1$ and $|z-1|\geq \epsilon$
for all $\epsilon>0$.


\end{question}
\begin{question}\label{4.9}
We work in ${\mathbb R}^{m}$ with the usual distance.

(i) Show that if $A_{1}$, $A_{2}$, are non-empty,
closed and bounded with $A_{1}\supseteq A_{2}\supseteq\dots$
then $\bigcap_{j=1}^{\infty}A_{j}$ is non empty.
Is this result true if we merely assume $A_{j}$ closed and non-empty?
Give reasons.

(ii) If $A$ is non-empty, closed and bounded show that we can find
${\mathbf a}',{\mathbf b}'\in A$ such that
\[\|{\mathbf a}'-{\mathbf b}'\|\geq\|{\mathbf a}-{\mathbf b}\|\]
for all ${\mathbf a},{\mathbf b}\in A$.
Is this result true if we merely assume $A$ bounded and non-empty?
Give reasons.


\end{question}
\begin{question}
We work in ${\mathbb R}^{m}$ with the usual distance.
Let $E$ be a closed non-empty subset of ${\mathbb R}^{m}$
and let $T$ be a map $T:E\rightarrow E$.

(i) Suppose
$\|T({\mathbf a})-T({\mathbf b})\|<\|{\mathbf a}-{\mathbf b}\|$
for all ${\mathbf a},{\mathbf b}\in E$
with ${\mathbf a}\neq{\mathbf b}$. We saw in
Example~\ref{no fixed} that $T$ need not have a fixed point.
Show that, if $T$ has a fixed point, it is unique.

(ii) Suppose
$\|T({\mathbf a})-T({\mathbf b})\|>\|{\mathbf a}-{\mathbf b}\|$
for all ${\mathbf a},{\mathbf b}\in E$
with  ${\mathbf a}\neq{\mathbf b}$.
In Question~\ref{4.16} it is shown that $T$ need not have a fixed
point. Show
that, if $T$ has a fixed point, it is unique.

(iii) Suppose
$\|T({\mathbf a})-T({\mathbf b})\|=\|{\mathbf a}-{\mathbf b}\|$
for all ${\mathbf a},{\mathbf b}\in E$.
Show that $T$ need not have a fixed point.
and that, if $T$ has a fixed point, it need not be unique.

(iv) Suppose now that $E$ is non-empty, closed and bounded and
\[\|T({\mathbf a})-T({\mathbf b})\|<\|{\mathbf a}-{\mathbf b}\|\]
for all ${\mathbf a},{\mathbf b}\in E$
with ${\mathbf a}\neq{\mathbf b}$.
By considering $\inf_{{mathbf x}\in E}\|{\mathbf x}-T({mathbf x})\|$,
or otherwise
show that $T$ has a fixed point.


\end{question}
\begin{question}
From time to time numerical analysts mention
the \emph{spectral radius}. It forms no part of any
of non-optional 1B courses but the reader may be interested
to see what it is.

(i) Give an example of a linear map
$\beta:{\mathbb R}^{m}\rightarrow{\mathbb R}^{m}$
such that $\beta^{m-1}\neq {\mathbf 0}$ but
$\beta^{m}={\mathbf 0}$.

(ii) Let $\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{m}$
be linear.
If $n=jk+r$ explain why
\[\|\alpha^{jk+r}\|\leq \|\alpha^{j}\|^{k}\|\alpha\|^{r}.\]

(iii) Continuing with the hypotheses of (ii), show that
$\Delta=\inf_{n}\|\alpha^{n}\|^{1/n}$ is well defined
and, by using the result of (ii), or otherwise, that
\[\|\alpha^{n}\|^{1/n}\rightarrow \Delta.\]
We call $\Delta$ the spectral radius of $\alpha$
and write $\rho(\alpha)=\Delta$.

(iv) If $\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{m}$
is diagonalisable show that
\[\rho(\alpha)=\max\{|\lambda|:\lambda\ \text{an eigenvalue
of $\alpha$}\}.\]

(v) Give an example of linear maps
$\alpha,\beta:{\mathbb R}^{2}\rightarrow{\mathbb R}^{2}$
such that $\rho(\alpha)=\rho(\beta)=0$ but
$\rho(\alpha+\beta)=1$.

(vi) Recalling Question~\ref{3.11} of the third sheet, show that
if $\rho(\iota-\gamma)<1$ then $\gamma$ is invertible.


\end{question}

\vspace{1\baselineskip}

The remaining questions are included for general interest
and not for relevance to the syllabus or to passing Tripos exams.

\begin{question}\label{4.12}
We work in ${\mathbb R}^{m}$, 
$\|{\mathbf x}-{\mathbf y}\|$ will represent the
usual Euclidean distance between ${\mathbf x}$ and
${\mathbf y}$.

(i) If $K$ is a closed non-empty bounded set and ${\mathbf x}$
is any point, show that there exists a point
${\mathbf k}'\in K$ such that  
\[\|{\mathbf x}-{\mathbf k}\|\geq \|{\mathbf x}-{\mathbf k}'\|\]
for all ${\mathbf k}\in K$.  Is ${\mathbf k}'$ 
necessarily unique?

(ii) If $E$ is a non-empty closed set and ${\mathbf x}$
is any point, show that there exists a point
${\mathbf e}'\in E$ such that  
\[\|{\mathbf x}-{\mathbf e}\|\geq \|{\mathbf x}-{\mathbf e}'\|\]
for all ${\mathbf e}\in E$. 
We write $d({\mathbf x},E)=\|{\mathbf x}-{\mathbf e}'\|.$

(iii) With the notation of (ii) show that
\[d({\mathbf x},E)+\|{\mathbf x}-{\mathbf y}\|
\geq d({\mathbf y},E).\]
Show that $d(\ ,E):{\mathbb R}^{m}\rightarrow{\mathbb R}$
is continuous.

(iv) If $E$ is closed and non-empty and $K$ closed,
bounded
and non-empty show that
there exist ${\mathbf e}'\in E$ and ${\mathbf k}'\in K$
such that
\[\|{\mathbf e}-{\mathbf k}\|\geq \|{\mathbf e}'-{\mathbf k}'\|.\]
Would this result be true if we only assumed $E$ and $K$
closed and non-empty. 

\end{question}

\begin{question}\label{4.13}
(i)  Show that if
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuous periodic function with period $2\pi$
such that $\hat{f}(n)=0$ for all $n$
then
\[\int_{-\pi}^{\pi}(1-\epsilon_{1}+\epsilon_{2}\cos t)^{N}f(t)\,dt=0\]
for all $N$.

(ii) Show that, given $\delta>0$ we can find 
$\epsilon_{1},\ \epsilon_{2}>0$
and an $\eta>0$ such that
\begin{align*}
(1-\epsilon_{1}+\epsilon_{2}\cos t)^{N}\rightarrow\infty&\ \text{uniformly
for $|t|\leq\eta$}\\
(1-\epsilon_{1}+\epsilon_{2}\cos t)^{N}\rightarrow 0&\ \text{uniformly
for $\delta\leq |t|\leq\pi$}
\end{align*}
as $N\rightarrow\infty$.

(iii) Show that if $f:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuous periodic function with period $2\pi$
such that $f(0)$ is real and $f(0)>0$ 
we can find $\epsilon_{1},\ \epsilon_{2}>0$ such that
\[\Re\int_{-\pi}^{\pi}(1-\epsilon_{1}+\epsilon_{2}\cos t)^{N}f(t)\,dt
\rightarrow\infty\]
as $N\rightarrow\infty$.

(iv) Show that if
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuous periodic function with period $2\pi$
such that $\hat{f}(n)=0$ for all $n$
then $f(t)=0$ for all $t$.

(v) By using Question~\ref{4.3}, or otherwise, show that if
if
$F:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuous periodic function with period $2\pi$
such that $\sum |\hat{F}(n)|$ converges then
\[F(t)=\sum_{n=-\infty}^{\infty}\hat{F}(n)\exp (int).\]

\end{question}
\begin{question}
(i)  Suppose that
$g:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuous periodic function with period $2\pi$.
Show that
\[\frac{1}{2\pi}\int_{0}^{2\pi}
\left|g(t)-\sum_{r=-N}^{N}\hat{g}(r)\exp(irt)\right|^{2}
\,dt
=\frac{1}{2\pi}\int_{0}^{2\pi}|g(t)|^{2}\,dt
-\sum_{r=-N}^{N}|\hat{g}(r)|^{2}.\]
Deduce that
\[\sum_{r=-N}^{N}|\hat{g}(r)|^{2}
\leq\frac{1}{2\pi}\int_{0}^{2\pi}|g(t)|^{2}\,dt.\]
(This is a version of Bessel's inequality.)

(ii) Now suppose that
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuously differentiable
periodic function with period $2\pi$.
If $f$ has derivative $g$, obtain a simple
relation between $\hat{f}(n)$ and $\hat{g}(n)$.
By applying the Cauchy-Schwarz inequality
to $\sum_{n\neq 0,\ |n|\leq N} |n||\hat{g}(n)|$ show that
\[\sum _{n\neq 0,\ |n|\leq N} |\hat{f}(n)|
\leq A \frac{1}{2\pi}\int_{0}^{2\pi}|g(t)|^{2}\,dt,\]
for some constant $A$.
Conclude that  $\sum |\hat{f}(n)|$ converges. 

(iii) Deduce, using Question~\ref{4.13}, that if
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuously differentiable
periodic function with period $2\pi$
then
\[f(t)=\sum_{n=-\infty}^{\infty}\hat{f}(n)\exp (int).\]

\end{question}
\begin{question}
We continue with the ideas of Question~\ref{4.12}.
From now on all sets will belong to the collection
$\mathcal{K}$
of closed bounded non-empty subsets of ${\mathbb R}^{m}$.
Define
\[d(E,F)=\sup_{e\in E}d(e,F)+\sup_{f\in F}d(f,E).\]
Show that $d$ is a metric on $\mathcal{K}$.

Show also that $d$ is a complete metric.

\end{question}
\begin{question}\label{4.16} If $(X,d)$ is a complete metric
space and $T:X\rightarrow X$ is a surjective
map such that
\[d(Tx,Ty)\geq Kd(x,y)\]
for all $x,y\in X$ and some $K>1$ show that
$T$ has a unique fixed point.


By considering the map 
$T:{\mathbb R}\rightarrow{\mathbb R}$ defined by
$T(x)=1+4n+2x$ for $0\leq x<1$ and $n$ an integer,
or otherwise show that the condition $T$ surjective
can not be dropped.
\end{question}
\begin{question}[A continuous nowhere differentiable function]
The following construction is due to Van der Waerden.

(i) Sketch the graph of the function $g$ given by the condition
\[g(x+k)=|x|\ \ \ \ \mbox{if $k$ is any integer and
$-1/2<x\leq 1/2$.}\]

(ii) Let $g_{n}(t)=2^{-n}g(8^{n}t)$. Sketch the graphs of
$g_{1}$, $g_{2}$ and $g_{3}$.

(iii) Let $f_{n}(t)=g_{1}(t)+g_{2}(t)+\ldots+g_{n}(t)$. Sketch the
graphs of $f_{1}$, $f_{2}$ and $f_{3}$.

(iv) Show that $f_{n}$ converges uniformly to a continuous function 
$f$.

(v) If $n$ is a positive integer and $r$ an odd integer give
an expression for $f(r2^{-n})$. Show that when $n$ is large
then $2^{n}|f((r+2)2^{-n})-f(r2^{-n})|$ is large.
Conclude that
if $r2^{-n}<t<(r+2)2^{-n}$ then
\[\max\left(\left|\frac{f(t)-f(r2^{-n})}{t-r2^{-n}}\right|,
\left|\frac{f((r+2)2^{-n})-f(t)}{(r+2)2^{-n}-t}\right|\right).\]

(vi) By developing the ideas of part (v) show that $f$ is
nowhere differentiable.
\end{question}
\begin{center}
{\bf Final Note To Supervisors}
\end{center}
Let me reiterate my request for corrections and improvements
\emph{particularly to the exercises}. The easiest
path for me is e-mail. My e-mail address is \verb+twk@dpmms+.
I have to express my gratitude to Drs Barden and Pinch
for finding a multitude of errors but I am sure that once the first
veil of error has been lifted a further veil of deeper errors
will appear.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limits and Convergence}

%\subsection{Review from N\&S}

$a_n\in\mathbb{R}$ is a sequence of real numbers.

\begin{definition}
We say $a_n\to a\in \R$ as $n\to \infty$ if given $\varepsilon>0$, $\exists N$ such that $|a_n-a|<\varepsilon$ for all $n\geq N$.
\end{definition}

\centertexdraw{
    
    \def\bdot {\fcir f:0 r:0.02 }
    
    \drawdim in
    \linewd 0.01 \setgray 0
    
    \move (-1 0) \lvec(1 0) 
   
    \move (0.2 0)\bdot
    \move (0 0)\bdot

    \htext (-0.5 -0.06){$($}
    \htext (0.5 -0.06){$)$}

    \htext (0 -0.1){$a$}
    \htext (0.2 0.05){$a_n$}
    \htext (0.5 0.1){$a+\varepsilon$}
    \htext (-0.6 0.1){$a-\varepsilon$}
	
    \move(0 0.2)
}

\begin{remark}
$N=N(\varepsilon)$.
\end{remark}

\begin{definition}
We say that $a_n$ is increasing if $a_{n+1}\geq a_n$ for all $n$, is decreasing if $a_{n+1}\leq a_n$ for all $n$.

strictly increasing: $a_{n+1}>a_n$ for all $n$ and strictly decreasing: $a_{n+1}< a_n$ for all $n$.
\end{definition}

monotonic is either increasing or decreasing.

{\flushleft\bf Fundamental Axiom of the real numbers}:

Suppose $a_n\in\mathbb{R}$ for all $n$ and there is $A\in\mathbb{R}$ such that $a_n\leq A$ for all $n$ and $a_n$ is increasing. Then there exists $a\in\mathbb{R}$ such that $a_n\to a$ as $n\to\infty$ (an increasing sequence bounded above converge)

\begin{remark}

1) We could have phrased the Axiom with decreasing sequences bounded below.

2) The Axiom is equivalent to the fact that every non-empty set of real numbers bounded above has a supremum (or least upper bound).
\end{remark}

\begin{lemma}\label{lem:basic_con}
(i) The limit is unique, that is, if $a_n\to a$ and $a_n\to b$, then $a=b$.

(ii) If $a_n\to a$, $n\to\infty$ and $n_1<n_2<\dots$, then $a_{n_j}\to a$ as $j\to \infty$. (subsequences converge to the same limit)

(iii) $a_n=c,\forall n$, then $a_n\to c$.

(iv) If $a_n\to a$ and $b_n\to b$, then $a_n+b_n\to a+b$

(v) If $a_n\to a$ and $b_n\to b$, then $a_nb_n\to ab$

(vi) If $a_n\to a, a_n\neq 0,\forall n$ and $a\neq 0$, then $\frac{1}{a_n}\to \frac{1}{a}$

(vii) If $a_n\leq A,\forall n$ and $a_n\to a$, then $a\leq A$

\end{lemma}
\begin{proof}[{\bf Proof}]
We will do just (i), (ii) and (v). The rest is an exercise.

{\flushleft (i)} $a_n\to a$ means given $\varepsilon>0$, $\exists N_1$ for all $n\geq N_1$ we have $|a_n-a|<\varepsilon$. Similarly, $a_n\to b$ means given $\varepsilon>0$, $\exists N_2$ for all $n\geq N_2$ we have $|a_n-b|<\varepsilon$. Thus, we have
\begin{equation*}
\underbrace{|a-b| \leq |a-a_n| + |b-a_n|}_{\text{triangle inequality}}< 2\varepsilon \text{ for }n\geq \max\{N_1,N_2\}
\end{equation*}

Hence we have $|a-b|<2\varepsilon$. If $a\neq b$, just take $\varepsilon=\frac{|a-b|}{3}$, then $|a-b|<\frac{2|a-b|}{3}$ which is assurd. Thua, $a=b$.

{\flushleft (ii)} $a_n\to a$ means given $\varepsilon>0$, $\exists N$ s.t. $|a_n-a|<\varepsilon, \forall n\geq N$. $a_{n_j}$, $n_j\geq j$, thus we have
\begin{equation*}
|a_{n_j}-a| < \varepsilon,\forall j\geq N
\end{equation*}
that is, $a_{n_j}\to a$ as $j\to\infty$.

{\flushleft (v)} $a_n\to a$ given $\varepsilon>0$, $\exists N_1$ s.t. $|a_n-a|<\varepsilon, \forall n\geq N_1$. $b_n\to b$ given $\varepsilon>0$, $\exists N_2$ s.t. $|b_n-b|<\varepsilon, \forall n\geq N_2$.
\begin{equation}
\underbrace{|a_nb_n-ab| \leq |a_nb_n-a_nb| + |a_nb-ab|}_{\text{triangle inequality}} = |a_n||b_n-b| + |b||a_n-a|\nonumber
\end{equation}

We have $|a_n|\leq |a_n-a|+|a|\leq 1+|a|,\forall n\geq N_1(1) \ \ (\varepsilon=1)$. Thus, we have
\begin{equation*}
|a_nb_n-ab| \leq (1+|a|)\varepsilon + |b|\varepsilon = \varepsilon(|a|+|b|+1), \ \forall n\geq \max\{N_1(1),N_1(\varepsilon), N_2(\varepsilon)\}.
\end{equation*} 
\end{proof}

\begin{lemma}
$\frac 1n\to 0$ as $n\to \infty$
\end{lemma}

\begin{proof}[{\bf Proof}]
The sequence $a_n=\frac 1n$ is decreasing and bounded below, so the $a_n\to a\in\mathbb{R}$ by the fundamental axiom.

We claim $a=0$, thus $\frac{1}{2n}=\frac 12 \frac1n\to \frac 12a$ by Lemma \ref{lem:basic_con} (v). But $\frac{1}{2n}$ is also a subsequence of $\frac 1n$, so by Lemma \ref{lem:basic_con} (ii), $\frac{1}{2n}\to a$, by uniqueness of the limit (Lemma \ref{lem:basic_con} (i)), $a=\frac 12a\ \Rightarrow \ a=0$.
\end{proof}

\begin{remark}
If we were considering instead subsequences of complex numbers, $a_n\in\mathbb{C}$, we can give essentially the same definition of limit. $a_n\to a\in\mathbb{C}$ as $n\to \infty$, if given $\varepsilon>0,\exists N$ s.t. 
\begin{equation*}
|a_n-a|<\varepsilon,\forall n\geq N.
\end{equation*}
here $|\cdot|$ stands for the modulus of a complex number.
\end{remark}

\centertexdraw{
    
    \def\bdot {\fcir f:0 r:0.02 }
    
    \drawdim in
    \linewd 0.01 \setgray 0
    
    \move (0 0) \lcir r:0.4 
    \move (0 0) \lvec(-0.2 -0.3426)
    \move (0 0)\bdot
    
    \htext (-0.1 -0.25){$\varepsilon$}
    
    \htext (0 -0.1){$a$}
    \htext (-0.3 -0.05){$a_n$}
}

All properties of Lemma \ref{lem:basic_con} carry to $\mathbb{C}$ except the last one (we have \underline{no order} in $\mathbb{C}$).

\begin{theorem}[Bolzano-Weierstrass Theorem\index{Bolzano-Weierstrass Theorem}]
If $x_n\in\mathbb{R}$ and there exists $K$ s.t. $|x_n|<K,\forall n$, then we can find $n_1<n_2<\dots$ and $x\in\mathbb{R}$ s.t. $x_{n_j}\to x$ as $j\to \infty$. (every bounded sequence has a convergent subsequence.)
\end{theorem}

\begin{remark}
We say nothing about uniqueness of $x$. i.e. $x_n=(-1)^n\ \Rightarrow \ x_{2n-1}=-1,x_{2n}=1$.
\end{remark}

\begin{proof}[{\bf Proof}]
We see $[a_n,b_n] = [-K,K]$, let $c=\frac{a_0+b_0}{2}$ (mid-point), thus, either

(i) $x_n\in[a_0,c]$ for infinitely many values of $n$;

(i) $x_n\in[c,b_0]$ for infinitely many values of $n$.

In case (i), set $a_1=a_0, b_1=c$; 

In case (ii), set $a_1=c,b_1=b_0$. 

Proceed inductively to obtain sequences $a_n$ and $b_n$ s.t. the following holds ($m\leq n$)

(i) $a_m\leq a_n\leq b_n\leq b_m$; ($a_n$ is a bounded increasing sequence, $b_n$ is a bounded decreasing sequence.)

(ii) $x_m\in[a_n,b_n]$ for infinitely many values of $m$;

(iii) $b_n-a_n=\frac{b_{n-1}-a_{n-1}}{2}$.

By the Fundamental Axiom, $a_n\to a$ and $b_n\to b$. By property (iii) passing to the limit,
\begin{equation*}
b-a = \frac{b-a}{2}\ \Rightarrow \ a=b
\end{equation*}

Having selecting $n_j$ s.t. $x_{n_j}\in[a_j,b_j]$, select $n_{j+1}>n_j$ s.t. $x_{n_{j+1}}\in[a_{j+1},b_{j+1}]$, we can always do this because $x_m\in[a_{j+1},b_{j+1}]$ for infinitely many values of $m$. In other words, 
\begin{equation*}
\left\{\begin{array}{c}
a_j\leq x_{n_j} \leq b_j, \forall j\\
\\
a_j\to a,\ b_j\to b=a
\end{array}\right.\ \Rightarrow \ x_{n_j}\to a.
\end{equation*}
\end{proof}

\begin{remark}
This argument (or method) is called bisection method or 'lion hunting'.
\end{remark}

%\subsection{Cauchy sequences}

\begin{definition}
We say that a sequence $a_n\in \mathbb{R}$ is a \underline{cauchy sequence}  if given $\varepsilon>0$, $\exists N(=N(\varepsilon))$ s.t. for any $n,m\geq N$,
\begin{equation*}
|a_m-a_n|<\varepsilon.
\end{equation*}
\end{definition}

\begin{lemma}
A convergent sequence is a Cauchy sequence.
\end{lemma}
\begin{proof}[{\bf Proof}]
$a_n\in\mathbb{R}, \ a_n\to a$ means that given $\frac{\varepsilon}{2}>0$, $\exists N$ s.t. $\forall n\geq N$, $|a_n-a|<\frac{\varepsilon}{2}$. Thus,
\begin{equation*}
|a_m-a_n|\leq |a_m-a| + |a_n-a|
\end{equation*}
so if $m,n\geq N$, we have 
\begin{equation*}
|a_m-a_n| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.
\end{equation*}
\end{proof}

As an application of the Bolzano-Weiastrass theorem, we now show that the converse is true.

\begin{theorem}
A Cauchy sequence is convergent.
\end{theorem}
\begin{proof}[{\bf Proof}]
First, we prove that if $a_n$ is a cauchy sequence, then it is bounded. 

$a_n$ is cauchy means that given $\varepsilon>0$, $\exists N$ s.t. $\forall m,n\geq N$, $|a_n-a_m|<\varepsilon$. Chose $\varepsilon=1$, we have 
\begin{equation*}
|a_n-a_m|<1,\ \forall m,n\geq N(1)
\end{equation*}
in particular, $m=N(1)$ 
\begin{equation*}
|a_n-a_N|<1,\ \forall n\geq N(1) \ \Rightarrow \ |a_n|\leq |a_n-a_N|+|a_N|\leq 1+|a_N|,\ \forall n\geq N(1)
\end{equation*}

Take $K=\max\{1+|a_N|,|a_i|,i=1,2,\dots,N-1\}$. For this choice of $K$, $|a_n|\leq K,\ \forall n$. Now, by the Bolzano-Weiastrass theorem, $a_n$ has a convergent subsequence $a_{n_j}\to a$.

Second, we show that in fact $a_n\to a$. We have 
\begin{equation*}
|a_n-a| \leq |a_n-a_{n_j}| + |a_{n_j}-a|
\end{equation*}

Choose $n_j$ large enough such that 
\begin{equation*}
\left\{\begin{array}{l}
\text{cauchy}:\quad |a_n-a_{n_j}| < \frac{\varepsilon}{2}, \forall n\geq N\left(\frac{\varepsilon}{2}\right) \left(\Rightarrow \ n_j\geq N\left(\frac{\varepsilon}{2}\right)\right)\\
\\
a_{n_j}\to a:\quad |a_{n_j}-a| < \frac{\varepsilon}{2}
\end{array}\right.\ \Rightarrow \ |a_n- a| < \varepsilon, \forall n\geq N\left(\frac{\varepsilon}{2}\right)
\end{equation*}
\end{proof}

\begin{remark}
Thus, for sequence of real numbers, convergence and cauchy property are \underline{equivalent}. This is called the {\bf General principle of convergence}.
\end{remark}

%\subsection{Series}

We begin with some generalities.

\begin{definition}
$a_n\in\mathbb{R},\mathbb{C}$. We say that $\sum^\infty_{j=1}a_j$ converges to $S$ if the sequence of partial sum $S_N=\sum^N_{j=1}a_j$ tends to $S$ as $N\to\infty$. In that case we write $\sum^\infty_{j=1}a_j=S$. If $S_N$ does not converge, we say that $\sum^\infty_{j=1}a_j$ diverges.
\end{definition}

\begin{remark}
Any question about series is nearly a question about the sequence of partial sums.
\end{remark}

\begin{lemma}
(i) If $\sum^\infty_{j=1}a_j$ and $\sum^\infty_{j=1}b_j$ converge, then so does $\sum^\infty_{j=1}(\lambda a_j+\mu b_j)$ where $\lambda,\mu\in\mathbb{C}$.

(ii) Suppose $\exists N$ s.t. $a_j=b_j,\ \forall j\geq N$, then either $\sum^\infty_{j=1}a_j$ and $\sum^\infty_{j=1}b_j$ both converge or they both diverge. (initial terms does not matter).
\end{lemma}
\begin{proof}[{\bf Proof}]
(i) $S_N=\sum^\infty_{j=1}(\lambda a_j+\mu b_j)=\lambda \sum^\infty_{j=1}a_j + \mu \sum^\infty_{j=1}b_j=\lambda c_N+\mu d_N$. 

If $c_N\to c$ and $d_N\to d$, then clearly, $S_N\to \lambda c + \mu d$ (by Lemma \ref{lem:basic_con}).

(ii) left as an exercise.
\end{proof}

\begin{lemma}\label{lem:con_sum}
If the series $\sum^n_{j=1}a_j$ converge, then $\lim_{n\to\infty}a_n=0$.
\end{lemma}
\begin{proof}[{\bf Proof}]
$S_n=S_{n-1}+a_n$. If $S_n=\sum^n_{j=1}a_j$ converges, $S_n\to S$. Therefore, 
\begin{equation*}
a_n=S_n-S_{n-1}\to S-S=0.
\end{equation*}
\end{proof}

\begin{remark}
Converse is not true.
\end{remark}

\begin{example}
$\sum^\infty_{n=1}\frac 1n$, $a_n=\frac 1n\to 0$, but series diverge. We have
\begin{equation*}
S_{2n} = \underbrace{1+\frac 12 + \dots + \frac 1n}_{S_n} + \underbrace{\frac{1}{n+1}}_{>\frac{1}{2n}} + \underbrace{\frac{1}{n+2}}_{>\frac{1}{2n}} \dots + \frac{1}{2n} \ \Rightarrow \ S_{2n}>S_n+\frac 12
\end{equation*}

If $\sum^\infty_{n=1}\frac 1n$ converges, then $S_n\to S$, but $S_{2n}>S_n+\frac 12\ \Rightarrow \ S > S+\frac 12$, which is \underline{absurd}.
\end{example}
\vspace{4mm}

\begin{example}
(Geometric Series) $\sum^\infty_{n=0}x^n$, $S_n =1+x+x^2+\dots+x^n$, $x\in\mathbb{R}$ 
\begin{equation*}
xS_n = \underbrace{x+x^2 + \dots + x^n}_{S_n-1} + x^{n+1} = S_n -1 + x^{n+1} \ \Rightarrow \ 1-x^{n+1} = S_n(1-x).
\end{equation*}

If $x\neq 1, S_n = \frac{1-x^{n+1}}{1-x}=\frac{1}{1-x}-\frac{x^{n+1}}{1-x}$. (If $x=1$, the series clearly diverge.)

Applying Lemma \ref{lem:con_sum}, we have $\sum^n_{i=0}x^i\text{ converges }\ \Rightarrow \ x^n\to 0 \ \Leftrightarrow \ |x|^n\to 0 \ \Leftrightarrow \ |x|<1$. Also, we can see that if $|x|<1$, $S_n\to\frac{1}{1-x}= \sum^\infty_{n=0}x^n$. Thus, $\sum^n_{i=0}x^i$ converges iff $|x|<1$.
\end{example}

{\flushleft\bf Seires of positive term (non-negative)}

For $a_n\geq 0$, $\sum a_n$ is convergent if $S_n$ is bounded above. This is the most powerful test comes out straight from the fundamental axiom.

\begin{theorem}[The comparison test]
Suppose $0\leq b_n\leq a_n,\ \forall n$, then if $\sum a_n$ converges, so does $\sum b_n$. 

($\sum b_n$ divergent $\Rightarrow$ $\sum a_n$ divergent). 
\end{theorem}
\begin{proof}[{\bf Proof}]
Let $S_N=\sum^N_{n=1}a_n,\ T_N=\sum^N_{n=1}b_n$. Thus, $S_N=S_{N-1}+a_N \geq S_{N-1}\ (a_n\geq 0)$. So $S_N$ ($T_N$) is an increasing sequence. 
\begin{equation*}
\left\{\begin{array}{l}
b_n\leq a_n \ \Rightarrow \ T_N\leq S_N\\
\\
\sum a_n \text{ converges } \ \Rightarrow \ S_N\to S
\end{array}\right.
\ \Rightarrow \ T_N \text{ is bounded above}\quad \underrightarrow{\text{  fundamental axiom  }}\quad  T_N \text{ has a limit}.
\end{equation*}
\end{proof}

\begin{remark}
Since initial terms do not matter for convergence in the theorem, it is enough to assume that $0\leq a_n\leq b_n,\ \forall n\geq n_0$.
\end{remark}

We will now derive two applications of the comparison test (Root test, ratio test).

\begin{theorem}[The root test / Cauchy]
If $a_n\geq 0$, suppose $(a_n)^{\frac 1n}\to a$ as $n\to\infty$, then
\begin{equation*}
\left\{\begin{array}{ll}
\sum a_n \text{ converges } \quad & a<1\\
\\
\sum a_n \text{ diverges } \quad & a>1
\end{array}\right.
\end{equation*}
\end{theorem}

\begin{remark}
If $a=1$, the test does not give any information. In fact later on, we will see examples with $a=1$ which convergent and divergent.
\end{remark}

\begin{proof}[{\bf Proof}]
Suppose $(a_n)^{\frac 1n}\to a$ with $a<1$. Take $r$ s.t. $a<r<1$. By the definition of convergence, given $\varepsilon>0$, $\exists N$ s.t. $\forall n\geq N$,
\begin{equation*}
\left|(a_n)^{\frac 1n}-a\right|<\varepsilon \ \Rightarrow \ a-\varepsilon< (a_n)^{\frac 1n} < a+\varepsilon
\end{equation*}

Take $\varepsilon=r-a$, 
\begin{equation*}
(a_n)^{\frac 1n} < r,\ \forall n\geq N \ \Rightarrow \ a_n < r^n, \ \forall n\geq N \ \Rightarrow \ \sum a_n < \sum r^n
\end{equation*}

Since the geometric series $r$ converges ($r<1$). The comparison test gives us right away that $\sum a_n$ converges.

Suppose now $(a_n)^{\frac 1n}\to a$ with $a>1$. By the definition of limit, $\exists N$ s.t. $(a_n)^{\frac 1n}>1,\ \forall n\geq N$. Thus, $a_n>1,\ \forall n\geq N$, in particular $a_n$ does not tend to zero. By Lemma \ref{lem:con_sum}, $\sum a_n$ diverges.
\end{proof}

\begin{example}
$\sum^\infty_{n=2}\left(\frac{1}{\log n}\right)^n$, $a_n=\left(\frac{1}{\log n}\right)^n\ \Rightarrow \ (a_n)^{\frac 1n}=\frac{1}{\log n}\to 0$ as $n\to\infty$.

Thus, $a=0<1$, $\sum^\infty_{n=2}\left(\frac{1}{\log n}\right)^n$ converges by the root test.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%% ratio test

\begin{theorem}[The ratio test]
Suppose $a_n> 0$, and $\frac{a_{n+1}}{a_n}\to l$ as $n\to\infty$.
\begin{equation*}
\left\{\begin{array}{ll}
\sum a_n \text{ converges } \quad & l<1\\
\\
\sum a_n \text{ diverges } \quad & l>1
\end{array}\right.
\end{equation*}
\end{theorem}

\begin{remark}
the test is inclusive if $l=1$.
\end{remark}

\begin{proof}[{\bf Proof}]
Suppose $\frac{a_{n+1}}{a_n}\to l$ with $l<1$. Take $r$ s.t. $l<r<1$. By the definition of convergence, given $\varepsilon>0$, $\exists N$ s.t. $\forall n\geq N$,
\begin{equation*}
\left|\frac{a_{n+1}}{a_n}-l\right|<\varepsilon \ \Rightarrow \ l-\varepsilon< \frac{a_{n+1}}{a_n} < l+\varepsilon
\end{equation*}

Take $\varepsilon=r-l$, thus $\forall n\geq N$
\begin{equation*}
\left\{\begin{array}{l}
\frac{a_{n+1}}{a_n} < r \\ 
a_n = \frac{a_n}{a_{n-1}}\cdot\frac{a_{n-1}}{a_{n-2}}\cdots \cdot \frac{a_{N+1}}{a_N} \cdot a_N 
\end{array}\right. \ \Rightarrow \ a_n < a_N r^{n-N}
\end{equation*}

In other words, there is a constant $K$ (independent of $n$) s.t. $a_n < K r^{n-N}, \ \forall n\geq N$. Since $r<1$, the series $\sum Kr^n$ converges and $\sum a_n$ also converges by the comparison test.

Suppose now $\frac{a_{n+1}}{a_n}\to l$ with $l>1$. By the definition of limit, $\exists N$ s.t. $\frac{a_{n+1}}{a_n}>1,\ \forall n\geq N$. This is saying that sequence $a_n$ increases after $N$. In particular, $a_n>a_N,\ \forall n\geq N$, which is saying $a_n$ does not tend to zero. By Lemma \ref{lem:con_sum}, $\sum a_n$ diverges.
\end{proof}

\begin{example}
$\sum^\infty_{n=1}\frac 1n$ diverges.

(i) root test: $\left(\frac{1}{n}\right)^{1/n}\to 1$ because $n^{1/n}\to 1$. We have 
\begin{equation*}
n^{1/n} = 1+\delta_n, \ \delta_n>0 \ \Rightarrow \ n=(1+\delta_n)^n> \binom{n}{2}\delta_n^2 = \frac{n(n-1)\delta_n^2}{2} \ \Rightarrow \ \delta_n< \sqrt{\frac{2}{n-1}} \to 0 \text{ as }n\to\infty
\end{equation*}

(ii) ratio test: $\frac{n}{n+1} \to 1$.

Thus, $a=l=1$.
\end{example}

\begin{example}
$\sum^\infty_{n=1}\frac{1}{n^2}$ converges.

(i) root test: $\left(\frac{1}{n^2}\right)^{1/n} = \left(\frac{1}{n^{1/n}}\right)^2\to 1$; (ii) ratio test: $\frac{n^2}{(n+1)^2} \to 1$. 

For this series it also holds that $a=l=1$. However, we have
\begin{equation*}
\sum^N_{n=1}\frac{1}{n^2} < 1 + \sum^N_{n=2}\frac{1}{n(n-1)} = 1+1-\frac 12 + \frac 12 - \frac 13 + \cdots + \frac{1}{N-1} - \frac{1}{N}= 2- \frac{1}{N} \to 2 \text{ as }N\to\infty 
\end{equation*}

Thus, the series $\sum^\infty_{n=2}\frac{1}{n(n-1)}$ converges, and by comparison $\sum^\infty_{n=1}\frac{1}{n^2}$ converges. This shows that for $a=1$ or $l=1$, we can not conclude anything.
\end{example}

\begin{example}
$\sum^\infty_{n=1}\frac{n^x}{2^n}, x\in\mathbb{R}$ converges.

ratio test:  
\begin{equation*}
\frac{a_{n+1}}{a_n} =\frac{(n+1)^x}{2^{n+1}}\cdot\frac{2^n}{n^x} = \frac 12 \left(\frac{n+1}{n}\right)^x \to \frac 12<1.
\end{equation*}

We conclude from the ratio test that the series converges.
\end{example}

\begin{theorem}[Cauchy's condensation test]
$a_n$ is a decreasing sequence of positive terms, then $\sum^\infty_{n=1}a_n$ converges iff $\sum^\infty_{n=1}2^na_{2^n}$ converges.
\end{theorem}

\begin{proof}[{\bf Proof}]
We have $a_{2^k}\leq a_{2^{k-1}+i}\leq a_{2^{k-1}},\ 1\leq i\leq 2^{k-1}(k\geq 1)$ since $a_n$ is decreasing. 

Suppose first that $\sum a_n$ converges. We have
\begin{equation*}
2^{n-1}a_{2^n}=\underbrace{a_{2^n} + a_{2^n} + \cdots + a_{2^n}}_{2^{n-1}\text{ terms}}\leq a_{2^{n-1}+1} + a_{2^{n-1}+2} + \cdots + a_{2^n} \Rightarrow \ 2^{n-1}a_{2^n}\leq \sum^{2^n}_{m=2^{n-1}+1} a_m 
\end{equation*}
\begin{equation*}
\Rightarrow \  \sum^N_{n=1}2^{n-1}a_{2^n}\leq \sum^N_{n=1}\sum^{2^n}_{m=2^{n-1}+1} a_m = \sum^{2^N}_{m=2}a_m \ \Rightarrow \ \sum^N_{n=1}2^na_{2^n}\leq 2\sum^{2^N}_{m=2}a_m \leq 2A
\end{equation*}
since $\sum a_n$ converges. So this (increasing) sequence of partial sum is bounded above, which implies $\sum^\infty_{n=1}2^na_{2^n}$ converges.

Suppose now conversely that $\sum^\infty_{n=1}2^na_{2^n}$ converges. Then
\begin{equation*}
\sum^{2^n}_{m=2^{n-1}+1} a_m = a_{2^{n-1}+1} + a_{2^{n-1}+2} + \cdots + a_{2^n} \leq a_{2^{n-1}} + a_{2^{n-1}} + \cdots + a_{2^{n-1}} = 2^{n-1}a_{2^{n-1}}
\end{equation*}
\begin{equation*}
\Rightarrow \ \sum^{2^n}_{m=2^{n-1}+1} a_m  \leq 2^{n-1}a_{2^{n-1}} \ \Rightarrow \  \sum^N_{n=1}\sum^{2^n}_{m=2^{n-1}+1} a_m \leq \sum^{N}_{n=1}2^{n-1}a_{2^{n-1}} \ \Rightarrow \ \sum^{2^N}_{m=2}a_m \leq \sum^N_{n=1}2^{n-1}a_{2^{n-1}}\leq B.
\end{equation*}
since $\sum 2^na_{2^n}$ converges. So this (increasing) sequence $\sum^{2^N}_{m=2}a_m$ is bounded above in $N$, which implies $\sum^{\infty}_{n=1}a_n$ converges.
\end{proof}

\begin{example}
$\sum^\infty_{n=1}\frac{1}{n^k}, k>0$. (if $k\leq 0$, $\frac{1}{n^k}$ does not tend to zero.)

$a_n=\frac{1}{n^k}$ is decreasing. Let's look at $\sum 2^na_{2^n}$. We have
\begin{equation*}
2^na_{2^n} = 2^n\frac{1}{2^{nk}} = 2^{n(1-k)} = \left(2^{1-k}\right)^n = r^n \quad(\text{geometric series})
\end{equation*}
Thus, $\sum 2^na_{2^n}$ converges iff $2^{1-k}<1 \ \Leftrightarrow \ k>1$. By the condensation test, $\sum^\infty_{n=1}\frac{1}{n^k}$ converges iff $k>1$.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}[The alternating series test]
$a_1\geq a_2\geq \dots \geq 0$ and $a_n\to 0$, then $\sum^\infty_{n=1}(-1)^{n+1}a_n$ converges.
\end{theorem}

\begin{example}
$\sum^\infty_{n=1}\frac{(-1)^n}{n}$, we have $\ a_n=\frac{1}{n}\to 0$ and $a_n$ is decreasing. Therefore, the series converges.
\end{example}

\begin{proof}[{\bf Proof}]
We have $S_n = a_1 - a_2 + \dots + (-1)^{n+1}a_n$. Also,
\begin{equation*}
\left\{\begin{array}{l}
S_{2n} = (a_1 - a_2) + (a_3 - a_4) + \dots + \underbrace{(a_{2n-1} - a_{2n})}_{\geq 0 (a_n \text{ decreasing})} \geq S_{2n-2}\\
S_{2n} = a_1 - (a_2 - a_3) - (a_4 - a_5) - \dots - (a_{2n-2} - a_{2n-1}) - a_{2n} \leq a_1.
\end{array}\right.
\end{equation*}

$S_{2n}$ is an increasing series bounded above. Now by the fundamental axiom, we have $S_{2n}\to S$. We know
\begin{equation*}
S_{2n+1} = S_{2n} + a_{2n+1} \ \Rightarrow \ S_{2n+1} \to S+0 = S, \quad (a_n\to 0). 
\end{equation*}

By the definition of convergence. $S_{2n}\to S$ means given $\varepsilon>0$, $\exists N_1,N_2 \ \forall n_1\geq N_1,n_2\geq N_2$
\begin{equation*}
|S_{2n_1} -S| < \varepsilon, \quad\quad  |S_{2n_2+1} - S| < \varepsilon. 
\end{equation*}

Thus, we take $N_3 = \max\{2N_1, 2N_2+1\}$, then $\forall n\geq N_3$, we have $|S_n -S| < \varepsilon$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Absolute convergence}

\begin{definition}
Take $a_n\in\mathbb{R},\mathbb{C}$. If $\sum|a_n|$ is convergent, then the series is called \underline{absolutely} \underline{convergent}.
\end{definition}

\begin{note}
The series $\sum^\infty_{n=1}\frac{(-1)^n}{n}$ is convergent, but not absolutely convergent.
\end{note}

\begin{theorem}\label{thm:abs_con}
If $\sum a_n$ is absolutely convergent, then it is convergent.
\end{theorem}

\begin{proof}[{\bf Proof}]
Suppose first $a_n\in\mathbb{R}$. Introduce 
\begin{equation*}
u_n=\frac{|a_n|+a_n}{2},\quad v_n = \frac{|a_n|-a_n}{2}
\end{equation*}
where $u_n,v_n\geq 0$ and $a_n = u_n - v_n, |a_n| = u_n + v_n\geq u_n,v_n$. By the comparison test and Lemma \ref{lem:basic_con} (iv), we have 
\begin{equation*}
\sum|a_n| \text{ converges } \ \Rightarrow \ \sum u_n,\ \sum v_n \text{ converges } \ \Rightarrow \ \sum a_n \text{ converges }. 
\end{equation*}

If $a_n\in\mathbb{C}$, we set $a_n=x_n+iy_n$ where $x_n,y_n\in\mathbb{R}$. By comparision test, we have 
\begin{equation*}
\left\{\begin{array}{l}
|x_n|,|y_n| \leq |a_n| = \sqrt{x_n^2+y_n^2} \\
\\
\sum|a_n| \text{ converges }
\end{array}\right.\ \Rightarrow \ \sum|x_n|,\ \sum|y_n| \text{ converges } 
\end{equation*}
\be
\underrightarrow{\ x_n,y_n\in\mathbb{R}\ } \ \sum x_n,\ \sum y_n \text{ converges.}\quad\quad\quad\quad
\ee

Since $a_n=x_n+iy_n$, we have $\sum a_n $ converges by Lemma \ref{lem:basic_con} (iv).
\end{proof}

\begin{remark}
There is an alternative 'quick' proof of this using Cauchy sequences. With $S_n = \sum^n_{i=1}a_i,\ T_n = \sum^n_{i=1}|a_i|$ and $m\geq 0$, we have $\exists N$ such that
\begin{equation*}
|S_{n+m}-S_n| = |\sum^{n+m}_{i=n+1}a_i| \leq \sum^{n+m}_{i=n+1}|a_n| = T_{n+m} - T_n < \varepsilon,\ \forall n\geq N 
\end{equation*}
since $T_n$ is Cauchy sequence $\Leftrightarrow\ T_n$ converges. Hence, $S_n$ is Cauchy, which implies that $S_n$ converges.
\end{remark}

\begin{definition}
If $\sum a_n$ converges, but $\sum|a_n|$ does not converge. We say that the series is \underline{conditionally} \underline{convergent}.
\end{definition}

\begin{theorem}
If $\sum a_n$ is absolutely convergent, then any series consisting of the same terms in any order (i.e. a rearrangement) has the same sum.
\end{theorem}

\begin{proof}[{\bf Proof}]
We will prove it for $a_n\in\mathbb{R}$ and leave the extension to $\mathbb{C}$ as an exercise.

Let $\sum a_n'$ be a rearrangement of $\sum a_n$ and $S_n = \sum^n_{i=1}a_i$, $S = \sum^\infty_{i=1}a_i$, $T_n = \sum^n_{i=1}a_i'$. Suppose fixed $a_n\geq 0$, then given $n$, $\exists m$ s.t. $S_m$ contains every term in $T_n$, thus $T_n\leq S_m \leq S,\ (a_n\geq 0)$. By the fundamental axiom, $T_n\to T\leq S$. By symmetry, we have $S\leq T \ \Rightarrow\ S=T$.

If $a_n$ has any sign, consider $u_n$ and $v_n$ from the proof of Theorem \ref{thm:abs_con} and $\sum a_n'$, $\sum u_n'$, $\sum v_n'$. Since $\sum |a_n|$ converges, both $\sum u_n$ and $\sum v_n$ converge. With the fact that $u_n,v_n\geq 0$, we have $\sum u_n$ ($\sum v_n$) and $\sum u_n'$ ($\sum v_n'$) converge to the same limit. 

Hence $a_n = u_n - v_n$ and $a_n' = u_n' - v_n'$ converge to the same limit.
\end{proof}

\begin{example}
Rearrangement cases with the different limits. 

$S_n = \sum \frac{(-1)^{n+1}}{n} \to 1-\frac 12 + \frac 13 - \frac 14 + \dots = \log (1+1) = \log 2$ (not absolutely convergent).

$T_n = \sum \left(\frac{1}{4n-3} + \frac{1}{4n-1}-\frac{1}{2n}\right) \to \left(1 + \frac 13 - \frac 12\right) + \left(\frac 15 + \frac 17 - \frac 14\right)  + \dots $ (absolutely convergent), which is a rearrangement of $\frac{(-1)^{n+1}}{n}$. However, we have
\begin{eqnarray*}
T_n & = & \sum \left(\frac{1}{4n-3} + \frac{1}{4n-1}-\frac{1}{4n-2} - \frac{1}{4n}\right) + \sum \left(\frac{1}{4n-2} + \frac{1}{4n}-\frac{1}{2n}\right) \\
& = & \sum \frac{(-1)^{n+1}}{n} + \sum \left(\frac{1}{4n-2} - \frac{1}{4n}\right) \\
& = & \sum \frac{(-1)^{n+1}}{n} + \frac 12\sum \left(\frac{1}{2n-1} - \frac{1}{2n}\right) \\
& = & \sum \frac{(-1)^{n+1}}{n} + \frac 12\sum \frac{(-1)^{n+1}}{n} = \frac 32\sum \frac{(-1)^{n+1}}{n} = \frac 32\log 2.
\end{eqnarray*}
\end{example}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Continuity}

%\subsection{Definition}

Define a function $f:E\mapsto \C$, $E\subset \C$ is non-empty subset. (also applies of course $f: E\subset \R\to\R$). Usually $E$ will be some intervals.

We check function $f:\R\mapsto\R$
\be f(x)=\left\{\ba {cl} 1 & x\geq 0\\ 0 & x<0
\ea \right. \ee
Thus, $f(0)=1$ is not continuous at 0 since $-\frac 1n\to 0,\ f\lob-\frac 1n\rob=0\nrightarrow 1$.

\begin{definition}\label{conti_def_1}
$a\in E$. We say $f$ is continuous at $a$ if given any sequence $z_n\in E$ s.t. $z_n\to a$, then $f(z_n)\to f(a)$.
\end{definition}

\begin{definition}\label{conti_def_2}
$a\in E$. We say $f$ is continuous at $a$ if given $\ve>0$, then $\exists \delta>0$, s.t. if $z\in E$ and $|z-a|<\delta$, then $|f(z)- f(a)|<\ve$.
\end{definition}

We now prove that Definition \ref{conti_def_1} $\Leftrightarrow$ Definition \ref{conti_def_2}.

\begin{proof}[{\bf Proof}]
Definition \ref{conti_def_2} $\Rightarrow$ Definition \ref{conti_def_1}:

We know tha given $\ve>0$, $\exists \delta>0$ s.t. for $z_n\in E$ and $|z_n-a|<\delta$, then $|f(z_n)- f(a)|<\ve$. Since $z_n\to a$, $\exists N$ s.t. $\forall n\geq N$, $|z_n-a|<\delta$. Thus, $|f(z_n)- f(a)|<\ve, \forall \ve>0$ implies $f(z_n)\to f(a)$.

Definition \ref{conti_def_1} $\Rightarrow$ Definition \ref{conti_def_2}:

Suppose Definition \ref{conti_def_2} is not true, $\exists \ve>0$ s.t. $\forall \delta>0$ we can find $z\in E$ with $|z-a|<\delta$ and $|f(z)- f(a)|\geq\ve$. Choose $\delta_n=\frac 1n$, thus we can find $z_n\in E$ with $|z_n-a|<\frac 1n$ (which implies $z_n \to a$) and $|f(z_n)- f(a)|\geq\ve$. So $f(z_n)$ does not tend to $f(a)$, which contradicts Defintion \ref{conti_def_1}.
\end{proof}

\begin{proposition}\label{pro:con}
$f,g:E\mapsto \C$ and $a\in E$ $f$ and $g$ are continuous at $a$, then $f(z)+g(z)$, $f(z)g(z)$, $\lm f(z)$ for any constant $\lm$, are also continuous at $a$. In addition, if $f(z)\neq 0,\forall z\in E$, then $\frac 1f$ is also continuous at $a$.
\end{proposition}

\begin{proof}[{\bf Proof}]
This is a direct consequence of Definition \ref{conti_def_1} and Lemma \ref{lem:basic_con} (about sequences). For example, to show that $f(z)+g(z)$ is continuous at $a$, we take $z_n\in E$ s.t. $z_n\to a$. Since $f$ and $g$ are continuous at $a$, $f(z_n)\to f(a)$ and $g(z_n)\to g(a)$. Then use Lemma \ref{lem:basic_con}, $f(z_n)+g(z_n)\to f(a) + g(a)$. That is, $f+g$ continuous at $a$. Similarly with the other claims.
\end{proof}

\begin{remark}
$f(z)=z$ is clearly continuous. By Proposition \ref{pro:con}, any ploynomial is continuous at every point of $\C$.

Quotent ploynomials (rational functions) are continuous at every point where the denominator does not vanish.

We say that $f$ is continuous in $E$ if it is continuous at any point in $E$.
\end{remark}

\begin{theorem}\label{thm:composition}
Let $f:A\mapsto \C$ and $g:B\mapsto \C$ be two functions s.t. $f(A)\subset B$ ($A$ and $B$ are subsets of $\C$). Suppose $f$ is continuous at $a\in A$ and $g$ is continuous at $f(a)$, thus $g\circ f:A\mapsto \C$ is continuous at $a$.
\end{theorem}

\begin{proof}[{\bf Proof}]
Take $z_n\in A$ with $z_n\to a$, RTP: $g(f(z_n))\to g(f(a))$. 

Since $f$ is continuous at $a$, $f(z_n)\to f(a)$. Call $y_n=f(z_n)$ and since $g$ is continuous at $f(a)$, $g(y_n)\to g(f(a))$.
\end{proof}

\begin{remark}
Of course, one can prove this with Definition \ref{conti_def_2}.
\end{remark}

\begin{example}
(1) Assume $\sin x$ is continuous
\be
f(x) = \left\{
\ba{cl}
\sin\lob\frac 1x\rob & x\neq 0\\
0 & x = 0
\ea \right.
\ee

At $x\neq 0$, $f(x)$ is continuous by Proposition \ref{pro:con} and Theorem \ref{thm:composition}.

However, $f$ is not continuous at 0. Take $x_n=\frac{1}{\lob 2n+\frac 12\rob \pi}\to 0$ and $f(x_n)=1\neq 0 = f(0)$.

(2) We know $\sin\lob\frac 1x\rob \leq 1$.
\be
f(x) = \left\{
\ba{cl}
x\sin\lob\frac 1x\rob & x\neq 0\\
0 & x = 0
\ea \right.
\ee

For $x\neq 0$, $f(x)$ is continuous. Let $x_n\to 0$, so 
\be
|f(x_n)-f(0)|\leq |f(x_n)|\leq |x_n|\to 0 \ \Rightarrow f(x_n)\to 0=f(0)
\ee
So the function is continuous at 0.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Limits of functions}

Assume $f:E\subset \C\mapsto\C$, we would like to make sense $\lim_{z\to a}f(z)$ in case in which $a$ may not be in $E$. For example, $\lim_{z\to 0}\frac{\sin z}{z}$.

\begin{definition}
$E\subset \C, f:E\mapsto \C$. Take $a\in \C$ and assume that there exist a sequence $z_n\in E$, $z_n\neq a$, $z_n\to a$. (Note: the point $a$ may be in $E$, but need not be in $E$) We say that $\lim_{z\to a}f(z)=l$ (or $f(z)\to l$ as $z\to a$) if given $\ve>0$, $\exists \delta>0$ s.t. whenever $z\in E$ and $0<|z-a|<\delta$, then $|f(z)-l|<\ve$.
\end{definition}

\begin{remark}
(1) $\lim_{z\to a}f(z)=l$ iff for every sequence, $z_n\in E$, $z_n\neq a$, $z_n\to a$, then $f(z)\to l$. (Proof of this is exactly as the proof of Definition \ref{conti_def_1} $\Leftrightarrow$ Definition \ref{conti_def_2})

(2) If $a\in E$ then $\lim_{z\to a}f(z)=f(a)$ iff f is continuous at $a$. (Nothing to prove really: straight from the definitions)
\end{remark}

The limit enjoys the properties which one would expect

(1) The limit is unique;

(2) If $f(z)\to A$ as $z\to a$ and $g(z)\to B$ as $z\to a$, then 
\bea
& & f(z)+g(z)\to A+B \nonumber\\
& & f(z)g(z)\to AB \nonumber\\
& & f(z)/g(z)\to A/B \text{ if }B\neq 0
\eea

\begin{theorem}[The intermediate value theorem]
$f:[a,b]\mapsto \R$ continuous, and $f(a)\neq f(b)$, then $f$ takes every value which lies between $f(a)$ and $f(b)$.
\end{theorem}

\begin{proof}[{\bf Proof}]
Without loss of generality, we assume $f(a)<f(b)$. Take $f(a)<\eta<f(b),\ \forall \eta$ and $S=\{x\in[a,b]: f(x)<\eta\}$. $S\neq \emptyset$ because $a\in S$ and $S$ is bounded above (by $b$ in fact). Thus $S$ has a supremum $c$, we need to prove $f(c)=\eta$.

\centertexdraw{
    
    \def\bdot {\fcir f:0 r:0.02 }
    
    \drawdim in
    \linewd 0.01 \setgray 0
    
    \move (-1 0) \lvec(1 0) 
   
    \move (-0.4 0)\bdot
    \move (0 0)\bdot

    \htext (-0.8 -0.06){$[$}
    \htext (0.8 -0.06){$]$}

    \htext (-0.8 -0.2){$a$}
    \htext (0.8 -0.2){$b$}
    \htext (0 -0.2){$c$}
    \htext (-0.6 -0.2){$c-1/n$}
       
    \move(0, 0.2)
    \move(0, -0.3)
}


Given $n$ a positive integer, $c-1/n$ is not an upper bound for $S$ (otherwise it would contradict the definition of supremum (least upper bound)). Then $\exists x_n\in S$ s.t. $x_n>c-1/n$. 

Thus, $c-1/n<x_n\leq c\ \Rightarrow \ x_n\to c$ as $n\to \infty$. By continuity of $f$, $f(x_n)\to f(c)$. Since $x_n\in S\ \Rightarrow \ f(x_n)<\eta$, we have $f(c)\leq \eta$.

Note $c\neq b$, otherwise $f(c)=f(b)<\eta$ but $\eta<f(b)$. Thus, $\forall n$ sufficiently large, $c+1/n\in [a,b]$ and $c+1/n\to c$. Again by continuity of $f$, $f(c+1/n)\to f(c)$. Since $c+1/n>c\ \Rightarrow \ c+1/n\notin S$, $f(c+1/n)\geq \eta$. Thus, $f(c)\geq \eta$. So we $f(c)=\eta$.
\end{proof}

\begin{example}
Existence of $N$-root of a positive number $\eta$ ($N$ is positive integer). 

Look at $f(x)=x^N,\ x\geq 0$. $f$ is continuous, look at $f$ in $[0,1+\eta]$. We have
\be
0=f(0)<\eta<f(1+\eta)=(1+\eta)^N
\ee

By the intermediate value theorem, $\exists c \in (0,1+\eta)$ s.t. $f(c)=\eta \ \Leftrightarrow \ c^N=\eta$. This is exactly that $\eta$ has a positive $N$-root. In fact this $N$-root is unique. If $d$ is another positive $N$-root with $d\neq c$. Without loss of generalities $d<c$, we have
\be
d^N<c^N\ \Leftrightarrow \ \eta <\eta \text{ (absurd).}
\ee
\end{example}

Now we prove that $f: [a,b]\mapsto \R$ is continuous, 

$\exists K>0$ s.t. $|f(x)|\leq K$, $\forall x\in[a,b]$. 

$\exists x_1,x_2\in [a,b]$ s.t. $f(x_1)\leq f(x) \leq f(x_2)$, $\forall x\in[a,b]$.

A continuous function on a closed bounded interval is bounded and attains its bounds.

\begin{theorem}\label{thm:conti_bound_1}
$f: [a,b]\mapsto \R$ is continuous, $\exists K>0$ s.t. $|f(x)|\leq K$, $\forall x\in[a,b]$. 
\end{theorem}

\begin{note}
$f(x)=1/x$ on $(0,1]$ is not bounded (needs a closed bounded interval)
\end{note}

\begin{proof}[{\bf Proof}]
Suppose the statement is not true (there is no such $K$). Then $\forall n$ (positive integer), there is $x_n\in [a,b]$ s.t. $|f(x_n)|>n$.

By Bolzano-Weierstrass theorem, $x_n$ has a convergent subsequence $x_{n_j}\to x$, then 
\bea
\left\{\ba{l}
a\leq x_{n_j}\leq b \\
\\
|f(x_n)|>n 
\ea\right. \ \Rightarrow \ 
\left\{\ba{l}
a \leq x\leq b \\
\\
|f(x_{n_j})|>n_j \to \infty
\ea\right.
\eea

Since $f$ is continuous, $x_{n_j}\to x \ \Rightarrow \ f(x_{n_j})\to f(x)$, but $|f(x_{n_j})|>n_j \to \infty$. This is contradiction.
\end{proof}

\begin{theorem}\label{thm:conti_bound_2}
$f: [a,b]\mapsto \R$ is continuous, $\exists x_1,x_2\in [a,b]$ s.t. $f(x_1)\leq f(x) \leq f(x_2)$, $\forall x\in[a,b]$.
\end{theorem}

\begin{proof}[{\bf Proof \#1}]
Let $M=\sup\{f(x), x\in[a,b]\}$ and $M<\infty$ by Theorem \ref{thm:conti_bound_1}.

Then $M-1/n<M$ and by definition of supremum, $\exists x_n\in[a,b]$ s.t. $M-1/n\leq f(x_n)\leq M$.

By Bolzano-Weierstrass theorem, we have a subsequence $x_{n_j}\to x_2$ and $a\leq x_{n_j}\leq b \ \Rightarrow \ a \leq x_2\leq b$. Also, we have $M-1/n\leq f(x_{n_j})\leq M$. Then let $j\to \infty$, we have 
\be
M\leq \lim_{j\to \infty}f(x_{n_j})\leq M \quad \underrightarrow{\text{continuity of $f$}} \quad M\leq f(x_2)\leq M \ \Rightarrow \ f(x_2)=M.
\ee
Similarly for $x_1$.
\end{proof}

\begin{proof}[{\bf Proof \#2}]
We know that $M=\sup\{f(x), x\in[a,b]\} <\infty$. By contradiction, suppose $f(x)<M,\ \forall x\in [a,b]$. Consider
\be
g(x) = \frac{1}{M-f(x)}, \quad\forall x \in [a,b]
\ee
$g$ is continuous since $f(x)$ is. By Theorem \ref{thm:conti_bound_1} applying to $g$, $\exists K>0$ s.t. $|g(x)|\leq K$, $\forall x\in[a,b]$. Since $g$ is positive, 
\be
g(x)\leq K \ \Rightarrow \ 1/K\leq M-f(x) \ \Rightarrow \ f(x)\leq M-1/K,\quad \forall x \in [a,b] 
\ee
which implies that $M-1/K$ is an upper bound of the set $\{f(x),\ x\in [a,b]\}$. This contradicts the definition of $M$ since $M-1/K<M$.
\end{proof}

%\subsection{Inverse functions}

\begin{definition}
$f: [a,b]\mapsto \R$ is said to be \underline{increasing} (\underline{decreasing}) if $x_1<x_2 \ \Rightarrow \ f(x_1)\leq (\geq)f(x_2)$ ($x_1,x_2\in[a,b]$). $f$ is \underline{strictly increasing} (\underline{strictly decreasing}) if $x_1<x_2 \ \Rightarrow \ f(x_1) <(>) f(x_2)$ ($x_1,x_2\in[a,b]$).
\end{definition}

\begin{theorem}\label{thm:inv}
$f: [a,b]\mapsto \R$ is continuous and strictly increasing. Let $c=f(a)$ and $d=f(b)$. Then $f: [a,b]\mapsto [c,d]$ is bijective and the inverse $g:=f^{-1}: [c,d]\mapsto [a,b]$ is also continuous and strictly increasing.
\end{theorem}

\begin{proof}[{\bf Proof}]
(i) $f$ is injective:

If $f(x_1)=f(x_2)$, then $x_1=x_2$. Otherwise, if $x_1<x_2$, $f(x_1)<f(x_2)$ and if $x_1>x_2$, $f(x_1)>f(x_2)$ (since $f$ is strictly increasing).


(ii) $f$ is surjective:

Take $k\in(c,d)$. By intermediate value theorem (IVT), $\exists h\in(a,b)$ s.t. $f(h)=k$. 

So $f$ is bijective and has an inverse function $g: [c,d]\mapsto [a,b]$.

(iii) $g$ is strictly increasing:

We have $y_1<y_2$ and $y_1=f(x_1), \ y_2=f(x_2)$. If $x_1\geq x_2$, it implies that $f(x_1)\geq f(x_2) \ \Rightarrow \ y_1\geq y_2$ since $f$ is increasing. Contradiction. Thus, $x_1 < x_2$.

(iv) $g$ is continuous: 

Given $\ve>0$, let $k_1=f(h-\ve),\ k_2=f(h+\ve)$, $k_1<k<k_2$ and $h-\ve < g(y) < h+\ve$ for any $k_1<y<k_2$. Take $\delta = \min\{k-k_1,k_2-k\}$. For this $\delta$, $|y-k|<\delta \ \Rightarrow \ y\in (k_1,k_2)\ \Rightarrow \ |g(y)-h|<\ve$. 

This was for $k\in(c,d)$. A similar argument gives continuity at the end points also.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Differentiability}

%\subsection{Definition}

$f: E\subseteq \C\mapsto\C$ is a function. We will look at the case: $E$ is the interval in $\R$ and $f$ is real-valued. $x\in E$ s.t. $\exists x_n\in E$, $x_n\neq x$ and $x_n\to x$.

\begin{definition}
$f$ is differentiable at $x$ with derivative $f'(x)$ if 
\be
\lim_{y\to x}\frac{f(y)-f(x)}{y-x} = f'(x)
\ee

$f$ is differentiable if it is differentiable at every point in $E$.
\end{definition}

\begin{remark}
\begin{enumerate}
\item We also write $h=y-x$
\be
\lim_{h\to 0}\frac{f(x+h)-f(x)}{h} = f'(x)
\ee

\item Consider $\ve(h)= \frac{f(x+h)-f(x)-hf'(x)}{h}$,
\be\left\{
\ba{l}
f(x+h)= f(x)+hf'(x) + \ve(h)h.\\
\\
\lim_{h\to 0}\ve(h) = 0
\ea\right.
\ee

Thus, an equivalent way of saying that $f$ is differentiable at $x$ with derivative $f'(x)$ is that there is a function $\ve(h)$ s.t. $f(x+h) = f(x) + hf'(x) + h\ve(h)$ with $\lim_{h\to 0}\ve(h) = 0$.

The other ways of writing the same thing:

$f(x+h) = f(x) + hf'(x) + \tilde{\ve}(h)$ with $\frac{\tilde{\ve}(h)}{h}\to 0$ as $h\to 0$.

$f(x) = f(a) + (x-a)f'(a) +(x-a)\ve(x)$ with $\lim_{x\to a}\ve(x) = 0$.

\item If $f$ is differentiable at $x$, then it is continuous at $x$:

$f(x+h) = f(x) + hf'(x) + h\ve(h)$ with $\lim_{h\to 0}\ve(h) = 0$ as $h\to 0$.

$\lim_{h\to 0}f(x+h) = f(x)$ i.e. $f$ is continuous at $x$.
\end{enumerate}
\end{remark}

\begin{example}
$f: \R\mapsto \R,\ f(x)=|x|$. If $x>0$, 
\be
\lim_{h\to 0}\frac{f(x+h)-f(x)}{h} = \lim_{h\to 0}\frac{x+h-x}{h} = 1 = f'(x)
\ee

If $x<0$, then $f$ is differentiable but $f'(x)=-1$.

But at $x=0$, $f$ is not differentiable, does $\lim_{h\to 0}\frac{f(h)-f(0)}{h}$ exist?
\bea
\lim_{h\to 0^+}\frac{f(h)}{h} & = & \lim_{h\to 0^+}\frac{h}{h} = 1 \\
\lim_{h\to 0^-}\frac{f(h)}{h} & = & \lim_{h\to 0^-}\frac{-h}{h} = -1 
\eea

However, $f$ is continuous at 0, $\lim_{x\to 0}|x|=0$.
\end{example}

\begin{proposition}
(i) If $f(x)=c$, $x\in E$, then $f$ is differentiable at $x$ with $f'(x)=0$;

(ii) $f$, $g$ is differentiable at $x$, then so is $f+g$ and $(f+g)'(x) = f'(x)+g'(x)$;

(iii) $f$, $g$ is differentiable at $x$, then so is $fg$ and $(fg)'(x) = f'(x)g(x) + f(x)g'(x)$;

(iv) $f$ is differentiable at $x$ and $f(t)\neq 0,\ \forall t\in E$, then $\frac 1f$ is differentiable at $x$ and 
\be
\lob \frac 1f\rob'(x) = -\frac{f'(x)}{f^2(x)}.
\ee
\end{proposition}

\begin{remark}
From (iii) and (iv), we get 
\be
\lob \frac fg\rob'(x) = \frac{f'(x)g(x)-f(x)g'(x)}{g^2(x)}.
\ee
\end{remark}

\begin{proof}[{\bf Proof}]
(i) 
\be
\lim_{h\to 0}\frac{f(x+h)-f(x)}{h} = \lim_{h\to 0}\frac{c-c}{h} = 0.
\ee

(ii) 
\bea
&  &\lim_{h\to 0}\frac{f(x+h)+g(x+h)-f(x)-g(x)}{h} \nonumber\\
& = &  \lim_{h\to 0}\frac{f(x+h)-f(x)}{h} + \lim_{h\to 0}\frac{g(x+h)-g(x)}{h} \nonumber\\
& = &  f'(x)+g'(x)
\eea

(iii) $\phi(x)=f(x)g(x)$,
\bea
\lim_{h\to 0}\frac{\phi(x+h)-\phi(x)}{h} & = & \lim_{h\to 0}\frac{f(x+h)g(x+h)-f(x)g(x)}{h} \nonumber\\
& = & \lim_{h\to 0}\frac{(f(x+h)-f(x))g(x+h)}{h} + \lim_{h\to 0}\frac{f(x)(g(x+h)-g(x))}{h} \nonumber\\
& = & f'(x)g(x)+f(x)g'(x) 
\eea

(iv) $\phi(x)=1/f(x)$,
\bea
\lim_{h\to 0}\frac{\phi(x+h)-\phi(x)}{h} & = & \lim_{h\to 0}\frac{1/f(x+h)-1/f(x)}{h} \nonumber\\
& = & \lim_{h\to 0}\frac{f(x)-f(x+h)}{hf(x+h)f(x)} = -\frac{f'(x)}{f^2(x)}
\eea
\end{proof}

\begin{example}
$f(x)=x^n$, $x\neq 0$ and $n$ is a integer, $f'(x) = nx^{n-1}$. 

Thus, all polynomials of rational functions are differentiable.
\end{example}

\begin{theorem}[The chain rule]
$f: U\mapsto \R$ is differentiable at $a\in U$ and $f(x)\in V, \forall x\in U$. $g: V\mapsto \R$ is differentiable at $f(a)$. Then $g\circ f: U\mapsto \R$ is differentiable at $a$ and 
\be
(g\circ f)'(a) = g'(f(a))f(a)
\ee
\end{theorem}

\begin{example}
$\phi(x)=\sin x^2: \ f(x)=x^2, \ g(x)=\sin x,\ \phi(x) = (g\circ f)(x)$. Thus, $\phi'(x) = 2x \cos x^2$.
\end{example}

\begin{proof}[{\bf Proof}]
$f$ is differentiable at $a$ means: $f(x)=f(a)+(x-a)f'(a)+(x-a)\ve_f(x)$ with $\lim_{x\to a}\ve_f(x)=0$.

Similarly, $g$ is differentiable at $f(a)$ means: 

$g(y)=g(f(a))+(y-f(a))g'(f(a))+(y-f(a))\ve_g(y)$ with $\lim_{y\to f(a)}\ve_g(y)=0$.

We substitute to obtain
\beast
g(f(x)) & = & g(f(a))+(f(x)-f(a))\left[g'(f(a))+\ve_g(f(x))\right] \nonumber\\
& = & g(f(a))+((x-a)f'(a)+(x-a)\ve_f(x))\left[g'(f(a))+\ve_g(f(x))\right] \nonumber\\
& = & g(f(a))+(x-a)\left[f'(a) + \ve_f(x)\right]\left[g'(f(a))+\ve_g(f(x))\right] \nonumber\\
& = & g(f(a))+(x-a)f'(a)g'(f(a))+ (x-a)\left[\ve_f(x)g'(f(a)) + (f'(a) + \ve_f(x))\ve_g(f(x))\right]
\eeast

Let $\sigma(x) =\ve_f(x)g'(f(a)) + (f'(a) + \ve_f(x))\ve_g(f(x))$, we have
\be
g(f(x)) = g(f(a))+(x-a)f'(a)g'(f(a))+ (x-a)\sigma(x)
\ee

Need to prove that $\lim_{x\to a}\sigma(x)=0$. Define $\ve_f(a)=0, \ve_g(f(a))=0$, then $\ve_f$ is continuous at $a$ and $\ve_g$ is continuous at $f(a)$. Thus, now $\sigma(x)$ is continuous at $a$ (because it is products, sums and compositions of continuous functions). So
\be
\lim_{x\to a}\sigma(x)=\sigma(a) = 0
\ee
\end{proof}

Up to now everything works for $f: E\subset \C\mapsto \C$.

%\subsection{The Mean Value Theorem}

We first prove the following basic existence result.

\begin{theorem}[Rolle's Theorem]
$f: [a,b]\mapsto \R$ is continuous and differentiable on $(a,b)$. If $f(a)=f(b)$, then $\exists c\in (a,b)$ s.t. $f'(c)=0$.
\end{theorem}

\begin{proof}[{\bf Proof}]
Define 
\be
M=\max_{x\in[a,b]}f(x), \quad m=\min_{x\in[a,b]}f(x)
\ee
(By Theorem \ref{thm:conti_bound_2}, the values $M$ and $m$ are achieved.) Let $k=f(a)=f(b)$. If $M=m=k$, then $f$ is constant, and $f'(c)=0,\ \forall c\in(a,b)$.

Otherwise, $M>k$ or $m<k$. Suppose $M>k$ (the proof of $m<k$ is similar). We know from Theorem \ref{thm:conti_bound_2}, $\exists c\in(a,b)$ s.t. $f(c)=M$. We want to prove that $f'(c)=0$. Since $f$ is differentiable at $c$, we can write
\be
f(c+h) = \underbrace{f(c)}_{M} + h(f'(c) + \ve(h)). \quad \lob \lim_{h\to 0}\ve(h)=0\rob 
\ee

If $f'(c)>0$, we have $f'(c) + \ve(h)>0$ for all $h$ sufficiently small $\lob \lim_{h\to 0}\ve(h)=0\rob $. In addition we take $h>0$, then $f(c+h) = M + h(f'(c) + \ve(h)) > M$ which is absurd because $M$ is the maximum of $f$. If $f'(c)<0$, the same argument shows that there are points at left of $c$, for which $f$ is strictly bigger than $M$. Again absurd, thus $f'(c)=0$.
\end{proof}

\begin{theorem}[Mean value theorem\index{Mean value theorem}]
$f: [a,b]\mapsto \R$ is continuous and differentiable on $(a,b)$. Then $\exists c \in (a,b)$ s.t. $f(b)-f(a)=f'(c)(b-a)$.
\end{theorem}

\begin{remark}
We can rewrite this as follows: 

$b=a+h$, $f(a+h)=f(a)+hf'(a+\theta h)$ where $\theta\in(0,1)$.
\end{remark}

\begin{proof}[{\bf Proof}]
We consider the auxilary function $\phi(x)=f(x)-kx$. Let's choose $k$ s.t. $\phi(a)=\phi(b)$. Thus,
\be
f(a)-ka = f(b)-kb \ \Rightarrow \ k=\frac{f(b)-f(a)}{b-a}
\ee

So for this choice of $k$, $\phi$ satisfies the hypothesis of Roller's theorem. Hence, $\exists c\in (a,b)$ s.t. $\phi'(c)=0$. Then
\be
\phi'(c) = f'(c) -k = 0 \ \Rightarrow \ f'(c) = k = \frac{f(b)-f(a)}{b-a}.
\ee
\end{proof}

We have the following important corollary.

\begin{corollary}\label{cor:diff}
$f: [a,b]\mapsto \R$ is continuous and differentiable, 

(i) If $f'(x)>0 \ \forall x\in (a,b)$, then $f$ is strictly increasing;

(ii) If $f'(x)\geq 0 \ \forall x\in (a,b)$, then $f$ is increasing;

(iii) If $f'(x)=0 \ \forall x\in (a,b)$, then $f$ is constant.
\end{corollary}

\begin{proof}[{\bf Proof}]
(i) Let $x,y\in[a,b]$ with $x<y$. The mean value theorem says 
\be
f(y)-f(x) = f'(c)(y-x), \quad \exists c\in (x,y).
\ee

Then $f'(c)>0 \ \Rightarrow \ f(y)-f(x) = f'(c)(y-x) >0 \ \Rightarrow \ f(y) > f(x)$.

(ii) Same argument.

(iii) Consider $f$ on the interval $[a,x],\ \forall x\in(a,b)$ and apply MVT to get 
\be
f(x) - f(a) = f'(c)(x-a) = 0, \ \exists c\in (a,x) \ \Rightarrow \ f(x) = f(a).
\ee
\end{proof}

%\subsection{Inverse Rule (Inverse Function Theorem)}

\begin{theorem}\label{thm:inverse}
$f: [a,b]\mapsto \R$ is continuous in $[a,b]$ and differentiable in $(a,b)$ with $f'(x)>0,\ \forall x\in(a,b)$. Let $c=f(a)$ and $d=f(b)$. The function $f: [a,b]\mapsto [c,d]$ is a bijection and $f^{-1}: [c,d]\mapsto [a,b]$ is continuous on $[c,d]$ and differentiable on $(c,d)$ with 
\be
\lob f^{-1}\rob'(x) = \frac{1}{f'\lob f^{-1}(x)\rob},\quad \forall x \in (c,d).
\ee
\end{theorem}

\begin{proof}[{\bf Proof}]
Since $f'(x)>0, \ \forall x\in(a,b)$, by Corollary \ref{cor:diff}, $f$ is strictly increasing. By Theorem \ref{thm:inv}, $f^{-1}: [c,d]\mapsto [a,b]$ exists and is bijective and continuous. 

Let $g(y)=f^{-1}(y)$, we need to prove that $g$ is differentiable with 
\be
g'(y)=\frac{1}{f'(x)}, \quad x=f^{-1}(y)
\ee

If $h\neq 0$ and small enough, then there exists a unique $k$ s.t. $y+k=f(x+h)$, $g(y+k) = x+h$ and $f(x+h)-f(x)=k$ ($k\neq 0$). Thus
\be
\frac{g(y+k)-g(y)}{k} = \frac{x+h-x}{f(x+h)-f(x)} = \frac{h}{f(x+h)-f(x)} = \frac{1}{\frac{f(x+h)-f(x)}{h}}
\ee

If $k\to 0$, then $h\to 0$ ($g$ continuous at $y$). Thus,
\be
g'(y) = \lim_{k\to 0} \frac{g(y+k)-g(y)}{k} = \lim_{h\to 0}\frac{1}{\frac{f(x+h)-f(x)}{h}} = \frac{1}{\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}} = \frac{1}{f'(x)}.
\ee
\end{proof}

\begin{example}
\begin{enumerate}
\item $f(x) = x^q$, $q$ is a positive integer and $g(x)=x^{1/q}$ ($x\geq 0$). $g(f(x))=x$, $f'(x) = qx^{q-1}>0$ if $x>0$. 

Inverse rule gives $g$ is differentiable in $(0,\infty)$ and
\be
g'(x) = \frac{1}{q(x^{1/q})^{q-1}} = \frac 1q x^{1/q-1}
\ee

\item $g(x)=x^{p/q}$, $p$ is any integer and $q$ is positive integer. We find $g'(x)$ using the chain rule,
\be
g(x) = (x^{1/q})^p  \ \Rightarrow \ g'(x) = p(x^{1/q})^{p-1} \frac{1}{q}x^{1/q-1} = \frac pq x^{\frac pq -1}
\ee

In other words, if $g(x)=x^r$, where $r$ is any rational number, then $g'(x)=rx^{r-1}$. Later on we will define $x^r$ for $r$ is \underline{real} and discover that $(x^r)'=rx^{r-1}$.
\end{enumerate}
\end{example}

\begin{theorem}[Cauchy's mean value theorem\index{Cauchy's mean value theorem}]\label{thm:mean_value}
$f,g: [a,b]\mapsto \R$ is continuous in $[a,b]$ and differentiable in $(a,b)$. Then $\exists t\in(a,b)$ s.t. $(f(b)-f(a))g'(t)=f'(t)(g(b)-g(a))$.
\end{theorem}

\begin{proof}[{\bf Proof}]
Consider the function $h: [a,b]\mapsto \R$. 
\be
\left|\ba{ccc}
1 & 1 & 1 \\
f(a) & f(x) & f(b) \\
g(a) & g(x) & g(b) 
\ea\right|
\ee

Thus $h$ is continuous in $[a,b]$ and differentiable in $(a,b)$ (It is just a product and sum of functions with the same property). $h(a)=h(b)=0$ because in either 2 columns are equal. By Rolle's Theorem, $\exists t\in (a,b)$ s.t. $h'(t)=0$. Now expand the determinant and differentiate to see that $h'(t)=0$ gives exactly what we want.
\end{proof}

\begin{example}
$\lim_{x\to 0}\frac{e^x-1}{\sin x}$. 

$f(x)=e^x$ and $g(x)=\sin x$. With Cauchy MVT, $\exists t\in (0,x)$, 
\be
\frac{e^x-1}{\sin x} = \frac{f(x)-f(0)}{g(x)-g(0)} = \frac{f'(t)}{g'(t)} = \frac{e^t}{\cos t}
\ee

Let $x\to 0\ \Rightarrow \ t\to 0$. $\frac{e^t}{\cos t}\to 1$. Thus,
\be
\lim_{x\to 0}\frac{e^x-1}{\sin x} = 1. \quad \text{(L'H\^opital Rule)}
\ee


\end{example}

\begin{theorem}[Taylor's theorem with Lagrange's remainder]\label{thm:taylor_lagrange}
Suppose $f$ and its derivative up to order $n-1$ are continuous on $[a,a+h]$ and $f^{(n)}$ exists for $x\in(a,a+h)$, then
\be
f(a+h) = f(a) + hf'(a) + \cdots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(a) + \frac{h^n}{n!}f^{(n)}(a+\theta h), \quad \theta\in (0,1).
\ee
\end{theorem}

\begin{note}
\begin{enumerate}
\item $R_n = \frac{h^n}{n!}f^{(n)}(a+\theta h)$ is Lagrange's form for the remainder.

\item For $n=1$ this is the mean value theorem.
\end{enumerate}
\end{note}

\begin{proof}[{\bf Proof}]
Consider $t\in [0,h]$
\be
\phi(t) = f(a+t) -f(a) - f'(a)t - \dots - \frac{t^{n-1}f^{(n-1)}(a)}{(n-1)!} - \frac {t^nB}{n!}
\ee
$B$ is chosen s.t. $\phi(h)=0$. $\phi(0) = f(a+0)=f(a)=0$.

We will apply Rolle's Theorem $n$ times. We apply it to $\phi$ first to get $h_1\in (0,1)$ s.t. $\phi'(h_1)=0$ and $\phi'(0) = f'(a) -f'(a)+0 = 0$.

Apply Rolle's Theorem to $\phi'$ in $[0,h_1]$ to get $h_2\in(0,h_1)$ s.t. $\phi''(h_2) =0$.

Note that in fact from definition of $\phi$, we see that $\phi(0)=\phi'(0)=\phi''(0)=\dots=\phi^{(n-1)}(0)=0$. We keep apply Rolle's Theorem to get $0< h_n < h_{n-1} < \dots < h$ at which $\phi^{(i)}(h_i)=0, \ 1\leq i\leq n$.

Now note $\phi^{(n)}(t)=f^{(n)}(a+t)-B\ \ra \ \phi^{(n)}(h_n)=0=f^{(n-1)}(a+h_n)-B =0$.

Write $h_n=\theta_n$ for $\theta\in(0,1)$ to get $B=f^{(n)}(a+\theta h)$.

Now put this value of $B$ in the definition of $\phi$ and the statement $\phi(h)=0$ is exactly the statement of the theorem.
\end{proof}

\begin{theorem}[Taylor's theorem with Cauchy's form of the remainder]
Let $f$ satisfy the same hypothesis as Theorem \label{thm:taylor_lagrange} and in addition suppose $a=0$. (This is just to simplify maths)
\be
f(h) = f(0) + hf'(0) + \cdots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(0) + R_n
\ee
where
\be
R_n = \frac{(1-\theta)^{n-1}h^n}{(n-1)!}f^{(n)}(\theta h), \quad \theta\in (0,1).
\ee
\end{theorem}

\begin{proof}[{\bf Proof}]
Consider $t\in [0,h]$
\be
F(t) = f(h) -f(t) - f'(t)(h-t) - \dots - \frac{(h-t)^{n-1}}{(n-1)!} f^{(n-1)}(t)
\ee
\be
F'(t) = -f'(t) + f'(t) -f''(t)(h-t) + f''(t)(h-t) -\frac {(h-t)^2}2 f'''(t) - \dots - \frac{(h-t)^{n-1}}{(n-1)!} f^{(n)}(t)
\ee

In summary:
\be
F'(t) = - \frac{(h-t)^{n-1}}{(n-1)!} f^{(n)}(t)
\ee

Set $\phi(0)=F(t) - \lob\frac{h-t}{h}\rob^p F(0), \ 1\leq p\leq n$, $\phi(0)=0$, $ \phi(h) = F(h) = 0$, (from definition of $F$). 

Rolle's Theorem applied to $\phi$ gives $\theta\in(0,1)$ s.t. $\phi'(\theta h)=0$. 

Now we compute 
\be
0 = \phi'(\theta h) = F'(\theta h) + p\lob\frac{h-\theta h}{h}\rob^{p-1}\lob\frac 1h\rob F(0) = F'(\theta h) + p\frac{\lob1-\theta\rob^{p-1}}{h} F(0) 
\ee
\be
\ra \ 0 = - \frac{h^{n-1}(1-\theta)^{n-1}}{(n-1)!} f^{(n)}(\theta h) +  p\frac{\lob1-\theta\rob^{p-1}}{h} \left[f(h) - f(0)- hf'(0) - \dots - \frac{h^{n-1}}{(n-1)!}f^{(n-1)(0)}\right] 
\ee

Rearranging 
\be
f(h) = f(0) + hf'(0) + \dots +  \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(0) + \frac{(1-\theta)^{n-1}h^n}{(n-1)!p(1-\theta)^{p-1}}f^{(n)}(\theta h)
\ee
\be
R_n =  \frac{(1-\theta)^{n-1}h^n}{(n-1)!p(1-\theta)^{p-1}}f^{(n)}(\theta h), \quad 1\leq p\leq n,\ p\in \Z,\ \theta\in(0,1)
\ee

Set $p=1$,
\be
R_n =  \frac{(1-\theta)^{n-1}h^n}{(n-1)!}f^{(n)}(\theta h), \quad \text{(Cauchy)}.
\ee
\end{proof}
Set $p=n$,
\be
R_n =  \frac{h^n}{n!}f^{(n)}(\theta h), \quad \text{(Lagrange)}.
\ee

Taylor's series needs $R_n\to 0$ as $n\to \infty$.
\be
f(h) = \sum^\infty_{n=0}\frac {f^{(n)}(0)}{n!}h^n
\ee

\begin{remark}
(a) $[0,h)$ is just to simplify matters.

(b) The same result holds in an interval $[h,0]$ ($h<0$).

(c) $a=0$. Mclaurin expansion.
\end{remark}

\begin{example}
binomial series
\be
f(x) = (1+x)^r,\ (r\in \Q), \quad f'(x) = r(1+x)^{r-1}
\ee
we have 
\be
(1+x)^r = \sum^\infty_{n=0}\underbrace{\binom{r}{n}x^n}_{a_n},\quad |x|<1.
\ee
If $r$ is a positive integer, then
\be
\binom{r}{n}=0 ,\quad n>r.
\ee

Let's prove that for $|x|<1$, the series is absolutely convergent. Ratio test
\be
\left|\frac{a_{n+1}}{a_n}\right| = \left|\frac{r(r-1)\dots(r-n+1)(r-n)x^{n+1}n!}{r(r-1)\dots(r-n+1)x^n(n+1)!}\right| = \left|\frac{r-n}{n+1}x\right| \to |x| \quad \text{as }n\to \infty. 
\ee
So, the series converges absolutely for $|x|<1$. In particular, $a_n\to 0$ i.e. $\binom{r}{n}x^n\to 0$ as $n\to \infty$ ($|x|<1$).

Now we study the remainders in the Taylor Theorem.
\be
f'(x) = r(1+x)^{r-1}, \quad f^{(n)}(x) = r(r-1)\dots(r-n+1)(1+x)^{r-n}\ \ra \ f^{(n)}(0)/n! = \binom{r}{n}.
\ee

We look at the Lagrange's form:
\be
R_n = (1+\theta x)^{r-n}\binom{r}{n}x^n,\quad \theta\in (0,1).
\ee

For $0<x<1$
\be
 1+\theta x>1 \ \ra \ (1+\theta x)^{n-r}>1,\ (n>r)\ \ra \ |R_n|\leq \left|\binom{r}{n}x^n\right|, \ (n>r) \ \ra \ R_n\to 0 \text{ as }n\to \infty.
\ee

For $-1<x<0$, the Cauchy form:
\beast
R_n & = & \frac{(1-\theta)^{n-1}r(r-1)\dots(r-n+1)(1+\theta x)^{r-n}x^n}{(n-1)!} = r\binom{r-1}{n-1}\frac{(1-\theta)^{n-1}}{(1+\theta x)^{n-r}}x^n \\
& & = r\binom{r-1}{n-1}\frac{(1-\theta)^{n-r}}{(1+\theta x)^{n-r}}(1-\theta)^{r-1}x^n
\eeast
\be
-1 <x <0 \ \ra \ \frac{1-\theta}{1+\theta x}<1 \ \ra \ |R_n|\leq \underbrace{r(1-\theta)^{r-1}}_{\text{bounded in }n}\left|\binom{r-1}{n-1}x^n\right| \to 0 \text{ as }n\to \infty \ \ra \ R_n \to 0.
\ee
\end{example}

Some comments on differentiability of functions $f: \C\to\C$.
\be
f'(z) = \lim_{w\to z}\frac{f(w)-f(z)}{w-z}
\ee
"Standard properties" work for both $\R$ and $\C$.

\begin{example}
$f:\C\to\C , \ f(z)=\bar{z}$
\be
z_n = z+\frac 1n\ \ra \ \frac{f(z_n)-f(z)}{z_n-z} = \frac{\bar{z}+\frac 1n -\bar{z}}{z+\frac 1n -z} = 1, \quad \quad z_n = z+\frac in\ \ra \ \frac{f(z_n)-f(z)}{z_n-z} = \frac{\bar{z}-\frac in -\bar{z}}{z+\frac in -z} = -1
\ee 
So $\lim_{w-z}\frac{f(w)-f(z)}{w-z}$ does not exist!
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Power Series}

Look at $\sum^\infty_{n=0}a_nz^n, \ a_n\in \C,\ z\in\C$.

\begin{lemma}\label{lem:com}
If $\sum^\infty_{n=0}a_nz_0^n$ converges, and $|z|<|z_0|$, then $\sum^\infty_{n=0}a_nz^n$ converges absolutely.
\end{lemma}

\begin{proof}[{\bf Proof}]
Since $\sum^\infty_{n=0}a_nz_0^n$ converges, $a_nz_0^n\to 0$ as $n\to\infty$. In particular, there is a constant $K$ s.t. $|a_nz_0^n|\leq K, \ \forall n$.
\be
|a_nz^n|= \left|a_nz^n\frac{z_0^n}{z_0^n}\right| \leq K\left|\frac{z}{z_0}\right|^n
\ee
Since $|z|<|z_0|$, the geometric series $\sum\left|\frac{z}{z_0}\right|^n$ converges. By comparison, $\sum^\infty_{n=0}|a_nz^n|$ converges, i.e. $\sum^\infty_{n=0}a_nz^n$ converges absolutely.
\end{proof}

\begin{theorem}
A power series either

(i) converges absolutely for all $z$, or

(ii) converges absolutely for all $z$ inside a circle $|z|=R$, and diverges for all $z$ outside it, or

(iii) converges for $z=0$, only.
\end{theorem}

\begin{definition}
The circle $|z|=R$ is called the circle of convergence and $R$ the radius of convergence. In case (i), we agree that $R=\infty$ and in case (iii), we agree that $R=0$.
\end{definition}

\begin{proof}[{\bf Proof}]
Let $S=\{x\in \R: x\geq 0 \text{ and }\sum a_n x^n \text{ converges}\}$, $0\in S\neq\emptyset$.

If $x_1 \in S$, then by Lemma \ref{lem:com}, $[0,x_1]\subset S$. 

If $S$ is unbounded, then $S=\R^+\{x\in \R: x\geq 0\}$ and we have case (i).

If $S$ is bounded, there exists a finite supremum for $S$, we call it $R$. 

If $R>0$, we will prove that if $|z_1|<R$, then $\sum a_n z_1^n$ converges absolutely: Choose $R_0$ s.t. $|z_1|<R_0<R$, then $R_0\in S$ implies that $\sum a_n R_0^n$ converges and by Lemma \ref{lem:com}, $\sum a_n z_1^n$ converges absolutely.

Finally, we show that if $|z_2|>R>0$, then $\sum a_n z_2^n$ diverges. Take $R_0$ s.t. $R<R_0<|z_0|$. If $\sum a_n z_2^n$ converges, again by Lemma \ref{lem:com}, $\sum a_n R_0^n$ converges. But this contradicts the definition of $R$ as supremum of $S$. Thus, $\sum a_n z_2^n$ diverges.
\end{proof}

The next lemma is useful for computing $R$.

\begin{lemma}
If $\left|\frac{a_{n+1}}{a_n}\right|\to l$ as $n\to \infty$, then $R=1/l$.
\end{lemma}

\begin{proof}[{\bf Proof}]
By the ratio test we have absolute convergence if 
\be
\lim \left|\frac{a_{n+1}z^{n+1}}{a_nz^n}\right|<1, \ \text{i.e. } |z|<1/l
\ee
and if $|z|>1/l$, then $\lim \left|\frac{a_{n+1}z^{n+1}}{a_nz^n}\right|>1$ implies that
\be
\left|a_{n+1}z^{n+1}\right| > \left|a_nz^n\right|,\ \forall n\geq n_0
\ee
and $a_nz^n$ does not tend to zero which implies that $R=1/l$.
\end{proof}

\begin{remark}
By the Root test, if $|a_n|^{1/n}\to l$, then $R=1/l$.
\end{remark}

\begin{example}
(i) $\sum^\infty_{n=0}\frac{z^n}{n!}$, $\left|\frac{n!}{(n+1)!}\right| = \frac {1}{n+1}\to 0\ \ra \ R=\infty$.

(ii) $\sum^\infty_{n=0} z^n$, $R=1$. Note that if $|z|=1$, then the series diverges, $|z|^n=1\nrightarrow 0$.

(iii) $\sum^\infty_{n=0} n!z^n$, $\left|\frac{(n+1)!}{n!}\right| = n+1\to \infty\ \ra \ R=0$.

(iv) $\sum^\infty_{n=1} \frac{z^n}{n}$, $\left|\frac{n}{n+1}\right| \to 1 \ \ra \ R=1$. If $z=1$, $\sum^\infty_{n=1}\frac 1n$ diverges. By Abel's Test (If $b_n$ is a decreasing sequence of positive terms tending to zero and $S_n = \sum^n_{i=1}a_i$ is bounded, $\sum^\infty_{i=1}a_ib_i$ converges), we have $a_n=z^n$, $b_n=1/n\to 0$. Thus, for $ z\neq 1$,
\be
S_n = \sum^n_{i=1}z^i = \frac {1-z^{n+1}}{1-z} -1 \ \ra \ |S_n| \leq 1+\frac{2}{|1-z|}
\ee
which is bounded in $n$. Hence, $\sum^\infty_{n=1} \frac{z^n}{n}$ converges for $|z|=1,\ z\neq 1$.

(v) $\sum^\infty_{n=1} \frac{z^n}{n^2}$, $\left|\frac{n^2}{(n+1)^2}\right| \to 1 \ \ra \ R=1$ but converge for every $|z|\leq 1$. 

(vi) $\sum^\infty_{n=1} n z^n$, $\left|\frac{n+1}n\right| \to 1 \ \ra \ R=1$ but on $|z|= 1$, $|nz^n|=n\to\infty$, we have divergence.

Conclusion: in general nothing can be said at $|z|=R$.
\end{example}

\begin{lemma}\label{lem:radius}
If $\sum a_n z^n$ has radius of convergence $R$, then so do $\sum na_nz^{n-1}$ and $\sum n(n-1)a_nz^{n-2}$,
\end{lemma}
\begin{proof}[{\bf Proof}]
$\sum a_nz^n$ has radius of convergence $R$, and $\sum na_nz^{n-1}$ has radius of convergence $R'$. RTP: $R=R'$.

Take $z$ with $|z|<R$, choose $R_0$ s.t. $|z_0|<R_0<R$. Since $R_0<R$, $a_nR_0^n\to 0$ as $n\to \infty$. In particular, there exists $K$ s.t. $|a_nR_0^n|\leq K,\ \forall n$.
\be
|na_nz^{n-1}| = \frac n{|z|}|a_nz^n|\frac {R_0^n}{R_0^n} \leq \frac K{|z|}n\left|\frac z{R_0}\right|^n
\ee
We see that $\sum n\left|\frac z{R_0}\right|^n$ converges. By comparison, $\sum na_nz^{n-1}$ converges absolutely which implies that $R\leq R'$.

But in fact we have equality because
\be
|a_nz^n|\leq n|a_nz^n| = \frac 1{|z|}n|a_nz^{n-1}|
\ee
by comparison, if $\sum na_nz^{n+1}$ converges absolutely, so does $\sum a_nz^n$. Thus, $R=R'$. What we proved also implies that $\sum n(n-1)a_nz^{n-2}$ has radius of convergence $R$.
\end{proof}

\begin{lemma}\label{lem:polynomial}
(i) $\binom{n}{r} \leq n(n-1)\binom{n-2}{r-2},\quad 2\leq r\leq n$
\vspace{2mm}

(ii) $|(z+h)^n-z^n-nhz^{n-1}|\leq n(n-1)(|z|+|h|)^{n-2}|h|^2,\quad \forall h,z\in \Z$
\end{lemma}
\begin{proof}[{\bf Proof}]
(i) 
\be
\frac{\binom{n}{r}}{\binom{n-2}{r-2}} = \frac{n!}{r!(n-r)!}\cdot \frac{(r-2)!(n-r)!}{(n-2)!} = \frac{n(n-1)}{r(r-1)} \leq n(n-1).
\ee

(ii)
\beast
(z+h)^n-z^n-nhz^{n-1} & = & \sum^n_{r=2}\binom{n}{r}z^{n-r}h^r\\
\ \ra \ |(z+h)^n-z^n-nhz^{n-1}| & \leq & \sum^n_{r=2}\binom{n}{r}|z|^{n-r}|h|^r \leq \sum^n_{r=2}n(n-1)\binom{n-2}{r-2}|z|^{n-r}|h|^r \quad (\text{by (i)})\\
 & = & n(n-1)|h|^2 \underbrace{\sum^n_{r=2}\binom{n-2}{r-2}|z|^{n-r}|h|^{r-2}}_{(|z|+|h|)^{n-2}} 
\eeast


\end{proof}

\begin{theorem}\label{thm:dif}
Suppose $\sum a_nz^n$ has radius of convergence $R$, so let
\be
f(z) = \sum^\infty_{n=0}a_nz^n,\ |z|<R
\ee
Then $f$ is differentiable and $f'(z)=\sum^\infty_{n=1}na_nz^{n-1}$.
\end{theorem}

\begin{remark}
Iterate this theorem, to get that $f$ can be differentiated infinitely many as if it was a polynomial.
\end{remark}

\begin{proof}[{\bf Proof}]
By Lemma \ref{lem:radius}, since $\sum na_nz^n$ has radius of convergence $R$, it defines a function $g(z)=\sum na_n z^{n-1}$ for $z$ with $|z|<R$. We would like to prove that 
\be
\frac{f(z+h)-f(z)-hg(z)}{h} \to 0 \text{ as }h\to 0.
\ee
This implies that $f$ is differentiable with $f'(z)=g(z)$.
\beast
\frac{f(z+h)-f(z)-hg(z)}{h} & = & \frac 1h\lob\sum^\infty_{n=0}a_n(z+h)^n-\sum^\infty_{n=0}a_nz^n - h\sum^\infty_{n=0}na_nz^{n-1}\rob \\
& = & \frac 1h\sum^\infty_{n=0}a_n\lob(z+h)^n-z^n - nhz^{n-1}\rob
\eeast

By Lemma \ref{lem:polynomial},
\be
\left|a_n\lob(z+h)^n-z^n - nhz^{n-1}\rob\right| \leq |a_n|n(n-1)(|z|+|h|)^{n-2}|h|^2
\ee

Take $r$ s.t. $|z|+r<R$. If $|h|<r$, we get 
\be
\left|a_n\lob(z+h)^n-z^n - nhz^{n-1}\rob\right| \leq |a_n|n(n-1)(|z|+r)^{n-2}|h|^2
\ee

By Lemma \ref{lem:radius}, we know that $\sum |a_n|n(n-1)(|z|+r)^{n-2}$ converges to some number $A$. Thus,
\be
\frac{|f(z+h)-f(z)-hg(z)|}{|h|} \leq \frac 1{|h|} A|h|^2 = A|h|\to 0 \text{ as }h\to 0.
\ee
\end{proof}

\begin{example}
$\sum^\infty_{n=0}\frac{z^n}{n!}$ has radius of convergence $R=\infty$. Thus, $e: \C\mapsto\C$, $e(z) = \sum^\infty_{n=0}\frac{z^n}{n!}$. The theorem tells us right away that $e$ is differentiable and 
\be
e'(z) = \sum^\infty_{n=1}\frac n{n!}z^{n-1} = e(z).
\ee
\end{example}

\begin{remark}
$F:\C\mapsto\C$ is differentiable. If $F'(z)=0,\ \forall z\in \C$, then $F$ is constant. Let $g(t)=F(tz)$, $g:[0,1]\mapsto \C$. Write 
\be
g(t) = u(t) + iv(t), \quad \text{where } u,v:[0,1]\mapsto \R.
\ee

With chain rule, we have $g'(t) = F'(tz)z = 0 \ \ra \ u'(t)=v'(t)=0$. By Corollary \ref{cor:diff}, we have $u$ and $v$ are constants.

Thus, we claim $e(a+b)=e(a)e(b), \ a,b\in\C$ and define $F:\C\mapsto\C$
\be
F(z) = e(a+b-z)e(z) \ \ra \ F'(z) = -e(a+b-z)e(z) + e(a+b-z)e(z) = 0 \ \ra \ F \text{ is constant} 
\ee

Thus, $F(z) = F(0) = e(a+b)e(0)$. From the definition of $e$, $e(0)=1$, so $F(z)= e(a+b),\ \forall z\in\C$. Let $z=b$, we have $e(a+b)=e(a)e(b)$.
\end{remark}

Now we restrict $e:\R\mapsto \R$ and prove

\begin{theorem}
(i) $e:\R\mapsto\R$ is differentiable everywhere;

(ii) $e(x+y) = e(x)e(y),\quad \forall x,y \in\R$;

(iii) $e(x)>0, \quad \forall x\in\R$;

(iv) $e$ is strictly increasing;

(v) $e(x)\to \infty$ as $x\to\infty$, $e(x)\to 0$ as $x\to -\infty$;

(vi) $e:\R\mapsto (0,\infty)$ is a bijection.
\end{theorem}

\begin{proof}[{\bf Proof}]
(i) and (ii) done.

(iii) clearly from the definition of $e(x)$, $e(x)>0$ if $x\geq 0$ and $e(0)=1$. We have
\be
1=e(0) = e(x-x) = e(x)e(-x)\ \ra \ e(x)>0, \ \forall x\in\R.
\ee

(iv) $e'(x) = e(x)>0$, then $e$ is strictly increasing by Corollary \ref{cor:diff}.

(v) From definition of $e(x)$, by $e(-x) = \frac 1{e(x)}$
\be
e(x) > 1+x,\quad x>0\ \ra \ e(x)\to \infty \text{ as }x\to\infty \ \ra \ e(x)\to 0 \text{ as }x\to-\infty.
\ee

(vi) $e$ is strictly increasing implies that $e$ is injective. 

For any $y\in (0,\infty)$, $\exists a,b\in \R$ s.t. $e(a)<y<e(b)$ since $e(x)\to \infty$ as $x\to\infty$, $e(x)\to 0$ as $x\to -\infty$. Thus, with Intermediate value theorem, $\exists x\in\R$ s.t. $e(x)=y$ which proves surjectivity.
\end{proof}

\begin{remark}
(vi) and (ii) are saying that the map $e:(\R,+)\mapsto ((0,\infty),\times)$ is group isomorphism. Since $e:\R\mapsto(0,\infty)$ is a bijection, $\exists l:(0,\infty)\mapsto \R$ s.t. $e(l(t))=t,\ \forall t\in(0,\infty)$ and $l(e(x))=x,\ \forall x\in\R$.
\end{remark}

\begin{theorem}
(i) $l: (0,\infty)\mapsto \R$ is a bijection and $l(e(x))=x,\ \forall x\in\R$, $e(l(t))=t,\ \forall t\in(0,\infty)$;

(ii) $l$ is differentiable and $l'(t)=1/t$;

(iii) $l(xy) = l(x) + l(y),\ \forall x,y\in(0,\infty)$. 
\end{theorem}

\begin{proof}[{\bf Proof}]
(i) Obvious.

(ii) $l$ is differentiable by the Inverse rule (Theorem \ref{thm:inverse})
\be
l'(t) = \frac{1}{e'(l(t))} = \frac1{e(l(t))} = \frac 1t.
\ee

(iii) By Groups, $l((0,\infty),x)\mapsto (\R,t)$ is a homomorphism.
\end{proof}

$\alpha\in\R$, $x$ is any positive number. We define 
\be
\gamma_\alpha (x) = e(\alpha l(x)),\quad \gamma_1(x) = x.
\ee 

\begin{theorem}
Suppose $x,y>0,\ \alpha,\beta\in\R$, then

(i) $\gamma_\alpha(xy) = \gamma_\alpha(x)\gamma_\alpha(y)$;

(ii) $\gamma_{\alpha+\beta}(x) = \gamma_\alpha(x)\gamma_\beta(x)$;

(iii) $\gamma_\alpha(\gamma_\beta(x)) = \gamma_{\alpha\beta}(x)$;
\end{theorem}

\begin{proof}[{\bf Proof}]
(i) $\gamma_\alpha(xy) = e(\alpha l(xy)) = e(\alpha (l(x)+l(y))) = e(\alpha l(x))e(\alpha l(y)) = \gamma_\alpha(x)\gamma_\alpha(y)$.

(ii) $\gamma_{\alpha+\beta}(x) = e((\alpha+\beta) l(x)) = e(\alpha l(x))e(\beta l(x)) = \gamma_\alpha(x)\gamma_\beta(x)$.

(iii) $\gamma_\alpha(\gamma_\beta(x)) = e(\alpha l(\gamma_\beta(x))) = e(\alpha l(e(\beta l(x)))) = e(\alpha\beta l(x))= \gamma_{\alpha\beta}(x)$.
\end{proof}

If $n$ is positive integer,
\be
\gamma_n(x) = \gamma_{1+1+\dots+1}(x) = x \cdots x = x^n,\quad \gamma_0(x) = 1,\quad \gamma_{-1}(x) = 1/x,\quad \gamma_{-n}(x) = x^{-n}
\ee

If $q$ is a positive integer, 
\be
(\gamma_{1/q}(x))^q = \gamma_{q/q}(x) = x \ \ra \ \gamma_{1/q} = x^{1/q} \ \ra \ \gamma_{p/q} = x^{p/q}
\ee

Thus, we have
\be
\gamma_\alpha(x) = x^\alpha,\quad \alpha \in \R.
\ee

This definition agrees with the one given before for $\alpha\in\R$, $\exp(x) = e^x = e(x)$ and $\log(x) = l(x)$. Define the real number, $e$ as 
\be
e = \sum^\infty_{n=0}\frac 1{n!}\quad\quad (\gamma_\alpha (x) = e(\alpha l(x)) \ \ra \ x^\alpha = e^{\alpha \log x})
\ee
\be
(x^\alpha)' = e^{\alpha \log x} \frac {\alpha}{x} = \alpha x^{\alpha -1}, \quad \alpha \in \R.
\ee

Define $f(x) = a^x = e^{x\log a}$, $a>0, a\in\R$
\be
f'(x) = e^{x\log a} \log a = a^x \log a.
\ee

{\flushleft \bf \underline{Trigonometric functions}}
\be
\left.\ba{rcl}
\sin z & = & \sum^\infty_{n=0}\frac{(-1)^nz^{2n+1}}{(2n+1)!}\\
\\
\cos z & = & \sum^\infty_{n=0}\frac{(-1)^nz^{2n}}{(2n)!}
\ea\right\}\text{both have infinite radius of convergence.}
\ee

$\sin z, \cos z: \C\mapsto \C$. By Theorem \ref{thm:dif}, they are differentiable and 
\be
(\sin z)' = \cos z,\quad (\cos z)' = -\sin z
\ee
\be
e^{iz} = \sum^\infty_{n=0}\frac {(iz)^n}{n!} = \sum^\infty_{n=0}\frac{(iz)^{2n}}{(2n)!} + \sum^\infty_{n=0}\frac{(iz)^{2n+1}}{(2n+1)!} = \cos z + i\sin z.
\ee
From defintion,
\be
\cos (-z) = \cos z,\quad \sin(- z) = -\sin z.
\ee
\be
\cos z = \frac{e^{iz}+e^{-iz}}{2},\quad \sin z = \frac{e^{iz}-e^{-iz}}{2}
\ee

Now we get using $e^{a+b} = e^a e^b$,
\be
\sin (z+w) = \sin z \cos w + \cos z \sin w,\quad \cos (z+w) = \cos z \cos w - \sin z \sin w, \quad z,w \in\C
\ee

Also, $\cos^2 z + \sin^2 z = 1$, $z\in\C$. If $x\in\R$, $|\sin x|\leq 1$ and $|\cos x|\leq 1$.

\underline{Warning} $|\cos z|\ (|\sin z|)$ are not bounded for $z\in \C$. If $z=iy$, $\cos(iy)=\frac{e^y+e^{-y}}{2}\to \infty$ as $y\to \pm\infty$.

Periodicity of the trigonometric functions:

\begin{proposition}
There is a smallest positive number $w$ (when $\sqrt{2} < w/2 <\sqrt{3}$) s.t. 
\be
\cos(w/2)=0.
\ee
\end{proposition}

\begin{proof}[{\bf Proof}]
If $0<x<2$,
\be
\sin x = \underbrace{\lob x - \frac {x^3}{3!}\rob}_{>0} + \underbrace{\lob \frac {x^5}{5!} - \frac {x^7}{7!}\rob}_{>0} + \dots \quad \lob \frac{x^{2n-1}}{(2n-1)!} >  \frac{x^{2n+1}}{(2n+1)!} \rob \ \ra \  \sin x>0.
\ee

But $(\cos x)'=-\sin x<0$, $x\in(0,2)$ implies that $\cos x$ is a strictly decreasing function in $(0,2)$. We will prove that $\cos \sqrt{2}>0, \cos\sqrt{3}<0$. By the Intermediate value theorem, there exists $w$ s.t. $w/2\in (\sqrt{2},\sqrt{3})$.
\beast
\cos x = 1 - \frac {x^2}{2!} + \frac {x^4}{4!} - \frac {x^6}{6!} + \dots \ \ra \ 
\left\{
\ba{rcl}
\cos \sqrt{2} & = &  \underbrace{\lob 1 - \frac {\sqrt{2}^2}{2!}\rob}_{>0} + \underbrace{\lob \frac {\sqrt{2}^4}{4!} - \frac {\sqrt{2}^6}{6!}\rob}_{>0} + \dots  > 0 \\
\cos \sqrt{3} & = & \underbrace{\lob 1 - \frac {\sqrt{3}^2}{2!} + \frac {\sqrt{3}^4}{4!} \rob}_{<0} - \underbrace{\lob \frac {\sqrt{3}^6}{6!} - \frac {\sqrt{3}^8}{8!} \rob}_{>0} + \dots < 0 
\ea\right.
\eeast
\end{proof}

\begin{corollary}
$\sin \frac w2 = 1$ 
\end{corollary}

\begin{proof}[{\bf Proof}]
\be
\underbrace{\cos^2 \frac w2}_{=0} + \sin^2 \frac w2 = 1 \ \ra \ \sin^2 \frac w2 = 1 \ \ra \ \sin \frac w2 = 1 \text{ since } \sin x>0.
\ee
\end{proof}

\begin{definition}
$\pi := w$.
\end{definition}

\begin{theorem}
(i) $\sin(z + \frac {\pi}2) = \cos z,\quad \cos(z + \frac {\pi}2) = - \sin z$;

(ii) $\sin(z + \pi) = -\sin z,\quad \cos(z + \pi) = - \cos z$;

(iii) $\sin(z + 2\pi) = \sin z,\quad \cos(z + 2\pi) = \cos z$.
\end{theorem}

Note that we $e^{z+2\pi i} = e^z$.

{\flushleft \bf \underline{Hyperbolic functions}}
\be
\cosh z = \frac 12(e^z + e^{-z}), \quad \sinh z = \frac 12(e^z - e^{-z}), \quad z\in\C
\ee

From this, 
\be
\left\{
\ba{rcl}
\cosh z = \cos iz\\
i\sinh z = \sin iz
\ea\right.,\quad 
\left\{
\ba{rcl}
(\cosh z)' = \sinh z\\
(\sinh z)' = \cosh z
\ea\right.,\quad \cosh^2 z - \sinh^2 z =1
\ee

\begin{remark}
\ben
\item The other trigonometric functions ($\tan, \sec$, etc.) are defined in the usual way.

\item Exponential bear powers $\frac{e^x}{x^k}\to \infty$ as $x\to\infty$, $k>0$. To prove this, take a positive integer $n$ s.t. $n>k$, and observe (from the definition of $e^k$ as a power series) that $e^x>\frac{x^n}{n!},\ x>0$
\be
\frac {e^x}{x^n} > \frac{x^{n-k}}{n!} \to \infty \text{ as } x\to \infty\ (n>k).
\ee 

\een
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Integration}

{\flushleft \bf \underline{Riemann integration}}

$f:[a,b]\mapsto \R$, bounded. There exists $K>0$ s.t. $|f(x)|\leq K, \ \forall x\in [a,b]$.

\begin{definition}
A dissection (or partition) of the interval $[a,b]$ is a finite subset $D$ of $[a,b]$ which contains $a$ and $b$. We write it as 
\be
D = \{a=x_0<x_1<\dots< x_{n-1}<x_n=b\}
\ee
\centertexdraw{
\drawdim in

\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0

\lpatt(0.05 0.05)

\move (1.5 0) \lvec(1.5 1.2) \lvec(1.1 1.2) \lvec(1.1 0) \lfill f:0.8
\move (0.2 0) \lvec(0.2 0.5) \move (2 0) \lvec(2 1.3)

\lpatt( )

\move (0.2 0.5) \clvec (0.5 0.8)(1 0)(1.5 1.2)
\move (1.5 1.2) \clvec (1.8 2)(1.9 1.5)(2 1.3)

\htext (0.15 -0.15){$a$}
\htext (1.95 -0.15){$b$}
\htext (1 -0.15){$x_{j-1}$}
\htext (1.4 -0.15){$x_j$}

\move (-0.2 0) \avec(2.3 0)
\move (0 -0.2) \avec(0 1.8)

}
We define the upper and lower sum of $f$ w.r.t. $D$ as 
\be
S(f,D) = \sum^n_{j=1}(x_j-x_{j-1})\sup_{x\in[x_{j-1},x_j]}f(x),\quad s(f,D) = \sum^n_{j=1}(x_j-x_{j-1})\inf_{x\in[x_{j-1},x_j]}f(x)
\ee

Clearly, $S(f,D)>s(f,D)$.
\end{definition}

\begin{lemma}\label{lem:interval}
If $D$ and $D'$ are dissections with $D'>D$, then 
\be
S(f,D) \geq S(f,D') \geq s(f,D') \geq s(f,D).
\ee
\end{lemma}

\begin{proof}[{\bf Proof}]
Suppose $D'$ has one extra point, let's say $y\in(x_{r-1},x_r)$ for some $r$. We know 
\be
\sup_{x\in[x_{r-1},y]}f(x), \sup_{x\in[y,x_r]}f(x)\leq \sup_{x\in[x_{r-1},x_r]}f(x).
\ee
Thus,
\be
(x_r-x_{r-1})\sup_{x\in[x_{r-1},x_r]}f(x) \geq (y-x_{r-1})\sup_{x\in[x_{r-1},y]}f(x)+(x_r-y)\sup_{x\in[y,x_r]}f(x) 
\ee
which implies that $S(f,D) \geq S(f,D')$. If now $D'$ has more than one $D$ extra point, just do the argument for each extra point. The proof for $s(f,D') \geq s(f,D)$ is similar to the one for the upper sums.
\end{proof}

\begin{lemma}\label{lem:interval_2}
If $D_1$ and $D_2$ are any two dissections, then 
\be
S(f,D_1) \geq S(f,D1\cup D_2) \geq s(f,D_1\cup D_2) \geq s(f,D_2).
\ee
\end{lemma}

\begin{proof}[{\bf Proof}]
$D1\cup D_2 \geq D_1, D_2$, so by Lemma \ref{lem:interval}, we get the conclusion as required.
\end{proof}

\begin{definition}
The upper and lower integrals are defined as 
\be
I^*(f) = \inf_D S(f,D),\quad\quad I_*(f) = \sup_D s(f,D).
\ee
\end{definition}

\begin{remark}
These numbers are well-defined because $f$ is bounded so 
\be
S(f,D) \geq -K(b-a),\quad s(f,D)\leq K(b-a),\quad \forall D
\ee
\end{remark}

\begin{definition}
We say that a bounded function $f$ is Riemann integrable (or just integrable) if $I^*(f) = I_*(f)$. In this case, we write 
\be
\int^b_a f(x)dx = I^*(f) = I_*(f)\quad\quad \lob\text{or just }\int^b_a f.\rob
\ee
\end{definition}

\begin{remark}
$I^*(f) \geq I_*(f)$ thanks to Lemma \ref{lem:interval_2}, 
\be
S(f,D_1) \geq s(f,D_2) \ \ra \ \inf_{D_1}S(f,D_1) \geq s(f,D_2), \quad S(f,D_1) \geq \sup_{D_2}s(f,D_2).
\ee
\end{remark}
Which functions are Riemann integrable?

\begin{example}
({\bf Dirichlet}) 
\be
f(x) = \left\{\ba{ll}
1 \quad \quad & x \text{ is rational in }[0,1]\\
0 & x \text{ is irrational in }[0,1]
\ea\right.\quad \text{bounded.}
\ee
$D$ is any partition, $S(f,D)=1$ because every $[x_{r-1},x_r]$ contains a rational. But $s(f,D)=0$ and 
\be
I^*(f) = 1, \quad I_*(f) = 0.
\ee
Thus, $f$ is not Riemann integrable. 
\end{example}

We now prove the following useful criterion for integrabilities.

\begin{theorem}[Riemann]\label{thm:riemann}
A bounded function $f:[a,b]\mapsto \R$ is integrable iff given $\ve>0$, $\exists D$ s.t. $S(f,D) - s(f,D)<\ve$.
\end{theorem}

\begin{proof}[{\bf Proof}]
Let's assume first that $f$ is Riemann integrable. Given $\ve>0$, by definition of $I^*(f)$, $\exists D_1$ s.t. 
\be
S(f,D_1) < I^*(f) + \ve
\ee

By definition of $I_*(f)$, $\exists D_2$ s.t. 
\be
s(f,D_2) > I_*(f) - \ve
\ee

Consider $D=D_1\cup D_2$, by Lemma \ref{lem:interval_2}, we have
\be
S(f,D_1\cup D_2) - s(f,D_1 \cup D_2) \leq S(f,D_1) - s(f,D_2) < I^*(f) + \ve - I_*(f) + \ve = 2\ve.
\ee

To prove a converse, assume that given $\ve>0$, $\exists D$ s.t. $S(f,D) - s(f,D)<\ve$. Applying the definition of $I^*$ and $I_*$,
\be
0\leq I^*(f) - I_*(f) \leq S(f,D) - s(f,D)<\ve.
\ee

Given this is true, for all $\ve>0$, $I^* = I_*$. Thus, $f$ is integrable.
\end{proof}

We now prove monotonic functions and continuous function are integrable.

\begin{remark}
Monotonic and continuous function on $[a,b]$ are bounded.
\end{remark}

\begin{theorem}
$f:[a,b]\mapsto \R$ is monotonic, then $f$ is integrable.
\end{theorem}

\begin{proof}[{\bf Proof}]
Assume $f$ is increasing. Take $D$ any partition of $[a,b]$
\be
S(f,D) = \sum^n_{j=1}\sup_{x\in[x_{j-1},x_j]}f(x)(x_j-x_{j-1}) = \sum^n_{j=1}f(x_j)(x_j-x_{j-1}) \quad(\text{since $f$ increasing})
\ee
Similarly,
\be
s(f,D) = \sum^n_{j=1}f(x_{j-1})(x_j-x_{j-1})
\ee
Thus,
\be
S(f,D) - s(f,D) = \sum^n_{j=1}(f(x_j)-f(x_{j-1}))(x_j-x_{j-1})
\ee

Now consider a positive integer $n$
\be
D=\left\{a,a+\frac{b-a}n, \dots, a+ \frac{(b-a)(n-1)}n, b\right\},\quad \quad x_j = a+ \frac{(b-a)j}n,\quad 0\leq j\leq n.
\ee
For this $D$,
\be
S(f,D) - s(f,D) = \frac {b-a}n \sum^n_{j=1}(f(x_j)-f(x_{j-1})) = \frac{(b-a)(f(b)-f(a))}{n}.
\ee
Given $\ve>0$, take $n$ large enough s.t. 
\be
\frac{(b-a)(f(b)-f(a))}{n} < \ve
\ee
By Theorem \ref{thm:riemann}, $f$ is integrable.
\end{proof}

\begin{lemma}\label{lem:uni_con}
$f:[a,b]\mapsto \R$ is continuous. Given $\ve>0$, $\exists \delta>0$ s.t. if $|x-y|<\delta$, ($x,y\in [a,b]$) then $|f(x)-f(y)|<\ve$ (uniform continuity).
\end{lemma}

\begin{remark}
Continuity at a point, let's say $x\in [a,b]$ means that given $\ve>0$, $\exists \delta(x)$ s.t. $|y-x|<\delta \ \ra \ |f(y)-f(x)|<\ve$.
\end{remark}

\begin{proof}[{\bf Proof}]
Suppose claim is not true. There exists $\ve>0$ s.t. for every $\delta>0$, we can find $x,y$ with $|x-y|<\delta$ but $|f(x)-f(y)|\geq \ve$.

Take $\delta = \frac 1n$ to get $x_n,y_n$ with $|x_n-y_n|<\frac 1n$, but $|f(x_n)-f(y_n)|\geq \ve$.

By Bolzano-Weierstrass theorem, $x_{n_k}\to c$ and $x_n \in [a,b]\ \ra \ c\in [a,b]$.
\be
|y_{n_k}-c| \leq |x_{n_k}-y_{n_k}| + |x_{n_k}-c| < \frac 1{n_k} + |x_{n_k}-c|
\ee
let $n_k\to \infty$, $\frac 1{n_k} + |x_{n_k}-c|\to 0$, so $y_{n_k}\to c$. 

On the other hand, $|f(x_{n_k})-f(y_{n_k})| \geq \ve$. Since $f$ is continuous,
\be
f(x_{n_k}) \to f(c),\quad f(y_{n_k}) \to f(c) \ \ra \ 0=|f(c)-f(c)|\geq \ve \quad (\text{absurd.})
\ee
\end{proof}

\begin{theorem}
$f:[a,b]\mapsto \R$ is continuous. Then $f$ is integrable.
\end{theorem}

\begin{proof}[{\bf Proof}]
Consider the partition $D$ with points ($n$ is a positive integer)
\be
x_j = a+ \frac{(b-a)j}n,\quad 0\leq j\leq n.
\ee
\be
S(f,D) - s(f,D) = \sum^n_{j=1}\lob \sup_{x\in[x_{j-1},x_j]}f(x) - \inf_{x\in[x_{j-1},x_j]}f(x)\rob \frac {b-a}n
\ee
Let $\ve>0$ be given and consider $\delta$ given in Lemma \ref{lem:uni_con}. Choose $n$ large enough s.t. $\frac{b-a}n<\delta$. Then for any $x,y\in [x_{j-1},x_j]$, $|f(x)-f(y)|<\ve$ which implies that
\be
\sup_{x\in[x_{j-1},x_j]}f(x) - \inf_{x\in[x_{j-1},x_j]}f(x) < \ve \ \ra \ S(f,D)-s(f,D)< \ve (b-a)
\ee
By Riemann's criterion, $f$ is integrable.
\end{proof}

\begin{example}
\be
f(x) = \left\{\ba{ll}
1/q \quad \quad & \text{if $x$ is $p/q$ in }[0,1]\\
0 & \text{if $x$ is irrational in }[0,1]
\ea\right.
\ee
Claim $f$ is integrable and $\int^1_0 f=0$.

Take $D$ as any partition, then $s(f,D) = 0$ (every $[x_{j-1},x_j]$ contains irrational numbers, where $f$ is zero). Clearly, $I_*(f)=0$. 

To prove the claim, it suffices to show that given $\ve>0$, $\exists D$ s.t. $S(f,D)<\ve$ (by Riemann's and $I_*(f)=0$).

Let $\ve>0$ be given, take an integer $N$ with $1/N<\ve$. $S=\{x\in [0,1], f(x)\geq 1/N\}$ is a finite set with a cardinality of $R$. Thus,
\be
x\in S=\{0=t_1<t_2 < \dots < t_R=1\} \ \Leftrightarrow \ \left\{\ba{l} 
x=p/q \\
1/q \geq 1/N \ (q\leq N)
\ea\right.
\ee
Choose a partition $D$ s.t. the point $t_i$ belong to the interval $[x_{i_1}, x_{i_2}]$ with length less than $\ve/R$. ($0=t_1=x_0$). Thus, we have
\beast
S(f,D) & = & \sum_{\text{intervals containing $t_i$}} \underbrace{\sup_D f(x)}_{\leq 1} (x_j-x_{j-1}) + \sum_{\text{all the other intervals}} \underbrace{\sup_D f(x)}_{< 1/N} (x_j-x_{j-1}) \\
& < & \sum_{\text{intervals containing $t_i$}} (x_j-x_{j-1}) +\quad 1/N \cdot\sum_{\text{all the other intervals}} (x_j-x_{j-1})\\
& < & \ve/R \cdot R + 1/N < 2\ve.
\eeast
\end{example}

{\flushleft \bf \underline{Elementary properties of the integral}}

$f,g$ are bounded, and integrable on $[a,b]$, then

(i) If $f\leq g$ on $[a,b]$, then $\int^b_a f\leq \int^b_a g$;

(ii) $f+g$ is integrable over $[a,b]$;

(iii) For any constant $k$, $kf$ is integrable and $\int^b_a kf = k\int^b_a f$;

(iv) $|f|$ is integrable and $\left|\int^b_a f\right| \leq \int^b_a |f|$; 

(v) The product $fg$ is integrable;

(vi) If $f(x)= F(x)$ except at finitely many points in $[a,b]$, then $F$ is integrable and $\int^b_a F = \int^b_a f$;

(vii) If $a<c<b$, $f$ is integrable over $[a,c]$ and $[c,b]$ and $\int^b_a f = \int^c_a f + \int^b_c f$.

\begin{proof}[{\bf Proof}]
(i) If $f\leq g$, then
\be
\int^b_a f = I^*(f) \leq S(f,D) \leq S(g,D) \forall D.
\ee
Take infimum over all $D$
\be
\int^b_a f \leq I^*(g) = \inf^b_a g.
\ee

(ii) observe first
\be
\sup_{x\in[x_{j-1},x_j]}(f+g)(x) \leq \sup_{x\in[x_{j-1},x_j]}f(x) + \sup_{x\in[x_{j-1},x_j]}g(x) \ \ra \ S(f+g,D) \leq S(f,D) + S(g,D)
\ee

Now take any two partitions $D_1$ and $D_2$
\be
I^*(f+g) \leq  S(f+g,D_1\cup D_2) \leq S(f,D_1\cup D_2) + S(g,D_1\cup D_2) \leq S(f,D_1) + S(g,D_2) \quad (\text{Lemma }\ref{lem:interval_2})
\ee

Keep $D_1$ fixed and take infimum over all $D_2$,
\be
I^*(f+g) \leq S(f,D_1) + I^*(g) \ \ra \  I^*(f+g) \leq I^*(f) + I^*(g) = \int^b_a f + \int^b_a g.
\ee
A similar argument for the lower sums gives 
\be
I_*(f+g) \geq \int^b_a f + \int^b_a g
\ee
With $I^*(f+g) \geq I_*(f+g)$, we have
\be
I^*(f+g)  = I_*(f+g) = \int^b_a f + \int^b_a g
\ee

(iii) observe first if $k\geq 0$
\be
\sup_{x\in[x_{j-1},x_j]}(kf)(x) = k \sup_{x\in[x_{j-1},x_j]}f(x)
\ee
with $k\geq 0$, we have
\be
I^*(kf) \leq  S(kf,D) = k S(f,D),\quad \forall D
\ee
take infimum over all $D$, we have $I^*(kf) \leq k I^*(f)$, Similarly, we have $I_*(kf) \geq k I_*(f)$ which implies that $I^*(kf) = I_*(kf)$. For $k\leq 0$, we use the similar argument.

(iv) Let $f^+(x)=\max\{f(x),0\}$ and $|f(x)| = 2f^+(x) - f(x)$. Claim that $f^+(x)$ is integrable.
\be
\sup_{x\in[x_{j-1},x_j]}f^+(x) - \inf_{x\in[x_{j-1},x_j]}f^+(x) \leq \sup_{x\in[x_{j-1},x_j]}f(x) - \inf_{x\in[x_{j-1},x_j]}f(x)
\ee
So for any partition $D$
\be
S(f^+,D)-s(f^+,D) \leq S(f,D)-s(f,D)
\ee
By Riemann's, $f$ is integrable, given $\ve>0$, $\exists D$ s.t. $S(f,D)-s(f,D) < \ve$ and then $S(f^+,D)-s(f^+,D) < \ve$. Thus, by Riemann's again, $f^+$ is integrable and $|f|$ is integrable. Also, follows from (i), we have $-|f|\leq f \leq |f| \ \ra \ \left|\int^b_a f\right| \leq \int^b_a |f|$.

(v) First we prove that if $f$ is integrable, then $f^2$ is integrable. Suppose first $f\geq 0$. Since $f$ is integrable, given $\ve>0$, $\exists D$ partition of $[a,b]$ s.t. $S(f,D)-s(f,D)<\ve$. Define
\beast
& & M_j = \sup_{x\in[x_{j-1},x_j]}f(x), \quad m_j = \inf_{x\in[x_{j-1},x_j]}f(x) \\
& & \ra \ \sup_{x\in[x_{j-1},x_j]}f^2(x) - \inf_{x\in[x_{j-1},x_j]}f^2(x) = M_j^2-m_j^2 \quad(\text{since }f\geq 0)
\eeast
Thus,
\beast
S(f^2,D)-s(f^2,D) & = & \sum^n_{j=1}(M_j^2-m_j^2)(x_j-x_{j-1}) \\
& = & \sum^n_{j=1}(M_j+m_j)(M_j-m_j)(x_j-x_{j-1})
\eeast
$f$ is bounded, $M_j+m_j\leq 2K$, where $f(x)\leq K,\ \forall x\in [a,b]$
\beast
S(f^2,D)-s(f^2,D) & < & 2K \sum^n_{j=1}(M_j-m_j)(x_j-x_{j-1}) \\
& = & 2K(S(f,D)-s(f,D)) < 2K\ve \ \ra \ f^2 \text{ is integrable.}
\eeast

If $f$ is now arbitrary (i.e. just integrable). Then $|f|$ is integrable by (iv), but $|f|^2= f^2$. Since $|f|\geq 0$, $f^2$ is integrable. 

To prove that $fg$ is integrable, just write 
\be
4f(x)g(x) = (f(x)+g(x))^2 - (f(x)-g(x))^2
\ee
Now $f,g$ are integrable $\ra$ $f+g$ and $f-g$ are integrable $\ra$ $(f+g)^2$ and $(f-g)^2$ are integrable. as required.

(vi) Let $h=f-F$, then $h(x)=0$, for every $x\in[a,b]$ except perhaps a finite set. (just from the definition of the integral) It must be integrable with $\int^b_a h=0$. But $F=f-h$, so $F$ is integrable with $\int^b_a F = \int^b_a f$.
\end{proof}

$\int^b_a f,\ a<b$. If $a>b$, define $\int^b_a f = -\int^a_b f$ and $\int^a_a f=0$.

How do we compute $\int^b_a f$? $f: [a,b]\mapsto \R$ is bounded Riemann integrable. $x\in [a,b]$
\be
F(x) = \int^x_a f(t)dt.
\ee 

\begin{theorem}
$F$ is continuous.
\end{theorem}

\begin{proof}[{\bf Proof}]
$x\in [a,b]$,
\be
F(x+h) - F(x) = \int^{x+h}_a f(t)dt - \int^x_a f(t)dt = \int^{x+h}_x f(t)dt 
\ee
\be
|F(x+h) - F(x)| = \left|\int^{x+h}_x f(t)dt \right| \leq \int^{x+h}_x |f(t)|dt \leq \int^{x+h}_x Kdt = Kh, \quad(h>0)
\ee
In fact for any $h$,
\be
|F(x+h) - F(x)| \leq K|h| \ \ra\ \lim_{h\to 0}F(x+h) = F(x).
\ee
\end{proof}

\begin{theorem}[The Fundamental Theorem of Calculus]\label{thm:calculus}
If in addition $f$ is continuous, then $F$ is differentiable and 
\be
F'(x) = f(x) \quad \forall x\in [a,b]
\ee
\end{theorem}

\begin{proof}[{\bf Proof}]
For $x\in [a,b], \ h\neq 0$,
\beast
\left|\frac{F(x+h)-F(x)}h-f(x)\right| & = & \frac 1{|h|}|F(x+h)-F(x) - hf(x)| = \frac 1{|h|}\left|\int^{x+h}_x f(t)dt - hf(x)\right|\\
& = & \frac 1{|h|}\left|\int^{x+h}_x [f(t)-f(x)]dt \right|
\eeast
Since $|f(t)-f(x)|\leq \max|f(x+\theta h)-f(x)|,\ \theta \in [0,1], \ \forall t\in [x,x+h]$. We have
\be
\left|\frac{F(x+h)-F(x)}h-f(x)\right| \leq  \frac 1{|h|}|h| \max_{\theta\in [0,1]}|f(x+\theta h)-f(x)| = \max_{\theta\in [0,1]}|f(x+\theta h)-f(x)|
\ee
Now if $h\to 0$, then by continuity of $f$
\be
\lim_{h\to 0}\max_{\theta\in [0,1]}|f(x+\theta h)-f(x)| =0.
\ee
\end{proof}

\begin{corollary}[Integration is the 'inverse' of differentiation]
If $f=g'$ is continuous on $[a,b]$, then
\be
\int^x_a f(t)dt = g(x) - g(a),\quad \forall x\in [a,b].
\ee
\end{corollary}

\begin{proof}[{\bf Proof}]
From Theorem \ref{thm:calculus}, we know $F'(x)=f(x)$. Therefore, 
\be
(F-g)'(x) = F'(x) -g'(x) = 0 \ \ra \ F-g \text{ is constant}.
\ee
Thus,
\be
F(x)-g(x) = F(a) -g(a) = 0 - g(a) \ \ra \ \int^x_a f = g(x) - g(a).
\ee
\end{proof}

This gives a way of computing $\int^b_a f$ if we know a 'primitive' for $f$, i.e. a function $g$ s.t. $g' =f$.

Primitives of continuous functions always exist (Theorem \ref{thm:calculus}) and they all differ by a constant.

\begin{corollary}[Integration by parts]
Suppose $f'$ and $g'$ exist and are continuous on $[a,b]$, then
\be
\int^b_a f'g = f(b)g(b) - f(a)g(a) - \int^b_a fg'.
\ee
\end{corollary}

\begin{proof}[{\bf Proof}]
Product rule gives
\be
(fg)' = f'g + fg'\ \ra \ \int^b_a (f'g + fg') = fg|^b_a = f(b)g(b) - f(a)g(a).
\ee
\end{proof}

\begin{corollary}[Integration by substitution]
$g: [\alpha, \beta]\mapsto [a,b]$ with $g(\alpha)=a,\ g(\beta)=b$, and $g'$ exists and is continuous on $[\alpha, \beta]$. Let $f:[a,b]\mapsto \R$ be continuous, then
\be
\int^b_a f(x)dx = \int^\beta_\alpha f(g(t))g'(t)dt.
\ee
\end{corollary}

\begin{proof}[{\bf Proof}]
Set $F(x)=\int^x_a f(t)dt$ for $x\in [a,b]$. Let $h(t)=F(g(t))$ ($g$ takes value in $[a,b]$). Apply chain rule,
\be
h'(t) = F'(g(t))g'(t) = f(g(t))g'(t).
\ee
Hence,
\beast
\int^\beta_\alpha f(g(t))g'(t)dt = h|^\beta_\alpha = h(\beta) - h(\alpha) = F(g(\beta)) - F(g(\alpha)) = F(b)-F(a) = \int^b_a f(x)dx.
\eeast
\end{proof}

We now revisit Taylor's theorem and find an integral form for the remainder.

\begin{theorem}[Taylor's theorem with remainders an integral]
Let $f^{(n)}$ be continuous for $x\in [0,h]$, then
\be
f(h) = f(0) + f'(0) h + \dots + \frac{f^{(n-1)}(0)h^{n-1}}{(n-1)!} + R_n
\ee
where
\be
R_n = \frac {h^n}{(n-1)!} \int^1_0 (1-t)^{n-1} f^{(n)}(ht)dt.
\ee
\end{theorem}

\begin{remark}
Note that here we assume continuity of $f^{(n)}$  and not just existence (as in our previous version), so that theorem is a little weaker, but just practical purposes.
\end{remark}

\begin{proof}[{\bf Proof}]
Let's make the substitution, $u=ht$.
\beast
R_n & = & \frac {h^n}{(n-1)!} \int^h_0 \lob 1-\frac uh \rob^{n-1} f^{(n)}(u)\frac {du}h = \frac 1{(n-1)!} \int^h_0 \lob h- u \rob^{n-1} f^{(n)}(u)du\\
& = & \frac 1{(n-1)!} \left.\lob h- u \rob^{n-1} f^{(n-1)}\right|^h_0 - \frac 1{(n-1)!} \int^h_0 -(n-1)\lob h- u \rob^{n-2} f^{(n-1)}(u)du \\
& = & -\frac {h^{n-1}}{(n-1)!} f^{(n-1)}(0) + \frac 1{(n-2)!} \int^h_0 \lob h- u \rob^{n-2} f^{(n-1)}(u)du\\
& = & -\frac {h^{n-1}}{(n-1)!} f^{(n-1)}(0) + R_{n-1}
\eeast
Integrate by parts $n-1$ times to get 
\be
R_n = -\frac {h^{n-1}}{(n-1)!} f^{(n-1)}(0) - \dots - hf'(0) + \underbrace{\int^h_0f'(u)du}_{f(h)-f(0)} .
\ee
Thus, integral form gives back Cauchy's and Lagrange's form.
\end{proof}

\begin{remark}
$F(x) = \int^x_af(t)dt, \ x\in[a,b]$ ($f$ is continuous). The mean value theorem applied to $F$, $\exists c\in (a,b)$ s.t.
\be
f(h) = f(0) + hf'(0) + \cdots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(0) + R_n
\ee
where
\be
R_n = \frac{h^n}{(n-1)!}\int^1_0 (1-t)^{n-1} f^{(n)}(ht)dt.
\ee
Let's apply this to $\int^1_0 (1-t)^{n-1} f^{(n)}(ht)dt$, $\exists \theta \in (0,1)$ s.t.
\beast
& & \int^1_0 (1-t)^{n-1} f^{(n)}(ht)dt = (1-\theta)^{n-1} f^{(n)}(\theta h)\\
& \ra \ & R_n = \frac{h^n}{(n-1)!}(1-\theta)^{n-1} f^{(n)}(\theta h) \quad \quad (\text{Cauchy's form})
\eeast

To get Lagrange's form we use the following. Suppose $g\geq 0$,
\be
\int^b_a f(x)g(x)dx = f(c) \int^b_a g(x)dx, \quad\quad \text{for some } c\in [a,b].
\ee
Thus,
\beast
& & \int^1_0 \underbrace{(1-t)^{n-1}}_{'g'} \underbrace{f^{(n)}(ht)}_{'f'}dt = f^{(n)}(\theta h) \int^1_0 (1-t)^{n-1}dt = f^{(n)}(\theta h) \frac 1n \\
& \ra \ & R_n = \frac{h^n}{(n)!}f^{(n)}(\theta h)  \quad \quad (\text{Lagrange's form})
\eeast
\end{remark}

Now we prove the above statement: $f,g:[a,b]\mapsto \R$ are continuous, $g(x)>0,\ \forall x\in (a,b)$, then $\exists c\in (a,b)$ s.t.
\be
\int^b_a f(x)g(x)dx = f(c) \int^b_a g(x)dx.
\ee

\begin{proof}[{\bf Proof}]
Let
\be
H(x) = \int^x_a f(t)g(t)dt,\quad G(x)=\int^x_a g(t)dt
\ee
By Fundamental theorem of calculus
\be
H'(x) = f(x)g(x),\quad G'(x) = g(x)
\ee

Cauchy's mean value theorem (Theorem \ref{thm:mean_value}) applied to $H$ and $G$: $\exists c\in (a,b)$ s.t.
\be
G'(c) (H(b)-H(a)) = H'(c) (G(b)-G(a)) \ \ra \ g(c) \int^b_a f(t)g(t)dt = f(c)g(c)\int^b_a g(t)dt
\ee
Since $g(c)\neq 0$, we get the required result.
\end{proof}

{\flushleft \bf \underline{Infinite integrals}} (Improper integrals)

We'd like to sense of $\int^\infty_a f(x)dx$. Suppose we have $f:[a,\infty)\mapsto \R$ s.t. for every $R\geq a$. $f$ is bounded and integrable on $[0,R]$.

\begin{definition}
If $\lim_{R\to \infty}\int^x_a f(x)dx = l$ we say that $\int^\infty_a f(x)dx$ exists and that its vlaue is $l$.

In this case, we also say that $\int^\infty_a f(x)dx$ converges. If $\int^\infty_a f(x)dx$ does not tend to a limit, we say $\int^\infty_a f(x)dx$ diverges.
\end{definition}

\begin{example}
$\int^\infty_1 \frac {dx}{x^k},\quad f(x)= \frac 1{x^k}$
\be
\int^R_1 \frac {dx}{x^k} = \left.\frac {x^{1-k}}{1-k}\right|^R_1 = \frac {R^{1-k}}{1-k}
\ee
If $k\neq 1$, let $R\to\infty$, we say that the limit is finite only if $k>1$. If $k=1$, $\int^R_1 \frac{dx}{x} = \log R \to \infty$ as $R\to\infty$. So $\int^\infty_1 \frac {dx}{x^k}$ iff $k>1$.

Notice the similarities with $\sum^\infty_{n=1}\frac 1{n^k} = \sum^\infty_{n=1}f(n)$.
\end{example}

\begin{remark}
\ben
\item If $f\geq 0$, $g\geq 0$, for $x\geq a$, and $f(x)\leq Kg(x)$, $K$ is constant, then if $\int^\infty_a g$ converges, then $\int^\infty_a f$ converges and $\int^\infty_a f \leq K \int^\infty_a g$. (analogue of comparison test)

\begin{proof}[{\bf Proof}]
\be
f(x) \leq Kg(x) \ \ra \ \int^R_a f(x)dx \leq K \int^R_a g(x)dx 
\ee
The function $R\mapsto \int^R_a f(x)dx$ is an increasing function. 
\be
R_1 \geq R_2, \quad \int^{R_1}_a f(x)dx = \int^{R_2}_a f(x)dx + \int^{R_1}_{R_2} f(x)dx \ \ra \ \int^{R_1}_{R_2} f(x)dx \geq 0.
\ee
If $\int^\infty_a g$ converges, then $R\mapsto \int^R_a f$ is bounded above, and so we can consider $\sup_{R\geq a}\int^R_a f(x)dx = l$ and claim $\lim_{R\to \infty}\int^R_a f=l$.

By defintion of supremum, given $\ve>0$, $\exists R_0$ s.t.
\be
\int^{R_0}_a f > l-\ve.
\ee

If $R\geq R_0$, 
\be
\int^R_a f \geq \int^{R_0}_a f \geq  l-\ve \ \ra \ 0\leq l-\int^R_a f <\ve \ \ra \ \lim_{R\to\infty}\int^R_a f = l.
\ee
\end{proof}

\item For series, we know if $\sum a_n$ converges then $a_n \to 0$. However, it is not ture that if $\int^\infty_a f$ converges, then $f(x)\to 0$ as $x\to \infty$.

\begin{example}

\centertexdraw{
\drawdim in

\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0

\move (0.5 0) \lvec(1 1) \lvec(1.5 0) 
\move (3 0) \lvec(3.5 1) \lvec(4 0) 

\htext (-0.15 0.95){1}
\htext (0.4 -0.2){1/2}
\htext (0.95 -0.15){1}
\htext (1.4 -0.2){3/2}
\htext (3.45 -0.15){$n$}

\htext (2 0.5) {$\dots$}
\htext (3 -0.5){$\underbrace{\quad\quad\quad\quad\quad\quad }_{\frac 2{(n+1)^2}}$}

\move (-0.2 0) \avec(4.3 0)
\move (0 -0.2) \avec(0 1.3)

\lpatt(0.05 0.05)
\move (0 1) \lvec (4.3 1)
\move (1 0) \lvec (1 1)
\move (3.5 0) \lvec (3.5 1)
}
$\sum \frac 1{(n+1)^2}$ converges $\ra $ $\int^\infty_a f$ converges, but $f(n)=1$ for $n \in \N$.
\end{example}
\een
\end{remark}

\begin{theorem}[The integral test]\label{thm:int_test}
$f: [1,\infty)\mapsto \R$ be a positive decreasing function, then

(i) $\int^\infty_1 f(x)dx$ and $\sum^\infty_{n=1}f(n)$ both converge or diverge;

(ii) $\sum^n_{r=1}f(r)-\int^n_1 f(x)dx$ tends to a limit $l$ as $n\to \infty$, and $0\leq l \leq f(1)$.
\end{theorem}

\begin{proof}[{\bf Proof}]
Note that since $f$ is decreasing, it is Riemann integrable in every interval $[1,R]$ and 
\be
f(n-1) \geq \int^n_{n-1}f(x)dx \geq f(n)  \ \ra \ \sum^{n-1}_{r=1} f(r)\geq \int^n_1 f(x)dx \geq \sum^n_{r=2}f(r).
\ee

Suppose $\int^\infty_1 f(x)dx$ converges, then $\int^\infty_1 f(x)dx$ is bounded. By the above inequality, $\sum^n_{r=2}f(r)$ is bounded and therefore it must converge by the fundamental axiom (increasing $n$).

Suppose $\sum^n_{r=1}f(r)$ converges, then $\sum^n_{r=1}f(r)$ is bounded and $\int^n_1 f(x)dx$ is bounded, thus since $R\mapsto \int^R_1 f(x)dx$ is increasing, $\int^\infty_1 f(x)dx$. Thus, proves (i).

For the proof of (ii), let
\be
\phi(n) = \sum^n_{r=1}f(r)-\int^n_1 f(x)dx 
\ee
we have
\beast
\phi(n) -  \phi(n-1) & = & \sum^n_{r=1}f(r)-\int^n_1 f(x)dx - \lob \sum^{n-1}_{r=1}f(r)-\int^{n-1}_1 f(x)dx \rob \\
& = & f(n) - \int^n_{n-1}f(x)dx \leq 0
\eeast
so $\phi(n)$ is decreasing. Also, from the above inequality 
\be
0\leq \phi(n) \leq f(1).
\ee

Thus, $\phi(n)$ is a bounded decreasing sequence and therefore it must converge to a limit $l$ with $0\leq l \leq f(1)$.
\end{proof}

\begin{example}
\ben
\item $\sum^\infty_{n=1}\frac 1{n^k}$. We just saw that $\int^\infty_1\frac {dx}{x^k}$ converges iff $k>1$. We can apply the integral test to deduce that
\be
\sum^\infty_{n=1}\frac 1{n^k} \text{ converge iff } k>1.
\ee

\item $\sum^\infty_{n=1} \frac 1{n\log n}$. Let $f(x) = \frac 1{x\log x}$, we have
\be
\int^R_2 \frac{dx}{x\log x} = \log(\log x)|^R_1 = \log(\log R) - \log(\log 2)\to \infty \text{ as }R\to\infty.
\ee
Thus, it diverges.
\een
\end{example}

\begin{corollary}[Eula's constant\index{Eula's constant}]
\be
1+\frac 12 + \frac 13 + \cdots + \frac 1n - \log n\to \gamma \text{ as } n\to \infty, \quad 0\leq \gamma \leq 1.
\ee
\end{corollary}

\begin{proof}[{\bf Proof}]
$f(x)=1/x$ and apply Theorem \ref{thm:int_test}.
\end{proof}

\begin{remark}
\ben
\item Still unknown if $\gamma$ is rational/irrational;

\item $\gamma \sim 0.577$;

\item $\frac 12 < \gamma < 1$.
\een
\end{remark}

Here we prove that $\frac 12 < \gamma < 1$.

\begin{proof}[{\bf Proof}]
\be
a_n := \int^1_0 \frac{t}{n(n-t)dt},\quad n\geq 2.
\ee

We will derive an estimate for $a_n$. Let $g(t) = t$, $f(t) = 1/(n-t)$, $\exists c\in (0,1)$,
\be
a_n = \frac 1n \frac 1{n-c}\int^1_0 tdt = \frac 1{2n(n-c)} < \frac 1{2n(n-1)}\ \ra \ 0 < a_n < \frac 1{2n(n-1)}
\ee
Thus,
\be
0 < \sum^N_{n=2}a_n < \sum^N_{n=2}\frac 1{2n(n-1)} = \frac 12 \sum^N_{n=2}\lob\frac 1{n-1} -\frac 1n\rob = \frac 12 \lob 1 -\frac 1N\rob\to \frac 12.
\ee

But $a_n$ can be easily computed integrating by parts. If you do the calculation, you can get
\be
a_n = \log\lob\frac n{n-1}\rob - \frac 1n
\ee
\beast
\sum^N_{n=2}a_n & = & -\sum^N_{n=2}\frac 1n + \lob \log 2 - \log 1\rob + \lob \log 3 - \log 2\rob + \dots + \lob \log N - \log (N-1)\rob \\
& = & -\sum^N_{n=2}\frac 1n + \log N \to 1 -\gamma.
\eeast
\end{proof}

{\flushleft \bf \underline{Final remarks on improper integrals}}

So far we looked at $\int^\infty_a f(x)dx$. We can also make perfect series of $\int^a_{-\infty}f(x)dx$
\be
\lim_{R\to\infty}\int^a_{-R}f.
\ee

We can also make sense of $\int^\infty_{-\infty}f(x)dx$.

\begin{definition}
$\int^\infty_{-\infty}f(x)dx$ converges if $\int^\infty_a f(x)dx$ converges and $\int^a_{-\infty}f(x)dx$ converges. Also, if $\int^\infty_a f(x)dx = l_1$, $\int^a_{-\infty}f(x)dx = l_2$. We let $\int^\infty_{-\infty}f(x)dx = l_1+l_2$.
\end{definition}

This definition is independent of $a$
\be
\left.\ba{rcl}
\int^\infty_{a'}f(x)dx & = & \int^a_{a'}f(x)dx + \int^\infty_{a'}f(x)dx \\
\\
\int^{a'}_{-\infty} f(x)dx & = & \int^a_{-\infty}f(x)dx + \int^{a'}_{a}f(x)dx 
\ea\right\} \text{the sume will not change } l_1 + l_2.
\ee

Not quite the same as saying that $\lim_{R\to \infty}\int^R_{-R}f(x)dx$ exists.

\begin{example}

\centertexdraw{
\drawdim in

\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0

\lpatt(0.05 0.05)
\move (0.5 0.5) \lvec (0.5 0) \lvec (-0.5 0) \lvec (-0.5 -0.5) \lvec (0.5 0.5) \lfill f:0.8 
\lpatt()
\move (-0.6 0) \avec(0.6 0)
\move (0 -0.5) \avec(0 0.5)

\move (0.5 0.5) \lvec (-0.5 -0.5)

\htext (-3.5 0.3) {$\lim_{R\to\infty}\int^R_{-R}xdx = 0$}
\htext (-3.5 0) {but $\int^\infty_{-\infty}xdx$ does not exist,}
\htext (-3.5 -0.3) {because neither $\int^\infty_0 xdx$ or $\int^0_{-\infty}xdx$}
}
\end{example}

\begin{example}
$f(x) = \frac 1{\sqrt{x}}, \ x \in (0,1]$. $\delta >0$,
\be
\int^1_\delta \frac 1{\sqrt{x}} dx = 2\sqrt{x}|^1_\delta = 2(1-\delta)\to 2 \text{ as }\delta \to 0. \ \ra \ \int^1_0 \frac {dx}{\sqrt{x}} = \lim_{\delta \to 0}\int^1_\delta \frac 1{\sqrt{x}}dx.
\ee

\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercises}


\begin{exercise}
Prove that if $a_n\to a$ and $b_n\to b$ then $a_n+b_n\to a+b$.
\end{exercise}

---------------------------------------------------------

Solution. $a_n \to a$ means given $\ve >0$, $\exists N_1 $ s.t. $\forall n \geq N_1$
\be
\abs{a_n -a} < \frac {\ve}2.
\ee

Similarly, $\exists N_2$ s.t. $\forall n \geq N_2$
\be
\abs{b_n -b} < \frac {\ve}2.
\ee

Thus, given $\ve>0$, $\forall n\geq \max\{N_1,N_2\}$,
\be
\abs{(a_n + b_n) -(a+b)} \leq \abs{a_n -a} + \abs{b_n -b}< \ve \ \ra \ a_n+b_n\to a+b.
\ee

\vspace{2mm}

------------------------------------------------------------------------------------------------------------------

\begin{exercise}
Sketch the graph of $y=x$ and $y=(x^4+1)/3$, and thereby illustrate the behaviour of the real sequence $a_n$ where $a_{n+1}=(a_n^4+1)/3$. For which of the three starting cases $a_1=0$, $a_1=1$ and $a_1=2$ does the sequence converge? Now prove your assertion.
\end{exercise}

Solution. First, we plot the diagram
\begin{figure}[thb]
\centering
\resizebox{10cm}{!}{\includegraphics{Analysis/Analysis_1_1.eps}}
\end{figure}

\centertexdraw{
    
    \def\bdot {\fcir f:0 r:0.03 }
    
    \drawdim in

    \arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
    \linewd 0.01 \setgray 0 

    \move (-1 -1) \lvec(2 2)
    \move (-1 0) \clvec (1 0.5)(1.5 1)(2 1.5)
  
    \move (-1 -1) \avec (-1 0 ) \avec(0 0) \avec(0 0.3) \avec(0.3 0.3) \avec(0.3 0.4)
    \move (2 2) \avec (2 1.5) \avec(1.5 1.5) \avec(1.5 1.05) \avec(1.05 1.05) \avec(1.05 0.75) \avec(0.75 0.75) \avec(0.75 0.6)\avec(0.6 0.6)
    
    \move (0.46 0.46) \bdot   
    \htext (-1.1 -1){0}
    \htext (0.4 0.2){$r_1$}

    \move (2 -1) \lvec(5 2)
    \move (2.2 -1) \clvec (3.2 0.5)(3.4 1)(3.7 2)

    \move (2.57 -0.43) \bdot   
    \move (3 0) \avec (3 0.3) \avec (3.3 0.3) \avec (3.3 0.85) \avec (3.85 0.85) \avec (3.85 2) 
    \htext (2.8 -0.5){$r_2$}
 
\move (0 -1.2)
}

Let the two roots of $f(x) = (x^4+1)/3 - x$ be $r_1$ and $r_2$ ($r_1 < r_2$). Because $f(0)>0$, $f(1) <0$ and $f(2) >0$, we have $r_1 \in (0,1)$ and $r_2\in (1,2)$.

Graphical analysis tells us that sequences starting with $a_1 =0,1$ converge to $r_1$ and $a_1 = 2$ diverges to infinity.

$a_1 = 0$. If $a_n \in (0,r_1)$, $f(a_n) = (a_n^4+1)/3 - a_n = a_{n+1} -a_n >0\ \ra \ a_{n+1} > a_n$. But
\be
a_{n+1} = (a_n^4+1)/3 < (r_1^4+1)/3 = r_1   
\ee

Thus, $a_n$ is increasing and bounded above so it is converging.

$a_1 = 1$. If $a_n \in (r_1,r_2)$, $f(a_n) = (a_n^4+1)/3 - a_n = a_{n+1} -a_n < 0\ \ra \ a_{n+1} < a_n$. But
\be
a_{n+1} = (a_n^4+1)/3 > (r_1^4+1)/3 = r_1   
\ee

Thus, $a_n$ is decreasing and bounded above so it is converging.

$a_1 = 2$. If $a_n \in (r_2,\infty)$, $f(a_n) = (a_n^4+1)/3 - a_n = a_{n+1} -a_n >0\ \ra \ a_{n+1} > a_n$. Thus, $a_n$ is increasing and unbounded above so it is diverging.

\vspace{2mm}

------------------------------------------------------------------------------------------------------------------

\begin{exercise}
Let $a_1>b_1>0$ and let $a_{n+1} = (a_n + b_n)/2$, $b_{n+1} = 2a_nb_n/(a_n+b_n)$ for $n\geq 1$. Show that $a_n>a_{n+1}>b_{n+1}>b_n$ and deduce the two sequences converge to a common limit. What limit?
\end{exercise}

Solution. Since $a_1>b_1>0$,
\be
\left\{\ba{l}
2a_1 > a_1 + b_1 \ \ra \ a_1 > \frac{a_1 + b_1}2 = a_2\\
\frac 1{a_1} < \frac 1{b_1} \ \ra \  \frac 1{a_1} + \frac 1{b_1}  < \frac 2{b_1} \ \ra \ b_1 < \frac 2{\frac 1{a_1} + \frac 1{b_1}} = b_2\\
a_2 - b_2 = \frac{a_1 + b_1}2 - \frac {2a_1b_1}{a_1 + b_1} = \frac {(a_1 - b_1)^2}{2(a_1+b_1)} >0
\ea\right. \ \ra \ a_1 > a_2 > b_2 > b_1.
\ee

By induction, we have $a_n>a_{n+1}>b_{n+1}>b_n$. Since $a_n$ is a decreasing sequence, and bounded below (by 0), thus $a_n$ converges to $a$. Similarly, $b_n$ is an increasing sequence and bounded above (by $a_1$). Thus,
\be
\left\{\ba{l}
a = \frac {a+b}2 \\
b = \frac {2ab}{a+b}
\ea\right. \ \ra \ a = b.
\ee
which means that $a_n$ and $b_n$ converge to a common limit $c$. We know that
\be
a_{n+1}b_{n+1} = (a_n + b_n)/2 \times 2a_nb_n/(a_n+b_n) = a_n b_n \ \ra \ c^2 = a_1 b_1 \ \ra \ c = \sqrt{a_1b_1}.
\ee

\begin{exercise}
Let $[a_n,b_n], \ n=1,2,\dots$ be closed intervals with $[a_n,b_n]\cap[a_m,b_m]\neq \emptyset$ for all $n,m$. Prove that $\cap^\infty_{n=1}[a_n,b_n]\neq \emptyset$.
\end{exercise}

Solution. First, we can not use induction to prove the statements about infinity! For example, consider the sets $A_n = [n,\infty),n\in \N$. We have
\be
\bigcap^k_{n=1} A_n = A_k \neq \emptyset = \bigcap^\infty_{n=1}A_n.
\ee

\begin{exercise}
The real sequence $a_n$ is bounded but does not converge. Prove that it has two convergent subsequences with different limits.
\end{exercise}

Solution. By Balzano-Weierstrass Theorem, $a_n$ is real and bounded, so $a_n$ contains a convergent subsequence. Let its limit be $l$. Because $a_n$ is not convergent, there exist $\ve>0$ s.t. there exists infinityly many terms of $a_n$ which satisfy 
\be
\abs{a_n -l} > \ve.
\ee
Now apply Balzano-Weierstrass Theorem again for these $a_n$. There exists another subsequence converges to another limit $m \neq l$. Thus, this real sequence $a_n$ has two convergent subsequences with different limits.

\begin{exercise}
Investigate the convergence of the following series. For those expressions containing the complex number $z$, find those $z$ for which convergence occurs.
\be
\sum_n\frac{\sin n}{n^2}\quad \quad \sum_n\frac{n^2z^n}{5^n}\quad\quad \sum_n\frac{(-1)^n}{4+\sqrt{n}}\quad\quad \sum_n \frac{z^n(1-z)}{n}
\ee
\end{exercise}

Solution. \ben
\item [(i)] It's obvious that $\abs{\frac {\sin n}{n^2}} \leq \frac 1{n^2}$ and $\sum_n \frac 1{n^2}$ converges. By comparison test, $\sum_n\frac{\sin n}{n^2}$ is absolutely convergent.

\item [(ii)] Using ratio test, we check
\be
\abs{\frac {a_{n+1}}{a_n}} = \abs{\frac{z(n+1)^2}{5n^2}} \to \abs{\frac z5}  \quad \text{as }n \to \infty.
\ee

Thus, we have
\be
\sum_n\frac{n^2z^n}{5^n}\quad
\left\{\ba{ll}
\text{converges absolutely }\quad \quad & \abs{z}<5\\
\text{inconclusive} & \abs{z} = 5\\
\text{diverges} & \abs{z} >5
\ea\right..
\ee

\item [(iii)] We know that $\frac 1{4+\sqrt{n}}$ is decreasing. Thus, by alternative test, $\sum_n\frac{(-1)^n}{4+\sqrt{n}}$ converges.

\item [(iv)] Use ratio test again, 
\be
\abs{\frac {a_{n+1}}{a_n}} = \abs{\frac{zn}{n+1}} \to \abs{z}  \quad \text{as }n \to \infty.
\ee
Thus, we have
\be
\sum_n \frac{z^n(1-z)}{n}\quad
\left\{\ba{ll}
\text{converges absolutely }\quad \quad & \abs{z}<1\\
\text{inconclusive} & \abs{z} = 1\\
\text{diverges} & \abs{z} >1
\ea\right..
\ee

\een

\begin{exercise}
Show that $\sum 1/(n\log^\alpha n)$ converges if $\alpha>1$ and diverges otherwise. Does $\sum 1/(n\log n\log\log n)$ converge?
\end{exercise}

Solution. Cauchy's condensation test. $a_n$ is decreasing sequence of positive terms, then $\sum^\infty_{n=1}a_n$ converges iff $\sum^\infty_{n=1}2^n a_{2^n}$ converges.

We know that $\frac 1{n\log^\alpha n}$ is decreasing. Using Cauchy' condensation test, we have
\be
\sum \frac 1{n\log^\alpha n} \text{ converges } \ \lra \ \sum \frac {2^n}{2^n\log^\alpha 2^n} \text{ converges } \ \lra \ \frac 1{\log^\alpha 2}\sum \frac {1}{n^\alpha } \text{ converges }
\ee

Thus $\sum_n \frac {1}{n^\alpha }$ converges iff $\alpha >1$, diverges otherwise.

For the decreasing sequence $\frac 1{n\log n\log\log n}$, we apply Cauchy's condensation test again,
\beast
\sum \frac 1{n\log n\log\log n} \text{ converges }& \lra & \sum \frac {2^n}{2^n\log 2^n\log\log 2^n} \text{ converges } \\
& \lra & \frac 1{\log 2}\sum \frac {1}{n\bb{\log n + \log\log 2}} \text{ converges }\\
& \lra & \sum \frac {2^n}{2^n\bb{n\log 2 + \log\log 2}} \text{ converges }\quad\quad \text{Cauchy's condensation test once more}\\
& \lra & \sum \frac {1}{n\log 2 + \log\log 2} \text{ converges }
\eeast

It's obvious that
\be
\frac {1}{n\log 2 + \log\log 2} \text{ diverges } \ \ra \ \sum \frac 1{n\log n\log\log n} \text{ diverges.}
\ee

\begin{exercise}
Let $a_n\in\C$ and let $b_n = \frac 1n\sum^n_{i=1}a_i$. Show that, if $a_n\to a$ as $n\to\infty$, then $b_n\to a$ also.
\end{exercise}

Solution. 

\begin{exercise}
Consider the two series $1-\frac 12 + \frac 13 - \frac 14+ \frac 15- \frac 16+\cdots$ and $1+ \frac 13 -\frac 12 + \frac 15 + \frac 17 - \frac 14+ \cdots$ having the same terms but taken in a different order. Let $s_n$ and $t_n$ be the corresponding partial sums to $n$ terms. Show that $s_{2n} = H_{2n}-H_n$ and $t_{3n} = H_{4n} - \frac 12H_{2n} -\frac 12H_{n}$, where $H_n = 1 + \frac 12 + \frac 13 + \frac 14 + \frac 15 + \cdots + \frac 1n$. Show that $s_n$ converges to a limit $s$ and that $t_n$ converges to $3s/2$.
\end{exercise}

Solution. We have
\beast
s_{2n} & = &  1 - \frac 12 + \frac 13 - \frac 14 \dots + \frac 1{2n-1} - \frac 1{2n}\\
& = & 1 + \frac 12 + \frac 13 + \frac 14 + \dots + \frac 1{2n-1} + \frac 1{2n} - 2\bb{\frac 12 + \frac 14 + \dots + \frac 1{2n}}\\
& = & H_{2n} - H_n.
\eeast

\beast
t_{3n} & = &  \bb{1 + \frac 13 - \frac 12 } + \bb{\frac 15 + \frac 17 - \frac 14 } + \bb{ \frac 1{4n-3} + \frac 1{4n-1}- \frac 1{2n}}\\
& = & \bb{1 + \frac 12 + \frac 13 + \frac 14 + \dots + \frac 1{4n-1} + \frac 1{4n}} - \bb{\frac 12 + \frac 14 + \dots + \frac 1{4n-2} + \frac 1{4n}} - \bb{\frac 12 + \frac 14 + \dots + \frac 1{2n}}\\
& = & H_{4n} - \frac 12 H_{2n} - \frac 12 H_n.
\eeast

Also,
\be
s_{n} \to 1 - \frac 12 + \frac 13 - \frac 14 \dots + \dots = \sum \frac {(-1)^{n+1}}{n} = \log(1+1) = \log 2.
\ee

\beast
t_{n} & \to & \sum \bb{ \frac 1{4n-3} + \frac 1{4n-1}- \frac 1{2n}} \\
& = & \sum \bb{ \frac 1{4n-3} + \frac 1{4n-1}- \frac 1{4n-2} - \frac 1{4n}} +  \sum \bb{ \frac 1{4n-2} + \frac 1{4n}- \frac 1{2n}}\\
& = & \sum \frac {(-1)^{n+1}}{n} + \frac 12 \sum \frac {(-1)^{n+1}}{n} = \frac 32 \sum \frac {(-1)^{n+1}}{n} = \frac 32 \log 2.
\eeast

\begin{exercise}
Suppose that $\sum a_n$ diverges and $a_n>0$. Show that there exist $b_n$ with $b_n/a_n\to 0$ and $\sum b_n$ divergent.
\end{exercise}

Solution. Since $\sum a_n$ is increasing and divergent, we take a subsequence $N_k$ s.t. $N_0 = 0$, $N_1$ is the first integer where $\sum a_n$ crosses 1, similarly
\be
N_k = \inf\left\{n:\sum^n_{i=N_{k-1}+1}a_i \geq 1,\ n\in \N\right\}.
\ee

Then we define
\be
b_n := \frac {a_n}{k},\quad N_{k-1} < n \leq N_k
\ee
so that $b_n/a_n$ converges to 0 as $n\to \infty$. Also,
\be
\sum b_n = \sum^\infty_{k=1} \sum^{N_{k}}_{N_{k-1}+1} b_n = \sum^\infty_{k=1} \sum^{N_{k}}_{N_{k-1}+1} \frac {a_n}k = \sum^\infty_{k=1} \frac 1k \sum^{N_{k}}_{N_{k-1}+1}a_n \geq \sum^\infty_{k=1} \frac 1k = \infty.
\ee

Hence, $b_n$ is divergent.

\begin{exercise}[Abel's test]
Let $a_n$ and $b_n$ be two sequences and let $S_n=\sum^n_{j=1}a_j$ and $S_0=0$. Show that for any $1\leq m\leq n$ we have
\be
\sum^n_{j=m}a_jb_j = S_nb_n - S_{m-1}b_m + \sum^{n-1}_{j=m}S_j(b_j-b_{j+1}).
\ee
Suppose now that $b_n$ is a decreasing sequence of positive terms tending to zero. Moreover, suppose that $S_n$ is a bounded sequence. Prove that $\sum^\infty_{j=m}a_jb_j$ converges. Deduce the alternative series test. Does the series $\sum^\infty_{n=1}\frac{\cos(n)}{n}$ converge or diverge?
\end{exercise}

Solution. First, we have
\be
S_n b_n = a_1b_n + a_2b_n + \dots + a_nb_n,\quad \quad S_{m-1}b_m = a_1b_m + a_2b_m + \dots + a_{m-1}b_m
\ee
and
\be
\sum^{n-1}_{j=m}S_j(b_j-b_{j+1}) = (a_1 + a_2+ \dots + a_m)b_m + (a_{m+1}b_{m+1} + \dots + a_{n-1}b_{n-1}) - (a_1 + a_2 + \dots + a_{n-1})b_n
\ee

Thus, we sum these up to get
\be
S_nb_n - S_{m-1}b_m + \sum^{n-1}_{j=m}S_j(b_j-b_{j+1}) = a_m b_m + (a_{m+1}b_{m+1} + \dots + a_{n-1}b_{n-1}) + a_n b_n = \sum^n_{j=m}a_jb_j.
\ee

Also, we know that $S_n$ is bounded, $\abs{S_n} \leq k$ for some $k$ and $\forall n\in \N$
\beast
\abs{\sum^n_{j=m} a_j b_j} & \leq & \abs{S_nb_n} + \abs{S_{m-1}b_m} + \abs{\sum^{n-1}_{j=m}S_j(b_j-b_{j+1})}\\
& \leq & k b_n + k b_m + k \abs{b_m - b_{n-1}} = 2k b_m.
\eeast

Since $\lim_{m\to\infty}b_m = 0$, $2kb_m$ can be arbitrarily small so by Cauchy's criterion, $\abs{\sum^n_{j=1} a_j b_j}$ converges, so does $\abs{\sum^n_{j=m} a_j b_j}$.

Now let $a_n = (-1)^{n+1}$ then $S_n = 0 \text{ or }1$ which is bounded. Then by Abel's test, the alternative series test holds. 

For the series $\sum^\infty_{n=1}\frac{\cos(n)}{n}$, we have $b_n = \frac 1n$, and 
\be
\sum^n_{k=1} a_k = \sum^n_{k=1} \cos k = \Re\bb{\sum^n_{k=1} e^{ki}} = \Re\bb{\frac{e^{(n+1)i}-1}{e^i - 1}}
\ee
is bounded. Thus, with Abel's test, we know that the series converges.

\begin{exercise}
For $n\geq 1$, let 
\be
a_n = \frac 1{\sqrt{n}} + \frac{(-1)^{n-1}}{n}.
\ee
Show that each $a_n$ is positive and that $\lim a_n =0$. Show also that $\sum^\infty_{n=1}(-1)^{n-1}a_n$ diverges. [This shows that, in the alternative series test, it is essential that the moduli of the terms decrease as $n$ increases.]
\end{exercise}

Solution. Since $n\geq 1$, $\frac 1{\sqrt{n}}\geq \frac 1n$. Thus, we have $a_n \geq 0$. Also,
\be
\lim_{n\to\infty}a_n = \lim_{n\to\infty}\frac 1{\sqrt{n}} + \lim_{n\to\infty}\frac{(-1)^{n-1}}{n} = 0 + 0 = 0.
\ee

Assume $\sum^\infty_{n=1}(-1)^{n-1}a_n$ converges, we have
\be
\sum^\infty_{n=1}(-1)^{n-1}a_n = \sum^\infty_{n=1}(-1)^{n-1}\frac 1{\sqrt{n}} + \sum^\infty_{n=1}\frac 1n.
\ee
The first part converges by alternative test and second part diverges. Hence, this contradiction gives the divergence of the series.

\begin{exercise}
Let $z\in \C$. Show that the series
\be
\frac{z}{1-z^2} + \frac{z^2}{1-z^4} + \frac{z^4}{1-z^8} + \frac{z^8}{1-z^{16}} + \cdots
\ee
converges to $z/(1-z)$ if $|z|<1$, converges to $1/(1-z)$ if $|z|>1$, and diverges if $|z|=1$.
\end{exercise}

Solution. Let $S_n$ be the partial sum of the $n$th partial sum and assume
\be
S_n = \frac 1{1-z} - \frac 1{1-z^{2^n}}.
\ee

Thus,
\be
S_1 = \frac 1{1-z} - \frac 1{1-z^{2}} = \frac z{1-z^2}
\ee
holds. Then by induction, 
\be
S_k = \frac 1{1-z} - \frac 1{1-z^{2^k}}  \ \ra \ S_{k+1} =  \frac 1{1-z} - \frac 1{1-z^{2^k}} + \frac {z^{2^k}}{1-z^{2^{k+1}}} = \frac 1{1-z} - \frac 1{1-z^{2^{k+1}}}. 
\ee
So $S_n$ is true for all $n\in \N$.

If $|z|<1$, as $n\to\infty$, $z^{2^n} \to 0$, thus $S_n \to \frac 1{1-z}-1 = \frac z{1-z}$ as $n\to\infty$.

If $|z|>1$, $\frac 1{|z^{2^n}|}\to 0$ as $n\to\infty$, so $\frac 1{1-z^{2^n}}\to 0$ and $S_n \to \frac 1{1-z}$ as $n\to\infty$.

If $|z|=1$, $S_n$ converges iff $\frac 1{1-z^{2^n}}\to 0$. We have
\be
\abs{\frac 1{1-z^{2^n}}-0} = \abs{\frac 1{1-z^{2^n}}} = \frac 1{\abs{1-z^{2^n}}} \geq \frac 1{1+\abs{z^{2^n}}} = \frac 12 \neq 0
\ee
Thus, we can say that $S_n$ diverges.

\begin{exercise}
Prove that every real sequence has a monotonic subsequence. Deduce the Bolzano-Weierstrass theorem. Show that the Bolzano-Weierstrass theorem is in fact equivalent to the fundamental axim, that is, give a proof of the fundamental axiom assuming Bolzano-Weierstrass.
\end{exercise}

Solution. Let $\{x_n\}$ be a real sequence. Let's call $x_N$ a 'peak' point if $x_N\geq x_i,\ \forall i >N$. 
\ben
\item [(i)] If there are inifinitely many 'peak' points, then these 'peak' points can form a monotonically decreasing subsequence.
\item [(ii)] If there are only finitely many 'peak' ponts, let them be $x_{N_1},x_{N_2},\dots,x_{N_k}$. Pick $m_1>N_k$, then there exists $m_2>m_10$ s.t. $x_{m_2} > x_{m_1}$ (since $x_{m_1}$ is not a 'peak' point). Again pick $m_3>m_2$ s.t. $x_{m_3} > x_{m_2}$. Continuing this procedure, we have a sequence $x_{m_i},\ i \in \N$ which is a monotonically increasing subsequence.
\een
Thus, every real sequence has a monotonic subsequence.

\vspace{4mm}

{\bf The Bolzano-Weierstrass Theorem}. If $x_n\in\mathbb{R}$ and there exists $K$ s.t. $|x_n|<K,\forall n$, then we can find $n_1<n_2<\dots$ and $x\in\mathbb{R}$ s.t. $x_{n_j}\to x$ as $j\to \infty$. (every bounded sequence has a convergent subsequence.)

{\bf Proof}. We see $[a_n,b_n] = [-K,K]$, let $c=\frac{a_0+b_0}{2}$ (mid-point), thus, either
\ben
\item [(i)] $x_n\in[a_0,c]$ for infinitely many values of $n$;
\item [(ii)] $x_n\in[c,b_0]$ for infinitely many values of $n$.
\een

In case (i), set $a_1=a_0, b_1=c$; 

In case (ii), set $a_1=c,b_1=b_0$. 

Proceed inductively to obtain sequences $a_n$ and $b_n$ s.t. the following holds ($m\leq n$)
\ben
\item [(i)] $a_m\leq a_n\leq b_n\leq b_m$; ($a_n$ is a bounded increasing sequence, $b_n$ is a bounded decreasing sequence.)
\item [(ii)] $x_m\in[a_n,b_n]$ for infinitely many values of $m$;
\item [(iii)] $b_n-a_n=\frac{b_{n-1}-a_{n-1}}{2}$.
\een

By the Fundamental Axiom, $a_n\to a$ and $b_n\to b$. By property (iii) passing to the limit,
\begin{equation*}
b-a = \frac{b-a}{2}\ \Rightarrow \ a=b
\end{equation*}

Having selecting $n_j$ s.t. $x_{n_j}\in[a_j,b_j]$, select $n_{j+1}>n_j$ s.t. $x_{n_{j+1}}\in[a_{j+1},b_{j+1}]$, we can always do this because $x_m\in[a_{j+1},b_{j+1}]$ for infinitely many values of $m$. In other words, 
\be
\left\{\ba{c}
a_j\leq x_{n_j} \leq b_j, \forall j\\
\\
a_j\to a,\ b_j\to b=a
\ea\right.\ \Rightarrow \ x_{n_j}\to a. \quad\quad \fbox{}
\ee

\begin{exercise}
Can we write the open interval $(0,1)$ as a disjoint union of closed intervals of positive length?
\end{exercise}

Solution. 

\vspace{2mm}

------------------------------------------------------------------------------------------------------------------

\begin{exercise}
Is there an enumeration of $\Q$ as $q_1,q_2,q_3,\dots$ such that $\sum(q_n-q_{n+1})^2$ converges?
\end{exercise}

Solution. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}
Define $f : \R \to \R$ by $f(x) = x$ if $x \in \Q$ and $f(x) = 1-x$ otherwise. Find $\{a : f \text{ is continuous at }a\}$.
\end{exercise}

Solution. $\forall \ve >0$, let $\delta = \ve$. Then for $x$ s.t. $\abs{x-\frac 12} < \delta$, 
\be
\abs{f(x)-f\bb{\frac 12}} = \left\{\ba{ll}
\abs{x-\frac 12} < \delta = \ve & \text{if }x\in \Q\\
\abs{1-x-\frac 12} = \abs{x-\frac 12}  < \delta = \ve  \quad\quad & \text{if }x\in \R\backslash\Q
\ea\right.
\ee
Thus $f(x)$ is continuous at $\frac 12$.



\emph{Approach 1}. Now if $f(x)$ is also continuous at $a>\frac 12$. Let $a=\frac 12 + \ve$, $\ve >0$ and $x,y >a$, $\exists \delta >0$ 
\be
\abs{x-a} < \delta, \ x \in \Q,\quad\quad \abs{y-a} < \delta, \ y \in \R\backslash\Q
\ee
such that
\be
\abs{f(x)-f(a)} < \ve,\quad \abs{f(y)-f(a)} < \ve \ \ra \ \abs{f(x)-f(y)} < 2\ve.
\ee

However,
\be
\abs{f(x)-f(y)} = \abs{x+y-1} = x+y-1 > 2a-1 = 2\bb{\frac 12+\ve} -1 = 2\ve \ \ra \ \text{Contradiction.}
\ee

\emph{Approach 2}. If $f(x)$ is continuous at $a\neq \frac 12$, then for any sequence $x_n \to a$, $f(x_n)\to f(a)$. Thus, we pick the sequence $y_n \in \R\backslash\Q$ s.t. $y_n \to a$, then $f(y_n) \to a$.
\be
a =  \lim_{n\to \infty}f(y_n) =  \lim_{n\to \infty}1-y_n = 1 -  \lim_{n\to \infty}y_n = 1-a \ \ra \ a = \frac 12. \quad \text{Contradiction.}
\ee

Hence, $f(x)$ is only continuous at $\frac 12$.

\begin{exercise}
Write down the definition of "$f(x) \to \infty$ as $x \to \infty$". Prove that $f(x) \to \infty$ as $x \to\infty$ if, and only if, $f(x_n) \to \infty$ for every sequence such that $x_n \to\infty$.
\end{exercise}

Solution. Definition. $f(x) \to \infty$ as $x \to \infty$ if $\forall \ve >0$, $\exists \delta >0$ s.t. $\forall x> \delta$, $f(x)>\ve$.

Proof. '$\Longrightarrow$'.
\beast
\left\{\ba {l}
f(x) \to \infty \text{ as }x \to \infty \ \ra \ \forall \ve >0,\ \exists \delta >0 \text{ s.t. }\forall x> \delta,\ f(x)>\ve\\
x_n \to \infty \ra \forall \delta >0,\ \exists N>0 \text{ s.t. } \forall n\geq N,\ x_n > \delta
\ea\right.\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\\
\ra
\left\{\ba{l}
\forall \ve >0,\ \exists N >0 \text{ s.t. }\forall n \geq N,\ f(x_n)>\ve\\
\text{ which means }f(x_n) \to \infty.
\ea\right.
\eeast

'$\Longleftarrow$'. We have
\be
f(x) \nrightarrow \infty \text{ as }x \to \infty\ \ra \ \forall \delta >0,\ \exists \ve>0 \text{ s.t. }\exists x > \delta,\ f(x)<\ve.
\ee

Choose $\delta_n = n$, thus there is a sequence $x_n > \delta_n$, so $x_n \to \infty$. As $n=\delta_n \to\infty$,
\be
\exists \ve>0, n>0, \ f(x_n) <\ve\ \ra \ f(x_n) \nrightarrow \infty.
\ee
%
%\beast
%\left\{\ba {l}
%f(x) \nrightarrow \infty \text{ as }x \to \infty\ \ra \ \forall \delta >0,\ \exists \ve>0 \text{ s.t. }\exists x > \delta,\ f(x)<\ve\\
%x_n \to \infty \ra \forall \delta >0,\ \exists N>0 \text{ s.t. } \forall n\geq N,\ x_n > \delta
%\ea\right. \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\\
%\\
% \ra 
%\forall \delta >0,\ \exists \ve>0, N>0 \text{ s.t. }\exists n \geq N, x_n > \delta,\ f(x_n)<\ve\quad\quad\quad\quad\quad\quad\quad\quad\\
%\\
% \ra \left\{\ba{l}
%\forall N>0,\ \exists \ve>0, n\geq N \text{ s.t. } \ f(x_n)<\ve \\
%\text{ which means }f(x_n) \nrightarrow \infty.
%\ea\right.
%\eeast

\begin{exercise}
Suppose that $f(x) \to l$ as $x \to a$ and $g(y) \to k$ as $y \to l$. Must it be true that $g(f(x)) \to k$ as $x \to a$?
\end{exercise}

Solution. No. If $f(x)$ is continuous at $a$ and $g(y)$ is not continuous at $l$, let's say
\be
f(x) = 2,\quad\quad g(y) = \left\{\ba{ll}
1 \quad\quad & y \neq 2\\
3 & y = 2
\ea\right.
\ee
which means that $l = 2$, $k=1$. However,
\be
g(f(x)) \to g(2) = 3 \neq 1 \ (k) \quad\quad \text{as }x\to a.
\ee

\begin{exercise}
Let $f_n : [0, 1] \to [0, 1]$ be continuous, $n \in \N$. Let $h_n(x) = \max\{f_1(x), f_2(x), \dots, f_n(x)\}$. Show that $h_n$ is continuous on $[0, 1]$ for each $n \in \N$. Must $h(x) = \sup\{f_n(x) : n \in \N\}$ be continuous?
\end{exercise}

Solution. Realizing for two continuous functions $f$ and $g$
\be
\max\{f,g\} = \frac{(f+g)+\abs{f-g}}2
\ee

We have $f+g$ and $abs{f-g}$ as continuous functions as well. Thus, $\max\{f,g\}$ is continuous. Also, we know that

\be
\max\{f_1(x), f_2(x), \dots, f_n(x)\} = \max\{f_1(x),\max\{f_2(x),\dots,f_n(x) \}\}.
\ee

By induction, we have that $h_n(x)$ is continuous.

Now consider $f_n(x) = \min\{nx,1\},\ x\in[0,1]$. Obviously, $f_n(x)$ is continuous. For any $x\in(0,1]$, there exists $N\in \N$ s.t. $Nx > 1$. Thus, $\forall n\geq N$, $f_n(x) = 1$. This implies that 
\be
h(x) = \sup\{f_n(x) : n \in \N\} = 1,\quad x\in (0,1].
\ee

Then consider $x=0$, we have $f_n(x) = 0$ for all $n$. Hence we have
\be
h(x) = \left\{\ba{ll}
1 \quad\quad & x\neq 0\\
0 & x= 0
\ea\right.
\ee
which is NOT continuous on $[0,1]$.

\begin{exercise}
The unit circle in $\C$ is mapped to $\R$ by a map $e^{i\theta} \mapsto f(\theta)$, where $f : [0, 2\pi] \to \R$ is continuous and $f(0) = f(2\pi)$. Show that there exist two diametrically opposite points that have the same image.
\end{exercise}

Solution. Consider $g(x) = f(x+\pi)-f(x)$, $x\in [0,\pi]$. Then we have three cases:
\ben
\item [(i)] If $f(0) = f(\pi)$, 0 and $\pi$ are diametrically oppositte points, done.
\item [(ii)] If $f(0) > f(\pi)$, 
\be
g(0) = f(\pi) -f(0) < 0, \quad g(\pi) = f(2\pi) - f(\pi) = f(0) - f(\pi) > 0.
\ee
By I.V.T., $\exists x^* $ such that $g(x^*) = 0$ which implies that $f(x^*+\pi) = f(x^*)$.
\item [(iii)] If $f(0) < f(\pi)$, use the same argument.
\een

\begin{exercise}
Let $f(x) = \sin^2 x + \sin^2(x + \cos^7 x)$. Assuming the familiar features of $\sin$ without justification, prove that there exists $k > 0$ such that $f(x) \geq k$ for all $x \in \R$.
\end{exercise}

Solution. First, we have $f(x)$ is non-negative and continuous on $[0,2\pi]$. Thus, there exists $k \geq 0$ such that $f(x)\geq k$. If $f(x) = 0$, we have 
\be
\left\{\ba{l}
\sin^2 x = 0\\
\sin^2(x + \cos^7 x) = 0
\ea\right. \ \ra \ 
\left\{\ba{l}
x = k\pi,\ k\in \Z \\
x + \cos^7 x = m\pi,\ m\in \Z
\ea\right. \ \ra \ \cos^7 x = (m-k)\pi
\ee
which gives
\beast
 -1\leq (m-k)\pi\leq 1 & \ra & k-\frac 1{\pi} \leq m \leq k+ \frac 1{\pi} \\
& \ra &  m=k \ \ra \ \cos^7x = 0 \ \ra \ \cos x = 0 \ \ra \ x = n\pi + \frac {\pi}2, \ n\in\Z \quad \text{contradiction}.
\eeast
Thus, there is no $x$ satisfying $f(x) = 0$ and $f(x) >0$. So $k$ can not be 0, then we have $k>0$.

\begin{exercise}
Suppose that $f : [0, 1] \to \R$ is continuous, that $f(0) = f(1) = 0$, and that for every $x \in (0, 1)$ there exists $0 < \delta < \min\{x, 1 - x\}$ with $f(x) = (f(x - \delta) + f(x + \delta))/2$. Show that $f(x) = 0$ for all $x$.
\end{exercise}

Solution. Since $f:[0,1]\mapsto \R$ is continuous, $\exists K$ s.t. $f(x)\leq K$ and $K$ can be achieved. ($K\geq 0$)

Let $S =\{x:f(x) = K, x\in [0,1]\}$. Since $S$ is bounded below (by zero), $\inf S$ exists. Let $r= \inf S$. By definition of infimum, $\exists x_n \in [0,1]$ s.t. 
\be
r+\frac 1n \geq x_n \geq r,\quad \quad f(x_n) = K.
\ee

By Bolzano-Weierstrass Theorem, $\exists x_{n_j} \to a$ as $j\to \infty$, thus
\be
r \leq x_{n_j} \leq r+ \frac1{n_j} \to r \text{ as }j\to \infty\ \ra \ r=a \ \ra \ \lim_{j}f(x_{n_j}) = f(a) =f(r) = K.
\ee

If $0<r<1$, $f(r-\delta) <K$ (since $r$ is infimum of $S$), we have
\be
f(x) = (f(x - \delta) + f(x + \delta))/2 \ \ra \ f(r) = (f(r - \delta) + f(r + \delta))/2 \ \ra \ f(r+\delta) > K.
\ee
This contradiction gives $r=0$, so $K = 0$. 

Similarly, $\exists L$ s.t. $f(x)\geq L$ and $L$ can be achieved. ($L\leq 0$). Using the same argument, we have $K=L=0$ which means that $f(x)=0$ for all $x$.

\begin{exercise}
Let $f : [a, b] \to \R$ be bounded. Suppose that $f((x + y)/2) \leq (f(x) + f(y))/2$ for all $x, y \in [a, b]$. Prove that $f$ is continuous on $(a, b)$. Must it be continuous at $a$ and $b$ too?
\end{exercise}

Solution. For $x\in (a,b)$, we consider $x,x+h,x+2h,\dots$. With the given inequality, we have
\beast
2f(x+h) \leq f(x) + f(x+2h) & \ra & 2\bb{f(x+h)-f(x)} \leq f(x+2h)-f(x) \\
& \ra & 2^k\bb{f(x+h)-f(x)} \leq f(x+2^k h)-f(x)
\eeast

We require that
\be
a \leq x+ 2^k h \leq b \ \ra \ h \leq 2^{-k}\min\{\abs{x-a},\abs{x-b}\}.
\ee

Also, we know that $f(x)$ is bounded, thus, $\exists M \geq 0$ such that
\be
\abs{f(x)} \leq M, \quad f(y)-f(x)\leq 2M, \ \forall x,y \in (a,b)
\ee

Thus, $\forall \ve >0$, if $f(x+h)-f(x) \geq \ve$, we have
\be
2^k\ve \leq 2^k\bb{f(x+h)-f(x)} \leq f(x+2^k h)-f(x) \ \ra \ f(x+2^k h)-f(x) \geq 2^k\ve
\ee

If we pick $k$ s.t.
\be
2^k\ve > 2M \ \bb{\text{i.e. }2^{-k} < \frac{\ve}{2M}},
\ee
then this is a contradiction for the bounded condition then implies $f(x+h)-f(x) < \ve$. So $\forall \ve>0$, we can have $f(x+h)-f(x) < \ve$ by choosing
\be
h \leq 2^{-k}\min\{\abs{x-a},\abs{x-b}\} < \frac{\ve}{2M} \min\{\abs{x-a},\abs{x-b}\}.
\ee

Hence, $f(x)$ is continuous on $(a,b)$. 

Now consider the case
\be
f(x) = \left\{\ba{ll}
1 \quad\quad & x=a,b\\
0 & x\in (a,b)
\ea\right.
\ee
The inequality 
\be
f((x + y)/2) \leq (f(x) + f(y))/2, \ \forall x, y \in [a, b]
\ee
is satisfied but $f(x)$ is NOT continuous at $a$ and $b$.

\begin{exercise}
Prove that $2x^5 +3x^4 +2x +16 = 0$ has no real solutions outside $[-2,-1]$ and exactly one inside.
\end{exercise}

Solution. First we have
\be
f(-1) = 15,\quad f(-2) = -4.
\ee

With I.V.T., $\exists c \in [-2,-1]$ s.t. $f(c) = 0$. Also, we have
\be
f'(x) = 10x^4+ 12x^3 + 2 = 2x^3(5x+6) + 2 =  2(x+1)(5x^3+x^2-x+1).% 10x^3 (x+1) + 2x^3 + 2=
\ee

Let $g(x) = 5x^3+x^2-x+1$. Thus,
\be
g'(x) = 15x^2 +2x -1 \ \ra g'(x) > 0, \ x \leq -1
\ee

With $g(-1) = -2$, we have $g(x) <0$, $x\leq -1$. Thus,
\be
f'(x) = 2(x+1)g(x) \geq 0,\ x\leq -1.
\ee

Hence, there is only one solution in $[-2,-1]$ and no solution in $(-\infty,-2]$. Furthermore, for $x\geq -1$, 
\be
f(x) = 2x^5 +3x^4 +2x +16 \geq -2 + 0 + -2 + 16 = 12 > 0.
\ee
Thus, there is no solution outline $[-2,-1]$.

\begin{exercise}
Let $f : [a, b] \to \R$ be continuous on $[a, b]$ and differentiable on $(a, b)$. Which of (1)-(4) must be true?
\ben
\item [(1)] If $f$ is increasing then $f'(x) \geq 0$ for all $x \in (a, b)$.
\item [(2)] If $f'(x) \geq 0$ for all $x \in (a, b)$ then $f$ is increasing.
\item [(3)] If $f$ is strictly increasing then $f'(x) > 0$ for all $x \in (a, b)$.
\item [(4)] If $f'(x) > 0$ for all $x \in (a, b)$ then $f$ is strictly increasing.
\een
[Increasing means $f(x) \leq f(y)$ if $x < y$, and strictly increasing means $f(x) < f(y)$ if $x < y$.]
\end{exercise}

Solution. \ben
\item [(1)] True. $\forall x\in(a,b)$, 
\be
f'(x) = \lim_{h\to 0} \frac{f(x+h)-f(x)}{h}
\ee
since
\be
\forall h>0,\ \frac{f(x+h)-f(x)}{h} \geq 0 \quad \text{(definition of increasing function)}
\ee
Thus,
\be
\lim_{h\to 0} \frac{f(x+h)-f(x)}{h} \geq 0 \ \ra \ f'(x) \geq, \ \forall x\in(a,b).
\ee
\item [(2)] True. Let $x,y\in [a,b]$ with $x<y$. The mean value theorem says 
\be
f(y)-f(x) = f'(c)(y-x), \quad \exists c\in (x,y).
\ee

Then $f'(c)\geq 0 \ \Rightarrow \ f(y)-f(x) = f'(c)(y-x) \geq 0 \ \ra \ f(y) \geq f(x)  \ra f(x) \text{ is increasing}$.

\item [(3)] False. Let $f(x) = x^3,\ x\in[-1,1]$. $f(x)$ is strictly increasing, but $f'(0) = 0$.
\item [(4)] True. Similar argument with (1).
\een

\begin{exercise}
Let $f : \R \to \R$ be differentiable for all $x$. Prove that if $f'(x) \to l$ as $x \to \infty$ then $f(x)/x \to l$. If $f(x)/x \to l$ as $x \to \infty$, must $f'(x)$ tend to a limit?
\end{exercise}

Solution. Without loss of generality, we assume $l=0$, since
\be
g(x) = f(x) - lx \ \ra\ g'(x) = f'(x) -l,\quad \frac{g(x)}{x} = \frac{f(x)}{x} -l.
\ee

With condition provided, we get $\forall \ve >0$, $\exists K >0$ s.t. 
\be
\abs{g'(x)} < \ve,\quad \forall x \geq K.
\ee

Now for any $y<x$,
\be
\abs{\frac{g(x)}{x}} \leq \abs{\frac{g(x)-g(y)}{x}} + \abs{\frac{g(y)}{x}}
\ee

Thus, $\forall \ve>0$, 
\be
\abs{\frac{g(x)-g(y)}{x}} \leq \abs{\frac{g(x)-g(y)}{x-y}} = \abs{g'(x^*)},\ x^*\in (y,x)\quad \bb{\text{by M.V.T. since $g(x)$ is differentiable}}
\ee
So $\exists K_1 >0$ s.t. 
\be
\abs{g'(x^*)} < \frac{\ve}2,\quad \forall x^* \geq K_1.
\ee

Also, $\exists K_2$ s.t.
\be
\abs{\frac{g(K_1)}{x}} < \frac {\ve}2, \quad \forall x\geq K_2
\ee

Thus, $\forall \ve>0$, $\exists K = \max\{K_1,K_2\}$ s.t.
\be
\abs{\frac{g(x)}{x}} \leq \abs{\frac{g(x)-g(K_1)}{x}} + \abs{\frac{g(K_1)}{x}} \leq \abs{g'(x^*)} + \abs{\frac{g(K_1)}{x}} < \frac{\ve}2 + \frac{\ve}2 = \ve, \quad \forall x\geq K.
\ee

It can be proved by L'H\^opital rule which is proved by using M.V.T..
\be
f(x)\to \infty,\ x \to \infty \ \ra \ \lim \frac{f'(x)}{x'} = \lim \frac{f(x)}{x} = l.
\ee

However, the inverse does not hold. For example, 
\be
f(x) = \sin x \ \ra \ \frac{f(x)}{x} = \frac{\sin x}{x} \to 0
\ee
but $f'(x) = \cos x$ does not converge.

\begin{exercise}
Let $f(x) = x + 2x^2 \sin(1/x)$ for $x \neq 0$ and $f(0) = 0$. Show that $f$ is differentiable everywhere and that $f'(0) = 1$, but that there is no interval around 0 on which $f$ is increasing.
\end{exercise}

Solution. For the function $f(x)$
\be
f'(x) = 1 + 4x\sin (1/x) - 2\cos(1/x),\quad x\neq 0
\ee
and
\be
f'(0) = \lim_{h\to 0} \frac{f(h)-f(0)}{h-0} = \lim_{h\to 0} \frac{f(h)}h = \lim_{h\to 0} \frac{h+ 2h^2 \sin(1/h)}{h} = 1+ 2\lim_{h\to 0}h \sin(1/h) = 1.
\ee

Thus, $f$ is differentiable everywhere. 

Now assume there is an interval around 0 on which $f$ is increasing. By the previous result, we have $f'(x)\geq 0$ for all $x$ on this interval.

However, $\forall \ve >0$, $\exists n$ s.t. $x = \frac 1{2n\pi} < \ve$ and
\be
f'(x) = 1 + 4x\sin (1/x) - 2\cos(1/x) = 1 + \frac2{n\pi} \sin (2n\pi) - 2\cos (2n\pi) = 1 -2 = -1 < 0.
\ee
The contradiction gives that there is no interval around 0 on which $f$ is increasing.

\begin{exercise}
Let $f :\R \to \R$ be a function which has the intermediate value property: If $f(a) < c < f(b)$, then $f(x) = c$ for some $x$ between $a$ and $b$. Suppose also that for every rational $r$, the set $S_r$ of all $x$ with $f(x) = r$ is closed, that is, if $x_n$ is any sequence in $S_r$ with $x_n \to a$, then $a \in S_r$. Prove that $f$ is continuous.
\end{exercise}

Solution. Proof by contradiction. Assume all the assumptions hold and $f(x)$ is NOT continuous at some points. That is saying there exists a convergent sequence $\{x_n\}$, $x_n \to a$ s.t. 
\be
\forall \delta >0, \ \exists \ve>0,\ \exists i \in \N \text{ s.t. } \abs{x_i -a}< \delta,\ \abs{f(x_i)-f(a)}>\ve.
\ee

Now without loss of generality, let $\{x_{n_j}\} =\{y_{j}\}$ be the subsequence s.t. $y_j<a$ satisfying the above condition. Then by the intermediate value property, $\forall k \in \N$, $\exists r \in (f(a),f(y_k))$ (or $(f(y_k),f(a))$) s.t. $f(z_k)=r$ for some $y_k < z_k < a$. 

Thus, we obtain a convergent sequence $\{z_k\}$ with $z_k \to a$ (since $y_k \to a$ and $y_k < z_k < a$). Also, $f(z_k) = r$, $\forall k\in\N$, $\lim_{k\to\infty}f(z_k) \neq f(a)$. Thus, $a\notin S_r$, so $S_r$ is not closed.

Hence, $f(x)$ must be continuous everywhere.

\begin{exercise}
Suppose that $f : \R \to \R$ satisfies $|f(x)-f(y)| \leq |x-y|^2$ for all $x, y \in \R$. Show that $f$ is constant.
\end{exercise}

Solution. Suppose $f$ is not constant. Take $a,b\in \R$ with $a<b$ and $f(a)\neq f(b)$, then choose $n\in\N$ with 
\be
n>\frac{(b-a)^2}{\abs{f(b)-f(a)}}
\ee
and for $i=0,1,\dots,n$ set 
\be
c_i = a + \frac in(b-a)
\ee
so that $c_0 = a$ and $c_n = b$. Then
\be
\abs{f(b)-f(a)} = \abs{f(c_n)- f(c_0)} \leq \sum^n_{i=1}\abs{f(c_i) -f(c_{i-1})} \leq \sum^n_{i=1}\abs{c_i - c_{i-1}}^2 = \sum^n_{i=1}\bb{\frac{b-a}n}^2 = \frac {(b-a)^2}n < \abs{f(b)-f(a)}.
\ee

The contradiction implies that $f$ must be constant. An alternative way to prove is to use $f'(x)$.

\begin{exercise}
Given $\alpha \in \R$, define $f_\alpha : [-1, 1] \to \R$ by $f_\alpha(x) = x^\alpha \sin(1/x)$ for $x \neq 0$ and $f_\alpha(0) = 0$. Is $f_0$
continuous? Is $f_1$ differentiable? Draw a table, with 4 columns labelled 0, 1, 2, 3 and with 6 rows labelled "$f_\alpha$ bounded", "$f_\alpha$ continuous2, "$f_\alpha$ differentiable", "$f'_\alpha$ bounded", "$f'_\alpha$ continuous", "$f'_\alpha$ differentiable". Place ticks and crosses at appropriate places in the table. 

Does $|x|^\alpha \sin(1/x)$ behave the same way? Complete 5 extra columns, for $\alpha = -\frac 12 , \frac 12 , \frac 32 , \frac 52 , \frac 72$.
\end{exercise}

Solution. The table is
\begin{center}
\begin{tabular}{l|ccccccccc}
 & \ $-\frac 12 $ \ & \ 0 \ & \ $\frac 12 $ \ & \ 1 \ & \ $\frac 32 $ \ & \ 2 \ & \ $\frac 52 $ \ & \ 3 \  & \ $\frac 72$ \ \\ \hline
$f_\alpha$ bounded & $\times$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$  \\ 
$f_\alpha$ continuous & $\times$ & $\times$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$  \\ 
$f_\alpha$ differentiable & $\times$ & $\times$ & $\times$ & $\times$ & $\surd$ & $\surd$ & $\surd$ & $\surd$ & $\surd$  \\ 
$f_\alpha'$ bounded & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\surd$ & $\surd$ & $\surd$ & $\surd$  \\ 
$f_\alpha'$ continuous & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\surd$ & $\surd$ & $\surd$  \\ 
$f_\alpha$ differentiable & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\surd$  
\end{tabular}
\end{center}

Both functions behave in the same way for $\alpha\in\Z$. 
\beast
|x|^\alpha \sin(1/x)\text{ is bounded } & \lra & \alpha \geq 0,\\
|x|^\alpha \sin(1/x)\text{ is continuous } & \lra & \lim_{h\to 0} \abs{h}^\alpha \sin \frac 1h = 0 \lra \alpha >0, \\
|x|^\alpha \sin(1/x)\text{ is differentiable} & \lra & \lim_{h\to 0}\frac{\abs{h}^\alpha \sin \frac 1h}{h} \text{ exists} \lra \alpha >1. 
\eeast

Thus, we obtain the table shown.

\begin{exercise}
By applying the mean value theorem to $\log(1 + x)$ on $[0, a/n]$ with $n > |a|$, prove carefully that $(1 + a/n)^n \to e^a$ as $n \to \infty$.
\end{exercise}

Solution. Set $f(x) = \log (1+x)$ and fix $a\in\R$. If $a =0$, then
\be
\bb{1+\frac an}^n = 1 = e^0 = e^a, \ \forall n \in \N.
\ee

So we assume $a\neq 0$, given $n > |a|$, apply the M.V.T. to either $\bsb{0,\frac an}$ or $\bsb{\frac an, 0}$ according as $a>0$ or $a<0$, there exists $x\in \bb{0,\frac an}$ with 
\be
f'(x) = \frac{f\bb{\frac an}-0}{\frac an - 0} \quad \text{i.e.},\quad \frac 1{1+x} = \frac na \log \bb{1+\frac an}.
\ee

As $n\to \infty$, since $|x| < \frac {\abs{a}}n$ we have $x\to 0$, so
\be
\frac 1{1+x} \to 1 \ \ra \ \frac na \log \bb{1+\frac an} \to 1 \ \log \bb{1+\frac an}^n \to a.
\ee

Finally, as $\exp$ is continuous, 
\be
\bb{1 + \frac an}^n \to e^a.
\ee

\begin{exercise}
Find $\lim_{n\to\infty} n(a^{1/n} - 1)$, where $a > 0$.
\end{exercise}

Solution. Set $f(x) = a^x$ and apply the M.V.T. to $\bsb{0,\frac 1n}$, there exists $x\in \bb{0,\frac 1n}$ with
\be
f'(x) = \frac{f\bb{\frac 1n} - f(0)}{\frac 1n - 0} \ \ra \ a^x \log a = n\bb{a^{\frac 1n} - 1}.
\ee

As $n\to \infty$, we have $x\to 0$. Thus,
\be
\lim_{n\to \infty} n\bb{a^{\frac 1n} - 1} = \lim_{x\to 0} a^x \log a = \log a.
\ee

\begin{exercise}
"Let $f'$ exist on $(a, b)$ and let $c \in (a, b)$. If $c + h \in (a, b)$ then $(f(c + h) - f(c))/h = f'(c + \theta h)$. Let $h \to 0$; then $f'(c + \theta h) \to f'(c)$. Thus $f'$ is continuous at $c$." Is this argument correct?
\end{exercise}

Solution. The argument is NOT correct as it can be seen by taking 
\be
f(x) = x^2\sin \frac 1x, \ x\neq 0,\quad f(0) = 0 \ \ra \ f'(x) = 2x\sin \frac 1x - \cos \frac 1x.
\ee

Thus, $\lim_{h\to 0} f'(c+\theta h) \neq f'(c)$ because $f'(c)$ contains $\cos \frac 1c$. 

Although as $h\to 0$ the values $c+\theta h$ tend to $c$, the $\theta$ may vary, so one cannot conclude that for all sequences $(x_n)$ tending to $c$ we have $f'(x_n) \to f'(c)$.

\begin{exercise}
Let $f : \R \to \R$ be defined by $f(x) = \exp(-1/x^2)$ for $x \neq 0$ and $f(0) = 0$. Show that $f$ is continuous and differentiable. Show that $f$ is twice differentiable. Indeed, show that $f$ is infinitely differentiable, and that $f^{(n)}(0) = 0$ for all $n\in \N$. Comment, in the light of what you know about Taylor series.
\end{exercise}

Solution. We prove by induction that for all $n\in \N$ the function $f$ is $n$ times differentiable, with $f^{(n)}(0)=0$ and $f^{(n)}(x) = P_n\bb{\frac 1x}f(x)$ for $x\neq 0$, where $P_n$ is some polynomial. Since 
\be
f'(0) = \lim_{h\to 0} \frac{f(h)-f(0)}{h} = \lim_{h\to 0} \frac{e^{-\frac 1{h^2}}-f(0)}{h} = \lim_{y\to \infty} \frac{y}{e^{y^2}} = 0,
\ee
while for $x\neq 0$ we have
\be
f'(x) = \frac 2{x^3}e^{-1/x^2} = P_1\bb{\frac 1x} f(x),
\ee
where $P_1(t) = 2t^3$, thus the statement is true if $n=1$. (The differentiability implies the continuity.)

Proof. $f$ is differentiable at $x_0$, which implies
\be
\lim_{x\to x_0} = \frac{f(x)-f(x_0)}{x-x_0} = f'(x_0).
\ee

We want to prove that $\lim_{x\to x_0}f(x) = f(x_0)$.
\be
\lim_{x\to x_0} f(x)-f(x_0) = \lim_{x\to x_0} (x-x_0) \frac{f(x)-f(x_0)}{x-x_0} = \lim_{x\to x_0} (x-x_0) \lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0} = 0 \cdot f'(x_0) = 0.
\ee
Thus, $f$ is continuous at $x_0$.

Now assume it is true for $n=k$, then 
\be
f^{(k+1)}(0) = \lim_{h\to 0} \frac{P_k\bb{\frac 1h}f(h)-0}h = \lim_{h\to 0} \frac 1h P_k\bb{\frac 1h}e^{-1/h^2} = \lim_{y\to \infty} \frac{yP_k(y)}{e^{y^2}} = 0,
\ee
while for $x\neq 0$ we have 
\be
f^{(k+1)}(x) = -\frac 1{x^2} P_k'\bb{\frac 1x} f(x) + \frac 2{x^3} P_k\bb{\frac 1x}f(x) = P_{k+1} \bb{\frac 1x}f(x),
\ee
where
\be
P_{k+1}(t) = -t^2 P_k'(t) + 2t^3 P_k(t).
\ee
So it is true for $n= k+1$. Thus, by induction $f$ is infinitely differentiable and $f^{(0)}(0) =0$ for all $n\in \N$. It follows that the Taylor expansion of $f$ about 0 is valid at non-zero points.

\begin{exercise}
Find the radius of convergence of each of these power series.
\be
\sum_{n\geq 0} \frac{ 2 \cdot 4 \cdot 6 \dots (2n + 2) }{1 \cdot 4 \cdot 7 \dots (3n + 1)} z^n,\quad \quad \sum_{n\geq 1} \frac{z^{3n}}{n2^n},\quad\quad \sum_{n\geq 0} \frac {n^nz^n}{n!},\quad\quad \sum_{n\geq 1} n^{\sqrt{n}} z^n
\ee
\end{exercise}

Solution. Let the $n$th term of each power series be denoted $A_n$.
\ben
\item \be
\frac{A_{n+1}}{A_n} = \frac{2n+4}{3n+4}z \to \frac 23 z \quad \text{as }n\to \infty \quad \ra \ \text{ radius of convergence is }\frac 32.
\ee
\item \be
\frac{A_{n+1}}{A_n} = \frac{nz^3}{2(n+1)} \to \frac 12 z^3 \quad \text{as }n\to \infty \quad \ra \ \text{ radius of convergence is }\sqrt[3]{2}.
\ee
\item \be
\frac{A_{n+1}}{A_n} = \frac{(n+1)^{n+1} n!}{(n+1)!n^n}z = \bb{\frac {n+1}{n}}^n z = \bb{1 + \frac 1n}^n z \to ez \quad \text{as }n\to \infty \quad \ra \ \text{ radius of convergence is }\frac 1e.
\ee
\item \be
A_n^{1/n} = n^{1/\sqrt{n}}z \to z \quad \text{as }n\to \infty \quad \ra \ \text{ radius of convergence is 1}.
\ee
Note that $n^{1/\sqrt{n}} = \bb{\sqrt{n}^{1/\sqrt{n}}}^2$, and $m^{1/m} \to 1$ as $m\to \infty$. 
\een

\begin{exercise}[L'H\^opital's rule]
Suppose that $f, g : [a, b] \to \R$ are continuous and differentiable on $(a, b)$. Suppose that $f(a) = g(a) = 0$, that $g'(x)$ does not vanish near $a$ and $f'(x)/g'(x) \to l$ as $x \to a$. 

Show that $f(x)/g(x) \to l$ as $x \to a$. Use the rule with $g(x) = x - a$ to show that if $f'(x) \to l$ as $x \to a$, then $f$ is differentiable at a with $f'(a) = l$.

Find a pair of functions $f$ and $g$ as above for which $\lim_{x\to a} f(x)/g(x)$ exists, but $\lim_{x\to a} f'(x)/g'(x)$ does not.

Investigate the limit as $x \to 1$ of
\be
\frac{x - (n + 1)x^{n+1} + nx^{n+2}}{(1 - x)^2}.
\ee
\end{exercise}

Solution. The generalized M.V.T. shows that for all $x\in (a,b)$ there exists $y\in (a,x)$ with 
\be
\frac{f'(y)}{g'(y)} = \frac{f(x)-f(a)}{g(x)-g(a)} = \frac {f(x)}{g(x)}.
\ee
Thus, as $x\to a$, we have
\be
y \to a \ \ra \ \frac{f(x)}{g(x)} = \frac{f'(x)}{g'(y)} \to l
\ee
as required. If we take $g(x) = x-a$, then $g(a) = 0$, while $g'(x)=1$ so that $g'(x)$ does not vanish near $a$. Thus if $f'(x)\to l$ as $x\to a$, then as $x\to a$ we have
\be
\frac{f'(x)}{g'(x)} \to l,
\ee
so that by the rule,
\be
\frac{f(x)-f(a)}{x-a} = \frac{f(x)}{g(x)} \to l \quad \text{i.e., $f$ is differentiable at $a$ with }f'(a) = l.
\ee

Now set $f = x^2\sin\frac 1x$, $g(x) =x$ and $a=0$. Then $f$ and $g$ are both continuous and differentiable on $\R$, with $f(0)=g(0)=0$, and $g'(x)$ does not vanish near $a$. We have 
\be
\lim_{x\to 0} \frac{f(x)}{g(x)} = \lim_{x\to 0}x\sin \bb{\frac 1x} = 0,
\ee
but if $x\neq 0$, then
\be
\frac{f'(x)}{g'(x)} = f'(x) = 2x\sin\frac 1x - \cos\frac 1x,
\ee
which does not tend to a limit as $x\to 0$.

Set 
\be
f(x) = x-(n+1)x^{n+1} +nx^{n+2},\quad\quad g(x) = (1-x)^2.
\ee

Then $f$ and $g$ are infinitely differentiable on $\R$, and we have
\be
\left\{\ba{l}
f'(x) = 1-(n+1)^2x^n + n(n+2)x^{n+1}\\
f''(x) = -n(n+1)^2x^{n-1}+n(n+1)(n+2)x^n
\ea\right.\quad\quad 
\left\{\ba{l}
g'(x) = - 2(1-x)\\
g''(x) = 2
\ea\right.
\ee

Thus,
\be
\frac{f''(x)}{g''(x)} = \frac 12 n(n+1)x^{n-1}\bb{(n+2)x - (n+1)} \to \frac 12 n(n+1) \quad \text{as }x\to 1.
\ee
so as $f'(1) = g'(1) = 0$, applying the rule to $f'$ and $g'$ gives
\be
\frac{f'(x)}{g'(x)} \to \frac 12 n(n+1) \quad \text{as }x\to 1.
\ee

Then as $f(1)=g(1) = 0$, applying the rule to $f$ and $g$ gives 
\be
\frac{f(x)}{g(x)} \to \frac 12 n(n+1) \quad \text{as }x\to 1.
\ee

\begin{exercise}
Find the derivative of $\tan x$. How do you know there is a differentiable inverse function $\arctan x$ for $x \in \R$? What is its derivative? Now let $g(x) = x-x^3/3+x^5/5+\dots$ for $|x| < 1$. By considering $g'(x)$, explain carefully why $\arctan x = g(x)$ for $|x| < 1$.
\end{exercise}

Solution. $\tan x = \frac{\sin x}{\cos x}$, so
\be
\tan'x = \frac{\cos x \cos x - \sin x(-\sin x)}{\cos^2 x} = \frac 1{\cos^2 x} = \sec^2 x.
\ee

Since $\tan:\ \bb{-\frac {\pi}2, \frac {\pi}2} \mapsto \R$ is strictly increasing (as the derivative is positive), bijective and differentiable with non-zero derivative, by the inverse function theorem the inverse map $\arctan:\ \R \mapsto \bb{-\frac {\pi}2, \frac {\pi}2} $ is also differentiable, and we have
\be
\bb{\arctan}'(\tan x) = \frac 1{\tan'x} = \frac 1{\sec^2 x} = \frac 1{1+\tan^2 x}, \quad \text{i.e., } \bb{\arctan}'y = \frac 1{1+y^2}.
\ee

If $\abs{x} <1$, then
\be
\frac 1{1+x^2} = 1-x^2 + x^4 - x^6 + \dots
\ee
which is a power series with radius of convergence 1, so if $\abs{x}<1$ we may integrate term by term to obtain
\be
\arctan x = c+x - \frac {x^3}3 + \frac {x^5}5 - \frac {x^7}7 + \dots
\ee
for some constant $c$. Evaluating both sides at 0, we have $c=0$, so
\be
\arctan x = x - \frac {x^3}3 + \frac {x^5}5 - \frac {x^7}7 + \dots,\quad \text{for }\abs{x}<1.
\ee

\begin{exercise}
The infinite product $\prod^\infty_{n=1}(1 + a_n)$ is said to converge if the sequence $p_n = (1 + a_1) \dots (1 + a_n)$ converges. Suppose that $a_n \geq 0$ for all $n$. Putting $s_m = a_1+\dots+a_m$, prove that $s_n \leq p_n \leq e^{s_n}$, and deduce that $\prod^\infty_{n=1}(1+a_n)$ converges if and only if $\sum^\infty_{n=1} a_n$ converges. Evaluate $\prod^\infty_{n=2} (1+1/(n^2-1))$.
\end{exercise}

Solution. If $p_n = (1 + a_1) \dots (1 + a_n)$ then
\be
p_n = 1 + \sum_i a_i + \sum_{i<j}a_ia_j + \dots > \sum_i a_i  = s_n.
\ee

Also,
\be
e^{a_i} =  1+a_i + \frac 12 a_i^2 + \geq 1+a_i \ \ra \ e^{s_n} = e^{a_1}\dots e^{a_n} \geq (1 + a_1) \dots (1 + a_n) = p_n.
\ee

Thus, $s_n \leq p_n \leq e^{s_n}$. Therefore, if $\sum a_n$ converges then $p_n$ is an increasing series bounded above by $e^{\sum^\infty_{n=1}a_n}$, so $\prod^\infty_{n=1}(1 + a_n)$ converges. 

Conversely if $\prod^\infty_{n=1}(1 + a_n)$ converges then $s_n$ is an increasing sequence bounded above by $\prod^\infty_{n=1}(1 + a_n)$, so $\sum a_n$ converges.

Now consider $a_n = \frac 1{n^2-1}$, then 
\be
p_n = \prod^n_{i=2} \frac {i^2}{i^2-1} = \prod^n_{i=2} \frac {i\cdot i}{(i-1)(i+1)} = \frac 21 \frac 23 \cdot \frac 32 \frac 34 \cdot \dots \frac n{n-1}\frac n{n+1} = \frac {2n}{n+1} \to 2 \quad \text{as }n \to \infty.
\ee

Hence, $\sum^\infty_{n=2} \frac 1{n^2-1}$ converges.

\begin{exercise}
Let $f$ be continuous on $[-1, 1]$ and twice differentiable on $(-1, 1)$. Let $\phi(x) = (f(x) - f(0))/x$ for $x \neq 0$ and $\phi(0) = f'(0)$. Show that $\phi$ is continuous on $[-1, 1]$ and differentiable on $(-1, 1)$. Using a second order mean value theorem for $f$, show that $\phi'(x) = f''(\theta x)/2$ for some $0 < \theta < 1$. Hence prove that there exists $c \in (-1, 1)$ with $f''(c) = f(-1) + f(1) - 2f(0)$.
\end{exercise}

Solution. For continuity on $[-1,1]$ and differentiability on $(-1,1)$, it clearly suffices to check the behaviour of $\phi$ at 0, since each other point lies in an interval on which $\phi$ is defined as a composition of continuous and differentiable functions.

We have 
\be
\lim_{x\to 0} \phi(x) = \lim_{x\to 0} \frac {f(x)-f(0)}x = f'(0) = \phi(0),
\ee
so $\phi$ is continuous at 0. 

Now take $z\neq 0$ and write $f(z) = f(0) + zf'(0) + \frac 12 z^2 A$. Set $g(u) = f(u)-f(0)-uf'(0) -\frac 12u^2A$, then $g(0)=g(z) = 0$. So by Rolle's theorem, there exists $y\in (0,z)$ with 
\be
g'(y) = 0,\quad \text{i.e., } f'(y)-f'(0) - Ay = 0, \ \ra \ A = \frac{f'(y)-f'(0)}{y}.
\ee

Then we have
\be
f(z) = f(0) + zf'(0) + \frac 12 z^2 \frac{f'(y)-f'(0)}{y} \ \ra \ \phi(z) = \frac{f(z)-f(0)}{z} = f'(0) + \frac 12z \frac{f'(y)-f'(0)}{y}
\ee

Thus,
\be
\frac{\phi(z)-\phi(0)}z = \frac 12 \frac{f'(y)-f'(0)}{y} \ \ra \ \lim_{z\to 0}\frac{\phi(z)-f(0)}{z} = \frac 12 \lim_{y\to 0}\frac{f'(y)-f'(0)}{y} = \frac 12 f''(0).
\ee
Thus, $\phi$ is differentiable at 0, and $\phi'(x) = \frac 12 f''(\theta x)$ holds for $x=0$. 

Now take $x\neq 0$, we have
\beast
f(0) = f(x+(-x)) & = & f(x) + (-x)f'(x) + \frac 12 (-x)^2f''(x + \theta(-x)) \quad \text{for some }\theta' \in (0,1)\\
& = & f(x) -xf'(x) + \frac 12 x^2f''(\theta x ) \quad \theta = 1-\theta' \in (0,1)\\
\eeast

Thus.
\be
\phi(x) = \frac{f(x)-f(0)}x \ \ra\ \phi'(x) = \frac 1{x^2}\bb{f'(x)x-(f(x)-f(0))} = \frac 12 f''(\theta x)
\ee
as required.

To conclude, apply the M.V.T. to $\phi$ on $[-1,1]$, there exists $a\in (-1,1)$ with $\phi'(a) = \frac{\phi(1)-\phi(-1)}{1-(-1)}$, so
\be
\frac 12 f''(\theta a) = \frac 12 \bb{\phi(1)-\phi(-1)} = \frac 12 \bb{f(1)-f(0) + f(-1) - f(0)}
\ee

Let $c=\theta a$, we have
\be
f''(c) = f(-1)+f(1)-2f(0).
\ee

\begin{exercise}
Prove the theorem of Darboux: that if $f : \R \to \R$ is differentiable then $f'$ has the "property of Darboux". (That is to say, if $a < b$ and $f'(a) < z < f'(b)$ then there exists $c,\ a < c < b$, with $f'(c) = z$.)
\end{exercise}

Solution. Take $a<b$ and suppose $f'(a)<z<f'(b)$. Set $g(x) = f(x)-zx$, then $g$ is differentiable and 
\be
g'(x) = f'(x) - z \ \ra \ g'(a) < 0 < g'(b).
\ee

We must show that there exists $c\in (a,b)$ with $g'(c) = 0$, since then $f'(c) =z$.

By replacing $g(x)$ by $g(a+b-x)$ if necessary (which preserves the assumption $g'(a) <0< g'(b)$) we may assume $g(a)\geq g(b)$. Take $\epsilon = \frac 12 g'(b)>0$, then $\exists \delta >0$ such that $\abs{x-b}<\delta$ s.t. 
\be
\abs{\frac{g(x)-g(b)}{x-b}-g'(b)} < \epsilon \ \ra \ \frac{g(x)-g(b)}{x-b} > \frac 12g'(b)
\ee

Set $x=b-\frac 12 \delta$, then $g(b)-g(x) > \frac 12 \delta g'(b)$, so $g(x)<g(b)$. Now by I.V.T. there exists $y\in (a,x)$ with $g(y)=g(b)$, then Rolle's theorem shows that there exists $c\in(y,b)$ with $g'(c)=0$ as required.


\begin{exercise}
Let $h : \R \mapsto \R$ be defined by $h(x) = \exp(-1/x^2)$ for $x \neq 0$ and $h(0) = 0$. Construct a function $g : \R \to \R$ that is infinitely-differentiable, positive on a given interval $(a, b)$ and zero elsewhere. Now set 
\be
f(x) = \frac{\int^x_{-\infty} g}{\int^\infty_{-\infty} g}.
\ee
Show that $f$ is infinitely-differentiable, $f(x) = 0$ for $x < a$, $f(x) = 1$ for $x > b$ and $0 < f(x) < 1$ for $x \in (a, b)$. [For this part of the question you may assume standard properties of integration, including that $f'(x) = g(x)/\int^\infty_{-\infty} g$.]

Finally, construct a function from $\R$ to $\R$ that is infinitely-differentiable, but is identically 1 on $[-1, 1]$ and identically 0 outside $(-2, 2)$.
\end{exercise}

Solution. From the previous question, we know 
\be
h:\ \R\mapsto \R, \quad h(x) = \left\{\ba{ll}
e^{-1/x^2} \quad\quad & x > 0\\
0 & x \leq 0
\ea\right.
\ee
is infinitely differentiable. Next, define 
\be
g:\ \R\mapsto \R, \quad g(x)= h(x-a)h(b-x).
\ee

Then $g$ is also infinitely differentiable, is positive on $(a,b)$ and zero elsewhere. Now define 
\be
f:\ \R\mapsto \R, \quad f(x) = \frac{\int^x_{-\infty} g(t)dt}{\int^\infty_{-\infty} g(t)dt}.
\ee

Then as 
\be
f'(x) = \frac{g(x)}{\int^\infty_{-\infty} g(t)dt}
\ee
we see that $f$ is infinitely differentiable.

If $x<a$, then
\be
f(x) = \frac{\int^x_{-\infty} g(t)dt}{\int^\infty_{-\infty} g(t)dt} = \frac{\int^x_{-\infty}0 dt}{\int^\infty_{-\infty} g(t)dt} = 0.
\ee

If $x>b$, then
\be
f(x) = \frac{\int^x_{-\infty} g(t)dt}{\int^\infty_{-\infty} g(t)dt} = \frac{\int^\infty_{-\infty}g(t) dt}{\int^\infty_{-\infty} g(t)dt} = 1.
\ee

If $a<x<b$, then
\be
0 < \int^x_{-\infty} g(t)dt < \int^\infty_{-\infty} g(t)dt \ \ra \ 0<f(x)<1.
\ee

Finally take $a=1$, $b=2$ and define
\be
k:\ \R\mapsto \R\quad k(x) = 1-f(\abs{x}).
\ee

Then $k$ is infinitely differentiable, and is identically 1 on $[-1,1]$ and identically 0 outside $[-2,2]$.

\begin{exercise}
Show directly from the definition of an integral that $\int^a_0 x^2 = a^3/3$ for $a > 0$.
\end{exercise}

Solution. Consider 
\be
D = \left\{0,\frac an, \frac {2a}n, \dots, \frac {(n-1)a}n, a\right\}
\ee

Then
\be
S(x^2,D) = \sum^n_{j=1} \sup_{x\in [x_{j-1},x_j]}x^2(x_j - x_{j-1}) = \sum^n_{j=1} \bb{\frac{aj}n }^2 \frac an = \frac {a^3}{n^3}\frac 16 n(n+1)(2n+1)= \frac {(n+1)(2n+1)}{6n^2}a^3,
\ee
\be
s(x^2,D) = \sum^n_{j=1} \inf_{x\in [x_{j-1},x_j]}x^2(x_j - x_{j-1}) = \sum^n_{j=1} \bb{\frac{a(j-1)}n }^2 \frac an = \frac {a^3}{n^3}\frac 16 (n-1)n(2n-1)= \frac {(n-1)(2n-1)}{6n^2}a^3.
\ee

Thus, we have
\be
I^*(x^2) = \inf_D S(x^2,D) = \frac {a^3}3 = \inf_D s(x^2,D) = I_*(x^2).
\ee

Hence, $x^2$ is integrable, (which can be achieved by monotonicity of $x^2$ (in the lecture notes)) and 
\be
\int^a_0 x^2 = \frac {a^3}3.
\ee

\begin{exercise}
Let $f(x) = \sin(1/x)$ for $x \neq 0$ and $f(0) = 0$. Does $\int^1_0 f$ exist?
\end{exercise}

Solution. Since $f(x)$ is continuous at $x\neq 0$, $\forall \ve >0$, $\exists$ a partition $D_1$ of $[\ve,1]$ s.t.
\be
S(f,D_1) - s(f,D_1) < \ve
\ee
by Riemann Theorem. Also, for any $D_2$ of $[0,\ve]$ s.t. 
\be
S(f,D_2) \leq \ve \sup_{x\in[0,\ve]} \sin \frac 1x \leq \ve, \quad\quad s(f,D_2) \geq \ve \inf_{x\in[0,\ve]} \sin \frac 1x \geq -\ve
\ee
with $f(0) = 0$. Thus,
\be
S(f,D_2) - s(f,D_2) \leq 2\ve.
\ee

Now let $D = D_1 \cup D_2$, we have
\be
S(f,D) - s(f,D) = S(f,D_1) - s(f,D_1) + S(f,D_2) - s(f,D_2) < \ve + 2\ve = 3\ve.
\ee

Thus, $f$ is integrable by Riemann Theorem and $\int^1_0 f$ exists.

\begin{exercise}
Give an example of a continuous function $f : [0,\infty) \to [0,\infty)$, such that $\int^\infty_0 f$ exists but $f$ is unbounded.
\end{exercise}

Solution. See the diagram. For each step, we double the height of the triangles and shrink the width to $\frac 14$ of previous one.

Thus, the area is 
\be
1\times \frac 12 + \frac 14 \times 1 + \frac 16 \times 2 + \dots = \frac 12 + \frac 14 + \frac 18 + \dots \to 1.
\ee

Thus, $\int^\infty_0 f$ exists but $f$ is unbounded.

\centertexdraw{
\drawdim in

\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0

\move (0.5 0) \lvec(1 0.25) \lvec(1.5 0) 
\move (1.75 0) \lvec(2 0.5) \lvec(2.25 0) 
\move (2.875 0) \lvec(3 1) \lvec(3.125 0) 

\htext(3.5 0.5){$\dots$}
\move (-0.2 0) \avec(4.3 0)
\move (0 -0.2) \avec(0 1.3)
}

\begin{exercise}\label{ques:vanish_identically} 
Give an example of an integrable function $f : [0, 1] \to \R$ with $f \geq 0$, $\int^1_0 f = 0$, and $f(x) > 0$ for some value of $x$. Show that this cannot happen if $f$ is continuous.
\end{exercise}

Solution. Here we give two examples
\be
f_1(x) = \left\{\ba{ll}
1 \quad\quad & x=\frac 12\\
0 & x \neq \frac 12
\ea\right.,\quad\quad\quad\quad 
f_2(x) = \left\{\ba{ll}
\frac 1q \quad\quad & x = \frac pq \in [0,1]\\
0 & x \text{ is irrational in }[0,1]
\ea\right..
\ee

Now consider a function $f$ which continuous at $[0,1]$ and $f(x)>0$ for some $x$, let's say, $x=a$. Choose $\ve>0$ s.t. $f(a) = 2\ve$. Since $f$ is continuous, $\exists \delta >0$ s.t.
\be
\forall \abs{x-a} < \delta \ \ra \ \abs{f(x)-f(a)}< \ve \ \ra \ f(x) > \frac 12 f(a)
\ee

Thus,
\be
\int^1_0 f = \int^{a-\delta}_0 f + \int^{a+\delta}_{a-\delta}f + \int^1_{a+\delta}f > \int^{a-\delta}_0 0 + \frac 12f(a) \int^{a+\delta}_{a-\delta}dx + \int^1_{a+\delta}0 = \delta f(a) > 0.
\ee

Hence, the contradiction gives the fact that $f$ is NOT continuous.

\begin{exercise}
Let $f : \R \to \R$ be monotonic. Show that $\{x \in \R : f \text{ is discontinuous at }x\}$ is countable. Let $x_n,\ n \geq 1$ be a sequence of distinct points in $(0, 1]$. Let $f_n(x) = 0$ if $0 \leq x < x_n$ and $f_n(x) = 1$ if $x_n \leq x \leq 1$. Let $f(x) = \sum^\infty_{n=1} 2^{-n}f_n(x)$. Show that this series converges for every $x \in [0, 1]$. Show that $f$ is increasing (and so is integrable). Show that $f$ is discontinuous at every $x_n$.
\end{exercise}

Solution. Without loss of generality, we have $f$ increasing. Let $S=\{x\in\R: \ x \text{ is a discontinuity of }f\}$, take $y\in S$ and define
\be
Ly = \{f(x):\ x<y\},\quad\quad Ry= \{f(x):\ x>y\}
\ee
then we have
\be
\sup Ly \leq f(y) \leq \inf Ry.
\ee

Since $f(x)$ is discontinuous at $y$, $\sup Ly < \inf Ry$. Since rational number is dense in $\R$, we can take any rational number $y_0 \in (\sup Ly , \inf Ry)$ then the map $y\mapsto y_0$ is an injection. Thus, rationals are countable implies that $S$ is countable.

Now consider the function $g_m(x) = \sum^m_{n=1} 2^{-n}f_n(x)$, then by fundamental axiom, $\forall x\in [0,1]$, the sequence $\{g_m(x)\}$ is non-decreasing and bounded above by $f(1)=\sum^\infty_{n=1}2^{-n} = 1$. So the sequence converges and the series converges for every $x\in [0,1]$.

If $x_1\leq x_2$, we have $f_n(x_1)\leq f_n(x_2)$, then
\be
f(x_1) = \sum^\infty_{n=1}2^{-n}f_n(x_1) \leq \sum^\infty_{n=1}2^{-n}f_n(x_2) = f(x_2).
\ee

Thus, $f$ is increasing and therefore integrable (since it is monotonic). 

Consider $y<x_n$, then
\be
f(x_n)-f(y) = \sum^\infty_{m=1}2^{-m} \bb{f_m(x_n)-f_m(y)} = \sum_{m\neq n} 2^{-m}\underbrace{\bb{f_m(x_n)-f_m(y)}}_{\geq 0\text{ since increasing}} + 2^{-n}\bb{\underbrace{f_n(x_n)}_{=1}-\underbrace{f_n(y)}_{=0}} = 2^{-n} >0
\ee

Thus, $\forall \delta >0$ s.t. $\abs{y-x_n}<\delta$, $\exists \ve= 2^{-n}$ s.t. $\abs{f(y)-f(x_n)}\geq \ve$, which means that $f$ is discontinuous at $x_n$.

\begin{exercise}
Let $f(x) = \log(1-x^2)$. Use the mean value theorem to show that $|f(x)| \leq 8x^2/3$ for $0 \leq x \leq 1/2$. Now let 
\be
I_n = \int^{n+1/2}_{n-1/2} \log x dx - \log n
\ee
for $n \in \N$. Show that 
\be
I_n = \int^{1/2}_0 f(t/n) dt
\ee
and hence that $|I_n| < 1/9n^2$. By considering $\sum^n_{j=1} I_j$, deduce that $n!/n^{n+1/2}e^{-n} \to l$ for some constant $l$.

[The bounds $8x^2/3$ and $1/9n^2$ are not best possible; they are merely good enough for the conclusion.]
\end{exercise}

Solution. First we know that $g(x) = \log x$ and $h(x) = 1-x^2$ are differentiable on $\left[0,\frac 12\right]$, then we have $f(x)$ is differentiable. Thus, $\exists c \in \bb{0,x}$ s.t.
\be
f(x)-f(0) = f'(c)(x-0) \ \ra \ \frac 1x f(x) = \frac {-2c}{1-c^2} \ \ra \ \abs{f(x)} = \abs{\frac{2cx}{1-c^2}} \leq \abs{\frac{2cx}{1-\bb{\frac12}^2}} \leq \abs{\frac{2x^2}{\frac 34}} = \frac 83 x^2.  
\ee

Now consider $I_n$,
\beast
I_n & = & \int^{n+1/2}_{n-1/2} \log x dx - \log n = \int^{n+1/2}_{n-1/2} \log \frac xn dx \\
& = & \int^{n+1/2}_n \log \frac xn dx + \int^n_{n-1/2} \log \frac xn dx = \int^{1/2}_0 \log \bb{1+\frac tn} dt + \int^{1/2}_0 \log \bb{1-\frac tn} dt\\
& = & \int^{1/2}_0 \log \bb{1-\frac {t^2}{n^2}} dt = \int^{1/2}_0 f(t/n) dt.
\eeast

Thus,
\be
\abs{I_n} = \abs{\int^{1/2}_0 f\bb{\frac tn} dt} \leq \frac 83 \abs{\int^{1/2}_0  \bb{\frac tn}^2 dt} = \frac 1{9n^2}.
\ee

Therefore, we have $\sum^n_{j=1} I_j$ is absolutely convergent and 
\beast
\sum^n_{j=1} I_j & = & \sum^n_{j=1}\bb{\int^{j+1/2}_{j-1/2} \log x dx - \log j} = \int^{n+1/2}_{1/2}\log xdx - \log n!\\
& = & \left. x\log x\right|^{n+1/2}_{1/2} - n -\log n! = \bb{n+ \frac 12}\log \bb{n+ \frac 12} - \frac 12 \log \frac 12 -n - \log n!\\
& = & \log \bb{\frac{\sqrt{2}\bb{n+ \frac 12}^{n+ \frac 12}}{e^n n!}} \to c \ \ra \ \frac{e^n n!}{\sqrt{2}\bb{n+ \frac 12}^{n+ \frac 12}} \to e^{-c}.
\eeast

Furthermore, we have
\be
\frac{e^n n!}{\sqrt{2}\bb{n+ \frac 12}^{n+ \frac 12}}= \frac{e^n n!}{\sqrt{2}\bb{n\bb{1+ \frac 1{2n}}}^{n+ \frac 12}} \to \frac{e^n n!}{\sqrt{2}n^{n+ \frac 12}e^{\frac 12}} \ \ra \ \frac{e^n n!}{n^{n+ \frac 12}} \to \sqrt{2}e^{\frac 12}e^{-c} = l.
\ee

\begin{exercise}
Let 
\be 
I_n = \int^{\pi/2}_0 \cos^n x dx.
\ee
Prove that $nI_n = (n - 1)I_{n-2}$, and hence $\frac{2n}{2n+1}\leq I_{2n+1}/I_{2n} \leq 1$. Deduce Wallis’s Product:
\be
\frac {\pi}2 = \lim_{n\to\infty}\frac{2 \cdot 2 \cdot 4 \cdot 4 \dots 2n \cdot 2n}{1 \cdot 3 \cdot 3 \cdot 5 \dots (2n - 1) \cdot (2n + 1)} = \lim_{n\to\infty} \frac{2^{4n}}{2n + 1} \binom{2n}{n}^{-2}.
\ee
By taking note of the previous exercise, prove that 
\be
n!/n^{n+1/2}e^{-n} \to \sqrt{2\pi}\quad\quad \text{(Stirling's formula)}.
\ee
\end{exercise}

Solution. We have
\beast
I_n & = & \int^{\pi/2}_0 \cos^n x dx =  \int^{\pi/2}_0 (1-\sin^2x)\cos^{n-2} x dx \\
& = & I_{n-2} + \int^{\pi/2}_0 \sin x\cos^{n-2} x d\cos x = I_{n-2} + \frac 1{n-1} \int^{\pi/2}_0 \sin x d \cos^{n-1} x\\
& = & I_{n-2} + \frac 1{n-1} \bb{\left. \sin x \cos^{n-1} x \right|^{\pi/2}_0 - \int^{\pi/2}_0  \cos^{n-1} x d\sin x}\\
& = & I_{n-2} - \frac 1{n-1} I_n \ \ra \ nI_n = (n - 1)I_{n-2}.
\eeast

Also,

\be
I_{2n+1} = \int^{\pi/2}_0 \cos^{2n+1} x dx \leq \int^{\pi/2}_0 \cos^{2n} x dx = I_{2n} \ \ra \ \frac{I_{2n+1}}{I_{2n}}\leq 1,
\ee
\be
\frac{I_{2n+1}}{I_{2n}} = \frac{2n I_{2n-1}}{(2n+1)I_{2n}} \geq \frac{2n }{2n+1} \cdot 1 = \frac{2n }{2n+1}.
\ee
So 
\be
\frac{I_{2n+1}}{I_{2n}} \to 1\quad \text{as }n\to \infty.
\ee

Thus,
\be
\lim_{n\to\infty}\frac{2 \cdot 2 \cdot 4 \cdot 4 \dots 2n \cdot 2n}{1 \cdot 3 \cdot 3 \cdot 5 \dots (2n - 1) \cdot (2n + 1)} = \lim_{n\to \infty} \frac{I_0}{I_2} \frac{I_3}{I_1} \dots \frac{I_{2n-2}}{I_{2n}} \frac{I_{2n+1}}{I_{2n-1}} = \lim_{n\to \infty} \frac{I_0I_{2n+1}}{I_1I_{2n}} \to \frac{I_0}{I_1} = \frac {\pi/2}{1} =  \frac {\pi}2.
\ee

Using the previous results, we have
\be
l = \lim_{n\to\infty} \frac{e^n n!}{n^{n+ \frac 12}} = \lim_{n\to\infty}\frac{e^{2n} (2n)!}{(2n)^{2n+ \frac 12}}
\ee
Thus,
\be
l = \frac{l^2}l = \left.\lim_{n\to\infty} \bb{\frac{e^n n!}{n^{n+ \frac 12}}}^2 \right/\lim_{n\to\infty}\frac{e^{2n} (2n)!}{(2n)^{2n+ \frac 12}} = \lim_{n\to \infty}\frac{2^{2n+\frac 12} (n!)^2}{n^{\frac 12}(2n)!}.
\ee

Then
\be
l^2 = \lim_{n\to \infty}\bb{\frac{2^{2n+\frac 12} (n!)^2}{n^{\frac 12}(2n)!}}^2 = \lim_{n\to \infty}\frac{2^{4n+1} (n!)^4}{n((2n)!)^2} = = \lim_{n\to\infty} \frac{2^{4n}}{2n + 1} \binom{2n}{n}^{-2} \lim_{n\to\infty} \frac{2(2n + 1)}n = \frac {\pi}2 \cdot 4 = 2\pi.
\ee

Thus, we have $l = \sqrt{2\pi}$.

\begin{exercise}
Do these improper integrals converge? 
\be
\text{(i)} \quad \int^\infty_1 \sin^2(1/x)dx,\quad\quad \text{(ii)}\quad \int^\infty_0 x^p \exp(-x^q)dx \quad \text{where }p, q > 0.
\ee
\end{exercise}

Solution. \ben
\item [(i)] We know that $\forall t\geq 0$, $\sin t \leq t$ (which can be checked by Taylor expansion), 
\be
\sin t = t+ \bb{-\frac {z^3}{3!} + \frac {z^5}{5!}} + \bb{-\frac {z^7}{7!} + \frac {z^9}{9!}} + \dots
\ee

For $t\geq 1$, the inequality is obvious, so for $t<1$, we have each term in the bracket,
\be
-\frac{t^{4n-1}}{(4n-1)!} + \frac{t^{4n+1}}{(4n+1)!} = \frac{t^{4n-1}}{(4n-1)!}\bb{-1 + \frac{t^2}{4n(4n+1)}} <0,\quad\quad n=1,2,\dots
\ee
So
\be
\int^\infty_1 \sin^2(1/x)dx \leq \int^\infty_1 \bb{\frac 1x}^2dx = -\int^\infty_1 d\frac 1x = 1.
\ee
Thus, the integral converges.

\item [(ii)] We have $x^{p+2} \leq \exp(x^q)$ for some $x\geq n$. Thus,
\be
\int^\infty_0 x^p \exp(-x^q)dx = \int^n_0 x^p \exp(-x^q)dx + \int^\infty_n x^p \exp(-x^q)dx \leq \int^n_0 x^p \exp(-x^q)dx + \int^\infty_n \frac 1{x^2}dx
\ee
So the integral converges.

\een

\begin{exercise}
Show that 
\be
\frac 1{n+1} + \frac 1{n+2} + \dots + \frac 1{2n} \to \log 2 \quad \text{as }n \to \infty,
\ee
and find 
\be
\lim_{n\to\infty} \frac 1{n+1} - \frac 1{n+2} + \dots + \frac{(-1)^{n-1}}{2n}.
\ee
\end{exercise}

Solution. Let  $g(x) =  \frac 1x$ on $[1,2]$, $\int^2_1 f = \log 2$. We choose partition
\be
D = \left\{1+\frac in:\ 0\leq i\leq n\right\} \ \ra \ L(g,D) = \sum^n_{i=1} \frac 1{1+ \frac in} \cdot \frac 1n = \sum^n_{i=1} \frac 1{n+i}.
\ee

Let $f(x) =  \frac 1{n+x}$, $f$ is decreasing and $\forall x\in [n-1,n]$,
\be
f(n-1) \geq f(x) \geq f(n) \ \ra \ f(n-1) \geq \int^n_{n-1} f(x)dx \geq f(n) \ \ra \ \sum^{n-1}_{i=1} f(i) \geq \int^n_1 f(x)dx \geq \sum^n_{i=2} f(i).
\ee

Thus, we have
\be
\frac 1{n+1} + \dots + \frac 1{2n-1} \geq \log \bb{\frac{2n}{n+1}} \geq \frac 1{n+2} + \dots + \frac 1{2n}.
\ee

Let $S_n =\frac 1{n+1} + \dots + \frac 1{2n-1}$, $T_n = \frac 1{n+2} + \dots + \frac 1{2n}$. We have $\forall \ve > 0$, $\exists N\geq 0$ s.t. $\abs{S_n - T_n} < \ve$ as $n\geq N$. Thus,
\be
S_n =\frac 1{n+1} + \dots + \frac 1{2n-1} \to \log 2 \ \ra \ \frac 1{n+1} + \frac 1{n+2} + \dots + \frac 1{2n} \to \log 2.
\ee

Alternatively, we know 
\be
1 + \frac 12 + \dots + \frac 1n - \log n \to \gamma \quad \text{as }n \to \infty, \quad\quad 1 + \frac 12 + \dots + \frac 1{2n} - \log 2n \to \gamma \quad \text{as }n \to \infty, 
\ee
So
\be
\frac 1{n+1} + \frac 1{n+2} + \dots + \frac 1{2n} - \log 2n + \log n \to 0 \quad \text{as }n \to \infty \ \ra \ \frac 1{n+1} + \frac 1{n+2} + \dots + \frac 1{2n} \to \log 2.
\ee

For $\lim_{n\to\infty} \frac 1{n+1} - \frac 1{n+2} + \dots + \frac{(-1)^{n-1}}{2n}$, if $n$ be even
\beast
\lim_{n\to\infty} \frac 1{n+1} - \frac 1{n+2} + \dots + \frac{(-1)^{n-1}}{2n} & = & \lim_{n\to\infty} \frac 1{(n+1)(n+2)} + \dots + \frac 1{(2n-1)2n} \\
& \leq & \frac 12 \lim_{n\to\infty} \frac n{(n+1)(n+2)} = 0.
\eeast

For $n$ odd it's similar. Alternatively, we let
\be
S_n = \frac 1{n+1} - \frac 1{n+2} + \dots + \frac{(-1)^{n-1}}{2n},\quad\quad T_n = \frac 1{n} - \frac 1{n+1} + \dots + \frac 1{2n} \ \ra \ T_{2n}-T_n = -\frac 1{2n} + S_{2n}
\ee

We know $T_n \to \log 2$, so $S_{2n} \to 0$, also $S_{2n+1} \to 0$, thus $S_n \to 0$.

\begin{exercise}
Let $f : [a, b] \to \R$ be continuous and suppose that $\int^b_a f(x)g(x) dx = 0$ for every continuous function $g : [a, b] \to \R$ with $g(a) = g(b) = 0$. Must $f$ vanish identically?
\end{exercise}

Solution. Let $g(x) = f(x)(x-a)^2(x-b)^2$. Then $fg \geq 0$ and $\int^b_a fg = 0$. Since $fg$ is continuous, we use the previous result (question \ref{ques:vanish_identically}) and conclude that $fg = 0$ on $(a,b)$. Thus $f=0$ on $(a,b)$ and $f=0$ at $a$ and $b$ by continuity.

\begin{exercise}
Suppose that $f :\ \R \mapsto \R$ has a continuous derivative, $f(0) = 0$ and $|f'(x)| \leq M$ for $x \in [0, 1]$. Show that $\abs{\int^1_0 f} \leq M/2$. Show that if, in addition, $f(1) = 0$ then $\abs{\int^1_0 f} \leq M/4$. What could you say if $\abs{f'(x)} \leq Mx$?
\end{exercise}

Solution. By the definition, we have
\be
\abs{f(x)} = \abs{f(x)-f(0)} = \abs{\int^x_0 f'(t)dt} \leq \abs{\int^x_0 M dt} = Mx.
\ee

Thus,
\be
\abs{\int^1_0f}  \leq \int^1_0 \abs{f} \leq \int^1_0 Mt dt = \frac M2.
\ee

Additionally, if $f(1)=0$, we have $x\geq \frac 12$,
\be
\abs{f(x)} = \abs{f(1)-f(x)} = \abs{\int^1_x f'(t)dt} \leq \int^1_x \abs{f'(t)}dt \leq \int^1_x M dt = M(1-x).
\ee

Then,
\beast
\abs{\int^1_0f} & = & \abs{\int^{\frac 12}_0 f + \int^1_{\frac 12} f } \leq \int^{\frac 12}_0 \abs{f} + \int^1_{\frac 12} \abs{f} \\
& \leq & \int^{\frac 12}_0 Mt dt + \int^1_{\frac 12} M(1-t) dt = \frac M8 + \frac M8 = \frac M4.
\eeast

If $\abs{f'(x)} \leq Mx$,
\be
\abs{f(x)} = \abs{f(x)-f(0)} = \abs{\int^x_0 f'(t)dt} \leq \abs{\int^x_0 Mt dt} = \frac 12 Mx^2.
\ee

Thus,
\be
\abs{\int^1_0f}  \leq \int^1_0 \abs{f} \leq \frac 12 \int^1_0 Mt^2 dt = \frac M6.
\ee

Additionally, if $f(1)=0$, we have $x\geq \frac 12$,
\be
\abs{f(x)} = \abs{f(1)-f(x)} = \abs{\int^1_x f'(t)dt} \leq \int^1_x \abs{f'(t)}dt \leq \int^1_x Mt dt = \frac 12 M(1-x^2).
\ee

Then,
\beast
\abs{\int^1_0f} & = & \abs{\int^{\frac 12}_0 f + \int^1_{\frac 12} f } \leq \int^{\frac 12}_0 \abs{f} + \int^1_{\frac 12} \abs{f} \\
& \leq & \frac 12\int^{\frac 12}_0 Mt^2 dt + \frac 12 \int^1_{\frac 12} M(1-t^2) dt = \frac M{48} + \frac {5M}{48} = \frac M8.
\eeast

\begin{exercise}
Let $f : [0, 1] \to \R$ be continuous. Let $G(x, t) = t(x - 1)$ for $t \leq x$ and $G(x, t) = x(t - 1)$ for $t \geq x$. Let $g(x) = \int^1_0 f(t)G(x, t)dt$. Show that $g''(x)$ exists for $x \in (0, 1)$ and equals $f(x)$.
\end{exercise}

Solution. First, we have
\beast
g(x) & = & \int^1_0 f(t)G(x,t)dt = \int^x_0 f(t)G(x,t)dt + \int^1_x f(t)G(x,t)dt = (x-1) \int^x_0 tf(t)dt + x\int^1_x (t-1)f(t)dt.
\eeast
Thus,
\be
g'(x) = \int^x_0 tf(t)dt + (x-1)xf(x) + \int^1_x (t-1)f(t)dt - x(x-1)f(x) = \int^x_0 tf(t)dt + \int^1_x (t-1)f(t)dt,
\ee
\be
g''(x) = xf(x) - (x-1)f(x) = f(x).
\ee

\begin{exercise}
Let 
\be
I_n(\theta) = \int^1_{-1} (1 - x^2)^n \cos(\theta x)dx.
\ee

Prove that 
\be
\theta^2 I_n = 2n(2n - 1)I_{n-1} - 4n(n - 1)I_{n-2} 
\ee
for $n \geq 2$, and hence that $\theta^{2n+1}I_n(\theta) = n!(P_n(\theta) \sin \theta +Q_n(\theta) \cos \theta)$, where $P_n$ and $Q_n$ are polynomials
of degree at most $2n$ with integer coefficients. Deduce that $\pi$ is irrational.
\end{exercise}

Solution. With the definition, we have
\be
I_n(\theta) = \int^1_{-1} (1 - x^2)^n \cos(\theta x)dx = \frac 1{\theta} \underbrace{\sin \theta x (1-x^2)^n|^1_{-1}}_{=0} + \frac n{\theta} \int^1_{-1}\sin \theta x (1-x^2)^{n-1}2x dx.
\ee

Then
\beast
\theta^2 I_n & = & 2\theta n\int^1_{-1}\sin \theta x (1-x^2)^{n-1}2x dx \\
& = & -2n\cos \theta x (1-x^2)^{n-1}|^1_{-1} + 2n\int^1_{-1}\cos \theta x \bb{(1-x^2)^{n-1}+ x(1-x^2)^{n-2}(n-1)(-2x)}dx\\
& = & 2n\int^1_{-1} (1-x^2)^{n-1}\cos \theta x dx - 4n(n-1) \int^1_{-1} \cos \theta x (1-x^2)^{n-2}x^2dx\\
& = & 2nI_{n-1} - 4n(n-1)\bb{I_{n-2}-I_{n-1}} = 2n(2n-1)I_{n-1} - 4n(n-1)I_{n-2}.
\eeast

For $\theta^{2n+1}I_n(\theta) = n!(P_n(\theta) \sin \theta +Q_n(\theta) \cos \theta)$, we show by induction,
\be
n=0:\ I_0 = \int^1_{-1}\cos \theta x dx = \frac 2{\theta}\sin\theta \ \ra \ \theta I_0 = 0!\bb{2\sin\theta + 0 \cos \theta}, \quad P_0(\theta) = 2,\ Q_0(\theta) = 0.
\ee
\beast
n=1:\ I_1 & = & \int^1_{-1}(1-x^2)\cos \theta x dx = \frac 1{\theta}\bb{(1-x^2)\sin\theta x|^1_{-1} - \int^1_{-1} \sin\theta x (-2x)dx }\\
& = & \frac 2{\theta} \int^1_{-1} \sin\theta x x dx = \frac 2{\theta^2} \bb{ - x \cos\theta x|^1_{-1} + \int^1_{-1} \cos\theta x dx}\\
& = & \frac 4{\theta^3} \sin\theta - \frac 4{\theta^2} \cos \theta \ \ra \ \theta^3 I_1 = 1!\bb{4\sin\theta - 4\theta \cos \theta},\quad P_1(\theta) = 4,\ Q_0(\theta) = -4\theta.
\eeast

Now if it holds for $I_k$ and $I_{k+1}$, then it is true for $I_{k+2}$,
\beast
& & I_k: \ \theta^{2k+1} I_k(\theta) = k! \bb{P_k(\theta)\sin\theta + Q_k(\theta)\cos\theta},\\
& & I_{k+1}: \ \theta^{2k+3} I_{k+1}(\theta) = (k+1)! \bb{P_{k+1}(\theta)\sin\theta + Q_{k+1}(\theta)\cos\theta},
\eeast
and 
\beast
I_{k+2}: \quad & & \theta^{2k+5} I_{k+2}(\theta) = \theta^{2k+3} \bb{\theta^2 I_{k+2}(\theta)}\\
& = & \theta^{2k+3} \bb{2(k+2)(2k+3)I_{k+1}(\theta) - 4(k+2)(k+1)I_k(\theta)}\\
& = & 2(k+2)(k+1)! \bb{(2k+3)P_{k+1}(\theta)\sin\theta + Q_{k+1}(\theta)\cos\theta} - 4\theta^2 (k+2)(k+1)k! \bb{P_k(\theta)\sin\theta + Q_k(\theta)\cos\theta}\\
& = & (k+2)! \bb{\bb{(4k+6) P_{k+1}(\theta)\sin\theta + (4k+6) Q_{k+1}(\theta)\cos\theta} - 4\theta^2 \bb{P_k(\theta)\sin\theta + Q_k(\theta)\cos\theta}}\\
& = & (k+2)! \bb{[(4k+6) P_{k+1}(\theta) - 4\theta^2 P_k(\theta)]\sin\theta  + [(4k+6) Q_{k+1}(\theta) - 4\theta^2 Q_k(\theta)]\cos\theta}
\eeast
as required.

If $\pi$ is rational, write $\pi = \frac ab$ and let $\theta = \frac {\pi}2 = \frac a{2b}$, then
\be
\bb{\frac{a}{2b}}^{2n+1} I_n\bb{\frac {\pi}2} = n! P_n\bb{\frac{a}{2b}} \ \ra \ \frac{a^{2n+1}}{n!} I_n\bb{\frac {\pi}2} = (2b)^{2n+1}P_n\bb{\frac{a}{2b}}.
\ee

$P_n\bb{\frac{a}{2b}}$ has at most order $2n$ so we have
\be
(2b)^{2n+1}P_n\bb{\frac{a}{2b}} \in \Z \ \ra \ \frac{a^{2n+1}}{n!} I_n\bb{\frac {\pi}2} \in \Z.
\ee

However, $a$ is fixed and $\exists n$ s.t. 
\be
\frac{a^{2n+1}}{n!} < \frac 12
\ee
and 
\be
I_n\bb{\frac {\pi}2} = \int^1_{-1} (1-x^2)^n \cos \frac{\pi x}2 dx \leq \int^1_{-1} dx = 2 \ \ra \ \frac{a^{2n+1}}{n!} I_n\bb{\frac {\pi}2} < 1
\ee

It is obvious that $\frac{a^{2n+1}}{n!} I_n\bb{\frac {\pi}2} > 0$, so this contradiction implies that $\pi$ is an irrational number.

\begin{exercise}
Let $f_1, f_2 : [-1, 1] \to \R$ be increasing and $g = f_1 - f_2$. Show that there exists $K$ such that, for any dissection $D = x_0 < \dots < x_n$ of $[-1, 1]$, 
\be
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| \leq K.
\ee

Now let $g(x) = x \sin(1/x)$ for $x \neq 0$ and $g(0) = 0$. Show that $g$ is integrable but is not the difference of two increasing functions.
\end{exercise}

Solution. For $g = f_1 - f_2$, 
\beast
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| & = & \sum^n_{j=1} \abs{f_1(x_j)-f_2(x_j)-f_1(x_{j-1})+f_2(x_{j-1})}\\
& \leq & \sum^n_{j=1} \abs{f_1(x_j)-f_1(x_{j-1})}+ \abs{f_2(x_j) - f_2(x_{j-1})} \\
& \leq & \sum^n_{j=1} f_1(x_j)-f_1(x_{j-1}) + f_2(x_j) - f_2(x_{j-1})\quad\quad \text{since $f_1,f_2$ increasing}\\
& = & f_1(x_n)-f_1(x_0) + f_2(x_n) - f_2(x_0) = K.
\eeast

Since $g(x) =x \sin(1/x)$, it is continuous and thus integrable. Now we want to prove it is not the difference of two increasing functions. It suffices to show that there exist a partition $D = x_0 < \dots < x_n$ of $[-1, 1]$ s.t. 
\be
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| \to \infty.
\ee

Since $g$ is an even function, we only consider the interval $[-1,0]$. We choose the partition $D$ of $[-1,0]$ s.t.
\be
x_0= -1,\quad x_n = -\frac 2{n\pi},\ n\geq 1.
\ee

Thus,
\be
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| = \abs{\sin 1 - \frac 2{\pi}} + \sum^n_{j=2} |g(x_j)-g(x_{j-1})|
\ee

If $n$ is even,
\be
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| = \sin 1 - \frac 4{\pi} + \frac 4{\pi}\sum^{n-1}_{j \text{ odd}}\frac 1j
\ee

If $n$ is odd,
\be
\sum^n_{j=1} |g(x_j)-g(x_{j-1})| = \sin 1 - \frac 4{\pi} + \frac 4{\pi}\sum^{n-2}_{j \text{ odd}}\frac 1j + \frac 2{n\pi}
\ee

Since
\be
\sum^\infty_{j \text{ odd}}\frac 1j = \frac 12 \sum^\infty_{j \text{ odd}}\frac 2j \geq \frac 12 \sum^\infty_{j \text{ odd}}\bb{\frac 1j+ \frac 1{j+1}} = \frac 12 \sum^\infty_{j=1}\frac 1j
\ee
which is divergent. Thus, $\sum^n_{j=1} |g(x_j)-g(x_{j-1})|$ is unbounded as $n\to \infty$, as required.

\begin{exercise}
Show that if $f : [0, 1] \to \R$ is integrable then $f$ has infinitely many points of continuity.
\end{exercise}

Solution. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Analysis II}

\section{Exercises}

\begin{exercise}
Which of the following sequences of functions converge uniformly on $X$?
\ben
\item [(a)] $f_n(x) = x^n$ on $X = \bb{0, \frac 12}$;
\item [(b)] $f_n(x) = \sin(n^2x)/ \log n$ on $X = \R$;
\item [(c)] $f_n(x) = x^n$ on $X = (0, 1)$;
\item [(d)] $f_n(x) = x^n - x^{2n}$ on $X = [0, 1]$;
\item [(e)] $f_n(x) = xe^{-nx}$ on $X = [0,\infty)$;
\item [(f)] $f_n(x) = e^{-x^2}\sin(x/n)$ on $X = \R$.
\een
\end{exercise}

Solution. \ben
\item [(a)] Yes. $\forall \ve >0$, take $N>\log_2\frac 1{\ve}$, then $\forall n\geq N$, $\forall x \in \bb{0,\tfrac 12}$, we have $f(x)=0$
\be
\abs{f_n(x)-f(x)} = \abs{x^n-0} = x^n < \frac 1{2^n} < \frac 1{2^N} < \ve.
\ee

\item [(b)] Yes. $\forall \ve >0$, take $N>e^{\frac 1{\ve}}$, then $\forall n\geq N$, $\forall x \in \R$, we have $f(x)=0$
\be
\abs{f_n(x)-f(x)} = \abs{\frac{\sin(n^2x)}{\log n}-0} \leq \frac 1{\log n} < \frac 1{\log N} < \ve.
\ee

\item [(c)] No. $\forall x\in(0,1)$, $\lim_{n\to\infty}f_n(x) = 0$, but $\forall n\in \N$, 
\be
f_n\bb{2^{-\frac 1n}} =\frac 12,
\ee
so the convergence is not uniform.

\item [(d)] No. $\forall x\in(0,1)$, $\lim_{n\to\infty}f_n(x) = 0$, but $\forall n\in \N$, 
\be
f_n\bb{2^{-\frac 1n}} =\frac 14,
\ee
so the convergence is not uniform. 

\item [(e)] Yes. Each $f_n$ is differentiable, with 
\be
f_n'(x) = e^{-nx} - nxe^{-nx} = (1-nx)e^{-nx},
\ee
so the maximum value occurs at $\frac 1n$ and $f_n\bb{\tfrac 1n} = \tfrac 1{ne}$, thus, the maximum value tends to 0 as $n\to \infty$.

\item [(f)] Yes. $\forall \ve >0$, choose $X>0$ such that $e^{-X^2}<\ve$, and take $N> \tfrac X{\ve}$, then $\forall n\geq N$, $\forall x \in \R$, if $\abs{x} > X$ we have 
\be
\abs{f_n(x)} = \abs{e^{-x^2}\sin(x/n)} \leq \abs{e^{-x^2}} < \abs{e^{-X^2}} < \ve,
\ee
while if $\abs{x} \leq X$, we have
\be
\abs{f_n(x)} = \abs{e^{-x^2}\sin(x/n)} \leq \abs{\sin \frac xn} < \sin \frac Xn < \frac Xn \leq \frac XN < \ve.
\ee

\een

\begin{exercise}
Suppose that $f : [0, 1] \mapsto \R$ is continuous. Show that the sequence $(x^nf(x))$ is uniformly convergent on $[0, 1]$ if and only if $f(1) = 0$.
\end{exercise}

Solution. "$\Longrightarrow$". Since a uniform limit of continuous functions is continuous, and the limit as $n\to \infty$ of $x^nf(x)$ is 0 if $x<1$ and $f(1)$ if $x=1$, for $x^n f(x)$ to converge uniformly requires $f(1)=0$.

"$\Longleftarrow$". Conversely, suppose $f(1)=0$, as $f$ is continuous on a closed bounded interval, it is bounded. So $\exists M$ s.t. $\abs{f(x)} \leq M$ for all $x\in [0,1]$. Given $\ve >0$, $\exists\delta>0$ s.t.
\be
\abs{x-1}<\delta, \ \abs{f(x)-f(1)} = \abs{f(x)} < \ve \quad(\text{definition of continuity at 1}).
\ee

Choose $N$ with 
\be
\bb{1-\frac 12 \delta}^N < \frac {\ve}M,
\ee

then $\forall n\geq N$, $\forall x\in [0,1]$, if $x< 1 -\tfrac 12 \delta $ we have
\be
\abs{x^nf(x)} < \bb{1-\frac 12 \delta}^nM \leq \bb{1-\frac 12 \delta}^NM < \ve,
\ee
while if $x\geq 1 -\tfrac 12 \delta$ we have
\be
\abs{x^nf(x)} \leq \abs{f(x)} < \ve.
\ee

\begin{exercise}\label{ques:x^2_uniform_continuous} 
Let $f$ and $g$ be uniformly continuous real-valued functions on a set $E \subseteq \R$. Show that the pointwise sum $f + g$ is uniformly continuous on $E$, and so is $\lm f$ for each real constant $\lm$. Give an example showing that the (pointwise) product $fg$ need not be uniformly continuous on $E$. Is
it possible to find such an example with $f$ bounded?
\end{exercise}

Solution. Since $f$ ang $g$ are uniformly continuous, $\forall \ve >0$, then $\exists \delta_1>0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta_1$,
\be
\abs{f(x)-f(y)} < \frac {\ve}2,
\ee
$\exists \delta_2>0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta_2$,
\be
\abs{g(x)-g(y)} < \frac {\ve}2.
\ee

Take $\delta = \min\{\delta_1,\delta_2\}$, we have $\forall \ve>0$, $\exists \delta>0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta$,
\be
\abs{f(x)+g(x)-f(y)-g(y)} \leq \abs{f(x)-f(y)} + \abs{g(x)-g(y)} < \frac {\ve}2 + \frac {\ve}2 = \ve.
\ee

so $f+g$ is uniformly continuous.

Now given $\lm \in \R$, if $\lm = 0$ the result is trivial, so assume $\lm \neq 0$. $\forall \ve >0$, $\exists \delta >0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta$,
\be
\abs{f(x)-f(y)} < \frac {\ve}{\abs{\lm}} \ \ra \ \abs{(\lm f)(x)-(\lm f)(y)} = \abs{\lm} \cdot \abs{f(x)-f(y)} < \abs{\lm}\cdot \frac {\ve}{\abs{\lm}} = \ve.
\ee
so $\lm f$ is uniformly continuous.

Let $E = \R$ and $f(x)=g(x)=x$, then $f$ and $g$ are both uniformly continuous (given $\ve>0$, we may let $\delta = \ve$), but we have
\be
fg(x) = x^2,\quad x\in \R.
\ee

Take $\ve = 1$, then $\forall \delta >0$, $\exists x > \frac 1{\delta}$ and set $y = x+ \frac {\delta}2$, then $\abs{x-y} = \frac {\delta}2 < \delta$ but
\be
\abs{fg(x)-fg(y)} = \abs{x^2 - \bb{x+\tfrac {\delta}2}^2} = \delta \abs{x+\tfrac{\delta}4} > \delta \abs{x} > \delta \frac 1{\delta} = 1 = \ve.
\ee
so $fg$ need not be uniformly continuous.

Now consider $E\in \R$ and $f(x)=\sin x$, $g(x) = x$ (note that $f$ is periodic with period $2\pi$, and on $[0,2\pi]$ (closed and bounded), it is continuous and hence uniformly continuous). Take $\ve =1$, then $\forall \delta >0$, $\exists$
\be
n > \frac 1{2\pi \abs{\sin \frac{\delta}2}},\ n \in \N, \quad x = 2n\pi,\quad y = 2n\pi + \tfrac {\delta}2,
\ee
then $\abs{x-y}=\frac{\delta}2 < \delta$ but
\beast
\abs{fg(x)-fg(y)} & = & \abs{x\sin x - \bb{x+\tfrac {\delta}2}\sin \bb{x+\tfrac {\delta}2}} = \abs{\bb{x+\tfrac {\delta}2}\sin \bb{x+\tfrac {\delta}2}} \\
& = & \abs{\bb{2n\pi +\tfrac {\delta}2}\sin \bb{2n\pi+\tfrac {\delta}2}} = \abs{\bb{2n\pi +\tfrac {\delta}2}\sin \tfrac {\delta}2} = \abs{\bb{2n\pi +\tfrac {\delta}2}}\abs{\sin \tfrac {\delta}2} > \delta \abs{x} > 1 = \ve.
\eeast
so $fg$ need nto be uniformly continuous if $f$ is bounded but $g$ is not.

\begin{exercise}
Let $(f_n)$ be a sequence of continuous real-valued functions on a closed, bounded interval $[a, b]$, and suppose that $f_n$ converges pointwise to a continuous function $f$.

Show that if $f_n \to f$ uniformly on $[a, b]$ and $(x_n)$ is a sequence of points in $[a, b]$ with $x_n \to x$, then $f_n(x_n) \to f(x)$. [Careful -- this is not quite as easy as it looks!]

On the other hand, show that if $f_n$ does not converge uniformly to $f$, then we can find a convergent sequence $x_m \to x$ in $[a, b]$ such that $f_n(x_n)$ does not converge to $f(x)$. 

[Hint: Bolzano-Weierstrass.]
\end{exercise}

Solution. Since $f$ is continuous and $x_n \to x$, we have $f(x_n) \to f(x)$ i.e. $\forall \ve>0$, $\exists N$ s.t. $n\geq N$,
\be
\abs{f(x_n)-f(x)} < \frac {\ve}2.
\ee

Also, if $f_n \to f$ uniformly on $[a, b]$, we have given $\ve>0$, $\exists N'$ s.t. 
\be
\abs{f_n(y) - f(y)} < \frac {\ve}2,
\ee
for all $y\in [a,b]$. Thus, $\forall \ve>0$, we take $n \geq \max\{N,N'\} $,
\be
\abs{f_n(x_n)-f(x)} \leq \abs{f(x_n)-f(x)} + \abs{f_n(x_n) - f(x_n)} < \frac {\ve}2 + \frac {\ve}2 = \ve.
\ee
so $f_n(x_n) \to f(x)$.

If $f_n$ does not converge uniformly to $f$, for each $n\in \N$ the continuous function $f_n-f$ is bounded on $[a,b]$ and attains its bounds, so we take $y_n$ such that $\abs{f_n(y_n) - f(y_n)}$ is maximal. Then
\be
f_n(y_n) - f(y_n) \nrightarrow 0 \quad \text{as }n\to \infty, \quad (\text{otherwise it converges uniformly})
\ee
so $\exists \ve>0$ s.t. $\forall N \in\N$, $\exists n \geq N$ with 
\be
\abs{f_n(y_n) - f(y_n)} > \ve.
\ee

Thus, there exists a sequence $n_1<n_2< \dots$ such that $\forall k \in \N$ we have
\be
\abs{f_{n_k}(y_{n_k}) - f(y_{n_k})} > \ve.
\ee

With Bolzano-Weierstrass Theorem, the bounded sequence $(y_{n_k})$ has a convergent subsequence $(y_{n_{k_i}})$ converging to $x$, say. For $m\in \N$, define
\be
x_m = \left\{\ba{ll}
y_{n_{k_i}} \quad\quad & m= n_{k_i}\\
x & m \neq \{ n_{k_i}:i\in \N\}
\ea\right.
\ee
then $x_m \to x$, so as $f$ is continuous $f(x_m)\to f(x)$, and thus given $\ve>0$, $\exists N'\in \N$ such that $\forall m \geq N'$,
\be
\abs{f(x_m) -f(x)} < \frac{\ve}2.
\ee

Thus, $\forall N$, there exist $\ve>0$ and $N'$, and then $\exists i \in\N$ s.t. $n_{k_i}\geq \max\{N,N'\}$ with
\be
\abs{f_{n_{k_i}}(y_{n_{k_i}}) - f(y_{n_k})} > \ve,\quad\quad \abs{f(y_{n_{k_i}}) -f(x)} < \frac{\ve}2.
\ee

Hence, 
\beast
\abs{f_{n_{k_i}}(y_{n_{k_i}}) - f(x)} & = & \abs{f_{n_{k_i}}(y_{n_{k_i}}) - f(y_{n_k}) + f(y_{n_k}) - f(x)} \\
& \geq & \abs{f_n(y_{n_{k_i}}) - f(y_{n_k})} - \abs{f(y_{n_{k_i}}) -f(x)} > \ve - \frac{\ve}2 =  \frac{\ve}2.
\eeast

so we have $f_n(x_n)\nrightarrow f(x)$.

\begin{exercise}
Suppose that $f :\ [0,\infty) \mapsto \to \R$ is continuous and that $f(x)$ tends to a (finite) limit as $x \to \infty$. Is $f$ necessarily uniformly continuous on $[0,\infty)$? Give a proof or a counter-example as appropriate.
\end{exercise}

Solution. Let $f(x)\to \ell$ as $x\to \infty$. Given $\ve>0$, $\exists M >0$ s.t. $x > M$ with
\be
\abs{f(x) - \ell} < \frac{\ve}2.
\ee

On $[0,M+1]$ (closed and bounded) $f$ is continuous and hence uniformly continuous, so $\exists \delta >0$ (we may assume $\delta <1$) such that if $0\leq x<y \leq M+1$ then
\be
\abs{y-x} \ \ra \ \abs{f(y)-f(x)} < \ve.
\ee

If $y>M+1$, then $\abs{y-x}<\delta$ implies $x > M$, thus,
\be
\abs{f(y)-f(x)} \leq \abs{f(y)-\ell} + \abs{f(x)-\ell} < \frac{\ve}2 + \frac{\ve}2 = \ve.
\ee

Thus, $f$ is uniformly continuous on $[0,\infty)$.

\begin{exercise}
Which of the following functions $f$ on $[0,\infty)$ are (a) uniformly continuous, (b) bounded?
\ben
\item [(i)] $f(x) = \sin x^2$;
\item [(ii)] $f(x) = \inf\{\abs{x - n^2}:\ n \in \N\}$;
\item [(iii)] $f(x) = (\sin x^3)/(x + 1)$.
\een
\end{exercise}

Solution. \ben
\item [(i)] (a) Take $\ve = \frac 12$, then $\forall \delta >0$ (we may assume that $\delta < \frac{\sqrt{\pi}}2$) and choose
\be
N > \frac{\sqrt{\pi}}{4\delta \sqrt{2}},\ N\in \N \quad\quad \delta' = \frac{\sqrt{\pi}}{4N\sqrt{2}}<\delta
\ee

Take $x = N\sqrt{2\pi}$ and $y = x + \delta'$ we have $\abs{x-y}< \delta$, but
\beast
\abs{f(x)-f(y)} & = & \abs{\sin y^2} = \abs{\sin (2\pi N^2 + 2\delta' \sqrt{2\pi} N + \delta'^2)} = \abs{\sin (\tfrac {\pi}2 + \delta'^2)}\\
& > & \sin \tfrac {3\pi}4 = \frac 1{\sqrt{2}} > \frac 12 = \ve. \quad\quad (\text{since $\delta'^2 < \delta^2 < \frac {\pi}4$})
\eeast
So it is not uniformly continuous. (b) It is bounded by $\pm 1$.

\item [(ii)] (a) $f$ is piecewise linear with linear sections of gradient $\pm 1$, so $\forall \ve >0$ we may take $\delta = \ve$ to prove that $f$ is uniformly continuous. (b) $\forall n\in\N$, we have $f(n^2 +n) = n$, which implies that $f$ is unbounded.

\item [(iii)] (a) Since $f:\ [0,\infty) \mapsto \to \R$ is continuous and that $f(x)$ tends to a limit 0 as $x \to \infty$, we can see that it is uniformly continuous by previous question. (b) Obviously, it is bounded by $\pm 1$.

\een

\begin{exercise}
Show that if $(f_n)$ is a sequence of uniformly continuous functions on $\R$, and $f_n \to f$ uniformly on $\R$, then $f$ is uniformly continuous. Give an example of a sequence of uniformly continuous functions $f_n$ on $\R$, such that $f_n$ converges pointwise to a continuous function $f$, but $f$ is not uniformly continuous.

[Hint for the last part: choose the limit function $f$ first.]
\end{exercise}

Solution. $\forall \ve >0$, by uniform convergence of $f_n$, we have $\exists N \in \N$ s.t. $\forall n\geq N$, $\forall x \in\R$, we have
\be
\abs{f_n(x)-f(x)} < \frac{\ve}3.
\ee

Then by uniform continuity of $f_N$, given $\ve$, $\exists \delta >0$ s.t. $\abs{x-y}<\delta $ with
\be
\abs{f_N(x) - f_N(y)} < \frac{\ve}3.
\ee

Thus, $\forall \ve>0$, $\exists \delta >0$ s.t. $\abs{x-y}<\delta$ with
\be
\abs{f(x)-f(y)} \leq \abs{f(x)-f_N(x)} + \abs{f_N(x) - f_N(y)} + \abs{f(y)-f_N(y)} < \frac{\ve}3 + \frac{\ve}3 + \frac{\ve}3 = \ve.
\ee

Hence, $f$ is uniform continuous.

Now consider function
\be
f_n(x) = \left\{\ba{ll}
x^2 \quad\quad & \abs{x} \leq n\\
n^2 & \abs{x} >n
\ea\right.,
\ee
then $f_n$ is uniformly continuous on $\R$, as its restriction to $[-n,n]$ is continuous and hence uniformly continuous, and it is constant outside $[-n,n]$. 

Let $f(x)=x^2$, then $f_n \to f$ pointwise, so the limit is continuous but not uniformly continuous by previous question (question \ref{ques:x^2_uniform_continuous}).

\begin{exercise}
Let $f_n(x) = n^\alpha x^n(1 - x)$, where $\alpha$ is a real constant.
\ben
\item [(i)] For which values of $\alpha$ does $f_n(x) \to 0$ pointwise on $[0, 1]$?
\item [(ii)] For which values of $\alpha$ does $f_n(x) \to 0$ uniformly on $[0, 1]$?
\item [(iii)] For which values of $\alpha$ does $\int^1_0 f_n(x)dx \to 0$?
\item [(iv)] For which values of $\alpha$ does $f'_n(x) \to 0$ pointwise on $[0, 1]$?
\item [(v)] For which values of $\alpha$ does $f'_n(x) \to 0$ uniformly on $[0, 1]$?
\een
\end{exercise}

Solution. \ben
\item [(i)] First $f_n(0)=f_n(1)=0$ for all $n\in\N$. Now for $x\in (0,1)$ we have
\be
\frac{f_{n+1}(x)}{f_n(x)} = \bb{\frac {n+1}n}^\alpha x = \bb{1+\frac 1n}^\alpha x,
\ee
so for large enough $n$ we have 
\be
\frac{f_{n+1}(x)}{f_n(x)} < \sqrt{x} < 1,
\ee
and thus $f_n(x)\to 0$. Thus $f_n(x) \to 0$ pointwise on $[0,1]$ for all $\alpha \in \R$.

\item [(ii)] Since 
\be
f_n'(x) = n^\alpha \bb{nx^{n-1} - (n+1)x^n},
\ee
the maximum value take by $f_n$ occurs at $\frac n{n+1}$ and is 
\be
f_n\bb{\tfrac n{n+1}} = n^\alpha \bb{\frac n{n+1}}^n\bb{1-\frac n{n+1}} = \frac{n^{n+\alpha}}{(n+1)^{n+1}}.
\ee

Thus, if $\alpha <1$ then given $\ve >0$ take $N\in \N$ s.t. $N^{\alpha -1} < \ve$, then we have $\forall n\geq N$ and $\forall x\in [0,1]$
\be
\abs{f_n(x)} \leq \frac{n^{n+\alpha}}{(n+1)^{n+1}} < n^{\alpha -1} < N^{\alpha -1} < \ve.
\ee
as required for uniform convergence. However, if $\alpha \geq 1$ then
\be
f_n\bb{\tfrac n{n+1}} = \frac{n^{n+\alpha}}{(n+1)^{n+1}} \geq \bb{\frac n{n+1}}^{n+1} = \bb{1+ \frac 1n}^{-n} \cdot \frac 1{1+ \frac 1n} \to \frac 1e \quad\text{as }n\to \infty,
\ee
so we do not have uniform convergence. Thus, $f_n \to 0$ uniformly on $[0,1]$ for $\alpha < 1$.

\item [(iii)] We have
\be
\int^1_0 f_n(x)dx = \int^1_0 n^\alpha x^n(1 - x) dx = \frac{n^\alpha}{(n+1)(n+2)},
\ee
so $\int^1_0 f_n(x)dx\to 0$ for $\alpha < 2$.

\item [(iv)] We already have
\be
f_n'(x) = n^\alpha \bb{nx^{n-1} - (n+1)x^n} = n^{\alpha+1} x^{n-1}\bb{1 - \frac {n+1}nx},
\ee
for $x \in [0,1)$, for large enough $n$ we have $\frac{n+1}n x <1$ so that $1-\frac {n+1}nx >0$, and then
\be
0\leq f_n'(x)\leq n^{\alpha+1} x^{n-1}.
\ee

So $f_n'(x) \to 0$ for any $\alpha\in \R$. However, $f_n'(1) = -n^{\alpha}$, so for $f_n'(1) \to 0$ we need $\alpha <0$. Thus $f_n'(x)\to 0$ pointwise on $[0,1]$ for $\alpha <0$.


\item [(v)] The minimum value taken by $f_n'$ is $f_n'(1)=-n^\alpha$, the maximum value occurs when $f_n''(x) = 0$, i.e., 
\be
n^\alpha \bb{n(n-1)x^{n-2} - n(n+1)x^{n-1}} = n^{\alpha+1}(n-1)x^{n-2}\bb{1 - \frac{n+1}{n-1}x} = 0 \ \ra \ x = \frac {n-1}{n+1}.
\ee
So the maximum value is 
\be
f_n'\bb{\tfrac {n-1}{n+1}} = n^{\alpha+1} \bb{\frac {n-1}{n+1}}^{n-1}\bb{1 - \frac {n+1}n \frac {n-1}{n+1}} = n^{\alpha} \bb{\frac {n-1}{n+1}}^{n-1}.
\ee

Thus, if $\alpha <0 $ then given $\ve >0$ take $N\in \N$ s.t. $N^{\alpha} < \ve$, then we have $\forall n\geq N$ and $\forall x\in [0,1]$
\be
\abs{f_n'(x)} \leq n^{\alpha} \bb{\frac {n-1}{n+1}}^{n-1} < n^{\alpha} < N^{\alpha} < \ve.
\ee
as required for uniform convergence. However, if $\alpha \geq 0$ then
\be
f_n'\bb{\tfrac {n-1}{n+1}} = n^{\alpha} \bb{\frac {n-1}{n+1}}^{n-1} \geq \bb{\frac {n-1}{n+1}}^{n-1}  = \bb{1+ \frac 2{n-1}}^{-(n-1)} \to e^{-2} \quad\text{as }n\to \infty,
\ee
so we do not have uniform convergence. Thus, $f_n' \to 0$ uniformly on $[0,1]$ for $\alpha < 0$.

\een

\begin{exercise}
Consider the sequence of functions $f_n :\ \R\backslash \Z \to \R$ defined by $f_n(x) = \sum^n_{m=-n}(x-m)^{-2}$. Show that $f_n$ converges pointwise on $\R\backslash \Z$ to a function $f$. Show that $f_n$ does not converge uniformly on $\R \backslash \Z$. Why can we nevertheless conclude that the limit function $f$ is continuous, and indeed differentiable, on $\R \backslash \Z$?
\end{exercise}

Solution. Given $x\in \R\bs\Z$ and $\ve>0$, choose $M\in\N$ s.t. 
\be
\sum^\infty_{k=M+1} \frac 1{k^2} < \frac{\ve}2
\ee
and take $N > \abs{x} + M, N\in\N$. Then if $\abs{m} = N+\ell$ with $\ell \in \N$, we have
\be
\abs{m-x} \geq \abs{m}-\abs{x} = N+\ell -\abs{x} > M + \ell,
\ee
so that if $n' > n>N$, we have
\beast
\abs{f_{n'}(x)-f_n(x)} & = & \sum^{-(n+1)}_{m=-n'} (x-m)^{-2} + \sum^{n'}_{m=n+1} (x-m)^{-2} <  \sum^{-(N+1)}_{m=-n'} (x-m)^{-2} + \sum^{n'}_{m=N+1} (x-m)^{-2}\\
& \leq & 2\sum^{n'-N}_{\ell=1} (M+\ell)^{-2} < 2\sum^{\infty}_{k=M+1} k^{-2} < \ve,
\eeast
so the sequence $\bb{f_n(x)}$ is Cauchy and hence converges. Thus, $f_n$ converges pointwise on $\R\bs\Z$ to function $f$.

Now we prove that the sequence $(f_n)$ does not converge uniformly to $f$ on $\R\bs\Z$. Take $\ve =1$, then $\forall N\in\N$ and take $x=N+\frac 12$, we have
\be
\abs{f_N(x)-f(x)} = \sum_{\abs{m}>N}(x-m)^{-2} > \bb{\frac 12}^{-2} = 4 > \ve.
\ee
Thus, $f_n$ does not converge to a function $f$. 

However, $f$ is differentiable and therefore continuous: take $x\in \R\bs\Z$ and choose $N>\abs{x}+2$, then as $f_N$ is a finite sum of differentiable functions, it suffices to show $f-f_N$ is differentiable at $x$. Take $y\in \R\bs\Z$ with $\abs{y-x}<1$, then we have
\beast
& & \frac{(f-f_N)(y)-(f-f_N)(x)}{y-x} + 2\sum_{\abs{m}>N}(x-m)^{-3} \\
& = & \frac 1{y-x}\sum_{\abs{m}>N}\bb{(y-m)^{-2}-(x-m)^{-2}} + 2\sum_{\abs{m}>N}(x-m)^{-3} \\
& = & \sum_{\abs{m}>N}\frac {(x-m)^3- (y-m)^2(x-m) + 2 (y-x)(y-m)^2 }{(y-x)(y-m)^2(x-m)^3} \\
& = & \sum_{\abs{m}>N}\frac {(x-m)\bb{(x-m)^2- (y-m)^2} + 2 (y-x)(y-m)^2 }{(y-x)(y-m)^2(x-m)^3} \\
& = & \sum_{\abs{m}>N}\frac { 2(y-m)^2 - (x-m)(x+y-2m)}{(y-m)^2(x-m)^3} = \sum_{\abs{m}>N}\frac { (2(y-m)+ (x-m))((y-m)-(x-m))}{(y-m)^2(x-m)^3}\\
& = & (y-x)\sum_{\abs{m}>N}\frac {2(y-m)+ (x-m)}{(y-m)^2(x-m)^3} = (y-x)\sum_{\abs{m}>N}\frac {2}{(y-m)(x-m)^3} + \frac {1}{(y-m)^2(x-m)^2}.
\eeast

since $\abs{x-m}>2$ and $\abs{y-m} \geq \abs{x-m}-\abs{x-y} > 1$ if $\abs{m}>N$, we have
\be
\sum_{\abs{m}>N}\frac {2}{(y-m)(x-m)^3} + \frac {1}{(y-m)^2(x-m)^2} < 2 \sum_{\abs{m}>N}\frac 1{(x-m)^2}
\ee
which converges, so as $y\to x$ we have 
\be
\frac{(f-f_N)(y)-(f-f_N)(x)}{y-x} + 2\sum_{\abs{m}>N}(x-m)^{-3} \to 0 \ \ra \ \frac{(f-f_N)(y)-(f-f_N)(x)}{y-x} \to - 2\sum_{\abs{m}>N}(x-m)^{-3}.
\ee
Thus, $f-f_N$ is differentiable at $x$ as required.

\begin{exercise}
Let $f$ be a differentiable, real-valued function on a (bounded or unbounded) interval $E \subseteq \R$, and suppose that $f'$ is bounded on $E$. Show that $f$ is uniformly continuous on $E$.

Let $g :\ [-1, 1] \mapsto \R$ be the function defined by $g(x) = x^2 \sin(1/x^2)$, for $x \neq 0$ and $g(0) = 0$. Show that $g$ is differentiable, but its derivative is unbounded. Is $g$ uniformly continuous on $[-1, 1]$?
\end{exercise}

Solution. Suppose $\forall x\in\R$, $\abs{f'(x)}<M$, given $\ve>0$ set $\delta = \frac{\ve}M$, then if $\abs{y-x}<\delta$ and with Mean Value Theorem, we have
\be
\abs{f(y)-f(x)} = \abs{y-x}\cdot \abs{f'(z)}
\ee
for some $z\in (x,y)$, so 
\be
\abs{f(y)-f(x)} < \delta M = \ve.
\ee
Thus, $f$ is uniformly continuous.

Now consider function $g$,
\be
\lim_{h\to 0} \frac{g(h)-g(0)}{h} = \lim_{h\to 0} \frac{h^2\sin \tfrac 1{h^2}}{h} = 0,
\ee
we see that $g$ is differentiable at 0. At $x\neq 0$, $g$ is clearly differentiable with 
\be
g'(x) = 2x\sin \tfrac 1x - \frac 2x \cos \tfrac 1x.
\ee

In particular,
\be
g'\bb{\tfrac 1{\sqrt{2n\pi}}}= -2\sqrt{2n\pi},
\ee
so $g'$ is unbounded. However, as $g$ is continuous on a closed bounded interval it is uniformly continuous.

\begin{exercise}
Suppose that a function $f$ has a continuous derivative on $(a, b) \subseteq \R$ and
\be
f_n(x) = n\bb{f\bb{x +\tfrac 1n} - f(x)}.
\ee
Show that $f_n$ converges uniformly to $f'$ on each interval $[\alpha , \beta] \subset (a, b)$.
\end{exercise}

Solution. Take an interval $[\alpha , \beta] \subset (a, b)$. Let
\be
c = \frac 12 (a + \alpha),\quad\quad d = \frac 12 (b+\beta)
\ee
so that $a<c<\alpha$ and $\beta <d<b$. Since $f'$ is continuous on $[c,d]$, it is uniformly continuous there. Thus, $\forall \ve>0$, $\exists \delta >0$ (and we may assume $\delta < \min(a-c,d-\beta)$) such that if $y,x\in [c,d]$, then
\be
\abs{y-x} < \delta \ \ra \ \abs{f'(y)-f'(x)}< \ve.
\ee

Take $N > \frac 1{\delta}$, then for all $x\in[\alpha,\beta]$, $\forall n> N$ we may write (since $f$ is continuous and differentiable on $(c,d)$),
\be
f_n(x) = n\bb{f\bb{x +\tfrac 1n} - f(x)} = \frac{f\bb{x +\tfrac 1n} - f(x)}{x+\frac 1n -x} = f'(y)
\ee
for some $y\in (x,x+\tfrac 1n)$ by M.V.T. (where $x+ \frac 1n \in (\alpha -\delta, \beta + \delta)\subseteq (c,d)$). Then as $\abs{y-x} < \frac 1n < \delta$, we have $y\in [c,d]$ and so
\be
\abs{f_n(x)-f'(x)} = \abs{f'(y)-f(x)} < \ve. 
\ee

Thus, $f_n$ converges uniformly to $f'$ on $[\alpha, \beta]$.

\begin{exercise}
Let $\sum^\infty_{n=1} a_n$ be an absolutely convergent series of real numbers. Define a sequence $(f_n)$ of functions on $[-\pi, \pi]$ by 
\be
f_n(x) = \sum^n_{m=1} a_m \sin mx
\ee
and show that each $f_n$ is differentiable with 
\be
f'_n(x) = \sum^n_{m=1} m a_m \cos mx.
\ee
Show further that 
\be
f(x) = \sum^\infty_{m=1} a_m \sin mx
\ee
defines a continuous function on $[-\pi, \pi]$, but that the series 
\be
\sum^\infty_{m=1} m a_m \cos mx
\ee
need not converge.
\end{exercise}

Solution. As $x\mapsto \sin mx$ is a differentiable function on $[-\pi,\pi]$ with derivative $x\mapsto m\cos mx$, it follows that $f_n$, being a finite sum of such functions, is differentiable with 
\be
f_n'(x) = \sum^n_{m=1} ma_m \cos mx.
\ee

As $\sum^\infty_{n=1}a_n$ is absolutely convergent, $\forall \ve>0$, $\exists N\in \N$ s.t. 
\be
\sum_{m>N}\abs{a_m} < \ve.
\ee

Thus, $\forall n\geq N$, $\forall x\in [-\pi,\pi]$,
\be
\abs{f_n(x)-f(x)} = \abs{\sum_{m>n}a_m \sin mx} \leq \sum_{m>n}\abs{a_m \sin mx} \leq \sum_{m>n} \abs{a_m} \leq \sum_{m>N}\abs{a_m} < \ve
\ee
so $f_n$ converges uniformly to $f$, thus $f$ is a uniform limit of continuous functions, hence is continuous. However, if we take $a_m = \frac 1{m^2}$, then $\sum a_m$ is absolutely convergent but 
\be
\sum m a_m \cos mx = \sum \frac 1m \cos mx,
\ee
which does not converge at 0.

\begin{exercise}
Let $f$ be a bounded function defined on a set $E \subseteq \R$, and for each positive integer $n$ let $g_n$ be a function defined on $E$ by
\be
g_n(x) = \sup\{\abs{f(y) - f(x)} :\ y \in E, \abs{y - x} < 1/n\}.
\ee

Show that $f$ is uniformly continuous on $E$ if and only if $g_n \to 0$ uniformly on $E$ as $n \to \infty$.
\end{exercise}

Solution. "$\Longrightarrow$". Suppose $f$ is uniformly continuous on $E$ and take $\ve>0$. Then $\exists \delta >0$ such that for all $x,y\in E$, $\abs{y-x}< \delta$, with $\abs{f(y)-f(x)}< \ve$. So $\forall \ve >0$, $\exists \delta >0$ and $n> \frac 1{\delta} \ \ra \ \abs{y-x}<\frac 1n < \delta$, we have $g_n(x) < \ve$, which means that $g_n \to 0$ uniformly on $E$.

"$\Longleftarrow$". Suppose $g_n \to 0$ uniformly on $E$ and take $\ve>0$. Then there exists $N\in \N$ s.t. $\forall x\in E$ and $\forall n \geq N$, we have
\be
\abs{g_n(x)} < \ve \ \ra g_n(x) < \ve.
\ee

So we set $\delta = \frac 1{N+1}$, then for $\forall x,y \in E$ with $\abs{x-y} < \delta = \frac 1{N+1} < \frac 1N$,
\be
\abs{f(y)-f(x)} \leq g_N(x) < \ve.
\ee

Thus, $f$ is uniformly continuous on $E$.

\begin{exercise}[Dini's theorem]
Let $f_n :\ [0, 1] \mapsto \R$ be a sequence of continuous functions converging pointwise to a continuous function $f :\ [0, 1] \mapsto \R$. Suppose that $f_n(x)$ is a decreasing sequence $f_n(x) \geq f_{n+1}(x)$ for each $x \in [0, 1]$. Show that $f_n \to f$ uniformly on $[0, 1]$.

[If you have done Metric and Topological Spaces then you may prefer to find a topological proof.]
\end{exercise}

Solution. Here we use two approaches.

\vspace{2mm}

{\bf Non-topological proof.} 

By replacing each $f_n$ by the continuous function $g_n = f_n -f$, we may assume $g=0$. Then $\forall n\in \N$ and $x\in [0,1]$ we have $g_n$ is decreasing sequence and $g_n(x) \geq 0$.

If $g_n$ does not converge to 0 uniformly, $\forall N\in \N$, $\exists \ve >0$ and $\exists n\geq N, \exists x_n\in [0,1]$ s.t.
\be
\abs{g_n(x_n)} \geq \ve.
\ee  
Then the bounded sequence $(x_n)$ has a convergent subsequence $(x_{n_i})$ with limit $\ell \in [0,1]$. 

But with the above argument, $\forall \delta >0$, $\exists \ve >0, i\in \N$ with
\be
\abs{x_{n_i}-\ell}<\delta,\quad\quad \abs{g_{n_i}(x_{n_i})} \geq \ve. \quad\quad (*)
\ee

Since $g_n \to 0$ pointwise, there exists $N\in \N$, $\forall n\geq N$ with $g_n(\ell) < \frac{\ve}2$. 

Since $g_n$ is continuous, there exists $\delta > 0$ with $\abs{y-\ell} < \delta \ \ra \ \abs{g_n(y)-g_n(\ell)} < \frac {\ve}2$. Thus,
\be
\abs{g_n(y)} \leq \abs{g_n(y) - g_n(\ell)} + \abs{g_n{\ell}} <  \frac{\ve}2 +  \frac{\ve}2 = \ve.
\ee

So $\forall m \geq n$ we have 
\be
\abs{y-\ell}< \delta \ \ra \ g_m(y) < \ve.
\ee

i.e., $\forall \ve >0$, $\exists \delta >0, N\in\N$, $\forall m \geq N$ with $\abs{y-\ell}< \delta$,
\be
g_m(y) < \ve.
\ee

This is a contradiction to $(*)$. Thus $g_n\to 0$ uniformly.

\vspace{4mm}

{\bf Topological proof.} 

Given $\ve>0$, let $E_n= \{x\in [0,1]: f_n(x)-f(x) < \ve\}$. For all $n\in\N$, since $f_n-f$ is continuous $E_n$ is open, and as $f_n(x)\geq f_{n+1}(x)$ for all $x$ we have $E_n\subseteq E_{n+1}$. Since $f_n\to f$, for all $x$ there exists $n\in \N$ with $x\in E_n$, so the $E_n$ form an open cover of the compact set $[0,1]$, and thus there is a finite subcover, and so there exists $N\in \N$ with $E_N=[0,1]$. Then $n> N$, $\forall x\in [0,1]$ we have
\be
\abs{f_n(x)-f(x)} = f_n(x)-f(x) < \ve.
\ee
So $f_n \to f$ uniformly on $[0,1]$.

\begin{exercise}[Abel's test]
Let $a_n$ and $b_n$ be real-valued functions on $E \subseteq \R$. Suppose that $\sum^\infty_{n=0} a_n(x)$ is uniformly convergent on $E$. Suppose further that the $b_n(x)$ are uniformly bounded on $E$ (this means there is a constant $K$ with $\abs{b_n(x)} \leq K$ for all $n$ and all $x \in E$), and that $b_n(x) \geq b_{n+1}(x)$ for all $n$ and all $x \in E$. Show that the sum 
\be
\sum^\infty_{n=0} a_n(x)b_n(x)
\ee
is uniformly convergent on $E$. 

[Hint: show first that 
\be
\sum^m_{k=n} a_k b_k = \sum^{m-1}_{k=n} (b_k-b_{k+1}) A_k + b_mA_m - b_nA_{n-1},
\ee
where $A_n = \sum^n_{k=0} a_k$.]

Deduce that if $a_n$ are real constants and $\sum^\infty_{n=0} a_n$ is convergent, then $\sum^\infty_{n=0} a_nx^n$ is uniformly convergent on $[0, 1]$. (But note that $\sum^\infty_{n=0} a_nx^n$ need not be convergent at $x = -1$; you almost certainly know a counterexample!)
\end{exercise}

Solution. Set 
\be
A_n =\sum^n_{k=0} a_k \ \ra \ a_n = A_n - A_{n-1}.
\ee

then if $m>n$, we have
\beast
& & \sum^{m-1}_{k=n}(b_k-b_{k+1}) A_k + b_mA_m - b_nA_{n-1} \\
& = & -b_nA_{n-1} + (b_n - b_{n+1})A_n + (b_{n+1}-b_{n+2})A_{n+1} + \dots + (b_{m-1}-b_m)A_{m-1} + b_mA_m \\
& = & b_n(A_n - A_{n-1}) + b_{n+1} (A_{n+1}-A_n) + \dots + b_m(A_m - A_{m-1}) = \sum^m_{k=n} a_k b_k.
\eeast

Thus,
\beast
\sum^m_{k=n} a_k b_k & = & \sum^{m-1}_{k=n}(b_k-b_{k+1}) A_k + b_mA_m - b_nA_{n-1} \\
& = & \sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + \sum^{m-1}_{k=n}(b_k-b_{k+1}) A_m + b_mA_m - b_nA_{n-1}\\
& = & \sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + (b_n-b_m) A_m + b_mA_m - b_nA_{n-1}\\
& = & \sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + b_n(A_m - A_{n-1}).
\eeast

Now take $\ve >0$, as $\sum^\infty_{n=0} a_n(x)$ uniformly convergent on $E$, there exists $N\in \N$ s.t. if $m>n\geq N$, then for all $x\in E$ we have
\be
\abs{A_m(x)-A_n(x)} < \frac {\ve}{3K}.
\ee

Then, if $m>n\geq N+1$, we have
\beast
\abs{\sum^m_{k=n} a_k b_k} & = & \abs{\sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + b_n(A_m - A_{n-1})}\\
& \leq & \sum^{m-1}_{k=n}(b_k-b_{k+1})\abs{A_k - A_m} + \abs{b_n(A_m - A_{n-1})}\\
& \leq & \sum^{m-1}_{k=n}(b_k-b_{k+1})\frac{\ve}{3K} + \abs{b_n}\abs{A_m - A_{n-1})}\\
& \leq & 2K \frac{\ve}{3K} + K \frac{\ve}{3K} = \ve.
\eeast
Thus, $\sum^\infty_{n=0} a_n(x)b_n(x)$ is uniformly convergent on $E$. 

Set $E =[0,1]$, $a_n(x)=a_n$ and $b_n(x) = x^n$. Since $a_n$ are real constants and $\sum^\infty_{n=0} a_n$ is convergent, then it is uniformly convergent. Thus, applying the Abel's test, we have $\sum^\infty_{n=0} a_nx^n$ is uniformly convergent on $[0,1]$.

\begin{exercise}
Define $\varphi(x) = |x|$ for $x\in [-1, 1]$ and extend the definition of $\varphi(x)$ to all real $x$ by requiring that
\be
\varphi(x + 2) = \varphi(x).
\ee
\ben
\item [(i)] Show that $\abs{\varphi(s) - \varphi(t)} \leq \abs{s - t}$ for all $s$ and $t$.
\item [(ii)] Define $f(x) = \sum^\infty_{n=0} \bb{\frac 34}^n \varphi(4^nx)$. Prove that $f$ is well-defined and continuous.
\item [(iii)] Fix a real number $x$ and positive integer $m$. Put $\delta_m = \pm \frac 12 4^{-m}$, where the sign is so chosen that no integer lies between $4^m x$ and $4^m(x + \delta_m)$. Prove that
\be
\abs{\frac {f(x + \delta_m) - f(x)}{\delta_m} } \geq \frac 12 (3^m + 1).
\ee
\item [(iv)] Conclude that $f$ is not differentiable at $x$. Hence there exists a real continuous function on the real line which is nowhere differentiable.
\een
\end{exercise}

\scutline

Solution. \ben
\item [(i)] We may assume $s>t$. Since $\varphi$ has period 2 and takes values in $[0,1]$, it suffices to consider $s,t$ s.t. 
\be
\abs{t} \leq 1,\quad\quad \abs{s-t} \leq 1. \quad (\text{the inequality is obvious when }\abs{s-t}> 1)
\ee
If $\abs{s} \leq 1$, then
\be
\abs{\varphi(s)-\varphi(t)} = \abs{\abs{s}-\abs{t}} \leq \abs{s-t} \quad\quad (\text{by triangle inequality}).
\ee

If $\abs{s} >1$, then $s\in (1,2]$ and hence $t\in (0,1]$. Hence,
\be
\abs{\varphi(s)-\varphi(t)} = \abs{(2-s) - t} \leq \abs{s-1}+\abs{t-1} = s-1 + 1-t = s-t = \abs{s-t}.
\ee

Thus for all $s$ and $t$, the inequality holds.

\item [(ii)] For all $x$, the partial sum of $\sum^\infty_{n=N+1} \bb{\frac 34}^n \varphi(4^nx)$ are bounded by 
\be
\sum^\infty_{n=0}\bb{\tfrac 34}^n = \frac 1{1-\frac 34} = 4.
\ee
so as the terms are all non-negative, the sum converges, thus $f$ is well-defined. For $m\in\N$, set
\be
f_m(x) = \sum^m_{n=0}\bb{\tfrac 34}^n \varphi(4^nx).
\ee

Given $\ve>0$, choose $N\in\N$ s.t. $\sum^\infty_{n=N+1}\bb{\frac 34}^n < \frac {\ve}3$, then $f_N$ is a finite sum of continuous functions, so is continuous. Thus given $x$ there exists $\delta>0$ s.t. 
\be
\abs{y-x}< \delta,\quad\quad\abs{f_N(y)-f_N(x)}< \frac {\ve}3.
\ee
Thus,
\beast
\abs{f(y)-f(x)} & \leq & \abs{f(x)- f_N(x)} + \abs{f(y)- f_N(y)} + \abs{f_N(y)-f_N(x)} \\
& \leq & \sum^\infty_{n=N+1}\bb{\tfrac 34}^n \bb{\varphi(4^nx)+\varphi(4^ny)} + \abs{f_N(y)-f_N(x)} \\
& \leq & 2 \sum^\infty_{n=N+1}\bb{\tfrac 34}^n + \abs{f_N(y)-f_N(x)} < \frac {2\ve}3 + \frac {\ve}3 = \ve.
\eeast

Thus, $f$ is continuous.

\item [(iii)] If $n>m$, then $4^n(x+\delta_m) = 4^nx \pm 2\cdot 4^{n-m-1}$, so that
\be
\varphi(4^nx) = \varphi(4^n(x+\delta_m)) \ \ra \ (f-f_m)(x+\delta_m) = (f-f_m)(x) \ \ra \ f(x+\delta_m)-f(x) = f_m(x+\delta_m) - f_m(x).
\ee

If $n\leq m$, as no integer lies between $4^nx$ and $4^n(x+\delta_m)$, the function $y\mapsto \bb{\tfrac 34}^n \varphi(4^ny)$ is linear between $x$ and $x+\delta_m$, and its gradient is $\pm \bb{\frac 34}^n 4^n = 3^n$. Thus $f_m$ is linear between $x$ and $x+\delta_m$ and the absolute value of its gradient is at least 
\be
3^m - 3^{m-1} - \dots - 3 -1 = 3^m - \frac 12 (3^m-1) = \frac 12\bb{3^m + 1}
\ee

Thus, we have
\be
\abs{\frac {f(x + \delta_m) - f(x)}{\delta_m} } = \abs{\frac {f_m(x + \delta_m) - f_m(x)}{\delta_m} } \geq \frac 12 (3^m + 1).
\ee

\item [(iv)] If $f$ is differentiable at $x$ with $f'(x) = M$, there exists $\delta>0$ s.t. $\abs{h}<\delta$, 
\be
\abs{\frac{f(x+h)-f(x)}{h}-M} < 1 \ \ra \ \abs{\frac{f(x+h)-f(x)}{h}} < \abs{M}+1,
\ee
but we may choose $m$ s.t. $\abs{\delta_m}< \delta$ and $\frac 12 (3^m+1)>\abs{M}+1$, and we have a contradiction. Thus $f$ is not differentiable at $x$. Since $x$ is arbitrary, $f$ is a real continuous function which is nowhere differentiable.

\een

\qcutline

Unless stated otherwise, the norm on $\R^n$ may be taken to be the Euclidean norm $\|x\|_2 = \sqrt{\sum^n_{i=1} x_i^2}$, and the spaces $\ell_0$ and $\ell_\infty$ may be assumed to have the sup-norm $\|x\|_\infty = \sup_i |x_i|$. ($\ell_0$ denotes the space of real sequences $(x_n)^\infty_{n=1}$ such that all but finitely many $x_n$ are zero.) 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}
Let $(x^{(m)})$ and $(y^{(m)})$ be sequences in $\R^n$ converging to $x$ and $y$ respectively. Show that $x^{(m)}\cdot y^{(m)}$ converges to $x \cdot y$. Deduce that if $f :\ \R^n \to \R^p$ and $g :\ \R^n \to \R^p$ are continuous at $x \in \R^n$, then so is the pointwise scalar product function $f \cdot g:\ \R^n \to \R$.
\end{exercise}

Solution. First observe that if $a,b\in \R^n$ then 
\be
\abs{a\cdot b}^2 = \bb{\sum a_i b_i}^2 \leq \bb{\sum a_i^2}\cdot \bb{\sum b_i^2} = \dabs{a}^2 \cdot\dabs{b}^2 \ \ra \ \abs{a\cdot b} \leq \dabs{a}\cdot \dabs{b}.
\ee 

Now given $\ve>0$, $\exists N_1 \in\N$ s.t. $m>N_1$ 
\be
\dabs{x^{(m)}-x} < \frac {\ve}{2\bb{\dabs{y}+1}}
\ee
and $\exists N_2 \in\N$ s.t. $m>N_2$ 
\be
\dabs{y^{(m)}-y} < \frac {\ve}{2\bb{\dabs{x}+\frac{\ve}{2\bb{\dabs{y}+1}}}}.
\ee

Thus, $m> \max\{N_1,N_2\}$, we have
\beast
\abs{x^{(m)}\cdot y^{(m)} - x\cdot y} & \leq & \abs{x^{(m)}\cdot y^{(m)} - x^{(m)}\cdot y} + \abs{x^{(m)}\cdot y - x\cdot y}\\
 & \leq & \dabs{x^{(m)}}\dabs{y^{(m)} - y} + \dabs{y}\dabs{x^{(m)} - x}\\
 & < & \bb{\dabs{x} + \frac {\ve}{2\bb{\dabs{y}+1}}}\dabs{y^{(m)} - y} + \dabs{y}\frac {\ve}{2\bb{\dabs{y}+1}} \\
& < & \frac {\ve}2 + \frac {\ve}2 = \ve.
\eeast

Thus, $x^{(m)}\cdot y^{(m)}$ converges to $x \cdot y$. 

Now if $f$, $g$ are continuous at $x$, take any sequence $x^{(m)}$ converging to $x$, then by continuity we have
\be
f(x^{(m)}) \to f(x),\quad \quad g(x^{(m)}) \to g(x) \quad  \ra\quad  (f\cdot g)(x^{(m)}) = f(x^{(m)}) \cdot g(x^{(m)}) \to f(x)\cdot g(x) = f\cdot g(x) 
\ee
by the previous result. Thus, $f\cdot g$ is continuous at $x$.

\begin{exercise}
Show that $\dabs{x}_1 = \sum^n_{i=1} \abs{x_i}$ defines a norm on $\R^n$. Show directly that it is Lipschitz equivalent to the Euclidean norm.
\end{exercise}

Solution. For $\dabs{x}_1 = \sum^n_{i=1} \abs{x_i}$, we check \ben
\item [(i)] Clearly $\dabs{\cdot}_1:\R^n \to \R$ takes non-negative values,
\item [(ii)] $\dabs{x}_1 = 0 \ \ra\ \sum^n_{i=1} \abs{x_i} = 0 \ \ra \ \forall i,\ x_i = 0 \ \ra \ x = (0,\dots,0)^T$,
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm x}_1 = \sum^n_{i=1} \abs{\lm x_i} = \abs{\lm} \sum^n_{i=1} \abs{x_i} = \abs{\lm}\cdot \dabs{x}_1$,
\item [(iv)] and
\be
\dabs{x+y}_1 = \sum^n_{i=1} \abs{(x+y)_i} = \sum^n_{i=1} \abs{x_i+y_i} \leq \sum^n_{i=1} \abs{x_i}+\abs{y_i} = \sum^n_{i=1} \abs{x_i} +\sum^n_{i=1} \abs{y_i} = \dabs{x}_1 + \dabs{y}_1.
\ee
\een
So $\dabs{\cdot}_1$ is a norm. 

We have
\be
\dabs{x}^2 = \sum^n_{i=1} x_i^2 \leq \bb{\sum^n_{i=1} \abs{x_i}}^2= \dabs{x}_1^2 \ \ra \ \dabs{x} \leq \dabs{x}_1.
\ee
and
\be
\dabs{x}_1^2 = \bb{\sum^n_{i=1} \abs{x_i}}^2 \leq \bb{\sum^n_{i=1} \abs{x_i}^2}\bb{\sum^n_{i=1} 1^2} = n\dabs{x}^2 \ \ra \ \dabs{x}_1 \leq \sqrt{n}\dabs{x}.
\ee
Thus, $\dabs{\cdot}_1$ is Lipschitz equivalent to the Euclidean norm $\dabs{\cdot}$.

\begin{exercise}\label{ques:norm_one} 
\ben
\item [(a)] Show that $\dabs{f}_1 = \int^1_0 \abs{f(x)} dx$ defines a norm on the space $C[0, 1]$. Is it Lipschitz equivalent to the uniform norm?
\item [(b)] Let $R[0, 1]$ denote the vector space of all integrable functions on $[0, 1]$. Does $\dabs{f} = \int^1_0 \abs{f(x)} dx$ define a norm on $R[0,1]$?
\een
\end{exercise}

Solution. \ben
\item [(a)] For $\dabs{f}_1 = \int^1_0 \abs{f(x)} dx$ on the space $C[0,1]$, we check \ben
\item [(i)] Clearly $\dabs{\cdot}_1:C[0,1] \to \R$ takes non-negative values,
\item [(ii)] $\dabs{f}_1 = 0 \ \ra\ \int^1_0 \abs{f} = 0 \ \ra \ f = 0$, (Analysis I Example Sheet 4)
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm f}_1 = \int^1_0 \abs{\lm f} = \abs{\lm } \int^1_0 \abs{f} = \abs{\lm}\cdot \dabs{f}_1$,
\item [(iv)] and
\be
\dabs{f+g}_1 = \int^1_0 \abs{f+g} \leq \int^1_0 \abs{f}+\abs{g} = \int^1_0 \abs{f} + \int^1_0 \abs{g} = \dabs{f}_1 + \dabs{g}_1.
\ee
\een
so $\dabs{\cdot}_1$ is a norm. Given $M>0$, choose $n\in \N$ with $n>M+1$ and define $f \in C[0,1]$ by
\be
f(x) = \left\{\ba{ll}
nx & 0\leq x< \frac 1n\\
2-nx \quad\quad & \frac 1n \leq x < \frac 2n\\
0 & \frac 2n \leq x \leq 1
\ea\right..
\ee 
then
\be
\dabs{f}_1 = \int^1_0 \abs{f(x)}dx = \int^{\frac 1n}_0 nx dx + \int^{\frac 2n}_{\frac 1n}(2-nx)dx = \frac n2\frac 1{n^2} + 2\frac 1n - \frac n2 \bb{\frac 4{n^2}-\frac 1{n^2}} = \frac 1n.
\ee
\be
\dabs{f}_\infty = \sup_{x\in [0,1]}\abs{f(x)} = 1.
\ee
so $\dabs{f}_\infty = n\dabs{f}_1 > M\dabs{f}_1$, so $\dabs{\cdot}_1$ is not Lipschitz equivalent to $\dabs{\cdot}_\infty$.

\item [(b)] No. Define $f\in R[0,1]$ by 
\be
f(x) = \left\{\ba{ll}
1 & x=0\\
0 \quad\quad & x>0
\ea\right. \ \ra \ f\neq 0, \text{ but }\dabs{f} = \int^1_0 \abs{f(x)} dx = 0.
\ee 

\een

\begin{exercise}
Which of the following subsets of $\R^2$ are open? Which are closed? (And why?)
\ben
\item [(i)] $\{(x, 0) :\ 0 \leq x \leq 1\}$;
\item [(ii)] $\{(x, 0) :\ 0 < x < 1\}$;
\item [(iii)] $\{(x, y) :\ y \neq 0 \}$;
\item [(iv)] $\{(x, y) :\ x \in \Q \text{ or }y \in \Q\}$;
\item [(v)] $\{(x, y) :\ y = nx \text{ for some }n \in \N \} \cup \{(x, y) :\ x = 0\}$;
\item [(vi)] $\{(x, f(x)) :\ x \in \R\}$, where $f :\ \R \to \R$ is a continuous function.
\een
\end{exercise}

Solution. In each case we denote the subset concerned by $S$.
\ben
\item [(i)] Not open. $S$ contains no open ball about any point. 

Closed. If $(x,y)\notin S$ then either $y\neq 0$ or $y=0$ and $x<0$, or $y=0$ and $x>1$. The open ball about $(x,y)$ of radius $\frac {\abs{y}}2$ or $-\frac x2$, or $\frac{x-1}2$ as appropriate lies outside $S$.

\item [(ii)] Not open. as in (i). 

Not closed. $(0,0)\notin S$ but any open ball about $(0,0)$ meets $S$.

\item [(iii)] Open. If $(x,y)\in S$ the open ball about $(x,y)$ of radius $\frac{\abs{y}}2$ lies in $S$.

Not closed. as in (ii).

\item [(iv)] Not open. as in (i).

Not closed. If $(x,y)\notin S$ then any open ball about $(x,y)$ meets $S$.

\item [(v)] Not open. $(1,1)\in S$ but $S$ contains no open ball about $(1,1)$.

Closed. If $(x,y)\notin S$ then $x\neq 0$ and $\frac yx \notin \N$. If $\frac yx <1$ set
\be
S_1 = \{(x',0):x'\in \R\},\quad\quad S_2 = \{(x',x'):x'\in \R\},
\ee
while if $\frac yx >1$ choose $n\in\N$ such that $n< \frac yx < n+1$ and set
\be
S_1 = \{(x',nx'):x'\in \R\},\quad\quad S_2 = \{(x',(n+1)x'):x'\in \R\}.
\ee

As in (iii), $S_1$ and $S_2$ are closed, so $S_1\cup S_2$ is closed and thus $\R^2\bs (S_1\cup S_2)$ is open, and as this set contains $(x,y)$ it contains an open about $(x,y)$, which then lies outside $S$.


\item [(vi)] Not open. as in (i). 

Closed. If $(x,y)\not\in S$ then $f(x) \neq y$. Let $\ve=\frac 12 \abs{f(x)-y}$, then $\exists \delta >0$ such that
\be
\abs{z-x}< \delta \ \ra \ \abs{f(z)-f(x)} < \ve \ \ra \ \abs{y-f(z)} \geq \abs{y-f(x)}-\abs{f(x)-f(z)} > 2\ve - \ve = \ve,
\ee
so the open ball about $(x,y)$ of radius $\min\{\delta, \ve\}$ lies outside $S$.

\centertexdraw{
\drawdim in
\def\bdot {\fcir f:0 r:0.02 }

\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0

\move (-0.2 0) \avec(2.3 0)
\move (0 -0.2) \avec(0 1.8)

\move (2.8 0) \avec(5.3 0)
\move (3 -0.2) \avec(3 1.8)

\lpatt( )
\move (0 0) \lvec (1.8 1.8)
\move (0 0) \lvec (0.9 1.8)

\move (3.2 0.5) \clvec (3.5 0.8)(4 0)(4.5 1.2)
\move (4.5 1.2) \clvec (4.8 2)(4.9 1.5)(5 1.3)

\htext (0.1 1.9){$x=0$}
\htext (0.8 1.9){$y = 2x$}
\htext (1.4 1.9){$y = x$}


\htext (3.2 0.6){$f$}
\htext (1 -0.2){(v)}
\htext (4 -0.2){(vi)}

\lpatt(0.05 0.05)

\move (1.5 1.2) \lcir r:0.2
\move (1.5 1.2) \bdot

\move (4.5 1.2) \lcir r:0.2
\move (4.5 1.2) \bdot
\move (0 2.2)
}

\een

\begin{exercise}
Is the set $\{f : f(1/2) = 0\}$ closed in the space $C[0, 1]$ with the uniform norm? What about the set $\{f : \int^1_0 f(x)dx = 0\}$? In each case, does the answer change if we replace the uniform norm with the norm $\dabs{\cdot}_1$ defined in Question \ref{ques:norm_one}?
\end{exercise}

Solution. Let $S=\{f : f(1/2) = 0\}$. Given $f\in C[0,1]\bs S$, set $r = \frac 12 \abs{f(\tfrac 12)} \ (\neq 0)$. Given $g\in C[0,1]$, if we have
\be
\dabs{g-f}_\infty < r \ \ra \ \sup_{x\in [0,1]}\abs{g(x)-f(x)} < r \ \ra \ g(\tfrac 12) - f(\tfrac 12) < r.
\ee

Thus,
\be
\abs{g(\tfrac 12)}  \geq \abs{f(\tfrac 12)} - \abs{g(\tfrac 12) - f(\tfrac 12)} > 2r -r = r \ \ra \ g \notin S.
\ee
So $C[0,1]\bs S$ is open and thus $S$ is closed in $C[0,1]$ with the uniform norm.

Now define $f\in C[0,1]\bs S$ by $f(x)=1$ and $g_1, g_2, \dots \in C[0,1]$ by
\be
g_n(x) = \left\{\ba{ll}
n\abs{x-\frac 12} \quad\quad & \abs{x-\frac 12}\leq \frac 1n\\
1 & \text{otherwise}
\ea\right. \ \ra \ g_n \in S.
\ee
and $\dabs{h}_1 = \int^1_0 \abs{h(x)}dx$
\be
\dabs{g_n - f}_1 = \int^1_0 \abs{g_n - f} = \frac 2n - n\int^{\frac 12 + \frac 1n}_{\frac 12 - \frac 1n}\abs{x-\frac 12}dx  = \frac 2n - 2n\int^{\frac 1n}_0 x dx = \frac 1n.
\ee

So $S$ meets every open ball about $f$, and hence is not closed on $C[0,1]$ with norm $\dabs{\cdot}_1$.

Now consider $S'= \{f : \int^1_0 f(x)dx = 0\}$. Given $f\in C[0,1]\bs S'$, set $r = \frac 12 \abs{\int^1_0 f} \ (\neq 0)$. Given $g\in C[0,1]$, if we have
\be
\dabs{g-f}_\infty < r \ \ra \ \sup_{x\in [0,1]}\abs{g(x)-f(x)} < r \ \ra \ \int^1_0 \abs{g-f} < r.
\ee
Thus,
\be
\abs{\int^1_0 g} \geq \abs{\int^1_0 f} - \abs{\int^1_0 (g-f)} \geq 2r -\int^1_0  \abs{g-f} > 2r -r = r \ \ra \ g \notin S'.
\ee
So $C[0,1]\bs S'$ is open and thus $S'$ is closed in $C[0,1]$ with the uniform norm.

Similarly, if $\dabs{g-f}_1 < r$ then $\dabs{h}_1 = \int^1_0 \abs{h(x)}dx$
\be
\int^1_0 \abs{g-f} < r \ \ra \ \abs{\int^1_0 g} \geq r \ \ra \ g\notin S'.
\ee
So $C[0,1]\bs S'$ is open and thus $S'$ is closed in $C[0,1]$ with the norm $\dabs{\cdot}_1$.

\begin{exercise}
Which of the following functions $f$ are continuous?
\ben
\item [(i)] The linear map $f :\ \ell_\infty \to \R$ defined by $f(x) = \sum^\infty_{n=1} x_n/n^2$.
\item [(ii)] The identity map from the space $C[0, 1]$ with the uniform norm to the space $C[0, 1]$ with the norm $\dabs{\cdot}_1$ defined in Question \ref{ques:norm_one}.
\item [(iii)] The identity map from $C[0, 1]$ with the norm $\dabs{\cdot}_1$ to $C[0, 1]$ with the uniform norm.
\item [(iv)] The linear map $f :\ \ell_0 \to \R$ defined by $f(x) = \sum^\infty_{i=1} x_i$.
\een
\end{exercise}

Solution. \ben
\item [(i)] Continuous. Given $x\in \ell_\infty$ and $\ve>0$, set $\delta = \frac {6\ve}{\pi^2}$, then 
\be
\dabs{y-x}_\infty < \delta \ \ra \ \forall n \in \N, \ \abs{y_n - x_n} < \frac {6\ve}{\pi^2}.
\ee

Thus,
\be
\abs{f(y)-f(x)} = \abs{\sum^\infty_{n=1} \frac{y_n-x_n}{n^2}} \leq \abs{y_n - x_n} \sum^\infty_{n=1} \frac 1{n^2} < \frac {6\ve}{\pi^2}\sum^\infty_{n=1} \frac 1{n^2} = \ve.
\ee

\item [(ii)] Continuous. Given $f\in C[0,1]$ and $\ve>0$, set $\delta = \ve$, then 
\be
\dabs{g-f}_\infty < \delta \ \ra \ \sup\abs{g(x)-f(x)} < \delta \ \ra \ \forall x \in [0,1], \ \abs{g(x)-f(x)} < \delta = \ve
\ee
Then
\be
\dabs{g-f}_1 = \int^1_0 \abs{g-f} < \int^1_0 \ve dx = \ve.
\ee

\item [(iii)] Not continuous. Take $f\in C[0,1]$ defined by $f(x)=1$ and set $\ve = \frac 12$. So $\forall \delta >0$ choose $n\in\N$ with $n> \frac 1{\delta}$, and define $g\in C[0,1]$ by 
\be
g(x) = \left\{\ba{ll}
n\abs{x-\frac 12} \quad\quad & \abs{x-\frac 12}\leq \frac 1n\\
1 & \text{otherwise}
\ea\right.
\ee
Then 
\be
\dabs{g-f}_1 = \int^1_0 \abs{g-f} = \frac 1n < \delta \quad (\text{by previous result})
\ee
but 
\be
\dabs{g-f}_\infty = \sup\abs{g(x)-f(x)} = 1 > \ve.
\ee

\item [(iv)] Not continuous. Take $x=0\in \ell_0$ and set $\ve= \frac 12$. Given $\delta>0$ choose $N>\frac 1{\delta}$ and take $y=(y_n)$ where 
\be
y_n = \left\{\ba{ll}
\frac 1N \quad\quad & n\leq N\\
0 & n>N
\ea\right. \ \ra \ \dabs{y-x} = \frac 1N< \delta, \quad \quad \text{but } \abs{f(y)-f(x)} = 1 > \ve.
\ee

\een

\begin{exercise}
If $A$ and $B$ are subsets of $\R^n$, we write $A + B$ for the set $\{a + b :\ a \in A,\ b \in B\}$. Show that if $A$ and $B$ are both closed and one of them is bounded then $A + B$ is closed. Give an example in $\R^1$ to show that the boundedness condition cannot be omitted. If $A$ and $B$ are both open, is $A + B$ necessarily open? Justify your answer.
\end{exercise}

Solution. If say $A$ is bounded, suppose $c\in \R^n$ is such that for all $\ve>0$, there exists $a\in A$, $b\in B$ with 
\be
\dabs{c-(a+b)} < \ve.
\ee

Taking $\ve = 1,\frac 12,\frac 13,\dots$, we obtain sequences $(a_m)$ in $A$ and $(b_m)$ in $B$ with $a_m+b_m\to c$. As the sequence $(a_m)$ is bounded, it has a convergent subsequence $(a_{m_i})$ (by Bolzano-Weierstrass Theorem) with limit $a$, and as $A$ is closed we have $a\in A$. Then
\be
\lim (a_{m_i} + b_{m_i}) = c \ \ra \ \lim b_{m_i} = c - \lim a_{m_i} = c-a, 
\ee
and as $B$ is closed we have $c-a \in B$, thus $c=a+(c-a) \in A+B$ and so $A+B$ is closed.

Let $A=\Z$ and $B=\{n+\frac 1n:n\geq 2,n\in \N\}$. Both sets are clearly closed. We have
\be
A+B = \{m+\tfrac 1n,\ m\in \Z,\ n\geq 2,n\in\N\}
\ee
So $0\notin A+B$ but $\forall n\geq 2$, $\frac 1n\in A+B$, so any open ball about 0 meets $A+B$, thus $A+B$ is not closed.

If $A,B\subset \R^n$ are open, $\forall c\in A+B$, then $\exists a\in A$, $b\in B$ with $c=a+b$. As $A$ is open, $\exists r>0$ s.t.
\be
\dabs{a'-a} < r \ra \ a' \in A, 
\ee
So 
\be
\dabs{c'-c} < r \ \ra \ \dabs{(c'-b)-a}<r \ \ra \ c'-b \in A \ \ra \ c'-b = a', a'\in A \ \ra \ c' = a'+b \in A+B.
\ee
Thus, $A+B$ is open.

\begin{exercise}
\ben
\item [(a)] Show that the space $\ell_\infty$ is complete. Show also that $c_0 = \{x\in \ell_\infty:\ x_n \to 0\}$, the vector subspace of $\ell_\infty$ consisting of all sequences converging to 0, is complete.
\item [(b)] Is the space $R[0, 1]$ of integrable functions on $[0, 1]$, equipped with the uniform norm, complete?
\een
\end{exercise}

Solution. \ben
\item [(a)] Take a Cauchy sequence $x^{(1)}, x^{(2)}, \dots, $ in $\ell_\infty$. Given $\ve>0$, $\exists N\in\N$ such that $n,m>N$,
\be
\dabs{x^{(n)}- x^{(m)}}_\infty < \ve \ \ra \ \forall t \in \N, \quad \abs{x^{(n)}_t - x^{(m)}_t} < \ve.
\ee
Thus, $\forall t\in \N$ the sequence $x^{(1)}_t, x^{(2)}_t,\dots$ is Cauchy and hence converges to some $x_t \in \R$. Set $x=(x_1, x_2,\dots)$. $\exists N\in \N$ such that $n>N$,
\be
\dabs{x^{(n)}- x^{(N)}} < 1, \quad \text{i.e., }\forall t\in\N,\quad \abs{x^{(n)}_t - x^{(N)}_t} < 1 \ \ra \ \abs{x_t-x^{(N)}_t} < 1 \ \ra\ \abs{x_t} < \abs{x^{(N)}_t}  + 1.
\ee

Thus, as $x^{(N)}$ is a bounded sequence, so is $x$, and $x\in \ell_\infty$. As above, $\forall \ve >0$, $\exists N\in \N$ such that $n,m>N$,
\be
\forall t\in \N,\quad \abs{x_t^{(n)}-x_t^{(m)}} < \ve,
\ee
taking the limit as $m\to\infty$ gives $\forall t\in \N$, 
\be
\abs{x_t^{(n)}-x_t} < \ve \ \ra \ \dabs{x^{(n)}-x}_\infty < \ve \ \ra\ x^{(n)}\to x. \ \ra \ \ell_\infty \text{ is complete.}
\ee

Now suppose $x^{(1)}, x^{(2)},\dots$ is a Cauchy sequence in $c_0$, then $x^{(n)}\to x$ for some $x\in \ell_\infty$. Given $\ve>0$, $\exists N\in\N$ such that $n,m>N$
\be
\forall t\in \N,\quad \abs{x^{(n)}_t -x^{(m)}_t} < \frac {\ve}2,
\ee
taking the limit as $m\to\infty$ gives 
\be
\forall t\in \N,\quad \abs{x^{(n)}_t -x_t} < \frac {\ve}2.
\ee
Then $\exists T\in\N$ such that $\forall t>T$,
\be
\abs{x^{(n)}_t} < \frac{\ve}2 \quad\quad (\text{by }x_n\to 0)
\ee
Thus,
\be
\abs{x_t} \leq \abs{x_t^{(n)}-x_t} + \abs{x_t^{(n)}} < \frac {\ve}2 + \frac {\ve}2 = \ve \ \ra \ x\in c_0 \ \ra \ c_0 \text{ is complete}.
\ee

\item [(b)] Take a Cauchy sequence $f_1,f_2,\dots$ in $R[0,1]$. Given $\ve >0$, $\exists N\in\N$ such that $n,m>N$
\be
\dabs{f_n - f_m}_\infty < \ve \ \ra \ \sup_{x\in [0,1]}\abs{f_n(x)-f_m(x)} < \ve.
\ee
Then $\forall x \in [0,1]$, the sequence $f_1(x),f_2(x),\dots$ is Cauchy and hence converges in $\R$. Define $f:[0,1]\to \R$ by $f(x)=\lim_{n\to\infty}f_n(x)$. Thus, given $\ve>0$, $\exists N\in\N$ such that 
\be
n>N,\ \forall x \in [0,1], \quad \abs{f_n(x)-f(x)} < \frac {\ve}3.
\ee
Choose $n>N$, then exists a dissection $\sD$ of $[0,1]$ such that 
\be
S_{\sD}(f_n) - s_{\sD}(f_n) < \frac {\ve}3.
\ee

On any interval $[a,b]\subseteq [0,1]$ we have
\be
\sup_{[a,b]}f \leq \sup_{[a,b]}f_n + \frac {\ve}3,\quad\quad \inf_{[a,b]}f \geq \inf_{[a,b]}f_n - \frac {\ve}3,
\ee
then
\be
S_{\sD}(f) \leq S_{\sD}(f_n) + \frac {\ve}3, \quad s_{\sD}(f) \geq s_{\sD}(f_n) - \frac {\ve}3 \ \ra \ S_{\sD}(f) - s_{\sD}(f) < \frac {\ve}3 + \frac {\ve}3 + \frac {\ve}3 = \ve \ \ra \ f\in R[0,1].
\ee

As above, $\forall \ve>0$, $\exists N\in\N$ such that $n,m > N$
\be
\forall x\in [0,1],\quad \abs{f_n(x)-f_m(x)} < \ve.
\ee
Taking the limit as $m\to \infty$ gives $\forall x\in [0,1]$, 
\be
\abs{f_n(x) -
 f(x)} < \ve \ \ra \ \dabs{f_n - f}_{\infty} < \ve \ \ra \ f_n \to f\ \ra \ R[0,1]\text{ is complete.}
\ee
\een

\begin{exercise}
Let $\alpha:\R^n \to \R^m$ be a linear map. Show that $\dabs{x}' = \dabs{x}+\dabs{\alpha x}$ defines a norm on $\R^n$. Using the fact that all norms on a finite-dimensional space are Lipschitz equivalent, deduce that $\alpha$ is continuous.
\end{exercise}

Solution. For $\dabs{x}' = \dabs{x}+\dabs{\alpha x}$ we check
\ben
\item [(i)] Clearly, $\dabs{\cdot}':\R^n\to\R$ takes non-negative values
\item [(ii)]
\be
\dabs{x}' = 0 \ \ra \ \dabs{x} = \dabs{\lm x} = 0 \ \ra \ x = 0,
\ee
\item [(iii)] For all $\lm \in\R$
\be
\dabs{\lm x}' = \dabs{\lm x} +\dabs{\alpha \lm x} = \abs{\lm} \dabs{x} + \abs{\lm \alpha}\dabs{x} = \abs{\lm}\dabs{x}',
\ee
\item [(iv)] and
\be
\dabs{x+y}' = \dabs{x+y}+\dabs{\alpha (x+y)} \leq \dabs{x} + \dabs{y} + \abs{\alpha} \dabs{x+y} = \dabs{x}' + \dabs{y}'.
\ee
\een
Thus, $\dabs{x}'$ defines a norm on $\R^n$. 

As $\dabs{\cdot}$ and $\dabs{\cdot}'$ are Lipschitz equivalent, $\exists M>0$ such that $\forall x\in\R^n$
\be
\dabs{x}' \leq M\dabs{x} \ \ra \ \dabs{x} + \dabs{\alpha x}\leq M\dabs{x} \ \ra \ \dabs{\alpha x} \leq (M-1)\dabs{x}
\ee

Given $\ve>0$, set $\delta = \frac {\ve}{M-1}$, then
\be
\dabs{y-x} < \delta \ \ra \ \dabs{\alpha(y)-\alpha(x)} = \abs{\alpha(y-x)} \leq (M-1)\dabs{y-x} < \ve,
\ee
thus, $\alpha$ is continuous.

\begin{exercise}
Which of the following vector spaces of functions, considered with the uniform norm, are complete? (Justify your answer.)
\ben
\item [(i)] The space $C_b(\R)$ of bounded continuous functions $f : \R \to \R$.
\item [(ii)] The space $C_0(\R)$ of continuous functions $f : \R \to \R$ such that $f(x) \to 0$ as $\abs{x} \to \infty$.
\item [(iii)] The space $C_c(\R)$ of continuous functions $f : \R \to \R$ such that $f(x) = 0$ for $\abs{x}$ sufficiently large.
\een
\end{exercise}

Solution. In each case, let $S$ be the space and $(f_n)$ be a Cauchy sequence in $S$. We know that the space of continuous function $\R\to\R$ is complete with respect to the uniform norm, so $f_n \to f$ for some continuous function $f$. It thus suffices to determine whether or not we must have $f\in S$.
\ben
\item [(i)] If $S=C_b(\R)$, there exists $N\in\N$ such that $\forall m>n\geq N$,
\be
\dabs{f_n-f_m}_\infty = \sup\{\abs{f_n - f_m}:x\in\R\} <1.
\ee
Since $f_N$ is bounded, there exists $K\in\R$ such that for all $x\in\R$, we have $\abs{f_N(x)}<K$, and then $\forall m>N$ and $x\in\R$, 
\be
\abs{f_m(x)} \leq \abs{f_m(x)-f_N(x)} + \abs{f_N(x)} < K+1 \ \ra \ \abs{f(x)}\leq \abs{f(x)-f_m(x)} + \abs{f_m(x)} 
\ee
we take $\ve = 1$, $\exists N'\in\N$ such that $\forall n > N, x\in\R$, $\abs{f_n(x) -f(x)} < 1$, then take $N'' = \max\{N,N'\}$, then $\forall n>N$
\be
\abs{f(x)}\leq \abs{f(x)-f_n(x)} + \abs{f_n(x)} < 1 + K+1 = K+2 \ \ra \ f \in C_b(\R).
\ee

\item [(ii)] If $S=C_0(\R)$, given $\ve>0$, there exists $N\in\N$ such that $\forall m>n\geq N,\ x\in S$,
\be
\sup\{\abs{f_n(x)-f_m(x)}:x\in\R\} < \frac {\ve}4, \ \ra \ \abs{f_N(x)-f_m(x)} < \frac {\ve}4,
\ee
and then by the definition of $C_0(\R)$ and $f_N\in C_0(\R)$, $\exists M>0$ such that $\abs{x}>M$, 
\be
\abs{f_N(x)} < \frac{\ve}4.
\ee
Take $x\in\R$ with $\abs{x}>M$, then 
\be
\abs{f_m(x)} \leq \abs{f_m(x)-f_N(x)} + \abs{f_N(x)} < \frac {\ve}4 + \frac {\ve}4 = \frac {\ve}2.
\ee
then since $f_m \to f$, $\exists N'\in\N$ such $\forall n \geq N'$, 
\be
\abs{f_m(x)-f(x)} < \frac {\ve}2.
\ee

Thus, $\forall \ve>0$, $\exists M>0$ such that $\abs{x}>M$ and $\exists N'' = \max\{N,N'\}$ such that $\forall n\geq N''$, 
\be
\abs{f(x)} \leq \abs{f_m(x)-f(x)} + \abs{f_m(x)} < \frac {\ve}2 + \frac {\ve}2 = \ve \ \ra \ f\in C_0(\R).
\ee

\item [(iii)] If $S=C_c(\R)$, define $f_n$ by 
\be
f_n(x)= \left\{\ba{ll}
1 & \abs{x}\leq 1 \\
\frac 1{\abs{x}} & 1<\abs{x}\leq n\\
\frac 1n\bb{n+1-\abs{x}} \quad\quad & n< \abs{x} \leq n+1\\
0 & n+1 < \abs{x}
\ea\right.
\ee

It is obvious that $f_n \in S$. Given $\ve>0$ choose $N > \frac 1{\ve}$, then if $m>n\geq N$ the function $f_m$ and $f_n$ agree on $[-n,n]$, while $\abs{x}>n$, we have
\be
\abs{f_m(x)-f_n(x)} \leq \frac 1n < \ve \ \ra \ (f_n) \text{ is a Cauchy sequence in }S.
\ee

Also, we know that $f_n \to f$ with 
\be
f(x) = \left\{\ba{ll}
1 & \abs{x}\leq 1 \\
\frac 1{\abs{x}} \quad\quad & \abs{x}>1
\ea\right. \quad \ra \ f\notin S \ \ra \ C_c(\R) \text{ is not complete.}
\ee

\een

\begin{exercise}
In lectures we proved that if $E$ is a closed and bounded set in $\R^n$, then any continuous function defined on $E$ has bounded image. Prove the converse: if every continuous real-valued function on $E \subseteq \R^n$ is bounded, then $E$ is closed and bounded.
\end{exercise}

Solution. Take $a\in E$ and define $f:E\to\R$ by $f(x) = \dabs{x-a}$, then $f$ is continuous and therefore bounded (by the assumption), there exists $r>0$ such that for all $x\in E$ we have $\abs{f(x)} \leq r$. So $E$ lies in the ball of radius $r$ about $a$, thus $E$ is bounded.

Given $b\in \R^n\bs E$ define $f:E\to \R$ by $f(x) = \frac 1{\dabs{x-b}},\ x\in E$, then $f$ is continuous therefore bounded (by the assumption), there exists $\ve>0$ such for all $x\in E$ we have 
\be
\abs{f(x)} < \frac 1{\ve} \ \ra \ \dabs{x-b} > \ve.
\ee
So the open ball of radius $\ve$ about $b$ does not meet $E$, thus $E$ is closed.

\begin{exercise}
Let $(x^{(m)})_{m\geq 1}$ be a bounded sequence in $\ell_\infty$. Show that there is a subsequence $(x^{(m_j)})_{j\geq 1}$ which converges in every coordinate; that is to say, the sequence $(x^{(m_j)}_i)_{j\geq 1}$ of real numbers converges for each $i$. Why does this not show that every bounded sequence in $\ell_\infty$ has a convergent subsequence?
\end{exercise}

Solution. Choose a subsequence $(x^{(m_i)})$ of $x^{(m)}$ as follow. The value $x_1^{(m)}$ form a bounded sequence, so they have a convergent subsequence $x_1^{(l_{1,m})}$. 
\begin{center}
\begin{tabular}{c|ccc}
 & \ bounded \  & \ bounded \  &  \\ 
 & \ sequence \  & \ sequence \  & $\quad$ \\ \hline
$x^{(1)}$ & $x_1^{(1)}$ & $x_2^{(1)}$ & $\dots$  \\ 
$x^{(2)}$ & $x_1^{(2)}$ & $x_2^{(2)}$ & $\dots$  \\ 
$\vdots$ & & &\\
$x^{(m)}$ & $x_1^{(m)}$ & $x_2^{(m)}$ & $\dots$  
\end{tabular} $\ \ra \ $
\begin{tabular}{c|ccc}
 & \ convergent \  & \ bounded \  &  \\ 
 & \ sequence \  & \ sequence \  & $\quad$ \\ \hline
$x^{(l_{1,1})}$ & $x_1^{(l_{1,1})}$ & $x_2^{(l_{1,1})}$ & $\dots$  \\ 
$x^{(l_{1,2})}$ & $x_1^{(l_{1,2})}$ & $x_2^{(l_{1,2})}$ & $\dots$  \\ 
$\vdots$ & & & \\
$x^{(l_{1,m})}$ & $x_1^{(l_{1,m})}$ & $x_2^{(l_{1,m})}$ & $\dots$  
\end{tabular} $\  \ra \ $
\begin{tabular}{c|ccc}
 & \ convergent \  & \ convergent \  &  \\ 
 & \ sequence \  & \ sequence \  & $\quad$ \\ \hline
$x^{(l_{2,1})}$ & $x_1^{(l_{2,1})}$ & $x_2^{(l_{2,1})}$ & $\dots$  \\ 
$x^{(l_{2,2})}$ & $x_1^{(l_{2,2})}$ & $x_2^{(l_{2,2})}$ & $\dots$  \\ 
$\vdots$ & & & \\
$x^{(l_{2,m})}$ & $x_1^{(l_{2,m})}$ & $x_2^{(l_{2,m})}$ & $\dots$  
\end{tabular}
\end{center}
Then for the bounded sequence $x_2^{(l_{1,m})}$, we have a convergent subsequence $x_2^{(l_{2,m})}$. Repeating these steps we can find a convergent subsequence $x_i^{(l_{i,m})}$ converges in the 1st, 2nd, $\dots,\ i$th co-ordinates. Thus, there is subsequence as required. 

This does not show that every bounded sequence in $l_{\infty}$ has a convergent subsequence. We have shown the statement
\be
\forall i \in \N,\ \forall \ve>0, \ \exists N\in\N \text{ such that }\forall m\geq N,\quad \abs{x_i^{(l_{i,m})}-\lim_{m\to\infty}x_i^{(l_{i,m})}}<\ve,
\ee
but this does not imply
\be
\forall \ve >0, \ \exists N\in\N \text{ such that }\forall m\geq N,\ \forall i\in \N,\quad \abs{x_i^{(l_{i,m})}-\lim_{m\to\infty}x_i^{(l_{i,m})}}<\ve.
\ee

For instance, take $\ve= \frac 12$,
\be
\left\{\ba{l}
x^{(m_1)} = \bb{1,1,1,\dots}\\
x^{(m_2)} = \bb{0,\frac 12,\frac 23,\dots}\\
\vdots\\
x^{(m_n)} = \bb{0,0,\dots, \frac 1n, \frac 2{n+1},\dots,}
\ea\right. \ \ra \ x^{(m_n)} = \bb{0,0,\dots, \frac 1n, \frac 2{n+1},\dots,\underbrace{\frac {n+1}{2n}}_{>\frac 12},\dots}.
\ee

\begin{exercise}
Show that $\dabs{x}_1 = \sum^\infty_{i=1} \abs{x_i}$ defines a norm on $\ell_0$ and that this norm is not Lipschitz equivalent to the uniform norm $\dabs{\cdot}$. Find a third norm on $\ell_0$ which is neither Lipschitz equivalent to $\dabs{\cdot}_1$, nor to $\dabs{\cdot}$. Is it possible to find uncountably many norms on $\ell_0$ such that no two are Lipschitz equivalent?
\end{exercise}

Solution. For $\dabs{x}_1 = \sum^\infty_{i=1} \abs{x_i}: \ell_0 \to\R$, we check \ben
\item [(i)] Clearly $\dabs{\cdot}_1:\ell_0 \to \R$ takes non-negative values,
\item [(ii)] $\dabs{x}_1 = 0 \ \ra\ \sum^\infty_{i=1} \abs{x_i} = 0 \ \ra \ \forall i,\ x_i = 0 \ \ra \ x = 0$,
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm x}_1 = \sum^\infty_{i=1} \abs{\lm x_i} = \abs{\lm} \sum^\infty_{i=1} \abs{x_i} = \abs{\lm}\cdot \dabs{x}_1$,
\item [(iv)] and
\be
\dabs{x+y}_1 = \sum^\infty_{i=1} \abs{(x+y)_i} = \sum^\infty_{i=1} \abs{x_i+y_i} \leq \sum^\infty_{i=1} \abs{x_i}+\abs{y_i} = \sum^\infty_{i=1} \abs{x_i} +\sum^\infty_{i=1} \abs{y_i} = \dabs{x}_1 + \dabs{y}_1.
\ee
\een
so $\dabs{\cdot}_1$ is a norm. 

For $n\in \N$ define $x^{(n)}\in \ell_0$ by 
\be
x_t^{(n)} = \left\{\ba{ll}
1 \quad\quad & t\leq n\\
0 & t>n
\ea\right. \ \ra \ \dabs{x^{(n)}}_\infty = 1,\quad \dabs{x^{(n)}}_1 = n.
\ee
Thus, $\forall M>0$, we may choose $n\in \N$ with $n>M$, and then
\be
\dabs{x^{(n)}}_1 > M \dabs{x^{(n)}}_\infty
\ee
So $\dabs{\cdot}_1$ is not Lipschitz equivalent to the uniform norm $\dabs{\cdot}_\infty$.

Note that if $x\in\ell_0$, then $\exists k\in\N$ such that 
\be
\sum_{i>k} \abs{x_i} < 1 \ \ra\ \forall i>k, \ \abs{x_i} <1 \ \ra \ \forall i>k,\ \abs{x_i}^2 < \abs{x_i} \ \ra \ \sum\abs{x_i} \text{ converges (increasing and bounded).}
\ee

We may therefore define the norm $\dabs{\cdot}_2: \ell_0 \to \R$ by
\be
\dabs{x}_2 = \bb{\sum^\infty_{i=1} x_i^2}^{\frac 12}.
\ee
We check \ben
\item [(i)] Clearly $\dabs{\cdot}_2:\ell_0 \to \R$ takes non-negative values,
\item [(ii)] $\dabs{x}_2 = 0 \ \ra\ \sum^\infty_{i=1} x_i^2 = 0 \ \ra \ \forall i,\ x_i = 0 \ \ra \ x = 0$,
\item [(iii)] For all $\lm \in\R$, 
\be
\dabs{\lm x}_2 = \bb{\sum^\infty_{i=1} (\lm x_i)^2}^{\frac 12} = \abs{\lm} \bb{\sum^\infty_{i=1} x_i^2}^{\frac 12} = \abs{\lm}\cdot \dabs{x}_2,
\ee
\item [(iv)] and
\beast
\dabs{x+y}_2 & = & \bb{\sum^\infty_{i=1} ((x+y)_i)^2}^{\frac 12} = \bb{\sum^\infty_{i=1} (x_i+y_i)^2}^{\frac 12} = \bb{\sum^\infty_{i=1} x_i^2 +y_i^2 + 2x_iy_i}^{\frac 12}\\
& \leq & \bb{\sum^\infty_{i=1} x_i^2 +y_i^2 + 2\sqrt{\sum^\infty_{i=1}x_i^2 \sum^\infty_{i=1}y_i^2}}^{\frac 12} \quad\quad (\text{by Cauchy-Schwarz inequality}) \\
& = & \bb{\bb{\sqrt{\sum^\infty_{i=1}x_i^2} + \sqrt{\sum^\infty_{i=1}y_i^2} }^2}^{\frac 12} = \sqrt{\sum^\infty_{i=1}x_i^2} + \sqrt{\sum^\infty_{i=1}y_i^2} = \dabs{x}_2 + \dabs{y}_2.
\eeast
\een
so $\dabs{\cdot}_2$ is a norm. Given $M>0$ we may choose $n\in\N$ with $n>M^2$, since $\dabs{x^{(n)}}_2 = \sqrt{n}$ we have 
\be
\dabs{x^{(n)}}_1 > M\dabs{x^{(n)}}_2,\quad\quad \dabs{x^{(n)}}_2 > M\dabs{x^{(n)}}_\infty
\ee
so $\dabs{\cdot}_2$ is Lipschitz equivalent to neither $\dabs{\cdot}_1$ nor $\dabs{\cdot}_\infty$.

In fact, for all $p\geq 1$ we observe as above that $\abs{x_i}^p$ converges (since $\abs{x_i}<1$), and hence we may define the map $\dabs{\cdot}_p:\ell_0\to\R$ by
\be
\dabs{x}_p = \bb{\sum^\infty_{i=1}\abs{x_i}^p}^{\frac 1p}.
\ee
We claim that $\dabs{\cdot}_p$ is a norm. Assuming this, whenever $p>q\geq 1$, Given $M>0$, we may choose $n\in\N$ with $n> M^{pq/(p-q)}$, then we have
\be
\dabs{x^{(n)}}_p = n^{\frac 1p}, \quad \dabs{x^{(n)}}_q = n^{\frac 1q} \ \ra \ \dabs{x^{(n)}}_q/\dabs{x^{(n)}}_p = n^{\frac 1q - \frac 1p} = n^{\frac {p-q}{pq}} > M \ \ra \ \dabs{x^{(n)}}_q > M \dabs{x^{(n)}}_p.
\ee
Thus $\dabs{\cdot}_p$ is not Lipschitz equivalent to $\dabs{\cdot}_q$. Thus the uncountably many norms $\dabs{\cdot}_p$ for $p\geq 1$ have the property that no two are Lipschitz equivalent.

It remains to check that $\dabs{\cdot}_p $ is a norm. The only property which is not obvious is the triangle inequality. Take $p$ and write $q=\frac p{p-1}$ so that $\frac 1p + \frac 1q = 1$. We first show the Young inequality:

Since the function $\log$ is concave, for all $a,b>0$ we have
\be
\log ab = \tfrac 1p \log a^p + \tfrac 1q \log b^q \leq \log \bb{\tfrac 1p a^p + \tfrac 1q b^q} \ \ra \ ab \leq \tfrac 1p a^p + \tfrac 1q b^q.
\ee

Next we use this to prove H\"older's inequality: given $x,y\in \ell_0$ we have
\be
\frac{\abs{\sum^\infty_{k=1} x_ky_k}}{\dabs{x}_p\dabs{y}_q} \leq \frac{\sum^\infty_{k=1} \abs{x_k}\cdot\abs{y_k}}{\dabs{x}_p\dabs{y}_q} \leq \sum^\infty_{k=1} \frac{\abs{x_k}}{\dabs{x}_p} \cdot \frac{\abs{y_k}}{\dabs{y}_q} \leq \frac 1p \sum^\infty_{k=1} \frac{\abs{x_k}^p}{\dabs{x}_p^p} + \frac 1q\frac{\abs{y_k}^q}{\dabs{y}_q^q} = \frac 1p + \frac 1q = 1.
\ee
Thus,
\be
\abs{\sum^\infty_{k=1}x_ky_k}\leq \dabs{x}_p\dabs{y}_q = \bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}\bb{\sum^\infty_{k=1}\abs{y_k}^q}^{\frac 1q}
\ee

Finally, given $x,y\in \ell_0$, for all $k\in\N$ we have
\be
\abs{x_k+y_k}^p = \abs{x_k + y_k} \cdot \abs{x_k + y_k}^{p-1} \leq \abs{x_k}\cdot \abs{x_k + y_k}^{p-1} + \abs{y_k}\cdot \abs{x_k + y_k}^{p-1}.
\ee

By H\"older's inequality, we have
\be
\sum^\infty_{k=1}\abs{x_k}\cdot \abs{x_k + y_k}^{p-1} \leq \bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p} \bb{\sum^\infty_{k=1}\abs{x_k+y_k}^{(p-1)q}}^{\frac 1q},
\ee
\be
\sum^\infty_{k=1}\abs{y_k}\cdot \abs{x_k + y_k}^{p-1} \leq \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p} \bb{\sum^\infty_{k=1}\abs{x_k+y_k}^{(p-1)q}}^{\frac 1q}.
\ee
Thus,
\beast
\sum^\infty_{k=1}\abs{x_k + y_k}^{p} & \leq & \bb{\bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}+ \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p}} \bb{\sum^\infty_{k=1}\abs{x_k+y_k}^p}^{\frac 1q}\\ 
\ra \ \bb{\sum^\infty_{k=1}\abs{x_k + y_k}^{p}}^{1-\frac 1q} & \leq & \bb{\bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}+ \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p}} \\
\ra \ \bb{\sum^\infty_{k=1}\abs{x_k + y_k}^{p}}^{\frac 1p} & \leq & \bb{\bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}+ \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p}} \\
\ra \ \dabs{x+y}_p & \leq & \dabs{x}_p + \dabs{y}_p
\eeast
as require.

\begin{exercise}
Let $V$ be a normed space in which every bounded sequence has a convergent subsequence.
\ben
\item [(a)] Show that $V$ must be complete.
\item [(b)] Show further that $V$ must be finite-dimensional.
\een
[Hint for (b): Show first that for every finite-dimensional subspace $V_0$ of $V$ there exists an $x \in V$ with $\dabs{x + y} > \dabs{x}/2$ for each $y \in V_0$.]
\end{exercise}

Solution. \ben
\item [(a)] Let $x_1,x_2,\dots$ be a Cauchy sequence in $V$. Take $\ve=1$, then $\exists N\in\N$ such that 

\be
\forall n,m>N, \quad \dabs{x_n-x_m} <1 \ \ra\ \forall n >N,\quad \dabs{x_n} \leq \dabs{x_n-x_{N+1}} + \dabs{x_{N+1}} < \dabs{x_{N+1}} + 1.
\ee
Thus, the sequence $(x_n)$ is bounded by $\max\{\dabs{x_1},\dots,\dabs{x_N},\dabs{x_{N+1}}+1\}$, so it has a convergent subsequenct $(x_{n_i})$ with limit $x\in V$. 

Now given $\ve>0$, $\exists M\in \N$ such that 
\be
\forall n,m>M,\quad \dabs{x_n-x_m}< \frac {\ve}2.
\ee
As $x_{n_i}\to x$, we may choose $n_i > M$ such that 
\be
\dabs{x_{n_i}-x} < \frac {\ve}2.
\ee
Then $\forall m>M$,
\be
\dabs{x_m-x} \leq \dabs{x_m - x_{n_i}} + \dabs{x_{n_i}-x} < \frac {\ve}2 + \frac{\ve}2 = \ve \ \ra \ x_m \to x \ \ra\ V \text{ is complete.}
\ee

\item [(b)] Let $V$ be an infinite-dimensional normed space. Suppose we have chosen $x_1,\dots,x_k\in V$ with $\dabs{x_i} = 1$ for all $i$ and $x_i$ has the bases
\be
\bb{1,0,0,\dots},\quad \bb{0,1,0,\dots}, \quad \dots, \quad (0,0,\dots,0,\underbrace{1}_{i\text{th}},0,\dots).
\ee
Let $S$ be the subspace spanned by $x_1,\dots,x_k$, and choose $y\in V\bs S$. Let
\be
r=\inf\{\dabs{y-s}:s\in S\},
\ee
then as $S$ is closed subspace (any finite dimensional subspace of a normed vector space is closed), we have $r>0$. Choose $s\in S$ with $r\leq \dabs{y-s}< 2r$ and set 
\be
x_{k+1} = \frac {y-s}{\dabs{y-s}} \ \ra \ \dabs{x_{k+1}} = 1.
\ee

Then, for all $i\leq k$, since $x_i \in S$ we have
\be
s+ \dabs{y-s}x_i \in S \quad \quad (\text{since $S$ is spanned by $x_1,\dots,x_k$}).
\ee
so that 
\be
\dabs{x_{k+1}-x_i} = \dabs{\frac {y-s}{\dabs{y-s}}-x_i} = \frac 1{\dabs{y-s}} \dabs{y- (s+\dabs{y-s}x_i)} \leq \frac 1{\dabs{y-s}}r > \frac 12.
\ee

Thus the sequence $(x_n)$ bounded, but for all $i<j$ we have $\dabs{x_i - x_j} > \frac 12$, so there can be no convergent subsequence. Therefore any normed space in which every bounded sequence has a convergent subsequence must be finite-dimensional.

\een

\begin{exercise}
Recall from the lectures the normed space $\ell_2$. The Hilbert cube is the subset of $\ell_2$ consisting of all the sequences $(x_n)^\infty_{n=1}$ such that for each $n$, $\abs{x_n} \leq 1/n$. Show that the Hilbert cube is closed in $\ell_2$, and that it has the Bolzano-Weierstrass property, that is, any sequence in the Hilbert cube has a convergent subsequence. (So the Hilbert cube is \emph{compact}.)
\end{exercise}

Solution. Let $H$ be the Hilbert cube, and take $(x_n)\in \ell_2\bs H$. Then there exists $k\in\N$ with $\abs{x_k}>\frac 1k$. Let 
\be
\ve = \frac 12 \bb{\abs{x_k}-\frac 1k},
\ee
then if $(y_n)\in \ell_2$ with $\abs{(y_n)-(x_n)}_2 < \ve$, then
\be
\ve^2 > \sum^\infty_{k=1}\abs{y_k -x_k}^2 \geq \abs{y_k - x_k}^2 \ \ra \ \abs{y_k - x_k} < \ve.
\ee
Thus,
\be
\abs{y_k}\geq \abs{x_k} - \abs{y_k - x_k} > \abs{x_k} -\ve = \frac 12 \bb{\abs{x_k}+\frac 1k} > \frac 1k \ \ra \ (y_n) \in \ell_2\bs H.
\ee
Therefore, $\ell_2\bs H$ is open, and hence $H$ is closed in $\ell_2$.

Let $(x_n^{(m)})$ be any sequence in $H$ (bounded). As previous question we may obtain a subsequence $(x_n^{m_j})$ which converges in every co-ordinate. For each $n\in \N$, let 
\be
\lim_{j\to \infty} x_n^{(m_j)} = x_n,
\ee
then as $\abs{x_n^{(m_j)}} \leq \frac 1n$ for all $j$, we have $\abs{x_n}\leq \frac 1n$. Thus $(x_n)\in H$.

Given $\ve>0$, take $N\in\N$ such that 
\be
\sum_{n>N} \frac 1{n^2} < \frac {\ve^2}8.
\ee

For each $n\leq N$ choose $M_n\in \N$ such that
\be
j>M_n \ \ra \ \abs{x_n^{(m_j)}-x_n} < \frac {\ve}{\sqrt{2N}},
\ee
and let $M = \max\{M_1,M_2,\dots, M_N\}$. Then if $j>M$, we have
\beast
\bb{\dabs{(x_n^{(m_j)})-(x_n)}}^2 & = & \sum^N_{n=1}\bb{x_n^{(m_j)}-x_n}^2 + \sum_{n>N}\bb{x_n^{(m_j)}-x_n}^2 \\
& \leq & \sum^N_{n=1}\bb{x_n^{(m_j)}-x_n}^2 + \sum_{n>N}\bb{\abs{x_n^{(m_j)}}+\abs{x_n}}^2 \\
& < & N\cdot \bb{\frac{\ve}{\sqrt{2N}}}^2 + \sum_{n>N} \bb{\frac 2{n}}^2 < \frac {\ve^2}2 + \frac {\ve^2}2 = \ve^2.
\eeast
so
\be
\dabs{(x_n^{(m_j)})-(x_n)} < \ve \ \ra \ (x_n^{(m_j)}) \to (x_n).
\ee
Hence $H$ has the Bolzano-Weierstrass property.

\begin{exercise}\label{ques:norm_derivative} 
Let $\dabs{\cdot}$ denote the usual Euclidean norm on $\R^n$. Show that the map sending $x$ to $\dabs{x}^2$ is differentiable everywhere. What is its derivative? Where is the map sending $x$ to $\dabs{x}$ differentiable and what is its derivative?
\end{exercise}

Solution. Define $f:\R^n\to \R$ by $f(x) = \dabs{x}^2$. Given $x,h\in\R$ we have
\be
f(x+h)-f(x) = \sum^n_{i=1}(x_i + h_i)^2 - \sum^n_{i=1}x_i^2 = \sum^n_{i=1} 2x_ih_i + \sum^n_{i=1}h_i^2.
\ee
Define $\alpha: \R^n\to \R$ by $\alpha(h) = 2x\cdot h$, then $\alpha$ is linear and 
\be
\abs{\frac {f(x+h)-f(x)-\alpha(h)}{\dabs{x+h-x}}} = \frac{\dabs{h}^2}{\dabs{h}} = \dabs{h} \to 0 \quad \text{as }h\to 0.
\ee

Thus, $f$ is differentiable at $x$ and the derivative is $D_xf(h) = 2x\cdot h$.

Now define $g:\R^n\to\R$ by $g(x) = \dabs{x}$. Then $g = j\circ f$ where $j:\R\to\R$ is defines by $j(r) = \sqrt{\abs{r}}$. We know that if $r>0$ then $j$ is differentiable at $r$ with $Dj(r)= \frac 1{2\sqrt{r}}$. Thus if $x\in\R^n\bs\{0,\dots,0\}$, by the chain rule $g$ is differentiable with
\be
D_xg(h) = D_{f(x)}j \cdot \bb{D_xf(h)} = \bb{D_{\dabs{x}^2}j} \cdot \bb{D_xf(h)} = \frac 1{2\dabs{x}} 2x\cdot h = \frac {x\cdot h}{\dabs{x}}
\ee

However, $g$ is not differentiable at $(0,\dots,0)$. If it had derivative $\alpha:\R^n\to \R$ with $\alpha(h) = \sum^n_{i=1} \lm_i \frac{h_i}{\dabs{h}}$, choosing $h= (\delta,0,\dots,0)$ gives
\be
\abs{\frac{g(h)-g(0)- \alpha(h)}{\dabs{h-0}}} = \abs{1-\lm_1 \frac{\delta}{\abs{\delta}}},
\ee
and for this to tend to 0 as $\delta \to 0^+$ we need $\lm_1 = 1$, but then it does not tend to 0 as $\delta \to 0^-$.

\begin{exercise}
At which points of $\R^2$ are the following functions $\R^2 \mapsto \R$ differentiable?
\ben
\item [(i)] $f(x, y) = \left\{\ba{ll} x/y \quad\quad & y \neq 0,\\ 0 & y = 0.\ea\right.$
\item [(ii)] $f(x, y) = \abs{x}\abs{y}$.
\item [(iii)] $f(x, y) = xy \abs{x - y}$.
\item [(iv)] $f(x, y) = \left\{\ba{ll} xy / \sqrt{x^2 + y^2}\quad\quad & (x, y) \neq (0, 0),\\ 0 & (x, y) = (0, 0). \ea\right.$
\item [(v)] $f(x, y) = \left\{\ba{ll} xy \sin(1/x) \quad\quad & x \neq 0, \\ 0 & x = 0.\ea\right.$
\een
\end{exercise}


Solution. \ben
\item [(i)] At $(x,y)$ with $y\neq 0$, $f$ is clearly differentiable, at $(x,0)$ with $x\neq 0$, $f(x,h) = \frac xh \nrightarrow 0$ as $h\to 0$, so $f$ is not even countinuous. At $(0,0)$, $f(h,h)=1 \nrightarrow 0$ as $h\to 0$, so again $f$ is not even continuous.
\item [(ii)] At $(x,y)$ with $x,y\neq 0$, write $\abs{x} = ex$, $\abs{y} = e'y$ with $e,e'\in\{\pm 1\}$, then in a neighourhood of $(x,y)$ we have $f(x,y) = ee'xy$ so $f$ is clearly differentiable. At $(0,y)$ with $y\neq 0$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to\R$ where $\alpha(h,k) = \lm h + \mu k$, then
\be
f(h,y) - f(0,y) = \abs{h}\abs{y}= \lm h + \epsilon(h,0)\abs{h} \ \ra\ \abs{y}-\lm \frac{h}{\abs{h}} = \epsilon(h,0).
\ee
If $h>0$, this forces $\lm = \abs{y}$ while if $h<0$, it forces $\lm = -\abs{y}$, a contradiction. Similarly, at $(x,0)$ with $x\neq 0$, $f$ is not differentiable. At $(0,0)$ it is differentiable with derivative 0, since 
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{f(h,k)}{\dabs{(h,k)}}} = \frac{\abs{h}\abs{k}}{\dabs{(h,k)}} = \frac{\abs{h}\abs{k}}{\sqrt{h^2+k^2}} \leq \frac 12 \bb{\frac{\abs{h}\abs{k}}{\sqrt{h^2}} + \frac{\abs{h}\abs{k}}{\sqrt{k^2}}} = \frac 12\bb{\abs{h}+\abs{k}} \to 0 
\ee
as $(h,k)\to (0,0)$.
\item [(iii)] At $(x,y)$ with $a \neq b$, there is neighbourhood on which $f(x,y) = exy(x-y)$ for some $e\in\{\pm 1\}$, so $f$ is clearly differentiable. At $(x,x)$ with $x\neq 0$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to\R$ where $\alpha (h,k) = \lm h + \mu k$, then
\be
f(x+h,x)-f(x,x) = x(x+h)\abs{x+h-x} - 0 = x(x+h)\abs{h} = \lm h + \epsilon(h,0)\abs{h}.
\ee
So
\be
x^2 + xh - \lm \frac {h}{\abs{h}} = \epsilon(h,0).
\ee
If $h>0$ this forces $\lm = x^2$ while if $h<0$ it forces $\lm = -x^2$, a contradiction. At $(0,0)$ it is differentiable with derivative 0, since
\beast
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} & = & \abs{\frac{hk\abs{h-k}}{\dabs{(h,k)}}} = \frac{\abs{h}\abs{k}\abs{h-k}}{\dabs{(h,k)}} = \frac{\abs{h}\abs{k}\abs{h-k}}{\sqrt{h^2+k^2}} \leq \frac 12 \abs{h-k} \bb{\frac{\abs{h}\abs{k}}{\sqrt{h^2}} + \frac{\abs{h}\abs{k}}{\sqrt{k^2}}} \\
& = & \frac 12\bb{\abs{h}+\abs{k}}\abs{h-k} \to 0 \quad \quad \text{as }(h,k)\to (0,0).
\eeast

\item [(iv)] At $(x,y)\neq (0,0)$, $f$ is clearly differentiable. At $(0,0)$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to \R$ where $\alpha(h,k)=\lm h + \mu k$, then
\be
f(h,0) - f(0,0) = 0 = \lm h + \epsilon(h,0)\abs{h}
\ee
forces $\lm = 0$, and similarly we must have $\mu = 0$, so that $\alpha(h,k)=0$, but
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{hk/\sqrt{h^2+k^2}}{\dabs{(h,k)}}} = \frac{\abs{h}\abs{k}}{h^2+k^2}.
\ee
Without loss of generality, we have $\abs{h}= M\abs{k}$. Thus,
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \frac{\abs{h}\abs{k}}{h^2+k^2} = \frac{M\abs{k}^2}{(1+M^2)k^2} = \frac M{1+M^2} \nrightarrow 0=\alpha(h,k).
\ee
\item [(v)] At $(x,y)$ with $x\neq 0$, $f$ is clearly differentiable. At $(0,y)$ with $y \neq 0$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to\R$ at $(0,y)$, where $\alpha(h,k)= \lm h + \mu k$, then
\be
f(h,y)-f(0,y) = hy\sin \tfrac 1h = \lm h + \epsilon(h,0)\abs{h} \ \ra \ \bb{y\sin \tfrac 1h - \lm}\frac{h}{\abs{h}} = \epsilon(h,0) \ \ra \ \lm = y\sin\tfrac 1h
\ee
which is a contradiction. At $(0,0)$ it is differentiable with derivative 0, since
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{hk\sin \tfrac 1h}{\dabs{(h,k)}}} \leq \frac{\abs{h}\abs{k}}{\dabs{(h,k)}} = \frac{\abs{h}\abs{k}}{\sqrt{h^2+k^2}} \leq \frac 12 \bb{\frac{\abs{h}\abs{k}}{\sqrt{h^2}} + \frac{\abs{h}\abs{k}}{\sqrt{k^2}}} = \frac 12\bb{\abs{h}+\abs{k}} \to 0 .
\ee
\een

\begin{exercise}
Let $f(x, y) = x^2y/(x^2 + y^2)$ for $(x, y) \neq (0, 0)$, and $f(0, 0) = 0$. Show that $f$ is continuous at $(0, 0)$ and that it has directional derivatives in all directions there (i.e. for any fixed $\alpha$, the function $t \mapsto f(t \cos \alpha, t \sin \alpha )$ is differentiable at $t = 0$). Is $f$ differentiable at $(0, 0)$?
\end{exercise}

Solution. Given $\ve>0$, take $\delta = \ve$, then
\be
\dabs{(x,y)-(0,0)} < \delta \ \ra \ \abs{y}<\ve \ \ra \ \abs{f(x,y)-f(0,0)}= \abs{\frac{x^2y}{x^2 + y^2}} \leq \abs{\frac{x^2y}{x^2}} = \abs{y}< \ve.
\ee
so $f$ is continuous at $(0,0)$. For fixed $\alpha$, let $f_\alpha:\R\to\R$ be the function defined by 
\be
f_\alpha(t) = f(t\cos\alpha,t\sin \alpha) = \frac{t^3\cos^2\alpha\sin\alpha}{t^2} = t\cos^2\alpha\sin\alpha
\ee
then $f_\alpha$ is linear and thus differentiable at $t=0$, i.e., $f$ has directional derivatives in all directions at $(0,0)$. Suppose $f$ had derivative $\alpha:\R^2\to \R$ at $(0,0)$ where $\alpha(h,k) = \lm h + \mu k$, then
\be
f(h,0)-f(0,0) = 0 = \lm h + \epsilon(h,0)\abs{h}
\ee
forces $\lm = 0$, and similarly we must have $\mu = 0$, thus $\alpha = 0$, but (ithout loss of generality, we have $\abs{h}= M\abs{k}$)
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{f(h,k)}{\dabs{(h,k)}}} = \abs{\frac{h^2k/(h^2+k^2)}{\dabs{(h,k)}}} = \abs{\frac{M^2}{(1+M^2)^{\frac 32}}} \nrightarrow 0 \quad\text{as }(h,k)\to (0,0).
\ee
So $f$ is not differentiable at $(0,0)$.

\begin{exercise}
We work in $\R^3$ with the usual inner product. Consider the map $f :\R^3 \mapsto \R^3$ given by $f(x) = x/\dabs{x}$ for $x \neq 0$ and $f(0) = 0$. Show that $f$ is differentiable except at 0 and 
\be
D_xf(h) = \frac h{\dabs{x}} - \frac{x(x \cdot h)}{\dabs{x}^3},\quad\quad x \neq 0, \ h \in \R^3
\ee

Verify that $D_xf(h)$ is orthogonal to $x$ and explain geometrically why this is the case.
\end{exercise}


Solution. Since $\dabs{f(x)} = 1$ for all $x\neq 0$ but $f(0)=0$, $f$ is not continuous at 0, so it is certianly not differentiable there. Define $g:\R^3\to \R^3$ by $g(x) = \dabs{x}$, $i:\R^3\to\R^3$ by $i(x)=x$, and $k:\R\bs\{0\}$ by $k(r) = \frac 1r$. Then by question \ref{ques:norm_derivative}, $g$ is differentiable on $\R^3\bs\{0\}$ with $D_xg(h) = \frac {x\cdot h}{\dabs{x}}$, and we know that $i$ and $k$ are differentiable at $x$ with
\be
D_xi(h) = h,\quad \quad D k(r) = -\frac 1{r^2}.
\ee

Since on $\R^3\to \{0\}$ we have $f=i \cdot (k\circ g)$, by the chain and product rules $f$ is differentiable at $x$ with
\beast
D_xf(h) & = & D_xi(h) \cdot (k\circ g)(x) + i(x) \cdot D_{g(x)}k \cdot D_xg(h)\\
& = & h \frac 1{\dabs{x}} + x \bb{-\frac 1{\dabs{x}^2} \frac {x\cdot h}{\dabs{x}}} = \frac h{\dabs{x}} - \frac{x(x \cdot h)}{\dabs{x}^3}.
\eeast

Thus,
\be
x\cdot D_xf(h) = x\cdot \bb{\frac h{\dabs{x}} - \frac{x(x \cdot h)}{\dabs{x}^3}} = \frac {x\cdot h}{\dabs{x}} - \frac{\dabs{x}^2(x \cdot h)}{\dabs{x}^3} =0.
\ee
so $D_xf(h)$ is orthogonal to $x$. The geometrical reason is that for fixed $x$ the value of $f$ is constant in the direction of $x$.

\begin{exercise}
\ben
\item [(i)] Suppose that $f : \R^2 \mapsto \R$ is such that $D_1 f= \partial f/\partial x$ is continuous in some open ball around $(a, b)$, and $D_2 f= \partial f/\partial y$ exists at $(a, b)$. Show that $f$ is differentiable at $(a, b)$.
\item [(ii)] Suppose that $f : \R^2 \mapsto \R$ is such that $\partial f/\partial x$ exists and is bounded near $(a, b)$, and that for a fixed, $f(a, y)$ is continuous as a function of $y$. Show that $f$ is continuous at $(a, b)$.
\een
\end{exercise}

Solution. \ben
\item [(i)] Take $r>0$ such that $\partial f/\partial x$ exists and is continuous at $(a+h,b+k)$ whenever $\dabs{(h,k)}<r$. Given $\ve>0$, $\exists \delta >0$ with $\delta<r$ such that
\be
\dabs{(h',k')} < \delta \ \ra \ \abs{D_1 f(a+h',b+k')-D_1 f(a,b)} < \ve. \quad (\text{by continuity of }\partial f/\partial x)
\ee
We may write 
\be
f(a,b+k) = f(a,b) + kD_2 f(a,b) + \epsilon_1(k)\abs{k}
\ee
where $\epsilon_1(k)\to 0$ as $k\to 0$. Take $(h,k)$ with $\dabs{(h,k)}<r$, then by the MVT $\exists \theta\in(0,1)$ with
\be
f(a+h,b+k) - f(a,b+k) = hD_1 f(a+\theta h,b+k).
\ee

Thus provided $\dabs{(h,k)}< \delta$ we have
\beast
& & \abs{f(a+h,b+k) -f(a,b) - hD_1 f(a,b) - kD_2 f(a,b) } \\
& = & \abs{\bb{f(a+h,b+k) -f(a,b+k) - hD_1 f(a,b)} + \bb{ f(a,b+k) - f(a,b) -kD_2 f(a,b) }}\\
& = & \abs{\bb{hD_1 f(a+\theta h,b+k) - hD_1 f(a,b)} + \bb{ f(a,b+k) - f(a,b) -kD_2 f(a,b) }}\\
& \leq & \abs{hD_1 f(a+\theta h,b+k) - hD_1 f(a,b)} + \abs{ f(a,b+k) - f(a,b) -kD_2 f(a,b)}\\
& < & \ve \abs{h} + \epsilon_1(k)\abs{k}
\eeast
Since we can make $\ve$ arbitrarily small, we have 
\be
\abs{f(a+h,b+k) -f(a,b) - hD_1 f(a,b) - kD_2 f(a,b) }/\dabs{(h,k)} \to 0
\ee
as $(h,k)\to (0,0)$. Thus $f$ is differetiable at $(a,b)$ with
\be
D_{(a,b)} f(h,k) = hD_1f(a,b) + kD_2f(a,b). 
\ee

\item [(ii)] Take $r>0$ such that $D_1f$ exists and is bounded by $M$ at $(a+h,b+k)$ whenever $\dabs{h,k}<r$. By MVT $\theta \in (0,1)$,
\be
f(a+h,b+k) - f(a,b+k) = h D_1 f(a+\theta h,b+k).
\ee

Given $\ve>0$, $\delta >0$ with $\delta<r$ such that $\abs{k}<\delta$
\be
\abs{f(a,b+k)-f(a,b)} < \frac {\ve}2. \quad (\text{by the continuity as a function of }y)
\ee

Then take $\delta < \frac {\ve}{2M}$, then we have $\abs{(h,k)} < \delta  \ra \ \abs{h},\abs{k}<\delta$, 
\beast
\abs{f(a+h,b+k)-f(a,b)} & \leq & \abs{f(a+h,b+k)-f(a,b+k)} + \abs{f(a,b+k)-f(a,b)} \\
& < & \abs{h D_1 f(a+\theta h,b+k)} + \frac {\ve}2\\
& \leq & \abs{h} M + \frac {\ve}2 < \delta M + \frac {\ve}2 < \frac {\ve}2 + \frac {\ve}2 = \ve.
\eeast
Thus $f$ is continuous at $(a,b)$.
\een

\begin{exercise}
Let $M_n = M_n(\R)$ be the space of $n \times n$ real matrices (it can be identified with $\R^{n^2}$). Show that the function $f : M_n \mapsto M_n$ defined by $f(X) = X^2$ is differentiable everywhere in $M_n$. Is it true that $D_Af = 2A$? If not, what is the derivative of $f$?
\end{exercise}

Solution. Given $A,H\in M_n$ we have 
\be
f(A+H)-f(A) = (A+H)^2 -A^2 = AH + HA + H^2. 
\ee
Define $\alpha (H) = AH + HA$, then $\alpha$ is linear and
\be
\dabs{\frac{f(A+H)-f(A)-\alpha(H)}{\dabs{H}}} = \dabs{\frac{H^2}{\dabs{H}}} = \dabs{H}\to 0\quad \text{as }H\to 0.
\ee
Thus, $f$ is differentiable at $A$ and 
\be
D_Af(H)= AH + HA.
\ee
(It would not even make sense to say that $D_A f = 2A$, since $D_af$ is linear map $M_n\to M_n$.)

\begin{exercise}
Let $A : \R^n \to \R^m$ be a linear map. Recall that the operator norm of $A$ is
\beast
\dabs{A}' = \sup \left\{ \dabs{Ax} : x\in \R^n,\ \dabs{x} \leq 1\right\} & = & \sup \left\{ \frac{\dabs{Ax}}{\dabs{x}}: 0 \neq x \in \R^n\right\} = \inf\{k\in\R: k \text{ is a Lipschitz constant for }A\}.
\eeast

Complete the proof that this defines a norm on the vector space $L(\R^n,\R^m)$ of all linear maps $\R^n \mapsto \R^m$.

Now assume $m = n$ and identify $L(\R^n,\R^n)$ with $M_n(\R)$, the space of $n\times n$ real matrices. Show that if the operator norm of $A \in M_n$ satisfies $\dabs{A}' < 1$, then the sequence $B_k = I +A+A^2+\dots + A^{k-1}$ converges (here $I$ is the identity matrix), and deduce that $I - A$ is then invertible. Deduce that the set $GL_n(\R)$ of all invertible $n\times n$ real matrices is an open subset of $M_n(\R)$.
\end{exercise}

Solution. Given $A$, set
\be
S=\left\{ \dabs{Ax} : x\in \R^n,\ \dabs{x} \leq 1\right\}, \quad\quad R = \left\{ \frac{\dabs{Ax}}{\dabs{x}}: 0 \neq x \in \R^n\right\},
\ee
\be
T = \{k\in\R:k \text{ is a Lipschitz constant for }A\}.
\ee

Given $k\in T$, for all $x\in\R^n$ we have $\dabs{Ax} \leq k\dabs{x}$, so 
\be
\dabs{x}\leq 1 \ \ra \ \dabs{Ax}\leq k \ \ra \ \forall s\in S,\ s\leq k \ \ra \ \sup S\leq k
\ee
and 
\be
x\neq 0 \ \ra \ \dabs{A\frac{x}{\dabs{x}}}\leq k \ \ra \ \forall r\in R,\ r\leq k \ \ra \ \sup R\leq k.
\ee
Since this is true for all $k\in T$ and $\sup S\leq \inf T$ and $\sup R \leq \inf T$. 

On the other hand, for all $x\in \R^n\bs\{0\}$ we have $\dabs{\frac{x}{\dabs{x}}}=1$, so that $\dabs{A\frac{x}{\dabs{x}}}\in S$ and hence 
\be
\dabs{A\frac{x}{\dabs{x}}} \leq \sup S \ \ra \ \dabs{Ax} \leq (\sup S)\dabs{x},
\ee
so $\sup S$ is a Lipschitz constant for A, i.e., $\sup S\in T$, and so $\sup S \geq \inf T$. 

Similarly, $\sup R \in T$ and $\sup R\geq \inf T$. 

Thus $\sup S = \sup R = \inf T$ as required.

If we set
Define $\dabs{\cdot}': L(\R^n,\R^m)\to \R$ by $\dabs{A}' = \sup S$.
\ben
\item [(i)] Clearly $\dabs{\cdot}'$ takes non-negative values,
\item [(ii)] \beast
\dabs{A}' = 0 & \ra & \sup\{\dabs{Ax}:\dabs{x}\leq 1\}= 0 \ \ra \ \forall x\in\R^n\bs\{0\}, \ \dabs{A\bb{\frac{x}{\dabs{x}}}}=0\\
& \ra & \forall x\in\R^n, \ \dabs{Ax}=0 \ \ra \ \forall x\in\R^n,\ Ax = 0 \quad(\text{since $\dabs{\cdot}$ is a norm}) \ \ra \ A = 0.
\eeast
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm A}' = \sup\{\dabs{\lm A x}:\dabs{x}\leq 1\} = \abs{\lm} \sup\{\dabs{A x}:\dabs{x}\leq 1\} = \dabs{A}'$,
\item [(iv)] and
\beast
\dabs{A+B}' & = & \sup\{\dabs{(A+B)x}:\dabs{x}\leq 1\} \leq \sup\{\dabs{Ax} + \dabs{Bx}:\dabs{x}\leq 1\}  \\
& \leq & \sup\{\dabs{Ax}:\dabs{x}\leq 1\} + \sup\{\dabs{Bx}:\dabs{x}\leq 1\}  = \dabs{A}' + \dabs{B}'.
\eeast
\een

Now given $A,B\in M_n$, for all $x\in \R^n$, $\dabs{x}\leq 1$ we have
\beast
\dabs{(AB)x} & = & \dabs{A(Bx)} \leq \dabs{A}'\dabs{Bx} \quad\quad (\text{by the set }R) \\
& \leq & \dabs{A}' \dabs{B}' \quad\quad\quad\quad\quad\quad (\text{by the set }S)
\eeast

Thus, we have
\be
\dabs{AB}' \leq \dabs{A}' \dabs{B}'.
\ee

Hence, for all $k\in \N$, we have $\dabs{A^k}' \leq \dabs{A}'^k$. So if $\dabs{A}'<1$, then the sequence
\be
B_k = I + A + A^2 + \dots + A^{k-1} 
\ee
is a Cauchy squence, because given $\ve>0$ we may choose $N\in\N$ with $\dabs{A}'^N < \ve(1-\dabs{A}')$, and then if $m>n\geq N$ we have
\be
\dabs{B_m - B_n}' = \dabs{A^{m-1} + A^{m-1} + \dots + A^n}' \leq \sum^{m-1}_{k=n} \dabs{A^k}'  \leq \sum^{m-1}_{k=n} \dabs{A}'^k < \sum^{\infty}_{k=N} \dabs{A}'^k = \frac{\dabs{A}'^N}{1-\dabs{A}'}< \ve.
\ee

Since $M_n$ is complete it follows that $B_k$ converges and let the limit be $B$. Since 
\be
\dabs{\lim_{k\to \infty} A^k}' = \lim_{k\to\infty} \dabs{A^k}' \leq \lim_{k\to\infty}\dabs{A}'^k = 0,
\ee
we have $\lim_{k\to\infty} A^k = 0$, whence 
\be
(I-A)B = (I-A)\lim_{k\to\infty}B_k = \lim_{k\to\infty} (I-A)B_k = \lim_{k\to\infty} I - A^k = I - \lim_{k\to\infty} A^k = I.
\ee
So $I-A$ is invertible. Thus there is an open ball about $I$ in $GL_n(\R)$. Given $X\in GL_n(\R)$, left multiplication by $X^{-1}$ is a linear and hence continuous map from $M_n$ to itself. 

Thus the preimage of the open ball $\{I-A : \dabs{A}'<1\}$ is an open set, i.e.,, $\{X(I-A):\dabs{A}'<1\}$ is an open set and it contains $X$, and lies in $GL_n(\R)$. Thus each element of $GL_n(\R)$ lies in an open ball in $GL_n(\R)$, so $GL_n(\R)$ is an open subset of $M_n$.

\begin{exercise}
We regard $GL_n(\R)$ as an open subset of $M_n(\R) \simeq \R^{n^2}$ (cf. the previous question). Define $g : GL_n(\R) \to M_n(\R)$ by $g(X) = X^{-1}$ for $X \in GL_n(\R)$. Show that $g$ is differentiable at the identity matrix $I \in GL_n(\R)$, and that its derivative there is the map $D_I g(H) = -H$.

Let $A \in GL_n(\R)$. By writing $(A + H)^{-1} = A^{-1}\bb{I + HA^{-1}}^{-1}$, or otherwise, show that $g$ is differentiable at $X = A$. What is $D_Ag$?

Show further that $g$ is twice differentiable at $I$, and find $D^2_I g$ as a bilinear map $M_n \times M_n \to M_n$.
\end{exercise}

Solution. From the previous question, we know that $\dabs{H}' < 1 \ \ra \ I+H \text{ is invertible}$, then
\be
(I+H)^{-1}-I^{-1} + H = (I+H)^{-1}\bb{I-I-H+H+H^2}= \bb{I+H}^{-1} H^2 = \epsilon(H)\dabs{H}'
\ee
where $\epsilon(H)\to 0$ as $H\to 0$, so
\be
g(I+H)=g(I)-H + \epsilon(H)\dabs{H}'
\ee
and hence $g$ is differentiable at $I$ with derivative given by $D_I g(H) = -H$.

Now given $A\in GL_n(\R)$, take $\delta >0$ such that $\dabs{H}<\delta \ \ra \ A+H\in GL_n(\R)$ (since $GL_n(\R)$ is an open set), then
\beast
(A+H)^{-1} & = & A^{-1}(I+HA^{-1})^{-1} = A^{-1}\bb{I+HA^{-1}}^{-1}\bb{I-(HA^{-1})^2 + (HA^{-1})^2}\\
& = & A^{-1}(I-HA^{-1} + (I+HA^{-1})^{-1}(HA^{-1})^2) \\
& = & A^{-1} - A^{-1}HA^{-1} + A^{-1}\bb{I + HA^{-1}}^{-1} (HA^{-1})^2\\
& = & A^{-1} - A^{-1}HA^{-1} + \bb{A + H}^{-1} (HA^{-1})^2
\eeast

Since 
\be
\frac 1{\dabs{H}'} \bb{A + H}^{-1} (HA^{-1})^2 \to 0 \quad \text{as }H\to 0,
\ee

$g$ is differentiable at A with derivative 
\be
D_Ag(H) = -A^{-1}HA^{-1}.
\ee

For all $H,K \in M_n$ with $\dabs{H}'<1$ (then $g$ is differentiable at $I+H$), we have
\beast
D_{I+H}g(K) - D_I g(K) & = & -(I+H)^{-1}K(I+H)^{-1} + K = (I+H)^{-1}\bb{-K + (I+H)K(I+H)}(I+H)^{-1} \\
& = &  (I+H)^{-1}\bb{ HK + KH + HKH}(I+H)^{-1}
\eeast

Set $\psi(H,K) = HK + KH$, then
\beast
& & \frac1{\dabs{H}'}\bb{D_{I+H}g(K) - D_I g(K)-\psi(H,K)}\\
& = & \frac1{\dabs{H}'}\bb{(I+H)^{-1}\bb{ HK + KH + HKH}(I+H)^{-1} - (HK+KH)}\\
& = & \frac1{\dabs{H}'}(I+H)^{-1}\bb{ HK + KH + HKH - (I+H)(HK+KH)(I+H)}(I+H)^{-1} \\
& = & \frac1{\dabs{H}'}(I+H)^{-1}\bb{ HK + KH + HKH - (HK + H^2K + KH + HKH )(I+H)}(I+H)^{-1} \\
& = & -\frac1{\dabs{H}'}(I+H)^{-1}\bb{ H^2K + HKH + H^2KH + KH^2 + HKH^2}(I+H)^{-1} \to 0 \quad \text{as }H\to 0.
\eeast

Thus $g$ is twice differentiable at $I$ and $D^2_Ig(H,K) = HK + KH$.

\begin{exercise}
\ben
\item [(i)] Define $f : M_n \to M_n$ by $f(X) = X^3$. Find the Taylor series of $f(A + H)$ about $A$.
\item [(ii)] Let again $g : GL_n(\R) \to GL_n(\R)$ be defined by $g(X) = X^{-1}$. Find the Taylor series of $g(I + H)$ about $I$.
\een
\end{exercise}


Solution. \ben
\item [(i)] Given $A,H\in M_n$ we have
\be
f(A+H)-f(A) = (A^2H + AHA + HA^2) + (AH^2 + HAH + H^2A + H^3).
\ee
Define $\alpha:M_n\to M_n$ by
\be
\alpha (H) = A^2H + AHA + HA^2,
\ee
then $\alpha$ is linear and 
\be
\frac 1{\dabs{H}'}\bb{f(A+H)-f(A)-\alpha (H)} \to 0 \quad \text{as }H\to 0.
\ee
Thus, $f$ is differentiable at $A$ and $D_Af = \alpha$.

Now given $A,H,K\in M_n$ we have
\beast
D_{A+H}f(K) - D_Af(K) & = & (A+H)^2K + (A+H)K(A+H) + K(A+H)^2 - \bb{A^2K + AKA + KA^2}\\
 & = & AHK + HAK + H^2K + HKA + AKH + HKH + KAH + KHA + KH^2 \\
 & = & \bb{AHK + HAK + HKA + AKH + KAH + KHA} + \bb{H^2K + KH^2 + HKH}
\eeast

Define $\psi(H,K) = AHK + HAK + HKA + AKH + KAH + KHA$, then $\psi$ is bilinear and 
\be
\frac 1{\dabs{H}'}\bb{D_{A+H}f(K) - D_Af(K)-\psi(H,K)} = \frac 1{\dabs{H}'}\bb{H^2K + KH^2 + HKH} \to 0 \quad \text{as }H\to 0.
\ee
Thus, $f$ is differentiable at $A$ and $D^2_Af = \psi$.

Finally, given $A,H,K,L\in M_n$ we have
\beast
& & D^2_{A+H}f(K,L) - D_Af(K,L) \\
& = & (A+H)KL + K(A+H)L + KL(A+H) + (A+H)LK + L(A+H)K + LK(A+H)\\ 
& & \quad -\bb{AKL + KAL + KLA + ALK + LAK + LKA }\\
 & = & HKL + KHL + KLH + HLK + LHK + LKH .
\eeast

Define $\chi(H,K,L) = HKL + KHL + KLH + HLK + LHK + LKH$, then $\chi$ is trilinear and 
\be
\frac 1{\dabs{H}'}\bb{D^2_{A+H}f(K,L) - D^2_Af(K,L)-\chi(H,K,L)} = 0.
\ee
Thus, $f$ is thrice differentiable at $A$ and $D^3_Af = \chi$. Hence the Taylor series of $(A+H)^3$ about $A$ is
\beast
f(A+H) & = & f(A) + D_Af(H) + \frac 12 D^2_Af(H,H) + \frac 16 D^3_A f(H,H,H)\\
& = & f(A) + A^2H + AHA + HA^2 + AH^2 + HAH + H^2A + H^3.
\eeast

\item [(ii)] For each $k\in\N$ we define a $k$-linear map $\psi_k:M_n\times M_n\times \dots\times M_n \to M_n$ by 
\beast
\psi_k(H_1,H_2,\dots,H_k) & = & (-1)^k \sum_{\sigma\in S_k}A^{-1}H_{\sigma(1)}A^{-1}H_{\sigma(2)}\dots A^{-1}H_{\sigma(k)}A^{-1} \\
& = & (-1)^k \sum_{\sigma\in S_k}A^{-1} \prod^k_{i=1} \bb{H_{\sigma(i)}A^{-1}}
\eeast
where $\sigma(k)$ is a permutation of $S_k$. We claim that $D^k_Ag = \psi_k$ for all $k\in\N$. By previous question, we know that this is true for $k=1$. For convenience we write $B=A^{-1}$. Note that
\beast
(A+H)^{-1} & = & A^{-1}(I+HA^{-1})^{-1} = A^{-1}\bb{I+HA^{-1}}^{-1}\bb{I-(HA^{-1})^2 + (HA^{-1})^2}\\
& = & A^{-1}(I-HA^{-1} + (I+HA^{-1})^{-1}(HA^{-1})^2) \\
& = & A^{-1} - A^{-1}HA^{-1} + A^{-1}\bb{I + HA^{-1}}^{-1} (HA^{-1})^2\\
& = & A^{-1} - A^{-1}HA^{-1} + \bb{A + H}^{-1} (HA^{-1})^2 \\
& = & B - BHB + (A+H)^{-1}(HB)^2.
\eeast

Thus, ignoring terms involving at least two factors $H_{k+1}$ we have
\beast
& & D^k_{A+H_{k+1}}g(H_1,\dots,H_k) - D^k_{A}g(H_1,\dots,H_k) - \psi_{k+1}(H_1,\dots,H_k) \\
& = & (-1)^k\bb{\sum_{\sigma\in S_k}\bb{(A+H_{k+1})^{-1} \prod^k_{i=1} \bb{H_{\sigma(i)}(A+H_{k+1})^{-1}} - B \prod^k_{i=1} \bb{H_{\sigma(i)}B}} + \sum_{\tau\in S_{k+1}}B \prod^{k+1}_{i=1} \bb{H_{\tau(i)}B}}\\
& =& (-1)^k\bb{\sum_{\sigma\in S_k}\bb{(B - BH_{k+1}B)\prod^k_{i=1} \bb{H_{\sigma(i)}(B - BH_{k+1}B)} - B \prod^k_{i=1} \bb{H_{\sigma(i)}B}} + \sum_{\tau\in S_{k+1}}B \prod^{k+1}_{i=1} \bb{H_{\tau(i)}B}}.
\eeast
The final step is made since the remainders are at least two factors $H_{k+1}$. If we multiply out the products in the first sum, each first term is of the form $B \prod^k_{i=1} \bb{H_{\sigma(i)}B}$ and thus is cancelled, each term involving one factor $BH_{k+1}B$ and $k$ factors $B$ cancels with one in the final sum, and all remaining terms involve at least two factors $H_{k+1}$, so
\be
\frac 1{\dabs{H_{k+1}}'} \bb{D^k_{A+H_{k+1}}g(H_1,\dots,H_k) - D^k_{A}g(H_1,\dots,H_k) - \psi_{k+1}(H_1,\dots,H_k)} \to 0 \quad \text{as }H_{k+1} \to 0.
\ee
Thus, $D^{k+1}_A g = \psi_{k+1}$ as required. Thus the Taylor series of $g(I+H)$ about $I$ is
\beast
\sum^\infty_{k=0}\frac 1{k!}D^k_I g(\underbrace{H,\dots,H}_{k \text{ terms}}) & = & \sum^\infty_{k=0}\frac 1{k!}(-1)^k \sum_{\sigma\in S_k}I^{-1} \prod^k_{i=1} \bb{HI^{-1}} = \sum^\infty_{k=0}\frac 1{k!}(-1)^k k! H^k\\
& = & I - H+ H^2 -H^3 + \dots.
\eeast

\een

\begin{exercise}
Show that $\det : M_n \to \R$ is differentiable at the identity matrix $I$ with $(D_I \det)(H) = \tr(H)$. 

Deduce that $\det$ is differentiable at any invertible matrix $A$ with $(D_A \det)(H) = \det(A) \tr(A^{-1}H)$. 

Show further that $\det$ is twice differentiable at $I$ and find $D^2_I \det$ as a bilinear map.
\end{exercise}

Solution. Given $H\in M_n$, each term of $\det(I+H)$ other than 1 is either $h_{ii}$ for some $i$ or a product of two or more entries $h_{ij}$, 
\beast
& & \det\bepm
1+h_{11} & h_{12} & \dots & h_{1n}\\
h_{21} & 1+h_{22} & \dots & h_{2n}\\
\vdots & \vdots & \ddots &\\
h_{n1} & h_{n2} & \dots & 1+ h_{nn} 
\eepm \\
& = & (1+h_{11}) \det\bepm
1+h_{22} & h_{23} & \dots & h_{2n}\\
h_{32} & 1+h_{33} & \dots & h_{3n}\\
\vdots & \vdots & \ddots &\\
h_{n2} & h_{n3} & \dots & 1+ h_{nn} 
\eepm + \underbrace{\sum^n_{i=2} (-1)^{1+i}h_{1i}\det\bepm
h_{21} & \dots\\
\vdots & \dots\\
h_{n1} & \dots
\eepm}_{\text{two or more entries of $h_{ij}$, $(i\neq j)$}}\\
& = & (1+h_{11})(1+h_{22}) \det\bepm
1+h_{33} & h_{34} & \dots & h_{3n}\\
h_{43} & 1+h_{44} & \dots & h_{4n}\\
\vdots & \vdots & \ddots &\\
h_{n3} & h_{n4} & \dots & 1+ h_{nn} 
\eepm + \underbrace{\text{two or more entries of $h_{ij}$}}_{i\neq j}\\
& = & (1+h_{11})(1+h_{22}) \dots (1+h_{nn}) + \underbrace{\text{two or more entries of $h_{ij}$}}_{i\neq j}\\
& = & 1+\underbrace{\bb{h_{11}+h_{22} \dots h_{nn}}}_{\tr H} + \underbrace{\text{two or more entries of $h_{ij}$}}_{i\neq j}
\eeast
thus,
\be
\frac 1{\dabs{H}'}\bb{\det(I+H)-\det I-\tr H} \to 0 \quad\text{as }H\to 0,
\ee
so as $H\to \tr H$ is a linear map we see that $\det$ is differentiable at $I$ with derivative $D_I\det(H) = \tr H$.

Thus, if $A\in GL_n(\R)$ we have
\be
\frac 1{\dabs{H}'} \bb{\det (A+H) -\det A - \det (A) \tr(A^{-1}H)} = \frac 1{\dabs{H}'} \det A \bb{\det (I+A^{-1}H) - \det I - \tr(A^{-1}H)} \to 0 
\ee
as $H\to 0$. So $\det$ is differentiable at $A$ with derivative $D_A \det (H) = \det(A) \tr(A^{-1}H) $.

Now given $H,K\in M_n$ such that $\dabs{H}'<1 \ \ra \ I+H\in GL_n(\R)$ (since $GL_n(\R)$ is open), we have
\beast
D_{I+H}\det(K) - D_I \det(K) & = & \det(I+H)\tr((I+H)^{-1}K) - \tr K \\
& = & \tr(\det(I+H)(I+H)^{-1}K) - \tr K \\
& = & \tr((I+H)^*K) - \tr K \\
& = & \tr\bb{\bb{(I+H)^* - I}K}
\eeast
where $X^*$ is the adjugate matrix of $X$. Write $H=(h_{ij})$, $K = (k_{ij})$ and work modulo terms involving 2 or more $h_{ab}$. Then if $i\neq j$, without loss of generality, we assume that $i<j$, the element corresponding to $C_{ij} = (-1)^{i+j}M$ (where $M$ is determinant of the $(n-1)\times(n-1)$ matrix that results from deleting row $i$ and column $j$ of $H$) is 
\be
(-1)^{i+j} \det\bepm
\underline{1+h_{11}} & h_{12} & \dots & \dots & \dots & \dots & h_{1,j-1} & h_{1,j+1} & \dots & h_{1n}\\
h_{21} & \underline{1+h_{22}} & \dots & \dots & \dots & \dots & h_{2,j-1} & h_{2,j+1} & \dots & h_{2n}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots &\\
h_{i-1,1} & h_{i-1,2} & \dots & \underline{1+ h_{i-1,i-1}} & \dots & \dots & h_{i-1,j-1} & h_{i-1,j+1} & \dots & h_{i-1,n}\\
h_{i+1,1} & h_{i+1,2} & \dots & h_{i+1,i-1}  & \underline{h_{i+1,i}} & 1 + h_{i+1,i+1} &  h_{i+1,j-1} &  h_{i+1,j+1} & \dots & h_{i-1,n}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots &\\
h_{j-1,1} & h_{j-1,2} & \dots & h_{j-1,i-1} &  \dots & \dots & 1+h_{j-1,j-1} & h_{j-1,j+1} & \dots & h_{j-1,n}\\
h_{j,1} & h_{j,2} & \dots & h_{j,i-1} &  h_{ji}  & \dots & \underline{h_{j,j-1}} & h_{j,j+1} & \dots & h_{j,n}\\
h_{j+1,1} & h_{j+1,2} & \dots & h_{j+1,i-1}  & h_{i+1,i} &  \dots & h_{j+1,j-1} &  \underline{1 + h_{j+1,j+1}} & \dots & h_{j-1,n}\\
\vdots & \vdots & \ddots &\ddots &\ddots &\ddots &\ddots &\ddots &\ddots &\\
h_{n1} & h_{n2} & \dots & \dots & \dots &  \dots & \dots & \dots &\dots & \underline{1+ h_{nn}}
\eepm 
\ee
where the underlined terms are the diagonal elements of matrix. Thus, its value is 
\be
(-1)^{i+j}(1+h_{11})\dots (1+ h_{i-1,i-1}) (1 + h_{j+1,j+1})\dots (1+ h_{nn})
\det\bepm
\underline{h_{i+1,i}} & 1 + h_{i+1,i+1} &  \dots & h_{i-1,n}\\
\vdots & \vdots & \vdots & \vdots \\
h_{j-1,i} & \dots & \dots & 1+h_{j-1,j-1} \\
h_{ji}  & \dots & \dots & \underline{h_{j,j-1}}
\eepm 
\ee
which is
\be
(-1)^{i+j} (-1)^{(j-i)+1} h_{ji} = -h_{ji}.
\ee

If $i = j$ we have
\be
C_{ii} = 1 + (h_{11}+h_{nn}) = 1 + \tr H - h_ii
\ee

so $(I+H)^* = C^T$ and
\be
\bb{(I+H)^* - I}_{ij} = \left\{\ba{ll}
\tr H - h_{ii} \quad\quad & i = j\\
-h_{ij} & i\neq j
\ea\right.
\ee

Then we have
\beast
\tr\bb{\bb{(I+H)^* - I}K} & = & \sum_{i,j} \bb{(I+H)^*-I)_{ij}k_{ji}} =  \sum_i \bb{\tr H -h_{ii}}k_{ii} - \sum_{j\neq i} h_{ij}k_{ji}\\
& = & \tr H \sum_i k_{ii} - \sum_i h_{ii}k_{ii} - \sum_{j\neq i} h_{ij}k_{ji}\\
& = & \tr H \tr K - \sum_{ij} h_{ij}k_{ji} = \tr H \tr K - \tr HK.
\eeast

Therefore, define $\psi: M_n\times M_n\to M_n$ by 
\be
\psi(H,K) =  \tr H \tr K - \tr HK,
\ee
then $\psi$ is bilinear and 
\be
\frac1{\dabs{H}'}\bb{D_{I+H}\det(K) - D_I \det(K) - \psi(H,K)} \to 0 \quad\text{as }H\to 0.
\ee

Hence, $\det$ is twice differentiable at $I$ and $D^2_I\det(H,K) = \tr H \tr K - \tr HK$.

\begin{exercise}
Show that there is a continuous square-root function on some neighbourhood of $I$ in $M_n$; that is, show that there is an open ball $B(I; r) \subset M_n$ for some $r > 0$ and a continuous function $g : B(I; r) \to M_n$ such that $g(X)^2 = X$ for all $X \in B(I; r)$.

Is it possible to define a square-root function on all of $M_n$? What about a cube-root function?
\end{exercise}

Solution. Define $f:M_n \to M_n$ by $f(A) = A^2$. By previous question, we have 
\be
D_Af(H) = AH + HA.
\ee

Thus, the map sending $A$ to $D_Af$ is linear and hence continuous. Since $D_If(H) = 2H$, the derivative $D_If$ is an isomorphism $M_n\to M_n$. Thus, by the inverse function theorem $f$ is a local $C^1$-diffeomorphism at $I$, i.e., there exists an open set $U$ in $M_n$ containing $I$ such that $f:U\to f(U)$ is bijective, $f(U)$ is open an both $f:U\to f(U)$ and $f^{-1}:f(U)\to U$ are continuously differentiable.

Since $f(U)$ is open and $f(I) = I$, $I\in U$, $\exists r>0$ such that $B(I;r) \subseteq f(U)$. Let $g: B(I,r)\to M_n$ be the restriction of $f^{-1}$, then $g$ is certainly continuous and for all $A\in B(I,r)$ we have
\be
f(g(A)) = f(f^{-1}A) = A \ \ra \ g(A)^2 = A.
\ee

It is not possible to define either a square-root or a cube root function on the whole of $M_n$, since not all matrices have square or cube root. To see this, let
\be
A = \bepm
0 & 1 & 0 & \dots & 0 & 0\\
0 & 0 & 1 & \dots & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 1\\
0 & 0 & 0 & \dots & 0 & 0
\eepm \ \ra \ A^{n-1} = \bepm
0 & 0 & 0 & \dots & 0 & 1\\
0 & 0 & 0 & \dots & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 0\\
0 & 0 & 0 & \dots & 0 & 0
\eepm \neq 0, \quad A^n = 0.
\ee

Thus if $B^r = A$ for $r\in\{2,3\}$, then $B^{rn} = A^n=0$, so that if we work over $\C$ then all eigenvalues of $B$ are 0, whence its Jordan normal form consists of blocks with diagonal entries 0. But raising such a matrix to the $n$th power then gives 0, so as $r(n-1)\geq n$ we have $0=B^{r(n-1)} = A^{n-1}$, a contradiction.

\begin{exercise}
Define $f : \R^2 \to \R^2$ by $f(x, y) = (x, x^3+y^3-3xy)$ and let $C = \{(x, y) \in \R^2 : x^3+y^3-3xy = 0\}$. Show that $f$ is locally invertible around each point of $C$ except $(0, 0)$ and $(2^{\frac 23}, 2^{\frac 13})$; that is, show that if $(x_0,y_0) \in C \left\backslash \left\{(0, 0), (2^{\frac 23}, 2^{\frac 13})\right\}\right.$ then there are open sets $U$ containing $(x_0, y_0)$ and $V$ containing $f(x_0, y_0)$ such that $f$ maps $U$ bijectively to $V$. What is the derivative of the local inverse function? Deduce that for each point $(x_0,y_0) \in C$ other than $(0, 0)$ and $(2^{\frac 23}, 2^{\frac 13})$ there exist open intervals $I$ containing $x_0$ and $J$ containing $y_0$ such that for each $x \in I$ there is a unique $y \in J$ with $(x, y) \in C$.
\end{exercise}

Solution. By computing partial derivatives we have
\be
D_{(x,y)}f(h,k) = (h,3hx^2-3hy + 3ky^2 - 3kx) \ \ra \ D_{(x,y)}f = \bepm
1 & 0 \\
3x^2 - 3y & 3y^2 - 3x
\eepm.
\ee

Therefore 
\be
D_{(x,y)}f \text{ is invertible } \ \lra \ 3y^2 -3x \neq 0 \ \lra \ x \neq y^2.
\ee

Thus if $(x,y)\in C$ and $D_{(x,y)}f$ is not invertible, then 
\be
x^3 + y^3 = 3xy, \quad x = y^2 \ \ra \ y^6 + y^3 = 3y^3 \ \ra \ y^3(y^3-2) = 0 \ \ra \ y =0,\ 2^{\frac 13} \ \ra \ (x,y) = \bb{0,0},\bb{2^{\frac 23},2^{\frac 13}}.
\ee

Therefore by the inverse function theorem if $(x_0,y_0)\in C \left\backslash \left\{(0, 0), (2^{\frac 23}, 2^{\frac 13})\right\}\right.$ then exist open sets $U$ containing $(x_0,y_0)$ and $V$ containing $f(x_0,y_0)$ such that $f:U\to V$ is a bijection with continuously differentiable inverse.

By the chain rule, 
\be
Df^{-1}(f(x,y))= (D_{(x,y)}f)^{-1} = \frac 1{3y^2 - 3x} \bepm
3y^2 - 3x & 0\\
3y - 3x^2 & 1
\eepm.
\ee

Suppose $(x_0,y_0)\in C \left\backslash \left\{(0, 0), (2^{\frac 23}, 2^{\frac 13})\right\}\right.$ and take open sets $U$ and $V$ as above. Then $\exists r>0$ such that $B((x,y);r)\subseteq U$. Let
\be
I' = \bb{x_0-\frac r2,x_0+\frac r2},\quad\quad J = \bb{y_0-\frac r2,y_0+\frac r2}
\ee  
then
\be
(x_0,y_0) \in I' \times J \subseteq U.
\ee

The set $f(I'\times J)\subseteq V$ is open and contains $f(x_0,y_0) = (x_0,0)$, so $\exists r'>0$ such that 
\be
B((x_0,0),r') \subseteq f(I'\times J).
\ee
Let
\be
I = \bb{x_0-\frac {r'}2,x_0+\frac {r'}2}, \quad\quad K = \bb{-\frac {r'}2,\frac {r'}2}
\ee
then
\be
(x_0,0) \in I\times K \subseteq f(I'\times J).
\ee

We know that $f: I\times J \to f(I\times J)$ is a bijection and $I\times K \subseteq f(I\times J)$ (since $f$ does not change first co-ordinates). Thus, $\forall x\in I$ we have
\be
(x,0)\in I\times K \subseteq f(I\times J)
\ee
so there is a unique $y\in J$ such that $f(x,y) = (x,0)$, i.e., we have $(x,y)\in C$.

\begin{exercise}
Let $f : \R^2 \to \R$ be a differentiable function and let $g(x) = f(x, c- x)$ where $c$ is a constant. Show that $g : \R \to \R$ is differentiable and find its derivative
\ben
\item [(i)] directly from the definition of differentiability

and also
\item [(ii)] by using the chain rule.
\een

Deduce that if $\partial f/\partial x = \partial f/\partial y$ holds throughout $\R^2$, then $f(x, y) = h(x + y)$ for some differentiable function $h$.
\end{exercise}

Solution. \ben
\item [(i)] Write 
\be
f(x+h,y+k) = f(x,y) + D_{1,(x,y)}f h + D_{2,(x,y)}f k + \epsilon(h,k)\dabs{(h,k)},
\ee
where $\epsilon(h,k)\to 0$ as $(h,k)\to (0,0)$. Then
\beast
g(x+h)-g(x) & = & f(x+h,c-x-h) - f(x,c-x) = D_{1,(x,c-x)}f h + D_{2,(x,c-x)}f(-h) + \epsilon(h,-h)\abs{h}\\
& = & \bb{D_{1,(x,c-x)}f - D_{2,(x,c-x)}f}h + \epsilon(h,-h)\abs{h},
\eeast
so as $\epsilon(h,-h) \to 0$ as $h\to 0$ we see that $g$ is differentiable and its derivative at $x$ is $D_{1,(x,c-x)}f - D_{2,(x,c-x)}f$.

\item [(ii)] Define $j:R\to \R^2$ by $j(x) = (x,c-x)$, so that $g= f\circ i$. Then $j$  is linear so it is differentiable with derivative given by 
\be
D_xj(h) = (h,-h).
\ee

Thus, $g$ is differentiable, and we have
\beast
D_xg(h) & = & D_{(x,c-x)}f D_xj(h) = D_{(x,c-x)}f(h,-h) \\
& = & D_{1,(x,c-x)}fh + D_{2,(x,c-x)}f(-h) \\
& = & \bb{D_{1,(x,c-x)}f - D_{2,(x,c-x)}f}h.
\eeast
Thus $D_xg = D_{1,(x,c-x)}f - D_{2,(x,c-x)}f$.
\een

Thus, if $D_1f = D_2f$ holds throughtout $\R^2$, then $g$ has derivative 0 and hence is constant (for each value of $c$). Define $h:\R\to\R$ by $h(t) = f(0,t)$, then $h$ is differentiable. Given $(x,y)\in\R^2$, set $c=x+y$ and let $g$ be as given then
\be
f(x,y) = f(x,c-x) = g(x) = g(0) = f(0,c) = f(0,x+y) = h(x+y).
\ee

\begin{exercise}
Let $U \subset \R^2$ be an open set that contains a rectangle $[a, b] \times [c, d]$. Suppose that $g : U \to \R$ is continuous and that the partial derivative $\partial g/\partial y$ exists and is continuous. Set $G(y) = \int^b_a g(x, y)dx$. Show that $G$ is differentiable on $(c, d)$ with derivative
\be
G'(y) = \int^b_a (\partial g/\partial y)(x, y)dx.
\ee

Show further that 
\be
H(y) = \int^y_a g(x, y)dx
\ee
is differentiable. What is its derivative $H'(y)$?

[Hint: consider a function $F(y, z) = \int^z_a g(x, y)dx$ before dealing with $H$.]
\end{exercise}

Solution. Given $x\in[a,b]$ and $y\in(c,d)$, choose $\delta >0$ such that $(y-\delta ,y+\delta) \subseteq (c,d)$. Then if $\abs{k}<\delta$ we have
\be
g(x,y+k) -g(x,y) = D_{2,(x,y)}g k + \epsilon(k)\abs{k}
\ee
where $\epsilon(k)\to 0$ as $k\to 0$. So 
\be
\frac{G(y+k)-G(y)}{k} = \frac 1{k}\bb{\int^b_a g(x,y+k)dx - \int^b_a g(x,y)dx} = \int^b_a \bb{D_2g(x,y) + \frac{\epsilon(k)\abs{k}}{k}}dx.
\ee

Thus 
\be
\lim_{k\to 0} \frac{G(y+k)-G(y)}{k} = \int^b_a D_2g(x,y) dx = \int^b_a \fp{}{y} g(x, y)dx,
\ee
i.e., $G$ is differentiable at $y$ with derivative $G'(y) = \int^b_a \fp{}{y} g(x, y)dx$.

Now assume that $[c,d]\subseteq [a,b]$ and define $F:[c,d]\times [c,d]\to \R$ by $F(y,z) = \int^z_a g(x,y)dx$. Then by above
\be
D_1F(y,z) = \int^z D_2g(x,y)dx \ \ra \ D_2F(y,z) = g(z,y) \quad\quad (\text{fundamental theorem of calculus})
\ee

Since both partial derivatives exist and are continuous on $[c,d]\times [c,d]$, it follows that $F$ is differentiable on $(c,d)\times (c,d)$.

Now define $j:\R\to \R^2$ by $j(y) = (y,y)$, so that $H=F\circ j$. Then $j$ is linear, so it is differentiable with derivative given by $D_yj(h) = (h,h)$. Thus $H$ is differentiable on $(c,d)$ and we have
\be
D_yH(h) = D_{(y,y)}F (D_y j(h)) = D_{(y,y)}F(h,h) = D_1 F(y,y)h + D_2 F(y,y)h = \bb{\int^y_a D_2g(x,y)dx + g(y,y)}h.
\ee
Thus, $H'(y) = \int^y_a \fp{}{y}g(x,y)dx + g(y,y)$.

\begin{exercise}
\ben
\item [(i)] For each of the following metric spaces $Y$
\be
\text{(a)}\ Y = \R,\quad\quad \text{(b)}\ Y = [0, 2],\quad\quad \text{(c)}\ Y = (1, 3),\quad\quad \text{(d)}\ Y = (1, 2] \cup (3, 4],
\ee
with metric $d(x, y) = \abs{x - y}$, is the set $(1, 2]$ an open subset of $Y$? Is it closed?

\item [(ii)] Suppose that $X$ is a metric space and $A_1,A_2$ are two closed balls in $X$ with radius respectively $r_1, r_2$, such that $r_1 > r_2 > 0$. Can $A_1$ be a proper subset of $A_2$ (i.e. $A_1 \subset A_2$ and $A_1 \neq A_2$)?
\een
\end{exercise}

Solution. \ben
\item [(i)] \ben 
\item [(a)] Not open. It contains 2 but no open ball about 2.

Not close. The complement contains 1 but no open ball about 1.

\item [(b)] Open. Given $x\in(1,2]$, $B(x;x-1)$ lies in the set.

Not closed. As in (a).

\item [(c)] Not open. As in (a).

Closed. The complement is (2,3), and given $x\in (2,3)$, $B(x;x-2)$ lies in it.

\item [(d)] Open. Given $x\in (1,2]$, $B(x;x-1)$ lies in the set. 

Closed. The complement is $(3,4]$, and given $x\in(3,4]$, $B(x;x-3)$ lies in it.
\een
\item [(ii)] Take $X=[0,\infty)$ with the usual metric. Let $A_1$ be the closed ball of radius 1 centred at 0, and $A_2$ be the closed ball of radius $\tfrac 34$ centred at $\frac 12$, $A_1=[0,1]$ is proper subset of $A_2 = [0,\frac 54]$.
\een

\begin{exercise}\label{ques:open_ball} 
For each of the following sets $X$, determine whether or not the given function d defines a metric on $X$. In each case where the function does define a metric, describe the open ball $B(x;r)$ for each $x \in X$ and $r > 0$ small.
\ben
\item [(i)] $X = \R^n$; $d(x, y) = \min\{\abs{x_1 -y_1}, \abs{x_2 - y_2},\dots,\abs{x_n - y_n}\}$.
\item [(ii)] $X = \Z$; $d(x, x) = 0$ and for $x \neq y$, $d(x, y) = 2^n$, where $x - y = 2^n a$ with $n$ a non-negative integer and $a$ an odd integer.
\item [(iii)] $X = \Q$; $d(x, x) = 0$ and for $x \neq y$, $d(x, y) = e^{-n}$, where $x - y = 3^{-n}a/b$ for $n, a, b \in \Z$ with both $a$ and $b$ not divisible by 3.
\item [(iv)] $X$ is the set of functions from $\N$ to $\N$; $d(f, f) = 0$ and for $f \neq g$, $d(f, g) = 2^{-n}$ for the least $n$ such that $f(n) \neq g(n)$.
\item [(v)] $X = \C$; $d(z, z) = 0$ and for $z \neq w$, $d(z,w) = \abs{z} + \abs{w}$.
\item [(vi)] $X = \C$; $d(z,w) = \abs{z - w}$ if $z$ and $w$ lie on the same straight line through the origin, $d(z,w) = \abs{z} + \abs{w}$ otherwise.
\een
\end{exercise}

Solution. \ben
\item [(i)] No. If $x=\bb{1,0,\dots,0}$, $y=\bb{0,0,\dots,0}$ then $x\neq y$ but $d(x,y) = 0$.
\item [(ii)] No. Take $n=0,a=1$, $d(0,1) + d(1,4) = 1 + 1 = 2 < 4 = d(0,4)$.
\item [(iii)] No. Take $x-y = 3^{-2}$ and $x-z = 3^{-1}$, then $y-z = 2\cdot 3^{-2}$. Thus
\be
d(x,z) = e^{-1} > e^{-1} \frac 2{e} = 2e^{-2} = d(x,y) + d(y,z).
\ee
\item [(iv)] Yes. We check
\ben
\item [(a)] $d(f,g)\geq 0$ for all $f,g\in X$. 
\item [(b)] If $d(f,g)=0\ \lra f=g$.
\item [(c)] The symmetry is obvious.
\item [(d)] Given $f,g,h$ the triangle inequality is clear if two of them are equal, while if $d(f,g) = 2^{-n}$ and $d(g,h) = 2^{-m}$ then $f$ and $g$ agree at $1,2,\dots \min(n,m)-1$ so 
\be
d(f,h) \leq 2^{-\min(n,m)} < 2^{-n} + 2^{-m} = d(f,g) + d(g,h).
\ee
\een
We have 
\be
B(f;r) = \{g:g(n)=f(n), \forall n \leq \log_2 \tfrac 1r\}.
\ee
\item [(v)] Yes. We check \ben
\item [(a)] $d(z,w)\geq 0$ for all $z,w\in X$. 
\item [(b)] If $d(z,w)=0\ \lra z=w$.
\item [(c)] The symmetry is obvious.
\item [(d)] Given $z,w,v$ the triangle inequality is clear if two of them are equal, while if $d(z,w) = \abs{z}+\abs{w}$ and $d(w,v) = \abs{w} + \abs{v}$ then \be
d(z,v) = \abs{z} + \abs{v} \leq \abs{z}+\abs{w}+\abs{w} + \abs{v} = d(z,w) + d(w,v).
\ee
\een
We have 
\be
B(0;r) = \{w:\abs{w}<r\}
\ee
while if $z\neq 0$ and $r<\abs{z}$ then $B(z;r) = \{z\}$.
\item [(vi)] Yes. We check
\ben
\item [(a)] $d(z,w)\geq 0$ for all $z,w\in X$. 
\item [(b)] If $d(z,w)=0\ \lra z=w$.
\item [(c)] The symmetry is obvious.
\item [(d)] Given $z,w,v$ the triangle inequality is clear if two of them are equal, or if all three lies on the same straight line through the orgin (by the standard triangle inequality). If $z,w$ do but not $v$ then
\be
d(z,v) = \abs{z} + \abs{v} \leq \abs{z-w} +\abs{w} + \abs{v} = d(z,w) + d(w,v).
\ee
If $v,w$ do but not $z$ the case is similar. If $z,v$ do but not $w$ then
\be
d(z,v) = \abs{z-v} \leq \abs{z}+\abs{v} \leq \abs{z}+\abs{w}+\abs{w} + \abs{v} = d(z,w) + d(w,v).
\ee
\een
If no two do the argument is as in previous question. We have 
\be
B(0;r) = \{w:\abs{w}<r\}
\ee
while if $z\neq 0$ and $r<\abs{z}$ then $B(z;r) = \left\{\lm z: \abs{\lm -1} < \frac r{\abs{z}}\right\}$.

\een

\begin{exercise}
Let $d$ and $d'$ denote the usual and discrete metrics respectively on $\R$. Show that all functions $f$ from $\R$ with metric $d'$ to $\R$ with metric $d$ are continuous. What are the continuous functions from $\R$ with metric $d$ to $\R$ with metric $d'$?
\end{exercise}

Solution. Given any $f:(\R,d') \to (\R,d)$, $\forall x\in\R$, $\forall \ve >0$ choose $\delta=\frac 12$, then
\be
d'(y,x) < \delta \ \ra \ y = x \ \ra \ d(f(y),f(x)) = 0< \ve,
\ee
so $f$ is continuous. 

Given $f:(\R,d) \to (\R,d')$, continous, if $f$ is not constant, $\exists a,b\in\R$ with 
\be
a<b,\quad f(a) \neq f(b).
\ee
Set $S = \{x>a:f(x)\neq f(a)\}$, then $b\in S$ and $a$ is a lower bound for $S$, $exists s = \inf S$. By continuity, $\exists \delta >0$ such that
\be
d(x,s) < \delta \ \ra \ d'(f(x),f(s)) < \frac 12 \ \ra \ f(x) = f(s).
\ee

If $s\in S$ then
\be
f(s-\tfrac{\delta}2) = f(s) \neq f(a)
\ee
which is contrary to $s$ being a lower bound for $S$, while if $s\notin S$, then $f(s) = f(a)$, so $\forall t\in [0,\delta)$ we have
\be
f(s+t) = f(a) \ \ra \ s+t\notin S \ \ra \ s+\delta \text{ is a lower bound for }S,
\ee
contrary to $s$ being the greatest such. Thus, $f$ must be constant. As constant functions are clearly continuous, the continuous functions $(\R,d)\to (\R,d')$ are the constant functions.

\begin{exercise}
\ben
\item [(a)] Show that the intersection of an arbitrary collection of closed subsets of a metric space must be closed.
\item [(b)] We define the \emph{closure} of subset $Y$ of a metric space $X$ to be the smallest closed set $cl(Y)$ containing $Y$. Why does the result of (a) tell us that this definition makes sense?
%if $Y$ is a subset of a metric space $X$, there is a unique closed subset $Z$ of $X$ such that $Z$ contains $Y$ and any closed subset of $X$ containing $Y$ also contains $Z$. The set $Z$ is called the closure of $Y$ in $X$, denoted $\bar{Y}$ or $cl(Y)$.
\item [(c)] Show that
\be
cl(Y) = \{x \in X : x_n \to x \text{ for some sequence $(x_n)$ in }Y \}.
\ee
\een
\end{exercise}

Solution. \ben
\item [(a)] Let $C_i$ for $i\in I$ be closed sets, then their complements $C_i^c$ are open. Given $x\in \bigcup_{i\in I}C_i^c$, take $i\in I$ such that $x \in C_i^c$, then $\exists r>0$ such that 
\be
B(x;r)\subseteq C_i^c \ \ra \ B(x;r)\subseteq \cup_{i\in I}C_i^c \ \ra \ \bigcup_{i\in I}C_i^c \text{ is open} \ \ra \ \bigcap_{i\in I}C_i = \bb{\bigcup_{i\in I}C_i^c}^c \text{ is closed}. 
\ee
\item [(b)] Given $Y$, let $C_i$ for $i\in I$ be the closed sets containing $Y$, then $\bigcap_{i\in I}C_i$ is closed by (a), contains $Y$, and lies in any closed set containing $Y$, so it is the smallest such.
\item [(c)] Let $S =\{x \in X : x_n \to x \text{ for some sequence $(x_n)$ in }Y \}$. Take $x\in X$. If there exists a sequence $(x_n)$ in $Y$ with $x_n\to x$, given $ve>0$, $\exists N\in\N$ such that
\be
n>N \ \ra \ d(x_n, x) < \ve \ \ra \ x_n \in B(x;\ve),
\ee
so that meets $Y$ and hence $cl(Y)$. Thus the open set $cl(Y)^c$ contains no open ball about $x$, so we must have $x\notin cl(Y)^c$, whence $x\in cl(Y)$ and $S\subseteq cl(Y)$.

Conversely, if $x\in cl(Y)$, then $\forall n \in\N$ the closed set $B(x;\frac 1n)^c$ does not contain $cl(Y)$, so cannot contain $Y$. So $\exists x_n \in B(x;\frac 1n)\cap Y$, and then $(x_n)$ is a sequence in $Y$ with $x_n\to x$. This gives $cl(Y) \subseteq S$.

\een

\begin{exercise}
Let $V$ be a normed space, $x \in V$ and $r > 0$. Prove that the closure of the open ball $B(x; r)$ is the closed ball $\{y \in V : \dabs{x - y} \leq r\}$. Give an example to show that, in a general metric space $(X, d)$, the closure of the open ball $B(x; r)$ need not be the closed ball $\{y \in X : d(x, y) \leq r\}$.
\end{exercise}

Solution. Let $A_r(x) = \{y \in V : \dabs{x - y} \leq r\}$. To show $A_r(x) \subseteq cl(B(x;r))$. Take $y\in V$. If $\dabs{x-y}\leq r$, define a sequence $(y_n)$ by 
\be
y_n = \frac 1n x + \frac {n-1}n y \ \ra \ \dabs{x-y_n} = \dabs{\frac{n-1}n(x-y)} = \frac {n-1}n \dabs{x-y} \leq \frac {n-1}n r<r \ \ra \ y_n \in B(x;r).
\ee
and
\be
\dabs{y_n -y} = \dabs{\frac 1n(x-y)} = \frac 1n \dabs{x-y} \to 0 \quad \text{as }n\to \infty \ \ra \ y_n \to y.
\ee
Thus by previous question we have $y\in cl(B(x;r))$. 

Now we need to show $cl(B(x;r)) \subseteq A_r(x) $. If $\dabs{x-y}>r$, let $r' = \dabs{x-y}-r$, then
\be
B(y;r')\cap B(x;r) = \emptyset \quad (\text{because if $\dabs{z-y}\leq r'$, then }\dabs{z-x}\geq \dabs{x-y}-\dabs{z-y} \geq \dabs{x-y} - r' = r)
\ee
so $B(y;r')^c$ is a closed set containing $B(x;r)$, so it contains $cl(B(x;r))$, and hence $y \notin cl(B(x;r))$. Thus,
\be
cl(B(x;r)) = \{y \in V : \dabs{x - y} \leq r\} = A_r(x).
\ee

Let $V=\Z$ with the discrete metric. If $r=1$ then $B(x;r)=\{x\}$ which is closed, so $cl(B(x;r))=\{x\}$, while $A_r(x) = V$.

\begin{exercise}
Show that the space of real sequences $a = (a_n)$, such that all but finitely many of the $a_n$ are zero, is not complete in the norm defined by $\dabs{a}_1 = \sum^\infty_{n=1} \abs{a_n}$. Is there an obvious 'completion'?
\end{exercise}


Solution. Let $\ell_0$ be the space of real sequence with finitely many non-zero terms, with the norm 
\be
\dabs{a}_1 = \sum^\infty_{n=1}\abs{a_n}.
\ee

Define
\be
a_n^{(k)} = \left\{\ba{ll}
\frac 1{2^n} \quad\quad & n\leq k\\
0 & n > k
\ea\right.
\ee
then each sequence $a^{(k)}$ lies in $\ell_0$. Given $\ve>0$, take $M\in \N$ with $\frac 1{2^M} < \ve$, then if $k>l>M$ we have
\be
\dabs{a^{(k)}-a^{(l)}} = \sum^\infty_{n=1} \abs{a_n^{(k)}-a_n^{(l)}} = \sum^k_{n=l+1} \frac 1{2^n} < \sum^\infty_{n=M+1} \frac 1{2^n} = \ve.
\ee

Thus $a^{(k)}$ is a Cauchy sequence in $\ell_0$. Given $a\in X$, take $N\in\N$ with $a_n = 0$ for all $n>N$, and let $\ve = \frac 1{2^{n+2}}$. Then if $k>N$ we have
\be
\dabs{a^{(k)}-a}_1 = \sum^\infty_{n=1}\abs{a_n^{(k)}-a_n} \geq \abs{a_{N+1}^{(k)} - a_{N+1}} = \frac 1{2^{N+1}} > \ve.
\ee

Thus $a$ is not the limit of the sequence $a^{(k)}$, so this Cauchy sequence has no limit in $\ell_0$, whence $\ell_0$ is not complete.

Set 
\be
X = \left\{(a_n): \sum^\infty_{n=1} \abs{a_n} <  \infty \right\},
\ee
so that $X$ contains $\ell_0$. We claim that $X$ is complete. Take a Cauchy sequence $a^{(k)}$ in $X$. For all $n\in \N$ the sequence $a_n^{(k)}$ is then Cauchy, so converges to some $a_n\in \R$. Let $a$ be the sequence whose $n$th term is $a_n$. 

We claim that $a\in X$ and that $a$ is the limit of the sequence $a^{(k)}$.

Take $M\in\N$ such that $k>l\geq M$, then $\sum^\infty_{n=1}\abs{a_n^{(k)}-a_n^{(l)}} < 1$. Let $\sum^\infty_{n=1} \abs{a_n^{(M)}}=R<\infty$. Take $N\in\N$, then for all $n\leq N$ there exists $K_n\in\N$ such that 
\be
k>K_n \ \ra \ \abs{a_n-a_n^{(k)}} < \frac 1N.
\ee

Let $K = \max\{K_1,\dots,K_N,M\} + 1$, then 
\be
\sum^N_{n=1} \abs{a_n - a_n^{(K)}} < N \cdot \frac 1N = 1, \quad\quad \sum^\infty_{n=1}\abs{a_n^{(K)}-a_n^{(M)}} < 1,
\ee
so
\be
\sum^N_{n=1} \abs{a_n} \leq \sum^N_{n=1} \abs{a_n - a_n^{(K)}} + \sum^N_{n=1} \abs{a_n^{(K)}-a_n^{(M)}} + \sum^N_{n=1} \abs{a_n^{(M)}} < 1+ 1+R = R+2.
\ee

Thus, $\sum^\infty_{n=1}\abs{a_n}$ is bounded by $R+2$, and so $a\in X$.

Now choose $M\in\N$ such that 
\be
\sum^\infty_{n=M+1} \abs{a_n} < \frac {\ve}4,\quad\quad \sum^\infty_{n=M+1} \abs{a_n^{(k)}} < \frac {\ve}4. \quad\quad (\text{since }a,a^{(k)}\in X)
\ee

For all $n\leq M$ take $L_n\in \N$ such that $l\geq L_n$, we have
\be
\abs{a_n - a_n^{(l)}} <  \frac {\ve}{4M}.
\ee

Let $l = \max\{L_1,\dots,L_M,K\}+1$, then
\beast
\sum^\infty_{n=1} \abs{a_n - a_n^{(k)}} & = & \sum^M_{n=1} \abs{a_n - a_n^{(k)}} +\sum^\infty_{n=M+1} \abs{a_n - a_n^{(k)}} \\
& \leq & \sum^M_{n=1} \abs{a_n - a_n^{(l)}} + \sum^M_{n=1} \abs{a_n^{(l)} - a_n^{(k)}} + \sum^\infty_{n=M+1} \abs{a_n} + \sum^\infty_{n=M+1} \abs{a_n^{(k)}} \\
& \leq & \sum^M_{n=1} \abs{a_n - a_n^{(l)}} + \sum^\infty_{n=1} \underbrace{\abs{a_n^{(l)} - a_n^{(k)}}}_{\text{Cauchy sequence}} + \sum^\infty_{n=M+1} \abs{a_n} + \sum^\infty_{n=M+1} \abs{a_n^{(k)}} \\
& < & M\cdot \frac{\ve}{4M} + \frac {\ve}4 + \frac {\ve}4 + \frac {\ve}4 = \ve.
\eeast
Thus $a$ is the limit of the sequence $a^{(k)}$ as required.

\begin{exercise}
Use the Contraction Mapping Theorem to show that the equation $\cos x = x$ has a unique real solution. Find this solution to some reasonable accuracy using a pocket calculator or the calculator on your computer (remember to work in radians!), and justify the claimed accuracy of your approximation.
\end{exercise}

Solution. Since $\abs{\cos x}\leq 1$, $\forall x\in\R$, any solution to $\cos x = x$ must lies in $[-1,1]$. Let $f(x) = \cos x$, then by the MVT, $\forall x,y \in [-1,1]$, $\exists z \in (x,y)$ such that
\be
\abs{f(y)-f(x)} = \abs{f'(z)}\cdot\abs{y-x} = \abs{\sin z}\cdot \abs{y-x} = (\sin 1) \abs{y-x}.
\ee
So $f$ is a contraction on $[-1,1]$ with constant $K =\sin 1 \simeq 0.841$, and hence $f$ has a unique fixed point. Take $x_0 =1$, 
\be
x_n = f(x_{n-1}) \quad \text{for }n\in\N \ \ra\ x_1 = 0.540 \dots \ \ra \ d(x_0,x_1) = 0.46.
\ee
Thus $m>n$, we have
\be
d(x_n,x_m) < d(x_0,x_1) \frac {K^n}{1-K} < 0.46 \frac{0.842^n}{0.158} < 3(0.842)^n,
\ee
so for 3 decimal place accuracy take $n > \frac{\log\bb{\frac{0.001}3}}{\log 0.842} = 46.55\dots$. Thus if we take $x_{47} = 0.739\dots$, this is correct to 3 decimal places. 

\begin{exercise}
Let $I = [0,R]$ be an interval and let $C(I)$ be the space of continuous functions on $I$. Show that, for any $\alpha \in \R$, we may define a norm by $\dabs{f}_\alpha = \sup_{x\in I} \abs{f(x)e^{-\alpha x}}$, and that the norm $\dabs{\cdot}_\alpha$ is Lipschitz equivalent to the uniform norm $\dabs{f} = \sup_{x\in I}\abs{f(x)}$.

Now suppose that $\varphi: \R^2 \to \R$ is continuous, and Lipschitz in the second variable $\abs{\varphi(t,x) - \varphi(t, y)} \leq K\abs{x - y}$, for all $t,x,y \in \R$. Consider the map $T$ from $C(I)$ to itself sending $f$ to $y_0+ \int^x_0 \varphi(t,f(t))dt$. Give an example to show that $T$ need not be a contraction under the uniform norm. Show, however, that $T$ is a contraction under the norm $\dabs{\cdot}_\alpha$ for some $\alpha$, and deduce that the differential equation $f' = \varphi(x,f(x))$ has a unique solution on $I$ satisfying $f(0) = y_0$.
\end{exercise}

Solution. First, we check
\ben
\item [(i)] Clearly, $\dabs{\cdot}_\alpha$ takes non-negative values.
\item [(ii)] If $\dabs{f}_\alpha = 0 \ \ra \ f=0$.
\item [(iii)] For $\lm \in\R$, 
\be
\dabs{\lm f}_\alpha = \sup_{x\in I} \abs{\lm f(x)e^{-\alpha x}} = \abs{\lm }\sup_{x\in I} \abs{f(x)e^{-\alpha x}} = \abs{\lm}\dabs{f}_\alpha.
\ee
\item [(iv)]
\be
\dabs{f+g}_\alpha = \sup_{x\in I} \abs{(f(x)+g(x))e^{-\alpha x}} \leq \sup_{x\in I} \abs{f(x)e^{-\alpha x}} + \sup_{x\in I} \abs{g(x)e^{-\alpha x}} = \dabs{f}_\alpha + \dabs{g}_\alpha.
\ee
\een
so $\dabs{\cdot}_\alpha$ is a norm.

$\forall x\in I$, we have
\be
\abs{f(x)e^{-\alpha x}} \leq \abs{f(x)}e^{\abs{\alpha} R} \leq e^{\abs{\alpha} R} \dabs{f} \ \ra \ \dabs{f}_\alpha = \sup_{x\in I} \abs{f(x)e^{-\alpha x}} \leq e^{\abs{\alpha} R}\dabs{f}.
\ee
Conversely, $\forall x\in I$, we have
\be
e^{-\abs{\alpha}R}\abs{f(x)} \leq \abs{f(x)e^{-\alpha x}} \leq \dabs{f}_\alpha \ \ra \ e^{-\abs{\alpha}R}\dabs{f} \leq \sup_{x\in I} e^{-\abs{\alpha}R} \abs{f(x)} \leq \dabs{f}_\alpha.
\ee
Thus 
\be
e^{-\abs{\alpha}R}\dabs{f} \leq \dabs{f}_\alpha \leq e^{\abs{\alpha}R}\dabs{f} \ \ra \ \dabs{\cdot}_\alpha \text{ is Lipschitz equivalent to }\dabs{\cdot}.
\ee

Take $R=1$, $\varphi(t,s) = 2s$ (which is continuous and Lipschitz in the second variable), $f(x)=1$, $g(x)=0$ for all $x$. Then $\dabs{f-g} =1$ while 
\be
\dabs{T(f)-T(g)} = \sup_{x\in[0,1]} \abs{\int^x_0 \bb{\varphi(t,f(t)) - \varphi(t,g(t))}dt} = \sup_{x\in[0,1]} 2 \abs{\int^x_0 \bb{f(t) - g(t)}dt} = 2 = 2\dabs{f-g},
\ee
so that $T$ is not a contraction under norm $\dabs{\cdot}$. Now take $\alpha >0$. Given $\varphi$ continuous and Lipschitz in the second variable with Lipschitz constant $K$, $\forall f,g\in C(I)$ we have
\beast
\dabs{T(f)-T(g)}_\alpha & = & \sup_{x\in I} \abs{e^{-\alpha x}\bb{\int^x_0(\varphi(t,f(t))- \varphi(t,g(t))dt}}\\
& \leq & \sup_{x\in I} \bb{\int^x_0 e^{-\alpha x}\abs{\varphi(t,f(t))- \varphi(t,g(t)}dt}\\
& \leq & \sup_{x\in I} \bb{\int^x_0 e^{-\alpha x}K\abs{f(t)- g(t)}dt} \leq \sup_{x\in I} \bb{\int^x_0 e^{-\alpha x}K\dabs{f-g}_\alpha e^{\alpha t}dt}\\
& \leq & K\dabs{f-g}_\alpha \sup_{x\in I} e^{-\alpha x} \bb{\int^x_0  e^{\alpha t}dt} = \frac K{\alpha}\dabs{f-g}_\alpha \sup_{x\in I} \bb{1-e^{-\alpha x}}\\
& < & \frac K{\alpha}\dabs{f-g}_\alpha.
\eeast
then $T$ is a contraction under the norm $\dabs{\cdot}_\alpha$. If $f$ is the fixed point of $T$ then
\be
f(x) = (T(f))(x) = y_0 + \int^x_0 \varphi(t,f(t))dt.
\ee

Differentiating gives 
\be
f'(x) = \varphi(x,f(x)), \quad\quad f(0)= y_0.
\ee

Conversely, if $f$ is a solution of $f'(x) = \varphi(x,f(x))$ and $f(0) = y_0$, then the fundamental theorem of calculus gives 
\be
f(x) = y_0 + \int^x_0 f'(t)dt = y_0 + \int^x_0 \varphi(t,f(t))dt.
\ee
So $f$ is a fixed point of $T$. Thus the differential equation $f'(x) = \varphi(x,f(x))$ has a unique solution on $I$ satisfying $f(0)=y_0$.

\begin{exercise}
Let $(X,d)$ be a non-empty complete metric space. Suppose $f : X \to X$ is a contraction and $g : X \to X$ is a function which commutes with $f$, i.e. such that $f(g(x)) = g(f(x))$ for all $x \in X$.

Show that $g$ has a fixed point. Must this fixed point be unique?
\end{exercise}

Solution. Let $x$ be the unique fixed point of $f$, then $f(g(x)) = g(f(x)) = g(x)$, so $g(x)$ is fixed point of $f$, by uniqueness we must have
\be
g(x) = x,
\ee
i.e., $x$ is a fixed point of $g$. It need not be unique, e.g. $g$ might be the identity map on $X$.

\begin{exercise}
Give an example of a non-empty complete metric space $(X,d)$ and a function $f : X \to X$ satisfying $d(f(x), f(y)) < d(x, y)$ for all $x, y \in X$ with $x \neq y$, but such that $f$ has no fixed point.

Suppose now that $X$ is a non-empty closed bounded subset of $\R^n$ with the Euclidean metric. Show that in this case $f$ must have a fixed point. If $g : X \to X$ satisfies $d(g(x), g(y)) \leq d(x, y)$ for all $x, y \in X$, must $g$ have a fixed point?
\end{exercise}

Solution. Take $X=[1,\infty)$ with usual metric $d$, and define $f:X\to X$ by 
\be
f(x) = x + \frac 1x.
\ee
Then
\be
d(f(x),f(y)) = \abs{x+\frac 1x - y - \frac 1y} = \abs{x-y - \frac{x-y}{xy}} = \abs{x-y}\bb{1-\frac 1{xy}} < \abs{x-y} = d(x,y),
\ee
but $f$ has no fixed point.

If $X$ is a closed bounded set in $\R^n$, define $h:X\to \R$ by $h(x) = d(x,f(x))$. Given $\ve>0$, if $d(y,x) < \frac {\ve}2$ then
\beast
d(f(y),f(x)) < \frac {\ve}2 \ \ra \ d(y,f(y)) & \leq & d(y,x) + d(x,f(x)) + d(f(x),f(y)) \\
& < & \frac {\ve}2 + d(x,f(x)) + \frac {\ve}2 = d(x,f(x)) + \ve.
\eeast

Similarly, $d(x,f(x)) < d(y,f(y)) + \ve$, so that
\be
\abs{h(y)-h(x)} < \ve.
\ee
Thus $h$ is continuous, so it is bounded and attains its bounds, and hence $\exists x\in X$ such that $\forall y\in X$ we have $h(y)\geq h(x)$. Now if $h(x)>0$ then
\be
h(f(x)) = d(f(x),f(f(x))) < d(x,f(x)) = h(x),
\ee
a contradiction (since $f(x)\in X$), so we must have $h(x) = 0 \ \ra \ d(x,f(x)) = 0 \ \ra\ x=f(x)$, and thus $x$ is a fixed point of $f$. 

Take $X= [-2,-1]\cup [1,2]$ and define $g:X\to X$ by $g(x)=-x$. Then $\forall x,y \in X$ we have
\be
d(g(x),g(y)) = d(x,y),
\ee
but $g$ has no fixed point.

\begin{exercise}
\ben
\item [(i)] Suppose that $(X, d)$ is a non-empty complete metric space, and $f : X \to X$ a continuous map such that, for any $x,y \in X$, the sum $\sum^\infty_{n=1} d(f^n(x),f^n(y))$ converges. ($f^n$ denotes the function $f$ applied $n$ times.) Show that $f$ has a unique fixed point.
\item [(ii)] By considering the function $x \mapsto \max \{x - 1, 0\}$ on the interval $[0,1) \subset \R$, show that a function satisfying the hypotheses of (i) need not be a contraction mapping.
\item [(iii)] Give an example to show that the result of (i) need not be true if $f$ is not assumed to be continuous.
\een
\end{exercise}

Solution. \ben
\item [(i)] Take $x_0 \in X$ and set $x_n = f^n(x_0)$ for $n\in\N$. Take $\ve>0$, since 
\be
\sum^\infty_{n=1}d(x_n,x_{n+1}) = \sum^\infty_{n=1}d(f^n(x_0),f^n(x_1)) 
\ee
converges, there exists $N\in\N$ such that 
\be
\sum_{n>N} d(x_n,x_{n+1}) < \ve.
\ee

Thus if $k>l>N$, we have 
\be
d(x_k,x_l) \leq \sum^{l-1}_{n=k}d(x_n,x_{n+1})\leq \sum_{n>N} d(x_n,x_{n+1}) < \ve \ \ra \ \text{the sequence $(x_n)$ is Cauchy.}
\ee

As $X$ is complete it therefore converges to some $x\in X$. Since $f$ is continuous we have 

\be 
f(x) = \lim_{n\to \infty} f(x_n) = \lim_{n\to\infty} x_{n+1} = x,
\ee
so that $x$ is a fixed point. If $y \neq x$ were any other fixed point then for all $n\in\N$, we would have
\be
d(f^n(x),f^n(y)) = d(x,y) \ \ra \ \sum^\infty_{n=1} d(f^n(x),f^n(y)) = \sum^\infty_{n=1} d(x,y) ,
\ee
which would not converge, so that $f$ has a unique fixed point.

\item [(ii)] Set $X=[0,\infty)$ with standard metric, and define $f:X\to X$ by $f(x)= \max\{x-1,0\}$. Given $x,y\in X$, we have
\be
d(f(x),f(y)) \leq d(x,y)
\ee
so $f$ is continuous, and if we choose $M\in\N$ with $M>\max{x,y}$, then
\be
f^M(x) = 0 = f^M(y) \ \ra\  \sum^\infty_{n=1}d(f^n(x),f^n(y)) \text{ is finite sum and hence converges.}
\ee
However, $f$ is not a contraction since 
\be
d(f(1),f(2)) = d(0,1) = 1 = d(1,2).
\ee

\item [(iii)] Let $X=[-1,1]$ with standard metric, and define $f: X\to X$ by 
\be
f(x) = \left\{\ba{ll}
\frac x2 \quad\quad x\neq 0\\
1 & x = 0
\ea\right.
\ee
then clearly, $f$ has no fixed point. However, given $x,y\in X$, for all $m\in \N$, we have $f^m(x), f^m(y)\neq 0$ whence ($f(x) \neq 0$)
\be
d\bb{f^{m+1}(x),f^{m+1}(y)} = \abs{f^{m+1}(x)-f^{m+1}(y)} = \abs{f^m(f(x))- f^m(f(y))} = \frac 12 \abs{f^m(x)- f^m(y)}.
\ee

Thus,
\be
\sum^\infty_{n=1} d\bb{f^n(x),f^n(y)} = d(f(x),f(y)) \sum^\infty_{n=0}\frac 1{2^n} = 2d(f(x),f(y)) \leq 3.
\ee
\een

\begin{exercise}
\ben
\item [(i)] Let $(X, d)$ be a metric space. For a nonempty subset $Y \subset X$ and $x \in X$ define
\be
d(x, Y) = \inf_{y\in Y} d(x, y).
\ee
Show that for fixed $Y$, the function $x \mapsto d(x, Y)$ defines a continuous function on $X$, and determine the subset of $X$ on which it vanishes.

\item [(ii)] For $Y, Z \subset X$ nonempty, define
\be
d(Y,Z) = \inf_{y\in Y} d(y,Z).
\ee

Show that if $Y$ and $Z$ are closed subsets of $\R^n$, and at least one of $Y$, $Z$ is bounded, then $d(Y,Z) > 0$ iff $Y$ and $Z$ are disjoint. Show that this conclusion can fail if the boundedness condition is removed.
\een
\end{exercise}

Solution. 

\begin{exercise}
A metric $d$ on a set $X$ is called an ultrametric if it satisfies the following stronger form of the triangle inequality:
\be
d(x, z) \leq \max \{d(x, y), d(y, z)\} \text{ for all }x, y, z \in X.
\ee

Which of the metrics in question \ref{ques:open_ball} are ultrametrics? Show that in an ultrametric space 'every triangle is isosceles' (that is, at least two of $d(x, z)$, $d(y, z)$ and $d(x, y)$ must be equal), and deduce that every open ball in an ultrametric space is a closed set. Does it follow that every open set must be closed?
\end{exercise}

Solution. The metric in question \ref{ques:open_ball}.(iv) is an ultrametric: in the notation of the answer above we have
\be
d(f,h) \leq 2^{-\min(n,m)} = \max\{2^{-n}, 2^{-m}\} = \max\{d(f,g), d(g,h)\}.
\ee

Those in question \ref{ques:open_ball}.(v) and (vi) are not ultrametric: in both cases 
\be
d(-1,0) = d(0,1) = 1, \quad d(-1,1) = 2.
\ee

Suppose $X$ is an ultrametric space. Take $x,y,z\in X$ and assume wlog that 
\be
d(x,y) \leq d(y,z) \ \ra \ d(x,z) \leq \max\{d(x,y),d(y,z)\} = d(y,z),
\ee
and $d(y,z) \leq \max\{d(y,x),d(x,z)\}$, we must have $d(y,z) \leq d(x,z)$, so $d(y,z) = d(x,z)$. Thus 'every triangle is isosceles', and indeed in any triangle the two longest sides have the same length. 

Now take $x\in X$ and $r>0$. If $y \notin B(x;r)$ then $d(x,y)\geq r$, then given $z\in B(y;r)$ we have
\be
d(z,y)< r\leq d(x,y) \ \ra \ d(x,z) = d(x,y) \geq r  \ \ra \ z\notin B(x;r).
\ee

Hence every point in $B(x;r)^c$ lies in an open ball contained in $B(x;r)^c$, so $B(x;r)^c$ is open, and thus $B(x;r)$ is closed.

It does not follow that every open set must be closed: take $X$ as in question \ref{ques:open_ball}.(iv) and choose $x\in X$, then $\{x\}^c$ is open since if $y\in \{x\}^c$ then $B(y;d(x,y)/2) \subseteq \{x\}^c$. But $\{x\}$ is not open since it contains no open ball, i.e., $\{x\}^c$ is not closed.

\begin{exercise}
There is (rumoured to be) a persistent 'urban myth' about the mathematics research student who spent three years writing a thesis about properties of 'antimetric spaces', where an antimetric on a set $X$ is a function $d : X \times X \to \R$ satisfying the same axioms as a metric except that the triangle inequality is reversed (i.e. $d(x, z) \geq d(x, y) + d(y, z)$ for all $x,y,z$). Why would such a thesis be unlikely to be considered worth a Ph.D.?
\end{exercise}

Solution. If $a,b\in X$ with $a\neq b$, then
\be
0 = d(a,a) \geq d(a,b) + d(b,a) =  2d(a,b) > 0,
\ee
a contradiction. So $\abs{X} \leq 1$.

\begin{exercise}
Let $X$ be the space of bounded real sequences. Is there a metric on $X$ such that a sequence $(x^{(k)})$ in $X$ converges to $x$ in this metric if and only if $(x^{(k)})$ converges to $x$ in every coordinate (i.e. $x^{(k)}_n \to x_n$ in $\R$ for every $n$)? Is there a norm with this property?
\end{exercise}

Solution. First, note that if $0\leq p\leq q$, then
\be
p(1+q) = p + pq \leq q + pq = q(1+p) \ \ra \ \frac p{1+p} \leq \frac q{1+q}.
\ee

Then if $0\leq r \leq p+q$ we have
\be
\frac r{1+r} \leq \frac{p+q}{1+p+q} = \frac p{1+p+q} + \frac q{1+p+q} \leq \frac p{1+p} + \frac q{1+q}.
\ee

Thus if we define $\delta:\R^2\to\R$ by 
\be
\delta(a,b) = \frac{\abs{a-b}}{1+\abs{a-b}},
\ee
then $\delta$ satisfies the triangle inequality, and hence $\delta$ is a metric on $\R$ and clearly $\forall a,b\in \R$ we have $\delta(a,b)<1$.

Thus if we define $d:X^2\to\R$ by 
\be
d(x,y) = \sum^\infty_{n=1} \frac 1{2^n}\delta(x_n,y_n),
\ee
it immediately follows that $d$ is metric on $X$. Suppose $x^{(k)}\to x$ in this metric. $\forall n\in\N$, $\forall \ve>0$,  $\exists K\in\N$ such that $\forall k>K$
\be
d(x^{(k)},x) < \frac 1{2^n} \frac {\ve}{1+\ve} \ \ra \  \frac 1{2^n} \frac {\ve}{1+\ve} > \sum^\infty_{m=1}\frac 1{2^m}\delta(x_m^{(k)},x_m) \geq \frac 1{2^n}\delta(x_n^{(k)},x_n) = \frac 1{2^n} \frac{\abs{x_n^{(k)}-x_n}}{1+\abs{x_n^{(k)}-x_n}},
\ee
giving 

\be
\frac {\ve}{1+\ve} > \frac{\abs{x_n^{(k)}-x_n}}{1+\abs{x_n^{(k)}-x_n}} \ \ra \ \abs{x_n^{(k)}-x_n} < \ve.
\ee
Thus, for all $n\in\N$, we have $x_n^{(k)}\to x_n$.

Conversely, assume that for all $n\in\N$ we have $x_n^{(k)}\to x_n$, and $\forall \ve>0$, we can choose $N\in\N$ such that $\frac 1{2^N} < \frac{\ve}2$. For all $n\leq N$, choose $K_n$ such that 
\be
\forall k > K \ \ra \ \abs{x_n^{(k)}-x_n} < \frac {\ve}2,
\ee
and set $K=\max\{K_1,\dots,K_N\}$. Then if $k>N$ we have
\beast
d(x^{(k)},x) & = & \sum^\infty_{n=1} \frac 1{2^n}\delta(x_n^{(k)},x_n) = \sum^N_{n=1}\frac 1{2^n}\delta(x_n^{(k)},x_n) + \sum^\infty_{n=N+1} \frac 1{2^n}\delta(x_n^{(k)},x_n) \\
& < & \sum^N_{n=1}\frac 1{2^n}\abs{x_n^{(k)}-x_n)} + \sum^\infty_{n=N+1} \frac 1{2^n} < \frac {\ve}2 \frac {\frac 12-\frac 1{2^N}}{1-\frac 12} + \frac 1{2^N} < \frac {\ve}2 + \frac {\ve}2 = \ve.
\eeast
Thus, $x^{(k)}\to x$ in the metric $d$.

However, if $\dabs{\cdot}'$ is any norm on $X$, for each $k\in\N$ let $y^{(k)}$ be the sequence with $y_n^{(k)} = \delta_{nk}$ and set $x^{(k)} = \frac{y^{(k)}}{\dabs{y^{(k)}}'}$ so that $\dabs{x^{(k)}}' = 1$. Let $x$ be the zero sequence, then for all $n\in\N$, as $k\to \infty$, we have $x_n^{k} \to 0 = x_n$, but for all $k\in\N$ we have $\dabs{x^{(k)}-x}' = \dabs{x^{(k)}}' = 1$ so that $x^{(k)}\nrightarrow x$. Thus there is no norm on $X$ with the requied property. 

\begin{exercise}
Metrics $d$, $d'$ on $X$ are said to be uniformly equivalent if the identity maps $(X, d) \to (X, d')$ and $(X, d') \to (X, d)$ are both uniformly continuous. Give an example of a pair of metrics on $\R$ which are uniformly equivalent but not Lipschitz equivalent. Show that for every metric space $d$
on a set $X$ there exists a metric $d'$ which is uniformly equivalent to $d$ and which is bounded.
\end{exercise}

Solution. 

\begin{exercise}
Let $(X, d)$ be a non-empty complete metric space and let $f : X \to X$ be a function such that for each positive integer $n$ we have
\ben
\item [(i)] if $d(x, y) < n + 1$ then $d(f(x), f(y)) < n$; and
\item [(ii)] if $d(x, y) < 1/n$ then $d(f(x), f(y)) < 1/(n + 1)$.
\een
Must $f$ have a fixed point?
\end{exercise}


Solution. Take $x\in X$ and define $x_0 = x$, $x_n = f(x_{n-1})$ for $n\in\N$. Take $n\in\N$ such that 
\be
d(x_0,x_1) < n+1 \ \ra \ d(f(x_0),f(x_1))=d(x_1, x_2) < n \ \ra \ \dots \ \ra \ d(x_n,x_{n+1}) < 1. \quad (\text{by (i)})
\ee
Write $y_i=x_{n+i}$ for $i\geq 0$, so $d(y_0,y_1)<1$. Given $k\in\N$, by (ii) we have
\be
d(y_k,y_{k+1})< \frac 1{k+1} < 1.
\ee
Assume $d(y_k,y_{k+n}) <1$ for some $n\in\N$, then
\be
d(y_k,y_{k+n+1}) \leq d(y_k,y_{k+1}) + d(y_{k+1},y_{k+n+1}) < \frac 1{k+1} + d(f(y_k),f(y_{k+n})) < \frac 1{k+1} + \frac 12 \leq 1.
\ee

Thus by induction $\forall n\in\N$, $d(y_k,y_{k+n}) < 1$, and this is true $\forall k\in\N$ (since $\frac 1{k+1} + \frac 12 \leq 1$).

Now $\forall \ve>0$, $\exists N > \frac 1{\ve} -1$, then $m,n>N$
\be
d(y_{m-N},y_{n-N}) < 1 \ \ra \ d(y_m,y_n) = d\bb{f^N(y_{m-N}),f^N(y_{n-N})} < \frac 1{N+1} < \ve \ \ra \ (y_n) \text{ is a Cauchy sequence}.
\ee

So as $X$ is complete we have $y_n \to y$ for some $y\in X$. As $f$ is Lipschitz with Lipschitz constant 1, it is continuous, so
\be
y_{n+1} = f(y_n) \to f(y) (\text{by continuity})
\ee
Thus by uniqueness of limits we must have $f(y)=y$, i.e., $y$ is fixed point of $f$.

