% analysis II

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Analysis II (draft)}


\section{Why do we bother?} It is surprising how many people
think that that analysis consists in the difficult
proofs of obvious theorems. All we need know, they say, is what
a limit is, the definition of continuity
and the definition of the derivative. All the
rest is `intuitively clear'.

If pressed they will agree that these definitions apply as
much to the rationals ${\mathbb Q}$ as to the real
numbers ${\mathbb R}$. They then have
to explain the following interesting
example.
\begin{example}\label{Rational} 
If $f:{\mathbb Q}\rightarrow{\mathbb Q}$
is given by
\begin{alignat*}{2}
f(x)&=-1&&\qquad\text{if $x^{2}<2$,}\\
f(x)&=1&&\qquad\text{otherwise,}
\end{alignat*}
then 

(i) $f$ is continuous function with $f(0)=-1$, $f(2)=1$ yet
there does not exist a $c$ with $f(c)=0$,

(ii) $f$ is a differentiable function with $f'(x)=0$ for
all $x$ yet $f$ is not constant.
\end{example}

What is the difference between ${\mathbb R}$ and ${\mathbb Q}$
which makes  
calculus work on one even though it fails on the other.
Both are `ordered fields', that is, both support operations
of `addition'
and `multiplication' together with a relation `greater than'
(`order') with the properties that we expect. If the
reader is interested she will find a complete list of 
the appropriate axioms in texts like the altogether
excellent book of Spivak~\cite{Spivak} and its
many rather less excellent competitors,
but, interesting as
such things may be, they are irrelevant to our purpose
which is not to consider the shared properties
of ${\mathbb R}$ and ${\mathbb Q}$ 
but to identify  a \emph{difference} between the
two systems which will enable us to exclude the
possibility of a function like that of Example~\ref{Rational}
for functions from ${\mathbb R}$ to ${\mathbb R}$.

To state the difference we need only recall a definition
from the beginning of the course~C5/6.
\begin{definition}\label{one convergence definition}
If $a_{n}\in{\mathbb R}$ for each $n\geq 1$
and $a\in{\mathbb R}$ then we say that $a_{n}\rightarrow a$
if given $\epsilon>0$ we can find an $n_{0}(\epsilon)$
such that
\[|a_{n}-a|<\epsilon\ \text{for all $n\geq n_{0}(\epsilon)$}.\]
\end{definition}
The key property of the reals, the \emph{fundamental axiom}
which makes everything work was also stated in 
the course~C5/6.
\begin{axiom}[The fundamental axiom of analysis]
If $a_{n}\in{\mathbb R}$ for each $n\geq 1$, $A\in{\mathbb R}$
and $a_{1}\leq a_{2}\leq a_{3}\leq \ldots$ and
$a_{n}<A$ for each $n$ then there exists an $a\in{\mathbb R}$
such that $a_{n}\rightarrow a$ as $n\rightarrow\infty$.
\end{axiom}
Less ponderously, and just as rigorously, the fundamental axiom
for the real numbers
says \emph{every increasing sequence bounded above tends
to a limit}.

Everything which depends on the fundamental axiom is
analysis, everything else is mere algebra.

\section{The axiom of Archimedes} In the course~C5/6 you
proved the following results on the limits of sequences.
\begin{lemma}\label{one sequences} 
(i) The limit is unique. That is, if $a_{n}\rightarrow a$
and $a_{n}\rightarrow b$ as $n\rightarrow\infty$
then $a=b$.

(ii) If $a_{n}\rightarrow a$ as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$a_{n(j)}\rightarrow a$ as $j\rightarrow\infty$.

(iii) If $a_{n}=c$ for all $n$ then $a_{n}\rightarrow c$
as $n\rightarrow\infty$.

(iv) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$ then
$a_{n}+b_{n}\rightarrow a+b$.

(v) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$ then
$a_{n}b_{n}\rightarrow ab$.

(vi) If $a_{n}\rightarrow a$
as $n\rightarrow\infty$ and $a_{n}\neq 0$ for each $n$,
$a\neq 0$ then $a_{n}^{-1}\rightarrow a^{-1}$.

(vii) If $a_{n}\leq A$ for each $n$ and
$a_{n}\rightarrow a$
as $n\rightarrow\infty$ then $a\leq A$.
\end{lemma}

Since these results were proved in the earlier course I shall
not prove them. However, it is \emph{essential} for
further progress that the reader should be able to prove
them for herself. If you can not prove these results
\emph{consult your supervisor at once}. You should do the
same if you cannot prove the following variation
on the fundamental axiom.
\begin{lemma}\label{Exercise 1.4} 
A decreasing sequence of real numbers bounded below tends
to a limit.
\end{lemma}
[Hint. If $a\leq b$ then $-b\leq -a$.]


Useful as the results of Lemma~\ref{one sequences} are,
they are also true of sequences in ${\mathbb Q}$.
They are therefore mere, if important, algebra.
Our first truly `analysis' result may strike the
reader as rather odd.
\begin{theorem}[Axiom of Archimedes]\label{Archimedes}
\[\frac{1}{n}\rightarrow 0\ \text{as $n\rightarrow\infty$}\]
\end{theorem}
Theorem~\ref{Archimedes} shows that there is no `exotic'
real number $\gimel$ say (to choose an exotic symbol)
with the property that $1/n>\gimel$ for all integers $n\geq 1$
yet $\gimel>0$ (that is, $\gimel$ is strictly positive
and yet smaller than all strictly positive rationals). 
There exist number systems with such exotic numbers
(the famous `non-standard analysis' of Abraham Robinson
and the `surreal numbers' of Conway constitute two
such systems) but, just as the rationals are, in
some sense, too
small a system for the standard theorems of analysis to hold
so these non-Archimedean systems are, in some sense,
too big. Archimedes and  Eudoxus
realised the need for an axiom
to show that there is no exotic number $\daleth$ bigger
than any integer\footnote{Footnote for passing historians,
this is a course in mathematics.}
(i.e. $\daleth>n$ for all integers $n\geq 1$;
to see the connection with our form of the axiom consider
$\gimel=1/\daleth$). However, in spite of its name, what
was an axiom for Archimedes is a theorem
for us. 
\begin{theorem} Given any real number $K$ we can find
an integer $n$ with $n>K$.
\end{theorem}
\section{The Bolzano--Weierstrass theorem} It is all very well
knowing that the fundamental axiom is the foundation
of mathematics but how can we find a proof technique
which will allow us to apply it.  One technique uses
the  Bolzano--Weierstrass theorem.
\begin{theorem}[Bolzano--Weierstrass]\label{one Bolzano}
If $x_{n}\in{\mathbb R}$ and there exists a $K$
such that $|x_{n}|\leq K$ for all $n$ then we can find
$n(1)<n(2)<\ldots$ and $x\in{\mathbb R}$ such that
$x_{n(j)}\rightarrow x$ as $j\rightarrow\infty$.
\end{theorem}
Recall from C5/6
that we say that a sequence converges if it tends to 
a limit.
The Bolzano--Weierstrass theorem thus
says that every bounded sequence of reals has a convergent
subsequence. Notice that we say nothing about uniqueness,
if $x_{n}=(-1)^{n}$ then $x_{2n}\rightarrow 1$ but
$x_{2n+1}\rightarrow -1$ as $n\rightarrow\infty$.

There is a proof of the Bolzano--Weierstrass theorem 
by repeated dissection along the lines of the proof
of the intermediate value theorem given in course~C5/6.
We shall give another proof based on the following
elegant combinatorial lemma.
\begin{lemma} If $x_{n}\in{\mathbb R}$ then at least
one of the following two statements must be true.

(A) There exist $n(1)<n(2)<\ldots$ such that
$x_{n(j)}\leq x_{n(j+1)}$ for each $j\geq 1$.

(B) There exist $m(1)<m(2)<\ldots$ such that
$x_{m(j)}\geq x_{m(j+1)}$ for each $j\geq 1$.
\end{lemma}

There are a variety of techniques for using the fundamental axiom:-
successive dissection, using Heine--Borel, looking for
the supremum and so on. When the reader is confident
in her control of the material in this course (and when she
knows the various techniques enumerated in
the last sentence) she should test her skill by
proving the main results of this course by using each
technique in turn. For the moment it seems sensible
to concentrate on one proof technique until
it is fully mastered. The technique chosen for this
exposition is that of Bolzano--Weierstrass.
We conclude this section with two important examples,
Theorem~\ref{supremum} on the existence of suprema
and Theorem~\ref{Cauchy} on the general principle of convergence.

It is an unfortunate fact of life that many, otherwise
perfectly well behaved sets of numbers do not have
greatest members (maxima). A simple example is
given by $\{x\in{\mathbb R}:0<x<1\}$. However,
we shall see in Theorem~\ref{supremum}
any non-empty bounded set of real numbers does have 
a least upper bound (supremum).
\begin{definition} Consider a non-empty set $A$ of real 
numbers. We say that $\alpha$ is a \emph{least upper bound}
(or \emph{supremum})
for $A$ if the following two conditions hold.

(i) $\alpha\geq a$ for all $a\in A$ (that is, $\alpha$ is
an upper bound).

(ii) If $\beta\geq a$ for all $a\in A$ then $\beta\geq\alpha$
(that is, $\alpha$ is no greater than any possible upper bound).
\end{definition}
\begin{lemma} The supremum is unique if it exists.
\end{lemma}

It is convenient to have the following alternative characterisation
of the supremum.
\begin{lemma} Consider a non-empty set $A$ of real 
numbers; $\alpha$ is a \emph{least upper bound}
for $A$ if and only if
the following two conditions hold.

(i) $\alpha\geq a$ for all $a\in A$.

(ii) Given $\epsilon>0$ there exists an $a\in A$ such that
$a+\epsilon\geq \alpha$.
\end{lemma}
We write $\sup A$ or $\sup_{a\in A}a$ for the supremum 
of $A$ if it exists.

Here is the promised theorem.
\begin{theorem}[Existence of the supremum]\label{supremum}
If $A$ is a non empty set of real numbers which is
bounded above (that is, there exists a $K$ such that
$a\leq K$ for all $a\in A$) then $A$ has a supremum.
\end{theorem}
That this is a genuine theorem of analysis is shown
by the fact that, if we work in $\mathbb Q$ the set
$\{x\in\mathbb Q:x^{2}<2\}$ has no least upper bound.
(See Question~\ref{square supremum} for a detailed
discussion.)

We leave it to the reader to define the \emph{greatest
lower bound} also called the \emph{infimum}
of a set $A$ and written $\inf A$ or $\inf_{a\in A}a$.
She should prove that
\[\inf_{a\in A}a=-\sup_{a\in A}(-a)\]
provided that either side of the equation exists.

As an example of its use, recall the following lemma
from course C5/6.
\begin{lemma} We work in ${\mathbb C}$.
If $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges
for $z=z_{0}$ then it converges for all $z$ with
$|z|<|z_{0}|$.
\end{lemma}
The use of Theorem~\ref{supremum} gives us a rather
transparent proof of the existence of the radius of convergence.
\begin{theorem} We work in ${\mathbb C}$. Consider
the sum $\sum_{n=0}^{\infty}a_{n}z^{n}$. Either
$\sum_{n=0}^{\infty}a_{n}z^{n}$ converges for all
$z$ (in this case we say that the series has infinite radius of
convergence) or there exists a real number $R\geq 0$
such that

(i) $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges for all $|z|<R$,

(ii) $\sum_{n=0}^{\infty}a_{n}z^{n}$ diverges for all $|z|>R$,

\noindent (in this case we say that the series has radius of
convergence R).
\end{theorem}

The nature of the supremum is illustrated by the
following example which we leave to the reader as an
exercise in the methods of the course C5/6. (As usual
contact your supervisor if you can not do it.)
\begin{example}\label{on and off}
(i) The sum $\sum_{n=1}^{\infty}n^{-2}z^{n}$
has radius of convergence $1$ and converges for all $|z|=1$.

(ii) The sum $\sum_{n=0}^{\infty}z^{n}$
has radius of convergence $1$ and diverges for all $|z|=1$.

(iii) The sum $\sum_{n=1}^{\infty}n^{-1}z^{n}$
has radius of convergence $1$, diverges for $z=1$
and converges for $z=-1$.
\end{example}
[When revising this part of the course you should also bear in
mind Theorem~\ref{Radius and uniform} discussed later.]

We turn now to the general principle of convergence.
\begin{definition}\label{one Cauchy sequence} 
We say that a sequence of real numbers
$x_{n}$ is a \emph{Cauchy sequence} if given any
$\epsilon>0$ we can find $n_{0}(\epsilon)$ such that
$|x_{p}-x_{q}|<\epsilon$ whenever $p,q\geq n_{0}(\epsilon)$.
\end{definition}
Our first lemma is merely algebraic. 
\begin{lemma} Any convergent sequence forms a Cauchy sequence.
\end{lemma}
The converse is a powerful theorem of analysis.
\begin{theorem} Any Cauchy sequence of real numbers
converges.
\end{theorem}
Combining the two results we get the general principle
of convergence.
\begin{theorem}[General principle of convergence]\label{Cauchy}
A sequence of real numbers converges if and only if
it is a Cauchy sequence.
\end{theorem}
The general principle of convergence is usually
too general to
be used in the same way as
the convergence tests of Course~C5/6
but has great theoretical importance as we shall see.

For those who can not wait here is a proof of the
uncountability of ${\mathbb R}$. The argument is
much closer to Cantor's original proof than the
standard one given in course C1. I present
it as a heavily starred exercise.
\begin{exercise}[Uncountability of the reals]%
\label{Cantor}
Let $y_{1}$, $y_{2}$, \dots be any
sequence of points in ${\mathbb R}$. Let $x_{0}=0$,
$\delta_{0}=1$.

(i) Show that you can construct inductively
a sequence of real numbers $x_{1}$, $x_{2}$, \dots
and positive numbers $\delta_{j}$ such that

\ \ (a) $|x_{n}-x_{n-1}|<\delta_{n-1}/4$,

\ \ (b) $x_{n}\neq y_{n}$,

\ \ (c) $0<\delta_{n}<|x_{n}-y_{n}|$,

\ \ (d) $\delta_{n}<\delta_{n-1}/4$.

(ii) Show that $\delta_{n+m}<4^{-m}\delta_{n}$ for $m,n\geq 0$
and deduce that the $x_{n}$ form a Cauchy sequence.
Conclude that $x_{n}$ tends to a limit $x$.

(iii) Show that $|x_{n+m}-x_{n}|<\delta_{n}/3$ for all 
$m,n\geq 0$. Deduce that $|x-x_{n}|\leq \delta_{n}/3$
for all $n\geq 0$. Why does this show that $y_{n}\neq x$
for all $n$?

(iv) Deduce that the real numbers are uncountable.
\end{exercise}

\section{Higher dimensions} In 1908, G.~H.~Hardy wrote
a textbook to introduce the new rigorous
analysis (or `continental analysis' as it was known
in a Cambridge even more insular than today)
to `first year students at the Universities
whose abilities approach something like what is
usually described as ``scholarship standard''\,'.
Apart from the fact that even the most hardened
analyst would now hesitate to call an introduction
to analysis  \emph{A Course of Pure Mathematics}
it is striking how close the book is in both
content and feel to a modern first course in
analysis. (And where there are changes it is
often not clear that the modern course\footnote{Indeed,
anyone who works through the exercises in Hardy
as a first course and the exercises in Whittaker
and Watson's even older 
\emph{A Course of Modern Analysis}~\cite{Whittaker}
as a second will have had a splendid education in analysis.}  
has the advantage.)
The only major difference is that Hardy only
studies the real line but later advances in mathematics
mean that we must now study analysis in ${\mathbb R}^{m}$
as well.

In the course C1/2 you saw that ${\mathbb R}^{m}$  has
a  Euclidean norm
\[\|{\mathbf x}\|=\left(\sum_{i=1}^{m}x_{j}^{2}\right)^{1/2}\]
(taking the positive square root) with the
following properties.
\begin{lemma}\label{Euclidean norm}
If $\|\ \|$ is the Euclidean norm on 
${\mathbb R}^{m}$  then

(i) $\|{\mathbf x}\|\geq 0$ for all ${\mathbf x}\in{\mathbb R}^{m}$,

(ii) If  $\|{\mathbf x}\|= 0$ then ${\mathbf x}={\mathbf 0}$,

(iii) If $\lambda\in{\mathbb R}$ and ${\mathbf x}\in{\mathbb R}^{m}$
then $\|\lambda {\mathbf x}\|=|\lambda|\|{\mathbf x}\|$.

(iv) (The triangle inequality)
If  ${\mathbf x},{\mathbf y}\in{\mathbb R}^{n}$
then $\|{\mathbf x}+{\mathbf y}\|
\leq\|{\mathbf x}\|+\|{\mathbf y}\|$.
\end{lemma}
The only non-trivial part of this lemma was the triangle inequality
which you proved using the Cauchy-Schwarz inequality.

In the course C5/6 you studied the notion of the
limit for sequences in ${\mathbb R}^{m}$.
\begin{definition}\label{many convergence definition}
If $\mathbf{a}_{n}\in{\mathbb R}^{m}$ for each $n\geq 1$
and $\mathbf{a}\in{\mathbb R}^{m}$ then we say that 
$\mathbf{a}_{n}\rightarrow \mathbf{a}$
if given $\epsilon>0$ we can find an $n_{0}(\epsilon)$
such that
\[\|\mathbf{a}_{n}-\mathbf{a}\|<\epsilon
\ \text{for all $n\geq n_{0}(\epsilon)$}.\]
\end{definition}
Notice that this shows that Definition~\ref{one convergence definition}
was about the \emph{distance} between two points and not
the absolute value of the difference of two numbers.

You proved the following results on sequences
in ${\mathbb R}^{m}$  in exactly the same way as
you proved the corresponding results in ${\mathbb R}$.
\begin{lemma}\label{many sequences}
(i) The limit is unique. That is, if 
$\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\mathbf{a}_{n}\rightarrow \mathbf{b}$
as $n\rightarrow\infty$
then $\mathbf{a}=\mathbf{b}$.

(ii) If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$\mathbf{a}_{n(j)}\rightarrow \mathbf{a}$ 
as $j\rightarrow\infty$.

(iii) If $\mathbf{a}_{n}=\mathbf{c}$ for all $n$ 
then $\mathbf{a}_{n}\rightarrow \mathbf{c}$
as $n\rightarrow\infty$.

(iv) If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\mathbf{b}_{n}\rightarrow\mathbf{b}$
as $n\rightarrow\infty$ then
$\mathbf{a}_{n}+\mathbf{b}_{n}
\rightarrow \mathbf{a}+\mathbf{b}$.

(v) Suppose $\mathbf{a}_{n}\in {\mathbb R}^{m}$,
$\mathbf{a}\in {\mathbb R}^{m}$,
$\lambda_{n}\in {\mathbb R}$
and $\lambda\in {\mathbb R}$. 
If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\lambda_{n}\rightarrow\lambda$ then
$\lambda_{n}\mathbf{a}_{n}\rightarrow 
\lambda\mathbf{a}$.


\end{lemma}
Once again I leave it to the reader to check that
she can indeed prove these results and to
consult her supervisor if she can not. She should also
make sure that she understands why certain results
in Lemma~\ref{one sequences} which dealt with
an ordered field do not appear in Lemma~\ref{many sequences}
which deals with a vector space.

Lemma~\ref{many sequences} is, of course, merely algebra
and applies to ${\mathbb Q}^{m}$ as much as to 
${\mathbb R}^{m}$. In order to do analysis we need
a more powerful tool  and, in keeping with the spirit
of this course, we extend the Bolzano--Weierstrass
theorem to ${\mathbb R}^{m}$. The extension is easy
since the real work has already been done in the one dimensional
case.
\begin{theorem}[Bolzano--Weierstrass]\label{many Bolzano}
If $\mathbf{x}_{n}\in{\mathbb R}^{m}$ and there exists a $K$
such that $\|\mathbf{x}_{n}\|\leq K$ for all $n$ then we can find
$n(1)<n(2)<\ldots$ and $\mathbf{x}\in{\mathbb R}$ such that
$\mathbf{x}_{n(j)}\rightarrow \mathbf{x}$ as $j\rightarrow\infty$.
\end{theorem}
Once again `any bounded sequence has a convergent subsequence'.

The proof of Theorem~\ref{many Bolzano} involves
extending a one dimensional result to several dimensions.
This is more or less inevitable because we stated the
fundamental axiom of analysis in a one dimensional form.
However the Bolzano--Weierstrass theorem itself 
contains no reference as to whether we are working in ${\mathbb R}$
or ${\mathbb R}^{m}$.  Thus \emph{exactly the same proofs}
as we used in the one dimensional case
give us a general principle of convergence in ${\mathbb R}^{m}$.
(Compare Definition~\ref{one Cauchy sequence} and
Theorem~\ref{Cauchy} with what follows.)
\begin{definition} We say that a sequence of points
$\mathbf{x}_{n}$ in ${\mathbb R}^{m}$ is a \emph{Cauchy sequence} 
if given any
$\epsilon>0$ we can find $n_{0}(\epsilon)$ such that
$\|\mathbf{x}_{p}-\mathbf{x}_{q}\|<\epsilon$ 
for all $p,q\geq n_{0}(\epsilon)$.
\end{definition}
\begin{theorem}[General Principle of Convergence]\label{many Cauchy}
A sequence in ${\mathbb R}^{m}$ converges if and only if
it is a Cauchy sequence.
\end{theorem}

Here is an example of the use of the general principle of convergence.
(Following the conventions of course C5/6 we say that 
$\sum_{j=1}^{\infty}\mathbf{a}_{j}$ converges if
the sequence of partial sums
$\sum_{j=1}^{N}\mathbf{a}_{j}$ tends to a limit  ${\mathbf s}$
say. We write $\sum_{j=1}^{\infty}\mathbf{a}_{j}={\mathbf s}$.)
\begin{theorem} Let $\mathbf{a}_{n}\in{\mathbb R}^{m}$ for each $n$.
If $\sum_{j=1}^{\infty}\|\mathbf{a}_{j}\|$ converges then so does
$\sum_{j=1}^{\infty}\mathbf{a}_{j}$.
\end{theorem}
This result substantially generalises the result in course C5/6
which says that an absolutely convergent series converges.
\section{Open and closed sets}\label{open and closed}
When we work in ${\mathbb R}$
the intervals are, in some sense, the `natural' sets
to consider. One of the problems that we face when we
try to do analysis in many dimensions is that the
types of sets with which we have to deal are much more
diverse. It turns out that the so called closed
and open sets are both sufficiently diverse
and sufficiently well behaved to be useful.
This short section is devoted to deriving some of their
simpler properties. Novices frequently find the
topic hard but eventually the reader will appreciate
that this section is a rather trivial interlude
in a deeper discussion.

The definition of a closed set is a natural one.
\begin{definition} A set $F\subseteq{\mathbb R}^{m}$
is closed if whenever $\mathbf{x}_{n}\in F$ for each
$n$ and $\mathbf{x}_{n}\rightarrow \mathbf{x}$
as $n\rightarrow\infty$ then $\mathbf{x}\in F$.
\end{definition}
Thus a set is closed in the sense of this course if
it is `closed under the operation of taking limits'.
An indication of why this is good definition is given
by the following version of the Bolzano--Weierstrass
theorem.
\begin{theorem} If $K$ is closed bounded set in
${\mathbb R}^{m}$ then every sequence in $K$ has
a subsequence converging to a point of $K$.
\end{theorem}
When working in ${\mathbb R}^{m}$ the words
`closed and bounded' should always elicit
the response `Bolzano--Weierstrass'. We shall
see important examples of this slogan in action
in the next section (Theorem~\ref{compact image}
and Theorem~\ref{uniform continuity}).

We turn now to the definition of an open set.
\begin{definition} A set $U\subseteq{\mathbb R}^{m}$
is open if whenever $\mathbf{x}\in U$ there exists
an $\epsilon({\mathbf x})>0$ such that whenever
$\|\mathbf{x}-\mathbf{y}\|<\epsilon({\mathbf x})$ we
have $\mathbf{y}\in U$.
\end{definition}
Thus every point of an open set lies `well inside the set'.
\begin{example} Consider sets in ${\mathbb R}$.
The interval $[a,b]=\{x\in{\mathbb R}:a\leq x \leq b\}$ is closed, 
the interval $(a,b)=\{x\in{\mathbb R}:a< x <b\}$ is open,
the interval $[a,b)=\{x\in{\mathbb R}:a\leq x <b\}$ is 
neither open nor closed, ${\mathbb R}$ is both open and
closed.
\end{example}
\begin{example} Consider sets in ${\mathbb R}^{m}$. Let
${\mathbf x}\in{\mathbb R}^{m}$ and $r>0$. 

(i) The set $B({\mathbf x},r)
=\{\mathbf{y}\in{\mathbb R}^{m}:
\|\mathbf{x}-\mathbf{y}\|<r\}$ is open.

(ii) The set $\bar{B}({\mathbf x},r)
=\{\mathbf{y}\in{\mathbb R}^{m}:
\|\mathbf{x}-\mathbf{y}\|\leq r\}$ is closed.
\end{example}
We call $B({\mathbf x},r)$ the open ball of radius $r$
and centre ${\mathbf x}$.
We call $\bar{B}({\mathbf x},r)$ the closed ball of radius $r$
and centre ${\mathbf x}$. Observe that the closed and and open
balls of ${\mathbb R}$ are precisely the closed
and open intervals.

The following restatement of the
definition helps us picture an open set.
\begin{lemma} A subset $A$ of ${\mathbb R}^{m}$ is open
if and only if each point of $A$ is the centre of
an open ball lying entirely within $A$.
\end{lemma}
Thus every point of an open set is surrounded by a ball
consisting only of points of the set.

The topics of this section are often treated
using the idea of \emph{neighbourhoods}. We shall
not use neighbourhoods very much but 
they come in useful from time to time.
\begin{definition} The set $N$ is a neighbourhood
of the point ${\mathbf x}$ if we can find an $r({\mathbf x})>0$
such that $B({\mathbf x},r({\mathbf x}))\subseteq N$.
\end{definition}
Thus a set is open if and only if it is neighbourhood
of every point that it contains.

Returning to the main theme we note the following
remarkable fact.
\begin{lemma}\label{complement open}
A subset $A$ of ${\mathbb R}^{m}$ is open
if and only if its complement ${\mathbb R}^{m}\setminus A$
is closed.
\end{lemma}
We observe the following basic results on open and closed sets.

\begin{lemma}\label{open familly} 
Consider the collection $\tau$ of open sets
in ${\mathbb R}^{m}$.

(i) $\emptyset\in\tau$, ${\mathbb R}^{m}\in \tau$.

(ii)  If $U_{\alpha}\in\tau$ for all $\alpha\in A$ then
$\bigcup_{\alpha\in A} U_{\alpha}\in\tau$.

(iii) If $U_{1},U_{2},\dots,U_{n}\in\tau$ then
$\bigcap_{j=1}^{n}U_{j}\in\tau$.
\end{lemma}
\begin{lemma}\label{closed familly}
Consider the collection $\mathcal{F}$ of closed sets
in ${\mathbb R}^{m}$.

(i) $\emptyset\in\mathcal{F}$, ${\mathbb R}^{m}\in \mathcal{F}$.

(ii)  If $F_{\alpha}\in\mathcal{F}$ for all $\alpha\in A$ then
$\bigcap_{\alpha\in A} F_{\alpha}\in\mathcal{F}$.

(iii) If $F_{1},F_{2},\dots,F_{n}\in\mathcal{F}$ then
$\bigcup_{j=1}^{n}F_{j}\in\mathcal{F}$.
\end{lemma}
In this context you should note the following examples.
\begin{example} (i) $\bigcap_{j=1}^{\infty}(-2-j^{-1},2+j^{-1})
=[-2,2]$, $\bigcap_{j=1}^{\infty}(-2+j^{-1},2-j^{-1})
=(-1,1)$, $\bigcap_{j=1}^{\infty}(-2+j^{-1},2+j^{-1})=(-1,2]$.

(ii) $\bigcup_{j=1}^{\infty}[-1-j^{-1},1+j^{-1}]
=[-2,2]$, $\bigcup_{j=1}^{\infty}[-1+j^{-1},1-j^{-1}]
=(-1,1)$, $\bigcup_{j=1}^{\infty}[-1+j^{-1},1+j^{-1}]=(-1,2]$.
\end{example}
Thus the restriction to finite collections
in part~(iii) of Lemmas~\ref{open familly}
and~\ref{closed familly} can not be removed.

So far in this course we have only dealt with
subsets and sequences. But analysis deals
with functions, or rather with `well behaved
functions'. The simplest such class was defined
in course C5/6.
\begin{definition}\label{definition continuity}
Let $E\subseteq{\mathbb R}^{m}$
we say that a function $f:E\rightarrow{\mathbb R}^{p}$
is continuous at some point ${\mathbf x}\in E$
if given $\epsilon>0$ we can find a 
$\delta(\epsilon,{\mathbf x})>0$ such that
whenever ${\mathbf y}\in E$ and
$\|{\mathbf x}-{\mathbf y}\|<\delta(\epsilon,{\mathbf x})$
we have 
\[\|f({\mathbf x})-f({\mathbf y})\|<\epsilon.\]
If $f$ is continuous at every point ${\mathbf x}\in E$
we say that $f$ is a continuous function on $E$.
\end{definition}
In other words $f$ is continuous at ${\mathbf x}$ if
$f({\mathbf y})$ can be made as close as we wish
to $f({\mathbf x})$ simply by taking ${\mathbf y}$ sufficiently
close to ${\mathbf x}$. In other words
$f$ is locally approximately constant. In less precise
terms `small changes produce small effects'.

\begin{exercise}\label{art of continuity} 
After looking at parts~(iii) to~(v)
of Lemma~\ref{many sequences} state the corresponding
results for continuous functions. (Thus part~(v) 
corresponds to the statement that if 
$\lambda:E\rightarrow{\mathbb R}$ and 
$f:E\rightarrow{\mathbb R}^{p}$ are continuous
at ${\mathbf x}\in E$ then so is $\lambda f$.)
Prove your statements directly from 
Definition~\ref{definition continuity}.
Consult your supervisor if you have any problems.
\end{exercise}

In this course we shall mainly use the following
simple consequence of the definition.
\begin{lemma} Let $E\subseteq{\mathbb R}^{m}$
and suppose that the function $f:E\rightarrow{\mathbb R}^{p}$
is continuous at ${\mathbf x}\in E$. Then if 
${\mathbf x}_{n}\in E$ and ${\mathbf x}_{n}\rightarrow{\mathbf x}$
it follows that $f({\mathbf x}_{n})\rightarrow f({\mathbf x})$.
\end{lemma}

However, more advanced work uses generalisations of the
following lemma. (Remember that if $f:X\rightarrow Y$ is a function
and $A\subseteq Y$
we write
\[f^{-1}(A)=\{x\in X\, :\, f(x)\in A\}\]
and this notation does not require $f$ to be invertible.)
\begin{lemma}\label{continuous open}
The function 
$f:{\mathbb R}^{m}\rightarrow {\mathbb R}^{p}$
is continuous if and only if $f^{-1}(O)$ is open
whenever $O$ is open.
\end{lemma}
As a simple example of how Lemma~\ref{continuous open}
can be used contrast the `$\epsilon$, $\delta$ proof'
of the next lemma
using Definition~\ref{definition continuity} with
a proof using  Lemma~\ref{continuous open}.
\begin{lemma}\label{composition} 
If $f:{\mathbb R}^{m}\rightarrow {\mathbb R}^{p}$
and $g:{\mathbb R}^{p}\rightarrow {\mathbb R}^{g}$
are continuous the so is their composition 
$g\circ f$.
\end{lemma}
(Recall that we write $g\circ f({\mathbf x})=g(f({\mathbf x}))$.)

Although we shall not make much use of it, the following
is an important consequence of the Bolzano--Weierstrass theorem.
\begin{theorem}\label{nested}
Suppose that $F_{1}$, $F_{2}$, \dots are 
non-empty bounded
closed sets in ${\mathbb R}^{m}$ such that 
$F_{1}\supseteq F_{2}\supseteq\dots$. 
Then $\bigcap_{j=1}^{\infty}F_{j}\neq\emptyset.$
\end{theorem}
That is, the intersection of bounded, closed, nested non-empty
sets is itself non-empty.
The example $F_{j}=[j,\infty)$ shows that `bounded'
can not be dropped from the hypothesis. 
The example $F_{j}=(0,j^{-1})$ shows that `closed'
can not be dropped from the hypothesis.

Finally, it is worth emphasising that openess is not an
intrinsic property of a set but depends on the space in which
the set finds itself.
\begin{example}\label{now you}  The set 
$\{(x,0)\in{\mathbb R}^{2}:0<x<1\}$ is not open
in ${\mathbb R}^{2}$.
\end{example}
(If you have a sufficiently logical mind Example~\ref{now you}
is otiose. Few of us have that kind of mind.)
\section{The central theorems of analysis} This section contains the core
of a first course in analysis. For completeness I shall include
some theorems already proved in course C5/6.
The first such theorem is the intermediate value theorem
proved in C5/6 by successive bisection. That proof is
entirely satisfactory but, as an exercise, we shall prove
the result using the method of Bolzano--Weierstrass.
\begin{theorem}[Intermediate value theorem]  If 
$f:[a,b]\rightarrow{\mathbb R}$ is continuous and
$f(a)\leq 0\leq f(b)$ then there exists a $c\in[a,b]$
such that $f(c)=0$.
\end{theorem}
You should compare this result with part~(i)
of Example~\ref{Rational}. Note also that
the result depends on $f$ taking values in ${\mathbb R}$
rather than ${\mathbb R}^{2}$ or ${\mathbb C}$.
\begin{example} If $f:[0,1]\rightarrow {\mathbb C}$ 
is given by $f(t)=\exp \pi it$ then $f$
is continuous,  $f(0)=1$, $f(1)=-1$ but $f(t)\neq 0$ for
all $t$.
\end{example}

Our next result looks a little abstract at first.
\begin{theorem}\label{compact image}
Let $K$ be a closed bounded subset of
${\mathbb  R}^{m}$ and $f:K\rightarrow {\mathbb  R}^{p}$
a continuous function. Then  $f(K)$ is closed and bounded.
\end{theorem}
Thus the continuous image of a closed bounded set is
closed and bounded. (The example $m=2$, $p=1$,
\[K=\{(x,y)\, : \, y\geq x^{-1}\ ,x>0\}\]
$f(x,y)=x$ shows that the continuous image of a closed
set need not be closed.
 
The example The example $m=1$, $p=1$, $K$ the open
interval $(0,1)$, $f(x)=x^{-1}$,
shows that the continuous image of a bounded
set need not be bounded.)

We derive a much more concrete corollary.
\begin{theorem}\label{maximum}
Let $K$ be a non-empty closed bounded subset of
${\mathbb  R}^{m}$ and $f:K\rightarrow {\mathbb  R}$
a continuous function. Then we can find ${\mathbf k}_{1}$
and ${\mathbf k}_{2}$ in $K$ such that
\[f({\mathbf k}_{1})\leq f({\mathbf k})\leq f({\mathbf k}_{2})\]
for all ${\mathbf k}\in K$.
\end{theorem}
In other words a real valued continuous function on a closed
bounded set is bounded and attains its bounds. Less usefully
we may say that, in this case $f$ actually has a maximum
and a minimum. Notice that there is no analogous result
for vector valued functions.  Much of economics consists
in an attempt to disguise this fact (there is unlikely to
be a state of the economy in which \emph{everybody}  is best
off\footnote{Of course economists know this but few of their
public pronouncements seem to mention it.}).

Theorem~\ref{maximum} has an even more concrete consequence.
\begin{theorem}\label{interval maximum}
Let $f:[a,b]\rightarrow {\mathbb  R}$
be a continuous function. Then we can find $k_{1},k_{2}\in [a,b]$ 
such that
\[f(k_{1})\leq f(k)\leq f(k_{2})\]
for all $k\in [a,b]$
\end{theorem}
In course C5/6 you used this result to prove  Rolle's
theorem.
\begin{theorem}[Rolle's theorem] If $f:[a,b]\rightarrow {\mathbb  R}$
is a continuous function with $f$ differentiable on $(a,b)$
and $f(a)=f(b)$ then we can find a $c\in (a,b)$ such that
$f'(c)=0$.
\end{theorem}
By `tilting Rolle's theorem' we obtain the mean value
(in one dimension).
\begin{theorem}[The one dimensional
mean value theorem]~\label{mean}
If $f:[a,b]\rightarrow {\mathbb  R}$
is a continuous function with $f$ differentiable on $(a,b)$
then we can find a $c\in (a,b)$ such that
\[f(b)-f(a)=(b-a)f'(c).\]
\end{theorem}
Historically, mathematicians were slow to grasp the
importance of the mean value theorem and the subtlety 
of its proof. Fallacious proofs still appeared in textbooks
(British textbooks, admittedly) in the early years of the 20th
century. (The proofs were fallacious in the strictest sense.
They `proved' the impossibility of the result given
in part~(ii) of Example~\ref{Rational}.)

As you saw in course C5/6, the mean value theorem  
Theorem~\ref{mean} has the following key corollaries.
\begin{theorem}\label{one growth} Suppose that
$f:(\alpha,\beta)\rightarrow {\mathbb  R}$
is a differentiable function.

(i) If $f'(x)\geq 0$ for all $x\in (\alpha,\beta)$
then $f$ is an increasing function 
on $(\alpha,\beta)$ (that is $f(x)\leq f(y)$ whenever
$\alpha<x<y<\beta$).

(ii) If $f'(x)> 0$ for all $x\in (\alpha,\beta)$
then $f$ is a strictly increasing function 
on $(\alpha,\beta)$ (that is $f(x)<f(y)$ whenever
$\alpha<x<y<\beta$).

(iii) If $f'(x)=0$ for all $x\in (\alpha,\beta)$
then $f$ is constant.

(iv) If $|f'(x)|\leq K$ for all $x\in (\alpha,\beta)$
then $|f(u)-f(v)|\leq K|u-v|$ for all $u,v\in (\alpha,\beta)$
\end{theorem}

Part~(iii) of of Theorem~\ref{one growth} is the result
that tells us that the solutions (if any)
of $f'(x)=g(x)$ on $(\alpha,\beta)$ differ only by
the addition of a constant (the so called
`constant of integration'). 

Part~(iv) of of Theorem~\ref{one growth} is an excellent
example of the way that the theorems of this section
(and many other theorems of analysis) convert \emph{local}
information into \emph{global} information. We know
that $|f'(x)|\leq K$ so that \emph{locally} the rate
of change of $f$ is no greater than $K$. We deduce that
$|f(u)-f(v)|\leq K|x-y|$ so that \emph{globally} the rate
of change of $f$ is no greater than $K$.

The final theorem of this section (Theorem~\ref{uniform continuity}
on uniform continuity) is another excellent example of the
conversion of local information to global. We need
a couple of definitions and examples. Recall first
from Definition~\ref{definition continuity} what
it means for a function to be continuous on a set
\begin{definition}\label{pointwise continuity}
Let $E\subseteq{\mathbb R}^{m}$
we say that a function $f:E\rightarrow{\mathbb R}^{p}$
is \emph{continuous} on $E$ if given any point ${\mathbf x}\in E$
and any $\epsilon>0$ we can find a 
$\delta(\epsilon,{\mathbf x})>0$ such that
whenever ${\mathbf y}\in E$ and
$\|{\mathbf x}-{\mathbf y}\|<\delta(\epsilon,{\mathbf x})$
we have 
\[\|f({\mathbf x})-f({\mathbf y})\|<\epsilon.\]
\end{definition}
Now compare that definition with our definition of
\emph{uniform} continuity.
\begin{definition}\label{definition uniform continuity}
Let $E\subseteq{\mathbb R}^{m}$
we say that a function $f:E\rightarrow{\mathbb R}^{p}$
is \emph{uniformly continuous} on $E$ 
if given any 
$\epsilon>0$ we can find a 
$\delta(\epsilon)>0$ such that
whenever ${\mathbf x},{\mathbf y}\in E$ and
$\|{\mathbf x}-{\mathbf y}\|<\delta(\epsilon)$
we have 
\[\|f({\mathbf x})-f({\mathbf y})\|<\epsilon.\]
\end{definition}

\begin{example} The following three functions are continuous
but not uniformly continuous.

(i) $f_{1}:{\mathbb R}\rightarrow{\mathbb R}$ given by
$f_{1}(x)=x^{2}$.

(ii) $f_{2}:(0,1)\rightarrow{\mathbb R}$ given by
$f_{2}(x)=x^{-1}$.

(iii) $f_{3}:(0,1)\rightarrow{\mathbb R}$ given by
$f_{2}(x)=\sin(x^{-1})$.
\end{example}
\begin{theorem}\label{uniform continuity}
Let $K$ be a closed bounded subset of
${\mathbb  R}^{m}$. If $f:K\rightarrow {\mathbb  R}^{p}$
is continuous on $K$ then $f$ is uniformly continuous
on K.
\end{theorem}

The results of this section are the foundations of all
analysis. It took 200 years to found analysis on the
same axiomatic basis as Greek geometry and the successful
completion of the project was a triumph of the human intellect.
If you can trace a compete path from the fundamental axiom
to the one dimensional mean value theorem you will
have mastered the essential point of this course.
\section{Differentiation}  A function is continuous if
it is locally approximately constant. A function is
differentiable if it is locally approximately linear.
More precisely, a function is continuous at a point ${\mathbf x}$
if it is locally approximately constant  with an error
which decreases to zero as we approach ${\mathbf x}$.
A function is
differentiable at a point ${\mathbf x}$
if it is locally approximately linear with an error
which decreases to zero \emph{faster than linearly}
as we approach  ${\mathbf x}$.
\begin{definition}\label{differentiation}
Suppose that $E$ is a subset of
of ${\mathbb R}^{m}$ and ${\mathbf x}$ a point such that
there exists a $\delta>0$ with 
$B({\mathbf x},\delta)\subseteq E$.
We say that $f:E\rightarrow  {\mathbb R}^{p}$,
is differentiable at ${\mathbf x}$ if we can find a linear
map $\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
such that, when $\|{\mathbf h}\|<\delta$
\begin{equation*}
f({\mathbf x}+{\mathbf h})=f({\mathbf x})+\alpha{\mathbf h}
+{\boldsymbol\epsilon}({\mathbf x},{\mathbf h})\|{\mathbf h}\|
\tag*{$\bigstar$}
\end{equation*}
where 
$\|{\boldsymbol\epsilon}({\mathbf x},{\mathbf h})\|\rightarrow 0$
as $\|{\mathbf h\|}\rightarrow 0$.
We write $\alpha=Df({\mathbf x})$
or $\alpha=f'({\mathbf x})$.

If $E$ is open and $f$ is differentiable at each point of
$E$ we say that $f$ is differentiable on $E$.
\end{definition}
Needless to say, the centre of the definition is
the formula $\bigstar$ and the reader should concentrate
on understanding the r\^{o}le of each term in that
formula. The rest of the definition is just supporting
waffle. Formula $\bigstar$ is sometimes rewritten
\[\frac{f({\mathbf x}+{\mathbf h})-f({\mathbf x})
-\alpha{\mathbf h}}{\|{\mathbf h}\|}\rightarrow 0\]
as $\|{\mathbf h}\|\rightarrow 0$.

You have already studied differentiation in
course C5/6 and you regularly use partial differentiation
in applied courses and elsewhere. I shall therefore
concentrate on recalling some of the key points.
\begin{lemma} Let $f$ be as in Definition~\ref{differentiation}.
If we use standard coordinates then
if $f$ is differentiable at ${\mathbf x}$ its
partial derivatives 
${\displaystyle  f_{i,j}({\mathbf x})=
\frac{\partial f_{i}}{\partial x_{j}}}$ exist and the
matrix of the derivative $Df({\mathbf x})$ is the 
Jacobian matrix $(f_{i,j}({\mathbf x}))$ of partial 
derivatives.
\end{lemma}
It is customary to point out that the existence
of the partial derivatives does not imply the
differentiability of the function (see Example~\ref{Jacob not}
below) but the main objections to over-reliance on
partial derivatives are that this makes formulae
cumbersome and stifles geometric intuition.
Let your motto be
{\bf `coordinates and matrices for calculation,
vectors and linear maps for understanding'.}

One key property of differentiation is the chain
rule. You have already seen this discussed and proved
in course C5/6.  Our discussion will run along the same lines
but is slightly neater.
\begin{lemma} If
$\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$ 
is a linear map we can find a constant $K$
such that $|\alpha{\mathbf x}|\leq K\|{\mathbf x}\|$
for all ${\mathbf x}\in {\mathbb R}^{m}$.
\end{lemma}
We can now make the following definition.
\begin{definition}\label{Definition operator norm}
If
$\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
is a linear map then
\[\|\alpha\|=\sup_{\|{\mathbf x}\|\leq 1}\|\alpha{\mathbf x}\|.\]
\end{definition}
The following easy exercise provides an equivalent definition.
\begin{exercise} Show that if
$\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
is a linear map then 
\[\|\alpha\|=\sup_{\|{\mathbf x}\|= 1}\|\alpha{\mathbf x}\|.\]
\end{exercise}
This `operator norm' has many pleasant properties.
\begin{lemma}\label{Operator norm}
Let  $\alpha,\beta:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
be linear maps.

(i) If ${\mathbf x}\in{\mathbb R}^{m}$ then
$\|\alpha{\mathbf x}\|\leq\|\alpha\|\,\|{\mathbf x}\|$.

(ii) $\|\alpha\|\geq 0$,

(iii) If  $\|\alpha\|= 0$ then $\alpha=0$,

(iv) If $\lambda\in{\mathbb R}$
then $\|\lambda \alpha\|=|\lambda|\|\alpha\|$.

(v) (The triangle inequality)
$\|\alpha+\beta\|\leq\|\alpha\|+\|\beta\|$.

(vi) If $\gamma:{\mathbb R}^{p}\rightarrow{\mathbb R}^{q}$
then $\|\gamma\alpha\|\leq\|\gamma\|\,\|\alpha\|$.
\end{lemma}

\begin{lemma}[The chain rule] Let $U$ be a neighbourhood
of ${\mathbf x}$ in ${\mathbb R}^{m}$, and
$V$ a neighbourhood of ${\mathbf x}$ in ${\mathbb R}^{p}$.
Suppose that $f:U\rightarrow V$ is differentiable
at ${\mathbf x}$ with derivative $\alpha$, that
$f:V\rightarrow {\mathbb R}^{q}$ is differentiable
at ${\mathbf y}$ with derivative $\beta$ and that
$f({\mathbf x})={\mathbf y}$. Then $g\circ f$ is
differentiable at ${\mathbf x}$ with derivative
$\beta\alpha$.
\end{lemma}
In more condensed notation
\begin{equation*}
D(g\circ f)({\mathbf x})=Dg(f({\mathbf x}))Df({\mathbf x}).
\tag*{$\bigstar\bigstar$}
\end{equation*}
It is important to see that formula $\bigstar\bigstar$
is exactly what we should expect and that its proof
is simple book-keeping.

\noindent{\bf Remark} One of the most troublesome
culture clashes between pure mathematics and applied
is that to an applied mathematician variables
like $x$ and $t$ have meanings such
as position and
time whereas to a pure mathematician 
all variables are `dummy variables' or `place-holders'
to be interchanged at will. To a pure mathematician
$v$ is an arbitrary function defined by its
effect on a variable so that $v(t)=At^{3}$ means
precisely the same thing as $v(x)=Ax^{3}$
whereas to an applied mathematician who thinks of
$v$ as a velocity the statements $v(t)=At^{3}$ and
$v(x)=Ax^{3}$ mean very different (indeed incompatible)
things. The applied mathematician writes
\[\frac{dv}{dt}=\frac{dv}{dx}\frac{dx}{dt}\]
but the nearest the pure mathematician can get
to this statement is
\[\frac{d\ }{dt}v(x(t))=v'(x(t))x'(t).\]
The same problems occur in still more confusing form
when we deal with partial derivatives. 
In particular the formula
\[\frac{\partial f}{\partial x_{i}}
=\sum_{k=1}^{n}\frac{\partial f}{\partial y_{k}}
\frac{\partial y_{k}}{\partial x_{i}}\]
makes no sense without using further
\emph{unspoken} conventions.
The only
remedy that I can suggest is `think like a pure mathematician
when doing pure mathematics and like an applied
mathematician when doing applied mathematics'.

The following simple result has a simple 
direct proof but
it is instructive to prove it by the chain rule.
\begin{lemma} Let $U$ be a neighbourhood
of ${\mathbf x}$ in ${\mathbb R}^{m}$.
Suppose that $f,g:U\rightarrow{\mathbb R}^{p}$
are differentiable at ${\mathbf x}$. Then
$f+g$ is differentiable at ${\mathbf x}$
and $D(f+g)({\mathbf x})=Df({\mathbf x})+Dg({\mathbf x})$. 
\end{lemma}

So far our study of differentiation in higher dimensions
has remained on the level of mere algebra. (The definition
of the operator norm used the supremum and so lay
deeper but we could have avoided the use of the
operator norm as was done in course C5/6.) The
next result is a true theorem of analysis.
\begin{theorem}[The mean value inequality] Suppose that
$U$ is an open set in ${\mathbb R}^{m}$ and that
$f:U\rightarrow{\mathbb R}^{p}$ is differentiable.
Consider the straight line segment 
\[L=\{(1-t){\mathbf a}+t{\mathbf b}:0\leq t\leq 1\}\]
joining ${\mathbf a}$ and ${\mathbf b}$. If $L\subseteq U$
(i.e. $L$ lies entirely within $U$) and
$\|Df({\mathbf x})\|\leq K$ for all 
${\mathbf x}\in L$ then
\[\|f({\mathbf a})-f({\mathbf b})\|
\leq K\|{\mathbf a}-{\mathbf b}\|.\]
\end{theorem}
The reader may be disappointed that a theorem involving
an equality in one dimension should be replaced by
one involving an inequality in many dimensions.
However, inspection of the use of the mean value
theorem in one dimension shows that its essential content
is the statement that `local growth bounds global
growth' and this statement is precisely the
mean value inequality. 

In any case the following example shows that we can
not hope for an exact analogue of the one dimensional
theorem.
\begin{example} Let $f:{\mathbb R}\rightarrow{\mathbb R}^{2}$
be given by $f(t)=(\cos t,\sin t)^{T}$. Then
$f(0)=f(2\pi)$ but $Df(t)\neq 0$ for all $t$.
\end{example}
It is also clear (though we shall not prove it,
and, indeed can not state it without using concepts
which we have not formally defined) that the
correct generalisation when $L$ is not a straight
line will run as follows.
`If  $L$ is a well behaved path lying entirely
within $U$  and
$\|Df({\mathbf x})\|\leq K$ for all
${\mathbf x}\in L$ then
$\|f({\mathbf a})-f({\mathbf b})\|
\leq K\times\operatorname{length}L$'.
\begin{example} Let
\[U=\{{\mathbf x}\in{\mathbb R}^{2}:\|{\mathbf x}\|>1\}
\setminus\{(x,0)^{T}:x\leq 0\}\}\]
If we take $\theta({\mathbf x})$ to be the unique solution 
of
\[\cos(\theta({\mathbf x}))=x,\ \sin(\theta({\mathbf x}))=y,
\ -\pi<\theta(\mathbf x)<\pi\]
for ${\mathbf x}=(x,y)^{T}\in U$ then
$\theta:U\rightarrow{\mathbb R}$ is everywhere differentiable
with $\|D\theta({\mathbf x})\|<1$. However
if $\mathbf{a}=(-1,10^{-1})^{T}$, $\mathbf{b}=(-1,-10^{-1})^{T}$
then 
\[|\theta(\mathbf{a})-\theta(\mathbf{b})|>                  
\|\mathbf{a}-\mathbf{b}\|.\]
\end{example}
\section{Local Taylor theorems} In this section we exploit
the mean value inequality to obtain information  about
the behaviour of reasonably well behaved functions
$f:{\mathbb R}^{m}\rightarrow{\mathbb R}$ in the neighbourhood
of a point ${\mathbf x}$. (Since we can always translate 
we shall sometimes put ${\mathbf x}={\mathbf 0}$ without
comment.) It may be worth alerting the reader to the
fact that we shall be using the one-dimensional
mean value inequality (see
Theorem~\ref{one growth}~(iv)) in forms like the following.
`If $|f_{,1}(x,c)|\leq K$  for some fixed $c$ and
all $a\leq x\leq b$ then $|f(a,c)-f(b,c)|\leq K|b-a|$.'
In this section we shall use row rather than column 
vectors.

Here is our first example.
\begin{theorem}[Continuity of partial derivatives implies
differentiability]\label{continuous first} Suppose $\delta>0$,
${\mathbf x}=(x,y)\in{\mathbb R}^{2}$,
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{2}$
and that $f:E\rightarrow{\mathbb R}$. If  the partial
derivatives $f_{,1}$ and $f_{,2}$ exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ then writing
\[f((x+h,y+k))=f(x,y)+f_{,1}(x,y)h+f_{,2}(x,y)k+
\epsilon(h,k)(h^{2}+k^{2})^{1/2}\]
we have $\epsilon(h,k)\rightarrow 0$ as
$(h^{2}+k^{2})^{1/2}\rightarrow 0$.  (In other words
$f$ is differentiable at ${\mathbf x}$.)
\end{theorem}
Although this is not one of the great theorems
of all time (or indeed of this course) it does provide
a useful short cut for proving functions differentiable.
The following easy extension is left to the reader.
\begin{theorem}
(i) Suppose $\delta>0$,
${\mathbf x}\in{\mathbb R}^{m}$
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{m}$
and that $f:E\rightarrow{\mathbb R}$. If  the partial
derivatives $f_{,1}$, $f_{,2}$, \dots $f_{,m}$ 
exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ then 
$f$ is differentiable at ${\mathbf x}$.

(ii)  Suppose $\delta>0$,
${\mathbf x}\in{\mathbb R}^{m}$
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{m}$
and that $f:E\rightarrow{\mathbb R}^{p}$. If  the partial
derivatives $f_{i,j}$ 
exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ 
$[1\leq i\leq p,\ 1\leq j\leq m]$ then 
$f$ is differentiable at ${\mathbf x}$.
\end{theorem}

The next result, although along similar lines, is
much more interesting. We write
\[f_{,ij}({\mathbf x})=(f_{,j})_{,i}({\mathbf x}),\]
or, in more familiar notation
\[f_{,ij}=\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}.\]
\begin{theorem}[Second order Taylor series]\label{2 Taylor}
Suppose $\delta>0$,
${\mathbf x}=(x,y)\in{\mathbb R}^{2}$
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{2}$
and that $f:E\rightarrow{\mathbb R}$. If  the partial
derivatives $f_{,1}$, $f_{,2}$, $f_{,11}$, $f_{,12}$,
$f_{,22}$ exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ then writing
\begin{align*}
f((x+h,y+k))=&f(x,y)+f_{,1}(x,y)h+f_{,2}(x,y)k\\
&+\tfrac{1}{2}(f_{,11}(x,y)h^{2}+2f_{,12}(x,y)hk
+f_{,22}(x,y)k^{2})+\epsilon(h,k)(h^{2}+k^{2})
\end{align*}
we have $\epsilon(h,k)\rightarrow 0$ as
$(h^{2}+k^{2})^{1/2}\rightarrow 0$.
\end{theorem}
We have the following important corollary.
\begin{theorem}[Symmetry of second partial derivative]%
\label{Symmetry derivative}
Suppose $\delta>0$,
${\mathbf x}=(x,y)\in{\mathbb R}^{2}$
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{2}$
and that $f:E\rightarrow{\mathbb R}$. If  the partial
derivatives $f_{,1}$, $f_{,2}$, $f_{,11}$, $f_{,12}$, $f_{,21}$
$f_{,22}$ exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ then 
$f_{,12}({\mathbf x})=f_{,21}({\mathbf x})$.
\end{theorem}
Stripped of all the fine (and for practical purposes
irrelevant) detail this tells us that we can  interchange
the order of partial differentiation for well behaved functions.

It is possible to produce plausibility arguments
for the symmetry of second partial derivatives.
Here are a couple.

(1) If $f$ is a multinomial, i.e.
$f(x,y)=\sum_{p=0}^{P}\sum_{q=0}^{Q}a_{p,q}x^{p}y^{q}$,
then $f_{,12}=f_{,21}$. But smooth functions are
very close to being polynomial so we would expect
the result to be true in general.

(2) Although we can not interchange limits in general
it is plausible that if $f$ is well behaved then
\begin{align*}
f_{,12}(x,y)&=\lim_{h\rightarrow 0}\lim_{k\rightarrow 0}
h^{-1}k^{-1}(f(x+h,y+k)-f(x+h,y)-f(x,y+k)+f(x,y))\\
&=\lim_{k\rightarrow 0}\lim_{h\rightarrow 0}
h^{-1}k^{-1}(f(x+h,y+k)-f(x+h,y)-f(x,y+k)+f(x,y))\\
&=f_{,21}(x,y).
\end{align*}

\noindent However, these are merely plausible arguments.
They do not make clear the role of the continuity
of the second derivative (in Example~\ref{symmetry fail}
we shall see that the result
may fail for discontinuous second partial derivatives).
More fundamentally they are \emph{algebraic} arguments
and, as the use of the mean value theorem indicates,
the result is one of \emph{analysis}. 

Theorems~\ref{continuous first} and~\ref{Symmetry derivative}
are inextricably linked in many examiners' minds with the
following two counter-examples. The present lecturer
would rather you forgot all the course than that
you forgot how to prove the theorems and remembered
the counter-examples but will not let his personal
feelings get in the way of his duty.

Suppose $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
has $f({\mathbf 0})=0$. We write 
$g(r,\theta)=f(r\cos\theta,r\sin\theta)$ 
(i.e. use polar coordinates). 
If $f$ is once differentiable then, to first order in $r$,

(i) $g(r,\theta)$ grows like $r$,

(ii) the contours $f(x,y)=c$ are parallel lines.

\noindent If we seek counter-examples it is reasonable
to look for an $f$ such that (i) is true but (ii) is not.
We might therefore be led to consider
$g(r,\theta)=r\sin 2\theta$. (Finding counter-examples
is more like tinkering with a meccano kit than anything else.)
\begin{example}\label{Jacob not} If
\begin{align*}
f(x,y)&=\frac{xy}{(x^{2}+y^{2})^{1/2}}\qquad\text{for $(x,y)\neq (0,0)$},\\
f(0,0)&=0,
\end{align*}
then $f$ has first partial derivatives everywhere,
$f$ is differentiable except at $(0,0)$ where
it is not.
\end{example}

Emboldened by our success we could well guess immediately
a suitable function to look for in the context of 
Theorem~\ref{Symmetry derivative}. If not
let us consider $f$ and $g$ as before. Suppose that
\[f({\mathbf 0})=f_{,1}({\mathbf 0})=f_{,2}({\mathbf 0})=0.\]
If $f$ obeys the conclusions of Theorem~\ref{2 Taylor}
(i.e. has a second order Taylor series) then,
to second order in $r$, 

(i) $g(r,\theta)$ grows like $r^{2}$,

(ii) the contours $f(x,y)=c$ are conics.

\noindent Once again
we look for an $f$ such that (i) is true but (ii) is not.
We might, for example, consider
$g(r,\theta)=r^{2}\sin 4\theta$.
\begin{example}\label{symmetry fail} If
\begin{align*}
f(x,y)&=\frac{xy(x^{2}-y^{2})}{(x^{2}+y^{2})}
\qquad\text{for $(x,y)\neq (0,0)$},\\
f(0,0)&=0,
\end{align*}
then $f$ has first and second partial derivatives everywhere
but $f_{,12}(0,0)\neq f_{,21}(0,0)$.
\end{example}     

It is profoundly unfortunate that the last contact
many mathematicians have with this topic is a couple
of complicated counter-examples. Multi-dimensional
calculus leads towards differential geometry and
infinite dimensional calculus (functional analysis).
Both subjects depend on \emph{understanding}
objects which we know to be well behaved but
which our limited geometric intuition makes
it hard for us to comprehend. Counter-examples
such as the ones just produced are simply irrelevant


What happens if a function is smooth (has partial
derivatives of all orders)? The following theorem
completes the discussion of this section. It is not
on the syllabus and left to the reader as an exercise
(but a very instructive one).
\begin{theorem}[The local Taylor's theorem]\label{local}
Suppose $\delta>0$,
${\mathbf x}\in{\mathbb R}^{m}$
$B({\mathbf x},\delta)\subseteq  E\subseteq {\mathbb R}^{m}$
and that $f:E\rightarrow{\mathbb R}^{p}$. If the
all partial
derivatives $f_{i,j}$, $f_{i,jk}$, $f_{i,jkl}$, \dots
exist in $B({\mathbf x},\delta)$
and are continuous at ${\mathbf x}$ then writing
\begin{align*}
f_{i}({\mathbf x}+{\mathbf h})=f_{i}({\mathbf x})&
+\sum_{j=1}^{m}f_{i,j}({\mathbf x})h_{j}+
\sum_{j=1}^{m}\sum_{k=1}^{m}f_{i,jk}({\mathbf x})h_{j}h_{k}\\
&+\dots+\text{sum up to $n$th powers}+
\epsilon_{i}(\mathbf{h})\|\mathbf{h}\|^{n}
\end{align*}
we have $\|{\boldsymbol\epsilon}(\mathbf{h})\|\rightarrow 0$ as
$\|\mathbf{h}\|\rightarrow 0$.
\end{theorem}

The wide awake reader may observe that I used (gasp,
horror!) coordinates in the statement of Theorem~\ref{local}.
However, the main formula
can be stated in a coordinate free way
as
\[
f({\mathbf x}+{\mathbf h})=f({\mathbf x})
+\alpha_{1}({\mathbf h})+\alpha_{2}({\mathbf h},{\mathbf h})+
+\dots+\alpha_{n}({\mathbf h},{\mathbf h},\dots,{\mathbf h})
+{\boldsymbol\epsilon}(\mathbf{h})\|\mathbf{h}\|^{n},
\]
where $\alpha_{k}:{\mathbb R}^{m}\times{\mathbb R}^{m}
\dots\times{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
is linear in each variable (i.e. a $k$-linear function).
For more details consult section~12
of chapter VIII of Dieudonn\'{e}'s
\emph{Foundations of Modern Analysis}~\cite{Dieudonn}
where the  higher derivatives are dealt with in a coordinate
free way. Like Hardy's book~\cite{Hardy}, Dieudonn\'{e}'s
is a masterpiece but in very different tradition.

Anyone who feels that the higher derivatives are best
studied using coordinates should reflect
that if $f:{\mathbb R}^{3}\rightarrow{\mathbb R}^{3}$
is well behaved then the `third derivative behaviour'
at of $f$ at a single point is apparently
given by the
$3\times3\times3\times3=81$ numbers
$f_{i,jkl}(\mathbf{x})$. By symmetry (see
Theorem~\ref{Symmetry derivative}) only $30$
of the numbers are distinct but these
$30$ numbers are independent  (consider
polynomials in three variables  for which
the total degree of each term is $3$).
How can we understand the information carried
by an array of $30$ real numbers?
\section{Riemann integration}\label{Riemann integration}
(Much of this section was sketched in the course C5/6
but this treatment will supply some important
extra detail.) Everybody knows what area is,
but then everybody knows what honey tastes like.
But does honey taste the same to you as it
does to me? Perhaps the question is unanswerable
but for many practical purposes it is sufficient
that we agree on what we call honey.
In the same way it is important that when
two mathematicians talk about area that
(1) they should agree on which sets $E$ actually have area,
and that
(2) when a set $E$ has area they should agree as
to what the area is.

At one time some mathematicians may well have
hoped that every bounded set in ${\mathbb R}^{2}$
could be given an area,
every bounded set in ${\mathbb R}^{3}$ could be given a volume,
and so on.
However Banach and Tarski showed that a ball
in ${\mathbb R}^{3}$ could be split into seven
parts which could be reassembled after rotation
and translation into a ball of smaller radius.
(The proof is non-trivial but there is a good discussion
in Wagon's book \emph{The Banach--Tarski Paradox}~\cite{Wagon}.)
If each of these seven parts had a volume then
repeated Banach Tarski decompositions would enable
us to get a quart into a pint pot.
Warned by this we shall not attempt to assign
area to every subset of ${\mathbb R}^{2}$ or
to integrate every function.

Turning specifically to the Riemann integral we
consider bounded functions $f:[a,b]\rightarrow{\mathbb R}$.
(Thus there exists a $K$ with $|f(x)|\leq K$ for all
$x\in[a,b]$). A dissection $\mathcal{D}$ of $[a,b]$
is a finite subset of $[a,b]$ containing the end
points $a$ and $b$. By convention we write
\[\mathcal{D}=\{x_{0},x_{1},\dots,x_{n}\}
\ \text{with}\ a=x_{0}\leq x_{1}\leq x_{2}\leq\dots\leq x_{n}=b.\]
We define the \emph{upper sum} and \emph{lower sum}
of $f$ associated with $\mathcal{D}$ by
\begin{align*}
S(f,\mathcal{D})=&\sum_{j=1}^{n}(x_{j}-x_{j-1})
\sup_{x\in [x_{j-1},x_{j}]}f(x),\\
s(f,\mathcal{D})=&\sum_{j=1}^{n}(x_{j}-x_{j-1})
\inf_{x\in [x_{j-1},x_{j}]}f(x)
\end{align*}

The next lemma is hardly more than an observation
but it is the key to the proper treatment of the integral.
\begin{lemma}[Key integration property] If
$f:[a,b]\rightarrow{\mathbb R}$
is bounded and
$\mathcal{D}_{1}$ and $\mathcal{D}_{2}$ are two dissections
then
\begin{equation*}
S(f,\mathcal{D}_{1})\geq S(f,\mathcal{D}_{1}\cup\mathcal{D}_{2})
\geq
s(f,\mathcal{D}_{1}\cup\mathcal{D}_{2})\geq s(f,\mathcal{D}_{2}).
\tag*{$\bigstar$}
\end{equation*}
\end{lemma}
The inequality $\bigstar$ tells us that, whatever dissection
you pick and whatever dissection I pick, your lower
sum can not exceed my upper sum. There is no way we
can put a quart in a pint pot and the Banach-Tarski
phenomenon is avoided.


Since $S(f,\mathcal{D})\geq -(b-a)K$
for all dissections $\mathcal{D}$
we can define the \emph{upper integral}
$I^{*}(f)=\inf_{\mathcal{D}}S(f,\mathcal{D})$.
We define the \emph{lower integral} similarly
as $I_{*}(f)=\sup_{\mathcal{D}}s(f,\mathcal{D})$.
The inequality $\bigstar$ tells us that these
concepts behave well.
\begin{lemma} If $f:[a,b]\rightarrow{\mathbb R}$
is bounded then $I^{*}(f)\geq I_{*}(f)$.
\end{lemma}
If $I^{*}(f)=I_{*}(f)$ we say that $f$ is Riemann
integrable and we write
\[\int_{a}^{b}f(x)\,dx=I^{*}(f).\]
We write $\mathcal{R}[a,b]$ or sometimes
just $\mathcal{R}$ for the set of Riemann integrable
functions on $[a,b]$.

The following lemma provides a convenient criterion
for Riemann integrability.
\begin{lemma}\label{Riemann criterion}
(i) A bounded function
$f:[a,b]\rightarrow{\mathbb R}$ is Riemann integrable
if and only if given any $\epsilon>0$ we can find
a dissection ${\mathcal D}$ with
\[S(f,{\mathcal D})-s(f,{\mathcal D})<\epsilon.\]

(ii) A bounded function
$f:[a,b]\rightarrow{\mathbb R}$ is Riemann integrable
with integral $I$
if and only if given any $\epsilon>0$ we can find
a dissection ${\mathcal D}$ with
\[S(f,{\mathcal D})-s(f,{\mathcal D})<\epsilon,
\ \text{and}\ S(f,{\mathcal D})\geq I\geq s(f,{\mathcal D}).\]
\end{lemma}
The reader who is tempted to start her treatment of
the Riemann integral with Lemma~\ref{Riemann criterion}~(ii)
should reflect that without the inequality $\bigstar$
she will find it hard to prove the uniqueness of $I$.

Even though we admit that we can not expect all functions
to be integrable, any satisfactory theory of integration
must have a large collection of integrable functions.
\begin{theorem}\label{Riemann class}
(i) Any monotonic (that is any increasing
or decreasing) function $f:[a,b]\rightarrow{\mathbb R}$
is Riemann integrable.

(ii) Any continuous function $f:[a,b]\rightarrow{\mathbb R}$
is Riemann integrable.
\end{theorem}
Part~(i) of Theorem~\ref{Riemann class} looks a little
odd. However most of the functions $f$ met with in `every day
life' including many discontinuous ones
are the difference of two increasing functions
(that is $f=f_{1}-f_{2}$ with $f_{1}$ and $f_{2}$ 
increasing). Such functions are called `functions of
bounded variation' and have a very pretty,
if old fashioned, theory of their own.
Lemma~\ref{add  Riemann} tells us that every
function of bounded variation is  Riemann 
integrable. It is worth noting that
Riemann did not have the theorem that every
continuous function on a closed bounded interval
is uniformly continuous and so was unable to
prove that every continuous function
is Riemann integrable.

The reader will recall from course C5/6 the standard
example of bounded function which is not Riemann
integrable.
\begin{example} If $f:[0,1]\rightarrow{\mathbb R}$ is
given by
\begin{align*} 
f(x)=0&\qquad\text{when $x$ is rational,}\\
f(x)=1&\qquad\text{when $x$ is irrational,}
\end{align*}
then $f$ is not Riemann integrable.
\end{example}

The following results are easy to prove.
\begin{lemma} Suppose that $a\leq c\leq b$ and
$f:[a,b]\rightarrow{\mathbb R}$. 
If $f|_{[a,c]}\in{\mathcal R}[a,c]$ and
$f|_{[c,b]}\in{\mathcal R}[c,b]$ then
$f\in{\mathcal R}[a,b]$ and
\[\int_{a}^{b}f(x)\,dx=\int_{a}^{c}f(x)\,dx
+\int_{c}^{b}f(x)\,dx .\]
\end{lemma}
If we adopt the convention that 
$\int_{b}^{a}f(x)\,dx=-\int_{a}^{b}f(x)\,dx$
then the formula
\[\int_{a}^{b}f(x)\,dx=\int_{a}^{c}f(x)\,dx
+\int_{c}^{b}f(x)\,dx \]
holds independently of the order of $a$, $b$ and $c$.
\begin{lemma}\label{add Riemann} 
If $\lambda,\mu\in{\mathbb R}$ and
$f,g:[a,b]\rightarrow{\mathbb R}$ are
Riemann integrable then so is $\lambda f+\mu g$
and 
\[\int_{a}^{b}\lambda f(x)+\mu g(x)\,dx
=\lambda\int_{a}^{b}f(x)\,dx+\mu \int_{a}^{b}g(x)\,dx.\]
\end{lemma}
In the language of linear algebra ${\mathcal R}[a,b]$
is a vector space and the integral is a linear functional
(i.e. a linear map from ${\mathcal R}[a,b]$ to
${\mathbb R}$.). 
\begin{lemma} If $f,g:[a,b]\rightarrow{\mathbb R}$ are
Riemann integrable and $f(x)\geq g(x)$ for all $x\in [a,b]$
then 
\[\int_{a}^{b}f(x)\geq \int_{a}^{b}g(x)\,dx.\]
\end{lemma}
\begin{lemma} If $f:[a,b]\rightarrow{\mathbb R}$ is
Riemann integrable then so is $|f|$.
\end{lemma}
The next result requires
a small idea.
\begin{lemma} (i) If $f:[a,b]\rightarrow{\mathbb R}$ is
Riemann integrable then so is $f^{2}$.

(ii) If $f,g:[a,b]\rightarrow{\mathbb R}$ are
Riemann integrable then so is $fg$.
\end{lemma}  

We are now in a position to prove the fundamental
theorem of the calculus.
\begin{theorem}[Fundamental theorem of the calculus]
Suppose that $u\in (a,b)$ and
$f:(a,b)\rightarrow{\mathbb R}$ is a
Riemann integrable function function which is
continuous at some $t\in(a,b)$. If we set
\[F(s)=\int_{u}^{s}f(x)\,dx\]
then $F$ is differentiable at $t$ and $F'(t)=f(t)$.
\end{theorem}

Sometimes we think of the fundamental theorem
in a slightly different way.
\begin{theorem} Suppose that $f:(a,b)\rightarrow{\mathbb R}$ is
continuous, that $u\in(a,b)$ and $c\in{\mathbb R}$.
Then there is a unique solution
to the differential equation $g'(t)=f(t)$ $[t\in(a,b)]$
such that $g(u)=c$.
\end{theorem}
Thus (under appropriate circumstances) integration
and differentiation are inverse operations and the
the theories of differentiation and integration
are subsumed in the greater theory of the calculus.
Under appropriate circumstances, if the graph of $F$
has tangent with slope $f(x)$ at $x$
\begin{align*}
\text{area under}&\ \text{the graph of slope of tangent of $F$}\\
&=\text{area under the graph of $f$}\\
&=\int_{a}^{b}f(x)\,dx=\int_{a}^{b}F'(x)\,dx=F(b)-F(a).
\end{align*}

Although it is not explicitly on the syllabus
the examiners seem deeply attached to the
next theorem. The result was stated in
course~C4 and the proof makes use of quite 
a lot of the methods developed in the present course.
\begin{theorem}[Differentiation under the integral]%
\label{under integral} 
Suppose $g:[a,b]\times[c,d]$ is
continuous and that the partial derivative $g_{2}$
exists and is continuous. Then writing 
$G(y)=\int_{a}^{b}g(x,y)\,dx$
we have $G$ differentiable on $(c,d)$ with
\[G'(y)=\int_{a}^{b}g_{,2}(x,y)\,dx.\]
\end{theorem}

In the last few years the examiners
have shown an excessive interest in the next lemma
probably because the proof
neatly combines several of our previous results.
\begin{lemma}~\label{double}
Suppose that $g:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is continuous and has continuous partial derivative
$g_{,2}$.

(i) If we set $G(x,y)=\int_{0}^{x}g(u,y)\,du$ then
$G$ is differentiable with partial derivatives
\[G_{,1}(x,y)=g(x,y)\ \text{and}
\ G_{,2}(x,y)=\int_{0}^{x}g_{,2}(u,y)\,du.\]

(ii) If we set $F(t)=\int_{0}^{t}g(u,t)\,du$ then
$F$ is differentiable with derivative
\[F'(t)=g(t,t)+\int_{0}^{t}g_{,2}(u,t)\,du.\]
\end{lemma}

\section{Further remarks on integration} We have
defined Riemann integration for bounded functions
on bounded intervals. However, the reader will already
have evaluated, as a matter of routine, integrals 
in the following manner
\[\int_{0}^{1}x^{-1/2}\,dx=\lim_{\epsilon\rightarrow 0+}
\int_{\epsilon}^{1}x^{-1/2}\,dx=
\lim_{\epsilon\rightarrow 0+}[2x^{1/2}]_{\epsilon}^{1}=2,\]
and
\[\int_{1}^{\infty}x^{-2}\,dx=\lim_{R\rightarrow \infty}
\int_{1}^{R}x^{-2}\,dx=
\lim_{R\rightarrow \infty}[-x^{-1}]_{1}^{R}=1.\]

A full theoretical treatment of such integrals with
the tools at our disposal is apt to lead into
a howling wilderness of `infinite integrals of the first kind',
`Cauchy principal values' and so on. Instead I shall give
a few typical theorems, definitions and counter-examples
from which the reader should be able to reconstruct
any parts of the theory that she needs.
\begin{definition} If $f:[a,\infty)\rightarrow{\mathbb R}$
is such that $f|_{[a,X]}\in{\mathcal R}[a,X]$ for each $X>a$
and $\int_{a}^{X}f(x)\,dx\rightarrow L$ as $X\rightarrow\infty$
then we say that $\int_{a}^{\infty}f(x)\,dx$ exists
with value $L$.
\end{definition}
\begin{lemma} Suppose $f:[a,\infty)\rightarrow{\mathbb R}$
is such that $f|_{[a,X]}\in{\mathcal R}[a,X]$ for each $X>a$.
If $f(x)\geq 0$ for all $x$ then
$\int_{a}^{\infty}f(x)\,dx$ exists if and only if
there exists a $K$ such that
$\int_{a}^{X}f(x)\,dx\leq K$ for all $X$.
\end{lemma}
\begin{lemma}\label{conditional}
Suppose $f:[a,\infty)\rightarrow{\mathbb R}$
is such that $f|_{[a,X]}\in{\mathcal R}[a,X]$ for each $X>a$.
If
$\int_{a}^{\infty}|f(x)|\,dx$ exists then
$\int_{a}^{\infty}f(x)\,dx$ exists.
\end{lemma}
It is natural to state Lemma~\ref{conditional} as saying
that `absolute convergence of the integral implies
conditional convergence'.

Additional problems arise when there are two limits
involved.
\begin{example} If $\lambda,\mu>0$ then
\[\int_{-\mu R}^{\lambda R}\frac{x}{1+x^{2}}\,dx
\rightarrow\log\frac{\lambda}{\mu}\]
as $R\rightarrow\infty$.
\end{example}
A pure mathematician gets round this problem
by making a definition along these lines.
\begin{definition} If $f:{\mathbb R}\rightarrow{\mathbb R}$
is such that $f|_{[-X,Y]}\in{\mathcal R}[X,Y]$ for each $X,Y>0$
$\int_{-\infty}^{\infty}f(x)\,dx$ exists
with value $L$ if and only if the following condition holds.
Given $\epsilon>0$ we can find an $X_{0}(\epsilon)>0$
such that
\[\left|\int_{-X}^{Y}f(x)\,dx-L\right|<\epsilon.\]
for all $X,Y>X_{0}(\epsilon)$.
\end{definition}
The physicist gets round the problem by ignoring it.
If she is a \emph{real physicist} with correct
physical intuition this works 
splendidly\footnote{In~\cite{Boas}
Boas reports the story of
a friend visiting the Princeton common room
`\dots where Einstein was talking to another man,
who would shake his head and stop him; Einstein
then thought for a while, then started talking
again; was stopped again; and so on. After
a while, \dots my friend was introduced to
Einstein. He asked Einstein who the other
man was. ``Oh,'' said Einstein, ``that's my mathematician.''\,'}
but if not, not.

Speaking broadly, infinite integrals $\int_{E}f(x)\,dx$
work well when they are absolutely convergent, that is
$\int_{E}|f(x)|\,dx<\infty$, but are full of traps for
the unwary otherwise.

So far we have dealt only with the integration
of functions $f:E\rightarrow{\mathbb R}$ with
$E$ a `well behaved' subset of ${\mathbb R}$.
It is not difficult to extend our definitions to
the full multidimensional case but the main users
of multidimensional integrals are either
applied mathematicians (in the broadest sense),
differential geometers and analysts (including probabilists).
The applied mathematicians are happy to take the
details on trust and both the differential geometers
and the analysts require more sophisticated approaches
(though of rather different kinds).

However the particular case of a function
$f:[a,b]\rightarrow{\mathbb R}^{m}$ is needed
in the complex variable courses (with $m=2$)
and elsewhere when line integrals are used.
The definition is simple.
\begin{definition} If $f:[a,b]\rightarrow{\mathbb R}^{m}$
is such that $f_{j}:[a,b]\rightarrow{\mathbb R}$
is Riemann integrable for each $j$ then 
$\int_{a}^{b}f(x)\,dx=\mathbf{y}$ where
${\mathbf y}\in{\mathbb R}^{m}$ and
\[y_{j}=\int_{a}^{b}f_{j}(x)\,dx.\]
\end{definition}
It is easy to obtain the properties of this
integral directly from its definition. Here
is an example.
\begin{lemma} If 
$\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{p}$
is linear and $f:[a,b]\rightarrow{\mathbb R}^{m}$
is Riemann integrable then so is $\alpha\circ f$
and
\[\int_{a}^{b}\alpha(f(x))\,dx=\alpha\left(
\int_{a}^{b}f(x)\,dx\right).\]
\end{lemma}
Taking $\alpha$ to be any orthogonal transformation
of ${\mathbb R}^{m}$ to itself we see that that
our definition is, in fact, coordinate independent.
Choosing a particular orthogonal transformation
we obtain the following nice result.
\begin{theorem} If $f:[a,b]\rightarrow{\mathbb R}^{m}$
is Riemann integrable then
\[\left\|\int_{a}^{b}f(x)\,dx\right\|
\leq (b-a)\sup_{x\in [a,b]}\|f(x)\|.\]
\end{theorem}
This result and its extensions are often quoted in the form
\[\text{integral}\leq\text{length}\times\text{$\sup$}.\] 
\begin{exercise} Show that the collection ${\mathcal R}$ of
Riemann integrable functions 
$f:[a,b]\rightarrow{\mathbb R}^{m}$ form
a real vector space. If we write
\[Tf=\int_{a}^{b}f(x)\,dx\]
show that $T:{\mathcal R}\rightarrow{\mathbb R}$
is a linear map and $|Tf|\leq (b-a)\|f\|$.
\end{exercise}

The Riemann integral is simple to define and easy to
use. So long as we only need to integrate continuous
functions (or functions continuous except at a few
simple discontinuities) it is perfectly satisfactory.
The great majority of mathematicians need nothing
else. However, the modern theory of probability
and many parts of modern analysis need the
more powerful integral of Lebesgue
(a hint as to why this might be is given
in Exercise~\ref{integral not complete}) and
its relatives. This is just as easy to use but
substantially harder to set up\footnote{I know
of two universities which teach undergraduates
Lebesgue theory in their
first year but of no university where
undergraduates learn Lebesgue theory in their
first year.}.
\section{Metric spaces}\label{section metric}
One of the unifying ideas
of modern analysis is that of distance.
We start with a definition modelled on those
properties of Euclidean distance which do not
refer to vector space structures. (Thus you
should both \emph{compare} and \emph{contrast}
Lemma~\ref{Euclidean norm}.)
\begin{definition}\label{metric}
We say that $(X,d)$ is a metric space if $X$ is
a set and $d:X^{2}\rightarrow\mathbb{R}$ is a function with the
following properties:-

(i) $d(x,y)\geq 0$ for all $x,y\in X$.

(ii) $d(x,y)=0$ if and only if $x=y$.

(iii) $d(x,y)=d(y,x)$ for all $x,y\in X$.

(iv) (The triangle inequality)
$d(x,z)\leq d(x,y)+d(y,z)$
\end{definition}
As ought to be the case, we have the following result.
\begin{lemma}~\label{natural Euclid}
If $d({\mathbf x},{\mathbf y})=\|{\mathbf x}-{\mathbf y}\|$
then $({\mathbb R}^{m},d)$ is a metric space.
\end{lemma}
I shall give later some examples of metric spaces which are
both useful and novel (in the context of this course).
The metric of Lemma~\ref{natural Euclid} is useful
but hardly novel. In the interest of balance 
let me give a metric which will be novel to most
of my readers even if it is not useful.
\begin{example}\label{Railway 1}
If 
$d({\mathbf x},{\mathbf y})=\|{\mathbf x}\|+\|{\mathbf y}\|$
when $\mathbf{x}\neq \mathbf{y}$ and 
$d({\mathbf x},{\mathbf x})=0$ then 
$({\mathbb R}^{m},d)$ is a metric space.
\end{example}
(This metric is called the British Railway metric.)
Here is an even odder railway metric.
\begin{example}\label{Railway 2}
If
${\mathbf x}$ and ${\mathbf y}$ are linearly dependent
(in more geometrical language) if 
${\mathbf x}$, ${\mathbf y}$ and ${\mathbf 0}$
are on the same straight line) then set
\[d({\mathbf x},{\mathbf y})=\|{\mathbf x}-{\mathbf y}\|.\]
Otherwise, set
\[d({\mathbf x},{\mathbf y})=\|{\mathbf x}\|+\|{\mathbf y}\|.\]
With this definition,
$({\mathbb R}^{m},d)$ is a metric space.
\end{example}

Surprising as it may seem the following rather trivial
metric is a rich source of intuition for communication
theory. (However it is of no analytic interest.)
\begin{lemma}[Hamming metric] Let  $M$ be the space
of sequences
\[{\mathbf x}=(x_{1},x_{2},\dots,x_{n})\]
with $x_{j}=0$ or $x_{j}=1$.  We call $M$ the space
of messages of length $n$. If  ${\mathbf x}$ and
${\mathbf y}$ are messages we define
\[d({\mathbf x},{\mathbf y})=\sum_{j=1}^{n}|x_{j}-y_{j}|\]
to be the \emph{Hamming distance} between the two messages.
$(M,d)$ is a metric space.
\end{lemma}

Much of the `mere algebra' which we did for the
Euclidean metric carries over with hardly any
change to the general metric case. Compare
the following definition with 
Definition~\ref{many convergence definition}.)
\begin{definition}\label{metric convergence definition}
Let $(X,d)$ be a metric space.
If $a_{n}\in X$ for each $n\geq 1$
and $a\in X$ then we say that 
$a_{n}\rightarrow a$
if given $\epsilon>0$ we can find an $n_{0}(\epsilon)$
such that
\[d(a_{n},a)<\epsilon
\ \text{for all $n\geq n_{0}(\epsilon)$}.\]
\end{definition}
However, we are not now dealing with a `norm 
on a vector space' so some results do not carry over.
The result corresponding to Lemma~\ref{many sequences}
has far fewer parts.
\begin{lemma}\label{metric sequences}
Let $(X,d)$ be a metric space.

(i) The limit is unique. That is, if 
$a_{n}\rightarrow a$
and $a_{n}\rightarrow b$
as $n\rightarrow\infty$
then $a=b$.

(ii) If $a_{n}\rightarrow a$
as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$a_{n(j)}\rightarrow a$ 
as $j\rightarrow\infty$.

(iii) If $a_{n}=c$ for all $n$ 
then $a_{n}\rightarrow c$
as $n\rightarrow\infty$.
\end{lemma}

The material on open and closed sets from 
Section~\ref{open and closed} goes through 
essentially unchanged.
\begin{definition} Let $(X,d)$ be metric space.
A set $F\subseteq X$
is closed if whenever $x_{n}\in F$ for each
$n$ and $x_{n}\rightarrow x$
as $n\rightarrow\infty$ then $x\in F$.
\end{definition}
\begin{definition} Let $(X,d)$ be metric space
A set $U\subseteq X$
is open if whenever $x\in U$ there exists
an $\epsilon>0$ such that whenever
$d(x,y)<\epsilon$ we
have $y\in U$.
\end{definition}
\begin{example} Let $(X,d)$ be metric space.
Let $x\in X$ and $r>0$. 

(i) The set $B(x,r)
=\{y\in X:d(x,y)<r\}$ is open.

(ii) The set $\bar{B}(x,r)
=\{y\in X:d(x,y)\leq r\}$ is closed.
\end{example}
We call $B(x,r)$ the open ball of radius $r$
and centre $x$.
We call $\bar{B}(x,r)$ the closed ball of radius $r$
and centre $x$.
\begin{lemma} Let $(X,d)$ be a metric space.
A subset $A$ of $X$ is open
if and only if each point of $A$ is the centre of
an open ball lying entirely within $A$.
\end{lemma}
\begin{definition} The set $N$ is a neighbourhood
of the point $x$ if we can find an $r>0$
such that $B(x,r)\subseteq N$.
\end{definition}
\begin{lemma}\label{complement open metric}
Let $(X,d)$ be a metric space.
A subset $A$ of $X$ is open
if and only if its complement $X\setminus A$
is closed.
\end{lemma}
\begin{lemma}\label{open familly metric}
Let $(X,d)$ be a metric space. 
Consider the collection $\tau$ of open sets
in $X$.

(i) $\emptyset\in\tau$, $X\in \tau$.

(ii)  If $U_{\alpha}\in\tau$ for all $\alpha\in A$ then
$\bigcup_{\alpha\in A} U_{\alpha}\in\tau$.

(iii) If $U_{1},U_{2},\dots,U_{n}\in\tau$ then
$\bigcap_{j=1}^{n}U_{j}\in\tau$.
\end{lemma}
\begin{lemma}\label{closed familly metric}
Let $(X,d)$ be a metric space.
Consider the collection $\mathcal{F}$ of closed sets
in $X$.

(i) $\emptyset\in\mathcal{F}$, $X\in \mathcal{F}$.

(ii)  If $F_{\alpha}\in\mathcal{F}$ for all $\alpha\in A$ then
$\bigcap_{\alpha\in A} F_{\alpha}\in\mathcal{F}$.

(iii) If $F_{1},F_{2},\dots,F_{n}\in\mathcal{F}$ then
$\bigcup_{j=1}^{n}F_{j}\in\mathcal{F}$.
\end{lemma}
 
The new definition of continuity only breaks the chain
of translations slightly because it involves 
\emph{two} metric spaces.
\begin{definition}\label{definition continuity metric}
Let $(X,d)$ and $(Z,\rho)$ be metric spaces.
We say that a function $f:X \rightarrow Z$
is continuous at some point $x\in X$
if given $\epsilon>0$ we can find a 
$\delta(\epsilon,x)>0$ such that
if $y\in X$ and
$d(x,y)<\delta(\epsilon,x)$
we have 
\[\rho[(f(x),f(y))<\epsilon.\]
If $f$ is continuous at every point $x\in X$
we say that $f$ is a continuous function on $X$.
\end{definition}
The reader may feel that 
Definition~\ref{definition continuity} is more general 
in than Definition~\ref{definition continuity metric}
because it involves a set $E$.

The following remark shows that this is not so.
\begin{lemma}\label{restriction metric}
Let $(X,d)$ be a metric space and $E\subseteq X$.
Let $d_{E}:E^{2}\rightarrow{\mathbb R}$ be given
by $d_{E}(u,v)=d(u,v)$ whenever $u,v\in E$. Then
$(E,d_{E})$ is a metric space.
\end{lemma}

We conclude this section with more results
taken directly from Section~\ref{open and closed}
\begin{lemma} Let $(X,d)$ and $(Z,\rho)$
be metric spaces
and suppose that the function $f:X\rightarrow Z$
is continuous at $x\in X$. Then if 
$x_{n}\in X$ and $x_{n}\rightarrow x$
it follows that $f(x_{n})\rightarrow f(x)$.
\end{lemma}
\begin{lemma}\label{continuous open metric}
Let $(X,d)$ and $(Z,\rho)$
be metric spaces. 
The function $f:X\rightarrow Z$
is continuous if and only if $f^{-1}(O)$ is open
whenever $O$ is open.
\end{lemma}
\begin{lemma} Let $(X,d)$ $(Y,\theta)$
and $(Z,\rho)$ be metric spaces.
If $f:X\rightarrow Y$
and $g:Y\rightarrow Z$
are continuous then so is their composition 
$g\circ f$.
\end{lemma}

Question~\ref{Odd railway} which asks you to look at
open balls and open sets in the  two railway metrics
given in
Examples~\ref{Railway 1} and~\ref{Railway 2}
emphasises the fact that open sets in different metrics
may be very different.

\section{A look to the future} In this short starred
section I look at a very important class of metrics.
The discussion will be informal, as indicated by the
replacement of the word `Theorem' by `Statement'.

Suppose we wish to build a road joining two points of the
plane. The cost of a building will depend on the
nature of the terrain. If the cost of building 
a short stretch of length $\delta s$ near a point $(x,y)$
is (to first order) $g(x,y)\delta s$ then
subject to everything being well behaved the cost
of  road  $\Gamma$ will be 
\[\int_{\Gamma}g(x,y)ds.\]
`But' cries the reader `you have not defined line
integrals yet!' To which I reply `Mathematical discourse
takes place on many levels and we must learn to move
freely between them'\footnote{`Shut up!' he explained.}.
It is tempting to define the distance $d(A,B)$ between
two points $A$ and $B$ in the plane as
\[d(A,B)=\inf\left\{\int_{\Gamma}g(x,y)ds:
\text{$\Gamma$ joining $A$ and $B$}\right\}.\]

If we include amongst our conditions that
$g$ is continuous and $g(x,y)>0$ everywhere
we see that $d$ is a metric.
\begin{statement}\label{geodesic distance}
(i) $d(A,B)\geq 0$ for all $A$ and $B$.

(ii) $d(A,B)=0$ if and only if $A=B$.

(iii) $d(A,B)=d(B,A)$ for all $A$ and $B$.

(iv) $d(A,C)\leq d(A,B)+d(B,C)$ for all $A$, $B$ and $C$.
\end{statement}

I have rather carefully kept to `$\inf$' rather than to `$\min$'.
One problem is that I have not specified the kind of
$\Gamma$ that is permissible. (Notice that the argument
we used to prove Statement~\ref{geodesic distance}
means that we could not just use `smooth'.)
However, it can be shown that if we choose a suitable
class for the possible $\Gamma$ and a suitably
well behaved $g$ then the minimum is attained.
If $\Gamma_{0}$ is a path from $A$ to $B$
such that
\[\int_{\Gamma_{0}}g(x,y)ds=d(A,B)\]
we call $\Gamma_{0}$ a geodesic. (The geodesic need
not be unique, consider road building for
two towns at diametrically
opposite points of a circular marsh.)
If any two points are joined by a geodesic then
$d(A,B)$ is the `length of the geodesic path joining
$A$ and $B$' where length refers to the metric $d$
and not to the Euclidean metric.

Let us try to use these ideas to find a metric
on the upper half-plane 
\[H=\{z\in{\mathbb C}:\Im z>0\}\]
which is invariant under M\"{o}bius transformation.
(See course C1/2 for background). More precisely
we want a metric $d$ such that if $T$ is a 
M\"{o}bius map mapping $H$ to itself bijectively,
then $d(Tz_{1},Tz_{2})=d(z_{1},z_{2})$ for all
$z_{1},z_{2}\in H$. Of course no such map may
exist but we shall see where the question leads.

\begin{statement} The set $\mathcal{H}$
of M\"{o}bius maps $T$ such that $T|_{H}:H\rightarrow H$
is bijective
is a subgroup of the group $\mathcal{M}$
of all M\"{o}bius transformations.
The subgroup $\mathcal{H}$ is generated by
the transformations $T_{a}$ with $a\in{\mathbb R}$,
$D_{\lambda}$ with $\lambda>0$ and $J$ where
\begin{align*}
T_{a}=&a+z\\
D_{\lambda}(z)=&\lambda z\\
J(z)=&-z^{-1}
\end{align*}
\end{statement}
By looking at the effect of elements of $\mathcal{H}$
on a small arc, we are led to the following.
(Throughout we write write $z=x+iy$ and identify
the plane ${\mathbb R}^{2}$ with ${\mathbb C}$ in
the usual manner.)
\begin{statement} Suppose $g:H\rightarrow {\mathbb R}$ is well 
behaved. Then
\[\int_{T(\Gamma)}g(x,y)ds=\int_{\Gamma}g(x,y)ds\]
for all nice paths $\Gamma$ in $H$ and all $T\in{\mathcal H}$
if and only if $g(x,y)=Ay^{-1}$ for some  $A>0$.
\end{statement}

Let us now fix $g(x,y)=y^{-1}$  and let $d$ be the
derived metric. 
\begin{statement} The metric just
described is invariant under M\"{o}bius transformation.
\end{statement}
We can use the calculus of
variations (course C10, Mathematical Methods) to find geodesics.
\begin{statement} The geodesics for the metric just
described are arcs of circles with centres
on the real axis and straight lines
which cut the real axis at right angles.
\end{statement}
Those who are interested in the axiom of parallels
for Euclidean geometry will note that, for $(H,d)$,
if we call circles with centres
on the real axis and straight lines
which cut the real axis at right angles
geodesic lines for $(H,d)$,  then given
any geodesic line $\Gamma$ and any point $A$ in $H$
not on $\Gamma$ we can find many  geodesic
lines through $A$ which do not intersect $\Gamma$.
More details are given in course  D1 (Geometry).

Of course the methods of C10 are indicative rather
than conclusive. (We have used a \emph{necessary}
condition and not a \emph{sufficient} one.)
However, in this particular case, it is not hard,
once the answer is known, to state and prove everything
rigorously. If we want general underpinning theorems
they can be found in Differential Geometry
but require a lot of hard work.

There is an interesting analogue of this kind
of metric for groups.
\begin{lemma} Let $G$ be a group and $A$ a subset
of $G$ generating $G$.  Write $A^{-1}=\{a^{-1}:a\in A\}$.
If we set
\[d(x,y)=\min\{n:
\text{we can find $a_{1},a_{2},\dots,a_{n}\in A\cup A^{-1}$
with $a_{1}a_{2}\dots a_{n}=xy^{-1}$}\},\]
then $d$ is a metric on $G$ with $d(xz,yz)=d(x,y)$
for all $x,y,z\in G$. (Thus $d$ is a right translation
invariant metric.)
\end{lemma}
If $d(x,y)$ is bounded we call $\max d(x,y)$ the
\emph{diameter} of the group. The notion of diameter
has an obvious relevance to the problem
`How many shuffles do we need to produce a well shuffled
pack?'.


\section{Completeness} If we examine the arguments
of Section~\ref{section metric}  we see that they
are all mere algebra. What do we need to
introduce to do genuine analysis on metric spaces.
We can not use a variant of the fundamental axiom
because there is no order on our spaces\footnote{There
is an appropriate theory  for objects with order
(lattices) but we shall not pursue it here.}. 
The correct variant of the Bolzano--Weierstrass
method is, it is generally agreed, the notion of compactness
which will be studied in the context of topological
spaces (a concept more general than metric spaces)
in course C~12. Instead we use a generalisation of
the general principle of convergence.
\begin{definition} If $(X,d)$ is a metric space
we say that a sequence of points $x_{n}\in X$
is Cauchy if given any $\epsilon>0$ we
can find $n_{0}(\epsilon)$ such that
$d(x_{p},x_{q})<\epsilon$ for all $p,q\geq n_{0}(\epsilon)$.
\end{definition}
\begin{definition} A metric space $(X,d)$ is complete
if every Cauchy sequence converges.
\end{definition}
Of course, ${\mathbb R}^{n}$ with the Euclidean metric
is complete.

To show that the notion of completeness is distinct
from `Bolzano--Weierstrass notions' we introduce a dull but
useful metric space.
\begin{lemma} Let $X$ be any set. If we define
$d:X^{2}\rightarrow\mathbb{R}$ by
\begin{align*}
d(x,y)=&1\qquad\text{if $x\neq y$}\\
d(x,x)=&0
\end{align*}
then  $(X,d)$ is a metric space.
\end{lemma}
We call the metric $d$ of the previous lemma the
\emph{discrete metric}.
\begin{lemma} Let $(X,d)$ be a space with the discrete
metric. Then $(X,d)$ is complete.

Suppose $X$ is infinite. Then although $X$ is bounded
(if $x_{0}\in X$ then $\bar{B}(x_{0},1)=X$)
we can find a sequence $x_{n}$ such that no subsequence converges.
\end{lemma}
Here is another property of the discrete metric.
\begin{lemma} If $(X,d)$ is a space with the discrete
metric then every subset of $X$ is both open and closed.
\end{lemma}

The contraction mapping theorem (Theorem~\ref{contraction})
and its applications will provide a striking example
of the utility of the concept of completeness. However this section
and the next are devoted more to examples of spaces
which are and are not complete. Students
who want to see completeness in action immediately
should do the next, starred, example.

\begin{exercise}
We say that a metric space $(X,d)$ has no isolated points
if given $y\in X$ and $\epsilon>0$ we can find
an $x\in X$ such that $0<d(x,y)<\epsilon$. Show
by the methods of Exercise~\ref{Cantor} that
a non-empty metric space with no isolated points
is uncountable.

Give an example of an infinite countable metric space.
Give an example of an uncountable metric space
all of whose points are isolated.
\end{exercise}

The next lemma gives a good supply of metric spaces
which are complete and of metric spaces
which are not complete. 
\begin{lemma}\label{closed complete}
Let $(X,d)$ be a complete metric space.
If $E$ is a subset of $X$ and we define 
$d_{E}:E^{2}\rightarrow{\mathbb R}$
by $d_{E}(u,v)=d(u,v)$ whenever $u,v\in E$
then $(E,d_{E})$ is complete if and only if
$E$ is closed in $(X,d)$.
\end{lemma}
Thus, for example, the closed interval $[a,b]$
is complete for the usual metric but the
open interval $(a,b)$ is not.

Here is a more interesting example of a metric which 
is not complete.
\begin{example}\label{one norm} 
Consider $C([-1,1])$ the set of continuous
functions $f:[-1,1]\rightarrow{\mathbb R}$. If we set
\[d(f,g)=\|f-g\|_{1}=\int_{-1}^{1}|f(x)-g(x)|\,dx\]
then $d$ is a metric. Let
\begin{alignat*}{2}
f_{n}(x)&=-1&&\qquad\text{for $-1\leq x\leq -1/n$}\\
f_{n}(x)&=nx&&\qquad\text{for $-1/n\leq x\leq 1/n$}\\
f_{n}(x)&=1&&\qquad\text{for $1/n\leq x\leq 1$}.
\end{alignat*}
The sequence $f_{n}$ is Cauchy but has no limit.
Thus $(C([-1,1]),d)$ is not complete.
\end{example}

The example just given leaves open the possibility
that the same type of metric might be be complete
when applied to Riemann integrable functions.
The following example which is long, heavily starred,
and only for the ambitious shows that this is not so.
\begin{exercise}\label{integral not complete}
Consider ${\mathcal R}([-1,1])$ the set of
Riemann integrable functions 
functions $f:[-1,1]\rightarrow{\mathbb R}$. If we set
\[d(f,g)=\|f-g\|_{1}=\int_{-1}^{1}|f(x)-g(x)|\,dx\]
show that $d$ satisfies conditions~(i), (iii) and
(iv) of the definition of a metric (see Definition~\ref{metric})
but give an example to show that condition~(ii)
may fail.  We could now stop since $d$ is not a metric
but this would be to miss the interesting part.

In many ways, condition~(ii) is the least important
part of the definition of a metric. We call something
satisfying all the other conditions a \emph{pseudo-metric}.
We can define limits for a pseudo-metrics in the same way
as for metrics. 

If $(X,\rho)$ is a pseudo-metric space 
show that if $x_{n}\rightarrow x$
then $x_{n}\rightarrow y$ if and only if $\rho(x,y)=0$.

In the same way we can define Cauchy sequences. A
pseudo-metric space is called complete if every
Cauchy sequence converges. Our object is to show
that $({\mathcal R}([-1,1]),d)$ is not complete.

(i) Set
\begin{alignat*}{2} 
\Delta_{n}(x)=&0&&\qquad\text{if $2^{-3n}\leq |x|$}\\
\Delta_{n}(x)=&1-2^{3n}|x|&&\qquad\text{if $|x|\leq 2^{-3n}$.}
\end{alignat*}
and $g_{n}(t)=\sum_{r=-2^{n}}^{2^{n}}\Delta_{n}(t-r2^{-n})$
for all $-1\leq t\leq 1$. Sketch $g_{n}$ and show that
$\|g_{n}\|_{1}\leq 2^{-2n+1}$.

(ii) If $f_{n}(x)=\max\{f_{1}(x),f_{2}(x),\dots,f_{n}(x)\}$
explain briefly why $f_{n}$ is a continuous function
on $[-1,1]$ with $0\leq f_{n}(x)\leq 1$. Show that
$\|f_{n+1}-f_{n}\|\leq 2^{-2n+3}$ and deduce that
$f_{n}$ is a Cauchy sequence in $({\mathcal R}([-1,1]),d)$.

(iii) Suppose if possible that there exists an 
$f\in {\mathcal R}([-1,1])$ such that $\|f_{n}-f\|\rightarrow 0$
as $n\rightarrow\infty$. Show that $\|f\|\leq 2/3$.

(iv) We now use the notation of Section~\ref{Riemann integration}.
Explain why there must exist a dissection 
$\mathcal{D}$ of $[-1,1]$ such that $S(f,\mathcal{D})\leq 1$.
Deduce that we can find $a$ and $b$ with $-1\leq a <b\leq 1$
such that $f(x)\leq 1/2$ for all $x\in [a,b]$. 

(v) Show that $\int_{a}^{b}|f(x)-f_{n}(x)|\,dx\nrightarrow 0$
and deduce that $\|f-f_{n}\|_{1}\nrightarrow 0$. This
contradiction shows that the sequence $f_{n}$ does not converge
and $({\mathcal R}([-1,1]),d)$ is not complete.

The reader who has got this far can feel well pleased
with herself but by working still harder
we can extract a little more juice from this example
and show that $f_{n}$ converges pointwise to a function
which is not Riemann integrable.

(v) Show that, for each $x$, $f_{n}(x)$ is a bounded increasing
function. Deduce that $f_{n}(x)$ tends to limit $F(x)$,
say, for each $x$. Show that $I^{*}(F)=2$.

(vi)  Let $Z_{n}=f_{n}^{-1}(0)$. Explain why $Z_{n}$
is closed and $Z_{1}\supseteq Z_{2}\supseteq \dots$.
If $\mathcal{D}$ is a fixed dissection of $[-1,1]$
show that for each $n$ we can find a collection
of  $\mathcal{I}_{n}$ of intervals $I=[x_{j},x_{j-1}]$
with end points successive points of the dissection
and of total length greater than $1$ such that
$I\cap Z_{n}\neq\emptyset$.

(vii) Let $\mathcal{I}=\bigcap_{n=1}^{\infty}\mathcal{I}_{n}$.
Show that $\mathcal{I}$ is a collection
of intervals $I=[x_{j},x_{j-1}]$
with end points successive points of the dissection
$\mathcal{D}$
and with total length greater than $1$ such that
$I\cap Z_{n}\neq\emptyset$ for each $n$.

(viii) Use Theorem~\ref{nested} to
show that $I\cap\bigcap_{n=1}^{\infty}Z_{n}\neq\emptyset$
whenever $I\in mathcal{I}$. Deduce that
$s(F,\mathcal{D})\leq 1$.

(ix) Conclude that $F$ is not Riemann integrable.
\end{exercise}

If we seek to embed $({\mathcal R}([-1,1]),d)$ in
some larger space of functions which is complete
we are led to Lebesgue theory.
\section{The uniform metric} This section is devoted
to one of the most important metrics on functions.
We shall write $\mathbb{F}$ to mean either
$\mathbb{R}$ or $\mathbb{C}$.
\begin{definition} If $E$ is a non-empty set
we write $\mathcal{B}(E)$ for the set of bounded
functions $f:E\rightarrow{\mathbb F}$.
The uniform norm $\|\ \|_{\infty}$ on $\mathcal{B}$
is defined by $\|f\|_{\infty}=\sup_{x\in E}|f(x)|$.
\end{definition}
\begin{lemma} If we use the standard operations
$\mathcal{B}(E)$  is a vector space
over $\mathbb{F}$. If $f,g,h\in\mathcal{B}(E)$ the following
results are true.

(i) $\|f\|_{\infty}\geq 0$,

(ii) If  $\|f\|_{\infty}= 0$ then $f=0$,

(iii) If $\lambda\in{\mathbb F}$
then $\|\lambda f\|_{\infty}=|\lambda|\|f\|_{\infty}$.

(iv) (The triangle inequality)
$\|f+g\|_{\infty}\leq\|f\|_{\infty}+\|g\|_{\infty}$.

(v) The pointwise product $fg\in \mathcal{B}(E)$
(where $fg(x)=f(x)g(x)$) and 
$\|fg\|_{\infty}\leq\|f\|_{\infty}\|g\|_{\infty}$.
\end{lemma}
We call the metric $d$ given by $d(f,g)=\|f-g\|_{\infty}$
the uniform metric.
\begin{theorem}\label{uniform complete}
The uniform metric on
$\mathcal{B}(E)$ is complete.
\end{theorem}

The space $\mathcal{B}(E)$ is not very interesting
in itself but if $E$ is a metric space it has 
a very interesting subspace.
\begin{definition} If $(E,d)$ is a 
non-empty metric space
we write $\mathcal{C}(E)$ for the set of bounded
continuous functions $f:E\rightarrow{\mathbb F}$.
\end{definition}
The next remark merely restates what we already know.
\begin{lemma}
If $(E,d)$ is a 
non-empty metric space
then $\mathcal{C}(E)$ is a vector subspace of 
$\mathcal{B}(E)$. Further if $f,g\in \mathcal{C}(E)$
the pointwise product $fg\in \mathcal{C}(E)$.
\end{lemma}

However the next result is new and crucial.
\begin{theorem}\label{uniform closed}
If $(E,d)$ is a 
non-empty metric space
then $\mathcal{C}(E)$ is a
closed subset of $\mathcal{B}(E)$
under the uniform metric.
\end{theorem}
This is the famous `$\epsilon/3$ Theorem'\footnote{Or
according to a rival school of thought the
`$3\epsilon$ Theorem'.}. Traditionally, it
has been presented in a different but
essentially equivalent manner.
\begin{definition}[Uniform convergence]
If $E$ is a non-empty set and
$f_{n}:E\rightarrow{\mathbb F}$  
and $f:E\rightarrow{\mathbb F}$ 
are functions
we say that $f_{n}$ \emph{converges uniformly}
to $f$ as $n\rightarrow\infty$
if, given any $\epsilon>0$
we can find an $n_{0}(\epsilon)$ such that
$|f_{n}(x)-f(x)|<\epsilon$ for all $x\in E$
and all $n\geq n_{0}(\epsilon)$.
\end{definition}
Theorem~\ref{uniform closed} now takes the following
form.
\begin{theorem} If $(E,d)$ is a non-empty metric space
and $f_{n}:E\rightarrow{\mathbb F}$  form
a sequence of continuous functions tending
uniformly to $f$ then $f$ is continuous.
\end{theorem}
More briefly, the uniform limit of continuous
functions is continuous. (To see the exact
equivalence observe that if $f_{n}\rightarrow f$
uniformly then we can find an $N$ such that
$f_{n}-f_{N}$ is bounded for all $n\geq N$.)

Since $\mathcal{C}(E)$ is a closed subset
of $\mathcal{B}(E)$, Theorem~\ref{uniform complete}
and Lemma~\ref{closed complete}
gives us another important theorem. 
\begin{theorem} If $(E,d)$ is a 
non-empty metric space
then the the uniform metric on $\mathcal{C}(E)$ is 
complete.
\end{theorem}
This result, also, has a more traditional form.
\begin{theorem}[General principle of uniform convergence]
Suppose that $(E,d)$ is a non-empty metric space
and $f_{n}:E\rightarrow{\mathbb F}$ is a continuous
function $[n\geq 1]$.
The sequence $f_{n}$ converge uniformly to
a continuous function $f$ if and only if
given any $\epsilon>0$ we can find an $n_{0}(\epsilon)$
such that $|f_{n}(x)-f_{m}(x)|<\epsilon$ for all
$n,m\geq n_{0}(\epsilon)$.
\end{theorem}
This theorem is also known as the GPUC by those who do
not object to theorems which sound as though they were
a branch of the secret police.

If $E$ is a closed bounded subset of ${\mathbb R}^{m}$
with the Euclidean metric then by Theorem~\ref{maximum}
all continuous functions $f:E\rightarrow{\mathbb F}$
are bounded. In these circumstances we shall
write $C(E)=\mathcal{C}$. 
For the rest of the course we will only
consider this special case. We start with a couple of
important examples.
\begin{example}[The witch's hat] Define 
$f_{n}:[0,2]\rightarrow{\mathbb R}$ by
\begin{alignat*}{2}
f_{n}(x)=&1-n|x-n^{-1}|&&\qquad\text{for $|x-n^{-1}|\leq n^{-1}$,}\\
f_{n}(x)=&0&&\qquad\text{otherwise.}
\end{alignat*}
Then the $f_{n}$ are continuous functions such that
$f_{n}(x)\rightarrow 0$ as $n\rightarrow \infty$
for each $x$ but $f_{n}\nrightarrow 0$ uniformly.
\end{example}
More briefly, pointwise convergence does not imply
uniform convergence.
\begin{example} Define $f_{n}:[0,1]\rightarrow{\mathbb R}$
by $f_{n}(x)=x^{n}$. Then $f_{n}(x)\rightarrow f(x)$
as $n\rightarrow\infty$ where $f(x)=0$ for $0\leq x<1$
but $f(1)=1$.
\end{example}
Thus the pointwise limit of continuous functions need
not be continuous.

Uniform convergence is a very useful tool when dealing
with integration.
\begin{theorem}\label{limit of integral}
Let $f_{n}\in C([a,b])$. 
If $f_{n}\rightarrow f$ uniformly then $f\in C([a,b])$
and
\[\int_{a}^{b}f_{n}(x)\,dx\rightarrow \int_{a}^{b}f(x)\,dx.\]
\end{theorem}
Students often miss the full force of this theorem
because the formula is so easy to prove. We also
need to prove that the second integral actually
exists.
The second half of Example~\ref{integral not complete} 
shows that the pointwise limit
of continuous functions need not be integrable.

Theorem~\ref{limit of integral} should be considered
in the context of the following two examples.
\begin{example}[The tall witch's hat] Define 
$f_{n}:[0,2]\rightarrow{\mathbb R}$ by
\begin{alignat*}{2}
f_{n}(x)=&n(1-n|x-n^{-1}|)&&\qquad
\text{for $|x-n^{-1}|\leq n^{-1}$,}\\
f_{n}(x)=&0&&\qquad\text{otherwise.}
\end{alignat*}
Then the $f_{n}$ are continuous functions such that
$f_{n}(x)\rightarrow 0$ as $n\rightarrow \infty$
but
\[\int_{0}^{2}f_{n}(x)\,dx\nrightarrow  0\]
as $n\rightarrow\infty$.
\end{example}
\begin{example}[Escape to infinity]\label{escape}
Define 
$f_{n}:{\mathbb R}\rightarrow{\mathbb R}$ by
\begin{alignat*}{2}
f_{n}(x)=&1-n^{-1}|x|&&\qquad
\text{for $|x|\leq n$,}\\
f_{n}(x)=&0&&\qquad\text{otherwise.}
\end{alignat*}
Then the $f_{n}$ are continuous functions such that
$f_{n}(x)\rightarrow 0$ uniformly as $n\rightarrow \infty$
but
\[\int_{-\infty}^{\infty}f_{n}(x)\,dx\nrightarrow  0\]
as $n\rightarrow\infty$.
\end{example}
Example~\ref{escape} must be well understood since
it represents a very common phenomenon which we
have to learn to live with.

Traditionally, Theorem~\ref{limit of integral}
is always paired with the following result
which is really a disguised theorem on integration!
\begin{theorem}\label{limit of derivative}
Suppose that 
$f_{n}:[a,b]\rightarrow{\mathbb R}$ is differentiable
on $[a,b]$ with continuous derivative $f_{n}'$
(we take the one-sided derivative at end points 
if necessary). Suppose that $f_{n}(x)\rightarrow f(x)$
as $n\rightarrow\infty$ for each $x\in [a,b]$
and suppose that $f_{n}'$ converges uniformly
to a limit $F$ on $[a,b]$. Then $f$ is differentiable
with derivative $F$.
\end{theorem}

The reader knows how to turn results on the limits of
sequences into results on infinite sums and vice versa.
Applied to Theorems~\ref{limit of integral} 
and~\ref{limit of derivative} the techniques produces
the following results.
\begin{theorem}[Term by term integration] Let 
$g_{j}:[a,b]\rightarrow{\mathbb R}$ be continuous.
If $\sum_{j=1}^{n}g_{j}$ converges uniformly as 
$n\rightarrow\infty$ then
\[\int_{a}^{b}\sum_{j=1}^{\infty}g_{j}(x)\,dx
=\sum_{j=1}^{\infty}\int_{a}^{b}g_{j}(x)\,dx.\]
\end{theorem}
\begin{theorem}[Term by term differentiation]
Let $g_{j}:[a,b]\rightarrow{\mathbb R}$ be differentiable
with continuous derivative. If $\sum_{j=1}^{n}g_{j}(x)$
converges for each $x$ and 
$\sum_{j=1}^{n}g_{j}'$ converges uniformly as 
$n\rightarrow\infty$ then $\sum_{j=1}^{\infty}g_{j}$ is
differentiable and
\[\frac{d\ }{dx}\left(\sum_{j=1}^{\infty}g_{j}(x)\right)
=\sum_{j=1}^{\infty}g_{j}(x).\]
\end{theorem}

Here is a starred application of 
Theorem~\ref{limit of derivative}.
The reader may, of course omit it but I include
it partly because it is often more useful than the
theorem (Theorem~\ref{under integral}) that it extends and
partly because it provides an excellent example
of how Theorem~\ref{limit of derivative} is used in practice.
\begin{theorem}[Differentiation under an infinite integral]%
\label{under infinite} 
Suppose $g:[0,\infty]\times[c,d]$ is
continuous and that the partial derivative $g_{,2}$
exists and is continuous.  Suppose further that
that there exists a continuous function 
$h:[0,\infty]\times[c,d]$ with $|g_{,2}(x,y)|\leq h(x)$
for all $(x,y)$ and such that $\int_{0}^{\infty}h(x)\,dx$
exists and is finite. Then, if 
$G(y)=\int_{0}^{\infty}g(x,y)\,dx$ exists for all $y\in (c,d)$
we have $G$ differentiable on $(c,d)$ with
\[G'(y)=\int_{0}^{\infty}g_{,2}(x,y)\,dx.\]
\end{theorem}

We conclude this section with a remark which may
appear rather isolated but will become more important
in course C12 and later work on complex variable.
\begin{theorem}\label{Radius and uniform}
We work in ${\mathbb C}$. Let
$\sum_{n=0}^{\infty}a_{n}z^{n}$ be a power series
with radius of convergence $R$. If $0\leq r< R$
then  $\sum_{n=0}^{N}a_{n}z^{n}$ converges uniformly
on $\bar{B}(0,r)$ as $N\rightarrow\infty$. However,
it need not be true that $\sum_{n=0}^{N}a_{n}z^{n}$ 
converges uniformly on $B(0,R)$.
\end{theorem}
\section{The contraction mapping theorem}
The next result is a beautiful example of the
power of abstraction. In it Banach transformed
a `folk-technique' into a theorem.
\begin{theorem}[The contraction mapping theorem]%
\label{contraction}
Let $(X,d)$ be a non-empty complete metric space
and $T:X\rightarrow X$ a mapping such that
there exists a $K<1$ with $d(Tx,Ty)\leq K d(x,y)$
for all $x,y\in X$. Then there exists a unique
$x_{0}\in X$ such that $Tx_{0}=x_{0}$.
\end{theorem}
More briefly, a contraction mapping on a complete
metric space has a unique fixed point.

Wide though the conditions are the reader should exercise
caution before attempting to widen them further.
\begin{example}\label{no fixed}
(i) If $X$ is the unit circle centre ${\mathbf 0}$
in the plane, $d$ Euclidean distance and $T$ a 
non-trivial rotation then $(X,d)$ is a complete
metric space and 
$d(T{\mathbf x},T{\mathbf y})=d({\mathbf x},{\mathbf y})$
for all ${\mathbf x},{\mathbf y}\in X$ but $T$ has
no fixed point.

(ii) If $X={\mathbb R}$, $d$ is Euclidean distance
and
\[Tx=1+x+g(x)\]
for some suitably chosen $g$ then $(X,d)$ is a complete
metric space and $d(Tx,Ty)< d(x,y)$
for all $x,y\in X$ but $T$ has
no fixed point.
\end{example}

We use the contraction mapping theorem to show
that a wide class of differential equations
actually have a solution. We shall be looking
at equations of the form
\[y'=f(x,y).\]
Our first, simple but important, result is that
this problem on differential equations can be turned into
a problem on integral equations.
\begin{lemma} If $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is continuous, $x_{0},y_{0}\in{\mathbb R}$ and $\delta>0$
then the following two statements
are equivalent.

(A) The function 
$y:(x_{0}-\delta,x_{0}+\delta)\rightarrow{\mathbb R}$
is differentiable and satisfies the equation $y'(x)=f(x,y(x))$
for all $x\in (x_{0}-\delta,x_{0}+\delta)$ together
with the boundary condition $y(x_{0})=y_{0}$.

(B) The function 
$y:(x_{0}-\delta,x_{0}+\delta)\rightarrow{\mathbb R}$
is continuous and satisfies the condition
\[y(x)=y_{0}+\int_{x_{0}}^{x}f(u,y(u))\,du\]
for all $x\in (x_{0}-\delta,x_{0}+\delta)$.
\end{lemma}

\begin{theorem}~\label{integral solution}
Suppose $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is continuous, $x_{0},y_{0}\in{\mathbb R}$ and $\delta>0$.
Suppose further that there exists a $K>0$ such that
$K\delta<1$ and
\begin{equation*}
|f(x,u)-f(x,v)|\leq K|u-v| \tag*{$\bigstar$}
\end{equation*}
for all $u,v\in (x_{0}-\delta,x_{0}+\delta)$.
Then there exists a unique
$y:(x_{0}-\delta,x_{0}+\delta)\rightarrow{\mathbb R}$
is continuous and satisfies the condition
\[y(x)=y_{0}+\int_{x_{0}}^{x}f(t,y(t))\,dt\]
for all $x\in (x_{0}-\delta,x_{0}+\delta)$.
\end{theorem}
Condition $\bigstar$ is called a Lipschitz condition.
It closely related to differentiability (consider
the mean value theorem) but does not imply differentiability
(consider $f(x,y)=|y|$). In the absence of such
a condition differential equations can have unexpected
properties.
\begin{example} There are an infinity of different
solutions to the equation
\[y'=3y^{2/3}\]
with $y(0)=0$.
\end{example}

The lecturer can not bear to let the topic go at 
this point but the rest of this section is heavily
starred.
As it stands Theorem~\ref{integral solution} is not
quite powerful enough for practical purposes. 
Careful thought, without the need for new ideas
shows that it can be extended to the following                              
result. 
\begin{theorem}\label{local solution}
Suppose $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is a continuous function satisfying the following condition.
There exists a $K:[0,\infty)\rightarrow{\mathbb R}$
such that 
\[|f(x,u)-f(x,v)|\leq K(R) |u-v| \]
whenever $|x|,|u|,|v|\leq R$.
Then given any $(x_{0},y_{0})\in{\mathbb R}^{2}$
we can find a $\delta(x_{0},y_{0})>0$ such that there
exists a unique differentiable function
$y:(x_{0}-\delta,x_{0}+\delta)\rightarrow{\mathbb R}$
which satisfies the equation $y'(x)=f(x,y(x))$
for all $x\in (x_{0}-\delta,x_{0}+\delta)$ together
with the boundary condition $y(x_{0})=y_{0}$.
\end{theorem}
Theorem~\ref{local solution} tells us that the
under very wide conditions the differential
equation has a \emph{local solution} through each
$(x_{0},y_{0})$. Does it have a \emph{global solution}, 
that is, can we find a solution for
the equation $y'(x)=f(x,y(x))$ which is defined for all 
$x\in{\mathbb R}$?

We finish our discussion by showing that the question
is non-trivial.
\begin{theorem}\label{nice global}
Suppose $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is a continuous function satisfying the following condition.
There exists a $K:[0,\infty)\rightarrow{\mathbb R}$
such that
\[|f(x,u)-f(x,v)|\leq K(R) |u-v| \]
for all $|x|\leq R$ and all $u$ and $v$.
Then given any $(x_{0},y_{0})\in{\mathbb R}^{2}$
we can find 
a unique $y:{\mathbb R}\rightarrow{\mathbb R}$
which is
is differentiable and satisfies the equation $y'(x)=f(x,y(x))$
for all $x\in {\mathbb R}$ together
with the boundary condition $y(x_{0})=y_{0}$
\end{theorem}

\begin{example} If $f(x,y)=1+y^{2}$ then
\[|f(x,u)-f(x,v)|\leq 2R |u-v| \]
whenever $|u|,|v|\leq R$. However, there does not exist
a differentiable function $y:{\mathbb R}\rightarrow{\mathbb R}$
with $y'=f(x,y)$ for all $x$.
\end{example}
\section{Green's function} (The contents of this section are
all starred)

In course C12 you saw how to solve the differential
equation
\begin{equation*}
y''+a(x)y'+b(x)y=f(x) \tag*{$(*)$}
\end{equation*}
subject to the conditions $y(0)=y(1)=0$ by using the
Green's function. There is no doubt in the present
author's mind that the `physicist's approach'
using impulses, delta functions and hand-waving
is the most important for the student to master.
However, it adds to our confidence in the hand-waving
approach to see that its results can be confirmed
rigourously.

We start by looking at solutions of
\begin{equation*}
y''+a(x)y'+b(x)y=0 \tag*{$(**)$}
\end{equation*}
Observe first that the results of the last section
on the existence of solutions of differential
equations can be extended to the vectorial case
with only the most minor variations in the proof.
Thus, for example, Theorem~\ref{nice global}
extends as follows.
\begin{theorem}\label{nice global many}
Suppose $f:{\mathbb R}\times{\mathbb R}^{m}
\rightarrow{\mathbb R}^{m}$
is a continuous function satisfying the following condition.
There exists a $K:[0,\infty)\rightarrow{\mathbb R}$
such that
\[\|f(x,{\mathbf w})-f(x,{\mathbf z})\|
\leq K(R) \|{\mathbf w}-{\mathbf z}\| \]
for all $|x|\leq R$
and all $\mathbf{w}$ and $\mathbf{z}$.
Then given any $x_{0}\in{\mathbb R}$
and ${\mathbf y}_{0}\in{\mathbb R}^{m}$
we can
find a unique
$\mathbf{y}:{\mathbb R}\rightarrow{\mathbb R}^{m}$
which is
is differentiable and satisfies the equation 
\[{\mathbf y}'(x)=f(x,{\mathbf y})\]
for all $x\in {\mathbb R}$ together
with the boundary condition $\mathbf{y}(x_{0})=\mathbf{y}_{0}$.
\end{theorem}

As the the reader knows from course C4 (Differential Equations)
it is easy to convert equation $(**)$ into a form appropriate
for the application of Theorem~\ref{nice global many}.
If we set
\[{\mathbf y}=\begin{pmatrix}y\\y'\end{pmatrix}
\ \text{and}\ f(x,{\mathbf w})=
\begin{pmatrix}0&1\\-a(x)&-b(x)\end{pmatrix}{\mathbf w}
=\begin{pmatrix}w_{2}\\-b(x)w_{1}-a(x)w_{2}\end{pmatrix},\]
then
\[{\mathbf y}'(x)=f(x,{\mathbf y}).\]
Now
\begin{align*}
\|f(x,{\mathbf w})-f(x,{\mathbf z})\|
&=\left\|\begin{pmatrix}
w_{2}-z_{2}\\-b(x)(w_{1}-z_{1})-a(x)(w_{2}-z_{2})
\end{pmatrix}\right\|\\
&\leq (1+|a(x)|+|b(x)|)\|{\mathbf w}-{\mathbf z}\|
\end{align*}
so that writing $K(R)=\sup_{t\in[-R,R]}(1+|a(t)|+|b(t)|)$
we have
\[\|f(x,{\mathbf w})-f(x,{\mathbf z})\|\leq K(R)
\|{\mathbf w}-{\mathbf z}\|\]
whenever $\|x|\leq K(R)$.
Thus there is one and only one solution $y_{[\lambda]}$, say,
of $(**)$ with $y(0)=0$ and $y'(0)=\lambda$. By linearity,
$y_{[\lambda]}=\lambda y_{[1]}$.

Let us write $y_{1}$ for the solution of $(**)$ with
$y_{1}(0)=0$, $y_{1}'(0)=1$. We make the following
\[\text{\bf key assumption}\qquad y_{1}(1)\neq 0.\]
We write $y_{2}$ for the solution of $(**)$ with
$y_{2}(0)=0$, $y_{2}'(0)=1$. The next lemmas are
familiar from courses~C4 and C10. We define
\[W(x)=y_{1}(x)y_{2}'(x)-y_{2}(x)y_{1}'(x).\]
The function $W$ is called the Wronskian.
\begin{lemma} (i) The Wronskian is differentiable
with
\[W'(x)=-a(x)W(x).\]

(ii) The Wronskian is never zero.
\end{lemma}

We can now define the Green's function 
$G:{\mathbb R}^{2}\rightarrow{\mathbb R}$ by
\begin{alignat*}{2}
G(s,t)=&y_{1}(s)y_{2}(t)W(t)^{-1}&&\qquad\text{for $s<t$}\\
G(s,t)=&y_{2}(s)y_{1}(t)W(t)^{-1}&&\qquad\text{for $t\leq s$.}
\end{alignat*}
(Why should we do any such thing? Because the intuitive
arguments of C10 tell us to.) Using Lemma~\ref{double}
it is easy to verify our main result.
\begin{theorem} If $f:{\mathbb R}\rightarrow{\mathbb R}$
is continuous then
\[y(x)=\int_{0}^{1}G(x,w)f(w)\,dw\]
is twice differentiable and satisfies
\begin{equation*}
y''+a(x)y'+b(x)y=f(x) \tag*{$(*)$}
\end{equation*}
together with the conditions $y(0)=y(1)=0$.
\end{theorem}
(Our proof is very direct and involves neither differentiation
under the integral nor Lemma~\ref{double}. However,
it is not hard to see that such ideas would
be relevant in more complicated situations.)

From the point of view of more advanced work
this shows how differential operators like
\[y\mapsto Sy=y''+a(x)y+b(x)y\]
can be linked with better behaved integral
operators like
\[f\mapsto Tf \ \text{with}\ Tf(x)=\int_{0}^{1}G(x,v)f(v)\,dv.\]
Note that we have shown $STf=f$ for $f$ continuous,
but note also
that, if $f$ is merely continuous, $Sf$ need not
be defined. The Green's function $G$ is an example
of an integral kernel\footnote{Most of the new words
in this last paragraph are well worth dropping.
\begin{verse}
You must lie among the daisies and discourse in novel
phrases of your complicated state of mind,\\
The meaning doesn't matter if it's only idle chatter
of a transcendental kind.\\
And everyone will say,\\
As you walk your mystic way,\\
`If this young man expresses himself in terms too deep for
me\\
Why, what a very singularly deep young man this deep
young man must be!'
\end{verse}}. More formally, if we
write
\[Tf(x)=\int_{0}^{1}K(x,v)f(v)\,dv,\]
then $T$ is called an integral operator with kernel $K$.
(So far as I know, there is no connection with the
`kernel' that you meet in the linear mathematics course~P1.)



We end with an example to show that things really
do go awry if our {\bf key assumption} fails.
\begin{example} The equation
\[y''+\pi^{2}y=x\]
has no solution satisfying $y(0)=y(1)=0$.
\end{example}
\section{Further reading} Since new introductions to
analysis pour off the printing presses in an unending
torrent, it may well be my fault that I can not
recommend one book to cover the whole course. 
Spivak's \emph{Calculus}~\cite{Spivak} and
J.~C.~Burkill's
\emph{A First Course in Mathematical Analysis}~\cite{Burkill 1}
are both excellent introductions to analysis
but only deal with the real line and not with ${\mathbb R}^{m}$.
J.~C.~Burkill and H.~Burkill's
\emph{A Second Course in Mathematical Analysis}~\cite{Burkill 2}
is also good but rather old fashioned. (Of course, this
has nothing to do with the authors' knowledge but a great
deal to do with what was then thought the appropriate
content for a second course.) Dieudonn\'{e}'s
\emph{Foundations of Modern Analysis}~\cite{Dieudonn}
is superb
but written by someone who neither knew nor cared
what others thought was the appropriate
content for a second course\footnote{Boas notes that
`There
is a test for identifying some of the future professional
mathematicians at an early age. These are students
who instantly comprehend a sentence beginning
``Let $X$ be an ordered quintuple 
$(a,T,\pi,\sigma,{\mathcal B})$ where \dots'. They are 
even more promising if they add, `I never really understood
it before.''\,' (\cite{Boas} page231.)}. 
A  suitable compromise is reached in Marsden and Hoffman's
\emph{Elementary Classical Analysis}~\cite{Marsden}.
This covers the material on multidimensional calculus
very clearly and is probably the book that most students
will find most useful. Make sure that your college
library has a copy of the second edition.

The material on metric spaces is covered
in Sutherland's \emph{Introduction to Metric and Topological
Spaces}~\cite{Sutherland} which is workmanlike though
a bit dull. (Once analytic topology is detached from its 
applications it takes a very gifted writer to 
make it exciting.)

I would be glad to hear of other suggestions for suitable books.
A completely unsuitable but interesting version of the
standard analysis course is given by Berlinski's
\emph{A Tour of the Calculus}~\cite{Berlinski}
--- Spivak rewritten by Sterne with additional purple
passages by the Ankh-Morpork tourist board.
\begin{thebibliography}{10}
\bibitem{Berlinski} D.~Berlinski
\emph{A Tour of the Calculus}
Mandarin Paperbacks 1997.
\bibitem{Boas} R.~P.~Boas 
\emph{Lion Hunting and Other Mathematical Pursuits}
(Editors G.~L.~Alexander and D.~H.~Mugler),
Dolciani Expositions, Vol 15, MAA, 1995.
\bibitem{Burkill 1} J.~C.~Burkill
\emph{A First Course in Mathematical Analysis}
CUP, 1962.
\bibitem{Burkill 2} J.~C.~Burkill and H.~Burkill
\emph{A Second Course in Mathematical Analysis}
CUP, 1970.
\bibitem{Dieudonn} J.~Dieudonn\'{e}
\emph{Foundations of Modern Analysis},
Academic Press, 1960.
\bibitem{Hardy} G.~H.~Hardy
\emph{A Course of Pure Mathematics}, CUP, 1908. 
(Still available in its 10th edition.)
\bibitem{Marsden} J.~E.~Marsden and M.~J.~Hoffman
\emph{Elementary Classical Analysis} (2nd Edition),
W.~H.~Freeman,
New York, 1993.
\bibitem{Spivak} M.~Spivak,
\emph{Calculus}
Addison-Wesley/Benjamin-Cummings, 1967.
\bibitem{Sutherland} W.~A.~Sutherland
\emph{Introduction to Metric and Topological Spaces},
OUP, 1975.
\bibitem{Wagon}S.~Wagon \emph{The Banach--Tarski Paradox},
CUP, 1993.
\bibitem{Whittaker} E.~T.~Whittaker and G.~N.~Watson
\emph{A Course of Modern Analysis} CUP, 1902
(Still available in its 4th edition.)
\end{thebibliography}
\newpage
\section{First Sheet of Exercises} 
I have tried to 
produce 12 rather routine questions for each sheet.
Any further questions are for interest
only. Ambitious
students and their supervisors should look at Tripos 
questions to supplement this meagre fare.
\vspace{1\baselineskip}
                                        
\begin{question}\label{1.1}
(We work with the same ideas as
in Example~\ref{Rational}.) 
(i) Find a differentiable function 
$f:{\mathbb Q}\rightarrow{\mathbb Q}$ such that
$f'(x)=1$ for all $x\in{\mathbb Q}$ but $f(0)>f(1)$.

(ii) If we define $g:{\mathbb Q}\rightarrow{\mathbb Q}$
by $g(x)=(x^{2}-2)^{-2}$ show that $g$ is continuous
but unbounded on $\{x\in{\mathbb Q}:|x|\leq 2\}$.
\end{question}
\begin{question}
In Question~\ref{1.3} below you are asked to
prove Lemma~\ref{one sequences}. Write a paragraph
on each of the following subjects.

(i) Would it have been better for the lecturer to prove
Lemma~\ref{one sequences} rather than leave it to you.
Why?

(ii) What benefit, if any, do you expect to derive from
doing Lemma~\ref{one sequences}? Give reasons.

(iii) What is the best way of approaching Question~\ref{1.1}.
For example should you look at your notes from last year
before attacking the question? Should you look at your notes 
from last year after attacking the question?
\end{question}
\begin{question}~\label{1.3}
Prove Lemma~\ref{one sequences}
(restated below for your convenience)

(i) The limit is unique. That is, if $a_{n}\rightarrow a$
and $a_{n}\rightarrow b$ as $n\rightarrow\infty$
then $a=b$.

(ii) If $a_{n}\rightarrow a$ as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$a_{n(j)}\rightarrow a$ as $j\rightarrow\infty$.

(iii) If $a_{n}=c$ for all $n$ then $a_{n}\rightarrow c$
as $n\rightarrow\infty$.

(iv) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$
as $n\rightarrow\infty$ then
$a_{n}+b_{n}\rightarrow a+b$.

(v) If $a_{n}\rightarrow a$ and $b_{n}\rightarrow b$ 
as $n\rightarrow\infty$ then
$a_{n}b_{n}\rightarrow ab$.

(vi) If $a_{n}\rightarrow a$
as $n\rightarrow\infty$ and $a_{n}\neq 0$ for each $n$,
$a\neq 0$ then $a_{n}^{-1}\rightarrow a^{-1}$.

(vii) If $a_{n}\leq A$ for each $n$ and
$a_{n}\rightarrow a$
as $n\rightarrow\infty$ then $a\leq A$.


\end{question}
\begin{question}
Prove
Lemma~\ref{Exercise 1.4} which states that 
a decreasing sequence of real numbers bounded below tends
to a limit.

\end{question}
\begin{question} (i) If $a_{j}$ is a integer 
with $0\leq a_{j}\leq 9$ show \emph{from the fundamental axiom}
that
\[\sum_{j=1}^{\infty}a_{j}10^{-j}\]
exists. Show that $0\leq \sum_{j=1}^{\infty}a_{j}10^{-j}\leq 1$
carefully quoting any theorems that you use.

(ii) If $0\leq x\leq 1$ show that we can find integers
$x_{j}$ with $0\leq x_{j}\leq 9$ such that
\[x=\sum_{j=1}^{\infty}x_{j}10^{-j}.\]
What important deep result do you use?

(iii) If $a_{j}$ and $b_{j}$ are integers 
with $0\leq a_{j},b_{j}\leq 9$ and
$a_{j}=b_{j}$ for $j<N$, $a_{N}>b_{N}$ show that
\[\sum_{j=1}^{\infty}a_{j}10^{-j}\geq \sum_{j=1}^{\infty}b_{j}10^{-j}.\]
Give the precise necessary and sufficient condition
for equality and prove it.

\end{question}
\begin{question} (i) Let us write
\[S_{n}=\sum_{r=0}^{n}\frac{1}{r!}.\]
Show, from first principles, that $S_{n}$ converges
to a limit (which, with the benefit of extra knowledge,
we call $e$).

(ii) Show that, if $n\geq 2$  and $r\geq 0$ then
\[\frac{n!}{(n+r)!}\leq \frac{1}{3^{r}}.\]
Deduce carefully that, if $m\geq n\geq 2$,
\[0\leq n!(S_{m}-S_{n})\leq \frac{1}{2}.\] 
and that
\[0<n!(e-S_{n})\leq \frac{1}{2}.\]
Deduce that $n!e$ is not an integer for any $n$  and
conclude that $e$ is irrational.

(iii) Show similarly that 
${\displaystyle\sum_{r=0}^{\infty}\frac{1}{(2r)!}}$
is irrational.

\end{question}
\begin{question}
In this question you should feel free
to use any results you know (provided you quote them 
correctly).

(i) By using the mean value theorem, show that 
$(1+x^{-1})^{x}$  is an increasing function of
$x$ for $x\geq 0$.

(ii) Show that $(1+n^{-1})^{n}$ tends to limit $L$
as $n\rightarrow\infty$.

(iii) Show that, if $n\geq N$,
\[e\geq \left(1+\frac{1}{n}\right)^{n}\geq 
\sum_{r=0}^{N}\binom{n}{r}n^{-r}.\]
Deduce that
\[e\geq L\geq \sum_{r=0}^{N}\frac{1}{r!},\]
and conclude that
\[\left(1+\frac{1}{n}\right)^{n}\rightarrow e\]
as $n\rightarrow\infty$.

(iv) Assuming the Taylor expansion for $e^{t}$ show
that
\[\left(1+\frac{t}{n}\right)^{n}\rightarrow e^{t}\]
as $n\rightarrow\infty$ for all $t>0$.

(v) Show that
\[1-\left(1+\frac{t^{2}}{n^{2}}\right)^{n}\rightarrow 0\]
and deduce that
\[1-\left(1-\frac{t^{2}}{n^{2}}\right)^{n}\rightarrow 0\]
as $n\rightarrow\infty$.
Hence deduce that
\[\left(1-\frac{t}{n}\right)^{n}\rightarrow e^{-t}\]
as $n\rightarrow\infty$ for all $t>0$.

(vi) Conclude that 
\[\left(1+\frac{t}{n}\right)^{n}\rightarrow e^{t}\]
as $n\rightarrow\infty$ for all real $t$.

\end{question}
\begin{question} (a) Prove the statements made in
Example~\ref{on and off}.

(i) The sum $\sum_{n=1}^{\infty}n^{-2}z^{n}$
has radius of convergence $1$ and converges for all $|z|=1$.

(ii) The sum $\sum_{n=0}^{\infty}z^{n}$
has radius of convergence $1$ and diverges for all $|z|=1$.

(iii) The sum $\sum_{n=1}^{\infty}n^{-1}z^{n}$
has radius of convergence $1$, diverges for $z=1$
and converges for $z=-1$.

(b) Suppose
that $\sum_{j=0}^{\infty}a_{j}z^{j}$ has radius of convergence
$R$ and $\sum_{j=0}^{\infty}b_{j}z^{j}$ has radius of
convergence $S$.

(i) Show that if $R\neq S$ then
$\sum_{j=0}^{\infty}(a_{j}+b_{j})z^{j}$ has radius of convergence
$\min(R,S)$.

(ii) Show that if $R=S$ then
$\sum_{j=0}^{\infty}(a_{j}+b_{j})z^{j}$ has radius of convergence
at least $R$.

(iii) If $T\geq 1=R=S$ give an example where
$\sum_{j=0}^{\infty}(a_{j}+b_{j})z^{j}$ has radius of convergence
$T$.
\end{question}
\begin{question} (i) Suppose that  $x_{n}$ is a bounded
sequence of real
numbers. Show that $y_{n}=\sup_{m\geq n}x_{m}$
is a well defined bounded decreasing sequence
and so $y_{n}$ tends to a limit $y$ say. We
write
\[\limsup_{n\rightarrow\infty}x_{n}=y.\]

(ii) Let $a_{n}$ be a sequence of complex numbers.
Show that if the sequence $|a_{n}|^{1/n}$ is unbounded, then
$\sum_{j=0}^{\infty}a_{j}z^{j}$ has radius of convergence $0$
and if the the sequence $|a_{n}|^{1/n}$ is bounded and non-zero, then
$\sum_{j=0}^{\infty}a_{j}z^{j}$ has radius of convergence 
$(\limsup_{n\rightarrow\infty}|a_{n}|^{1/n})^{-1}$.
State, with reasons, what happens when 
$\limsup_{n\rightarrow\infty}|a_{n}|^{1/n}=0$

\end{question}
\begin{question}
Suppose that $A$ and $B$ are non-empty
bounded subsets of ${\mathbb R}$. Show that
\[\sup\{a+b:a\in A,\ b\in B\}=\sup A+\sup B.\]
The last formula is more frequently written
\[\sup_{a\in A,\ b\in B}a+b=\sup_{a\in A}a+\sup_{b\in B}b.\]

Suppose, further that $a_{n}$ and $b_{n}$ are bounded
sequences of real numbers. For each of the following
statements either give a proof that it is always true
or an example to show that it is sometimes false.

(i) $\sup_{n}(a_{n}+b_{n})=\sup_{n}a_{n}+\sup_{n}b_{n}$.

(ii) $\sup_{a\in A,\ b\in B}ab=(\sup_{a\in A}a)(\sup_{b\in B}b)$.

(iii) $\inf_{a\in A,\ b\in B}a+b=\inf_{a\in A}a+\inf_{b\in B}b$.

\end{question}
\begin{question}
Prove all the statements made
in Lemma~\ref{many sequences}, viz:

(i) The limit is unique. That is, if 
$\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\mathbf{a}_{n}\rightarrow \mathbf{b}$
as $n\rightarrow\infty$
then $\mathbf{a}=\mathbf{b}$.

(ii) If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
as $n\rightarrow\infty$
and $n(1)<n(2)<n(3)\ldots$ then
$\mathbf{a}_{n(j)}\rightarrow \mathbf{a}$ 
as $j\rightarrow\infty$.

(iii) If $\mathbf{a}_{n}=\mathbf{c}$ for all $n$ 
then $\mathbf{a}_{n}\rightarrow \mathbf{c}$
as $n\rightarrow\infty$.

(iv) If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\mathbf{b}_{n}\rightarrow\mathbf{b}$
as $n\rightarrow\infty$ then
$\mathbf{a}_{n}+\mathbf{b}_{n}
\rightarrow \mathbf{a}+\mathbf{b}$.

(v) Suppose $\mathbf{a}_{n}\in {\mathbb R}^{m}$,
$\mathbf{a}\in {\mathbb R}^{m}$
$\lambda_{n}\in {\mathbb R}$,
and $\lambda\in {\mathbb R}$. 
If $\mathbf{a}_{n}\rightarrow \mathbf{a}$
and $\lambda_{n}\rightarrow\lambda$ then
$\lambda_{n}\mathbf{a}_{n}\rightarrow 
\lambda\mathbf{a}$.

\end{question}
\begin{question}
Use the Bolzano--Weierstrass theorem to prove 
the general principle of convergence in ${\mathbb R}^{m}$
(Theorem~\ref{many Cauchy}).
\end{question}

\vspace{1\baselineskip}

The remaining questions are included for general interest
and not for relevance to the syllabus or to passing Tripos exams.

\vspace{1\baselineskip}

\begin{question}\label{square supremum} By comparing
\[\frac{a}{b}\ \text{with}\ \frac{3a+4b}{2a+3b}\]
show that given any $x\in{\mathbb Q}$ with $x^{2}\leq 2$
we can find a $y\in{\mathbb Q}$ with $y>x$ and $y^{2}\leq 2$.
Deduce that
the set $\{x\in{\mathbb Q}:x^{2}<2\}$ has no supremum
in ${\mathbb Q}$. (The idea of comparing $a/b$ with
$(3a+4b)/(2a+3b)$ goes back as far as Euclid.)
\end{question}


\vspace{1\baselineskip}

\begin{question}
If $a<c<b$ and $E$ is a subset of ${\mathbb R}$
explain why if $E\cap [a,c]$ and $E\cap [c,b]$ are finite
then so is $E\cap [a,b]$. Use this to give a proof by bisection
of Bolzano's theorem.

\end{question}
\begin{question}
Do Exercise~\ref{Cantor}
showing the uncountability of the reals.

Let $y_{1}$, $y_{2}$, \dots be any
sequence of points in ${\mathbb R}$. Let $x_{0}=0$,
$\delta_{0}=1$.

(i) Show that you can construct inductively
a sequence of real numbers $x_{1}$, $x_{2}$, \dots
and positive numbers $\delta_{j}$ such that

\ \ (a) $|x_{n}-x_{n-1}|<\delta_{n-1}/4$,

\ \ (b) $x_{n}\neq y_{n}$,

\ \ (c) $0<\delta_{n}<|x_{n}-y_{n}|$,

\ \ (d) $\delta_{n}<\delta_{n-1}/4$.

(ii) Show that $\delta_{n+m}<4^{-m}\delta_{n}$ for $m,n\geq 0$
and deduce that the $x_{n}$ form a Cauchy sequence.
Conclude that $x_{n}$ tends to a limit $x$.

(iii) Show that $|x_{n+m}-x_{n}|<\delta_{n}/3$ for all 
$m,n\geq 0$. Deduce that $|x-x_{n}|\leq \delta_{n}/3$
for all $n\geq 0$. Why does this show that $y_{n}\neq x$
for all $n$.

(iv) Prove that the real numbers are uncountable.

\end{question}
\begin{question}
Suppose we consider the collection $\mathcal Q$
of rational functions
\[f(X)=\frac{a_{0}+a_{1}X+\dots+a_{n}X^{n}}
{b_{0}+b_{1}X+\dots+b_{m}X^{m}}\]
(where the $a_{j}$ and $b_{j}$ are real
and where $b_{m}\neq 0$ and $a_{n}\neq 0$ if $n\geq 1$)
with the usual rules for addition and multiplication.
Suppose that we say that $f(X)>0$ if
$a_{n}b_{m}>0$ and that $f>g$ if $f-g>0$.
Convince yourself that  $\mathcal Q$
has all the standard properties of arithmetic
and order we expect. (Some students will only be able
to convince themselves by looking up the axioms for
an ordered field in, for example,~\cite{Spivak}
and then verifying each axiom in turn. Good luck
to them, I say.)

Show that $1/n>1/X>0$ for all integers $n$ and
so Archimedes' Axiom is false for $\mathcal Q$.
\end{question}

\newpage
\section{Second Sheet of Exercises}
I have tried to
produce 12 rather routine questions for each sheet.
Any further questions are for interest
only. Ambitious
students and their supervisors should look at Tripos
questions to supplement this meagre fare.

\vspace{1\baselineskip}

\begin{question}
Here is an alternative proof of Theorem~6.8

(i) Let $K\geq 0$ and $\epsilon>0$.
If $f:[a,b]\rightarrow{\mathbb R}$  has the property
that
\[f(b)-f(a)\geq (K+\epsilon)(b-a)\]
and $N$ is strictly
positive integer show that there exists an integer $r$
with $1\leq r\leq N$ and
\[f(a+r(b-a)/N)-f(a+(r-1)(b-a)/N)\geq (K+\epsilon)(b-a)/N\]

(ii) Under the conditions of part (i) deduce that there
exists a sequence of closed intervals $[x_{n},y_{n}]$
with
\[[a,b]=[x_{0},y_{0}]\supseteq [x_{1},y_{1}]\supseteq
[x_{2},y_{2}]\supseteq \dots\]
such that $f(y_{n})-f(x_{n})\geq (K+\epsilon)(y_{n}-x_{n})$
and $y_{n}-x_{n}\rightarrow 0$ as $n\rightarrow\infty$.

(iii) Continuing with the same notation, show that
$x_{n}\rightarrow c$ for some $c\in [a,b]$. Show
that $c\in [x_{n},y_{n}]$ for all $n$ and that,
if $c\neq x_{n},y_{n}$ then
\[\max\left(
\frac{f(y_{n})-f(c)}{y_{n}-c},
\frac{f(c)-f(x_{n})}{c-x_{n}}
\right)\geq K+\epsilon.\]
Conclude that if $f$ is differentiable at $c$ then
$f'(c)\geq K+\epsilon$.

(iv) Deduce all the results of Theorem 6.8.

\end{question}
\begin{question} (i) If $E$ is a set show that the set
\[E^{\circ}=\bigcup\{U:\text{$U$ is open and $U\subseteq E$}\}\]
has the following properties.

\ \ (a) $E^{\circ}$ is open.

\ \ (b) $E^{\circ}\subseteq E$.

\ \ (c) If $A$ is open and $A\subseteq E$
then $A\subseteq E^{\circ}$.

(ii) Suppose that $E'$ is such that

\ \ (a) $E'$ is open.

\ \ (b) $E'\subseteq E$.

\ \ (c) If $A$ is open and $A\subseteq E$
then $A\subseteq E'$.

\noindent
Show that $E'=E^{\circ}$.

(iii) Explain why (i) and (ii) make it possible to define
the \emph{interior} of $E$ as the largest open set
contained in $E$.

(iv) Show why it is possible to define the
\emph{closure} of $E$ as the smallest closed set
containing $E$.

(v) Find the interior and closure of $(a,b)$, $[a,b)$,
$(a,b]$ and $[a,b]$ where $a<b$.

\end{question}
\begin{question}\label{2.3}
Do the Exercise~\ref{art of continuity}
restated below.
 
After looking at parts~(iii) to~(v)
of Lemma~\ref{many sequences} state the corresponding
results for continuous functions. (Thus part~(v) 
corresponds to the statement that if 
$\lambda:E\rightarrow{\mathbb R}$ and 
$f:E\rightarrow{\mathbb R}^{p}$ are continuous
at ${\mathbf x}\in E$ then so is $\lambda f$.)
Prove your statements directly from 
Definition~\ref{definition continuity}.

\end{question}
\begin{question}
If $f,g:{\mathbb R}^{p}\rightarrow{\mathbb R}^{m}$
are continuous
show that 
\[\begin{pmatrix}
f\\g
\end{pmatrix}
:{\mathbb R}^{p}\rightarrow{\mathbb R}^{2m}
={\mathbb R}^{m}\times{\mathbb R}^{m}\]
defined by
\[\begin{pmatrix}
f\\g
\end{pmatrix}({\mathbf x})=
\begin{pmatrix}
f({\mathbf x})\\g({\mathbf x})
\end{pmatrix}\]
is continuous.

Show that the map $A:{\mathbb R}^{m}\times{\mathbb R}^{m}
\rightarrow {\mathbb R}^{m}$ defined by
\[A\begin{pmatrix}
\mathbf{x}\\ \mathbf{y}
\end{pmatrix}=\mathbf{x}+\mathbf{y}\]
whenever $\mathbf{x},\mathbf{y}\in {\mathbb R}^{m}$
is continuous. By applying the continuity of compositions
of continuous functions (Lemma~\ref{composition}) to 
${\displaystyle A\circ
\begin{pmatrix}
f\\g
\end{pmatrix}}$
deduce that $f+g$ is continuous.

Obtain the other results of Question~\ref{2.3} similarly. 

\end{question}
\begin{question}
Let $f:{\mathbb R}\rightarrow{\mathbb R}$.
State which of the following statements are always
true and which may be false giving a proof or a counter
example as appropriate.

(i) If $f^{-1}(E)$ is closed whenever $E$ is closed
then $f$ is continuous.

(ii) If $f$ is continuous then $f(U)$ is open whenever
$U$ is open.

(iii) If $f$ is continuous then $f(E)$ is bounded whenever
$E$ is bounded.

(iv) If $f(E)$ is bounded whenever
$E$ is bounded then $f$ is continuous.

\end{question}
\begin{question}
Suppose that $E$ is a subset of ${\mathbb R}^{n}$.
Show that $E$ is closed and bounded if and only if
every continuous function $f:E\rightarrow{\mathbb R}^{n}$
is bounded.                      

\end{question}
\begin{question}
If $A$ and $B$ are subsets of ${\mathbb R}^{m}$
we write
\[A+B=\{{\mathbf a}+{\mathbf b}:
{\mathbf a}\in A,\ {\mathbf b}\in B\}\]

By considering the case $m=1$, 
\[A=\{-n:n\in{\mathbb N}\}\ \text{and}
\ B=\{n+1/n:n\in{\mathbb N},\ n\geq 2\}\]
show that if $A$ and $B$ are closed it does not follow
that $A+B$ is closed.

Show however that if $A$ is closed and bounded and
$B$ is closed then $A+B$ is closed.

If $A$ and $B$ are open, does it follow that $A+B$
is open? Give reasons.  

\end{question}
\begin{question}
Consider the map 
$\Omega:{\mathbb R}^{6}\rightarrow{\mathbb R}^{3}$
given by
\[\Omega\begin{pmatrix}{\mathbf x}\\{\mathbf y}
\end{pmatrix}=
{\mathbf x}\wedge{\mathbf y}\]
for all ${\mathbf x},{\mathbf y}\in{\mathbb R}^{3}$.
(Here $\wedge$ is the `vector product' of mathematical
methods.)
Show directly from
the definition that $\Omega$ is differentiable.

Having done this, find the Jacobian matrix of $\Omega$.

\end{question}
\begin{question}[Very traditional] Consider the map
$f:{\mathbb R}^{2}\rightarrow{\mathbb R}$ given 
by
\[f(x,y)=xy|x-y|.\]
At which points is $f$ differentiable? Prove
your statements.

\noindent
[Deal quickly with the easy part. Do not be surprised
if the hard part reveals a special case that must
be dealt with separately.]

\end{question}
\begin{question}[Also very traditional]
(i) Consider the function $f:{\mathbb R}\rightarrow{\mathbb R}$
given by $f(t)=t^{2}\sin 1/t$ for $t\neq 0$, $f(0)=0$.
Show that $f$ is everywhere differentiable and find its derivative.
Show that $f'$ is not continuous.
\noindent
[Deal quickly with the easy part and then
go back to the definition to deal with $t=0$.
There are wide selection of counter-examples
obtained by looking at $t^{\beta}\sin t^{\alpha}$
for various values of $\alpha$ and $\beta$.]

(ii) Find an infinitely differentiable
function $g:(1,\infty)\rightarrow{\mathbb R}$
such that $g(t)\rightarrow 0$ but $g'(t)\nrightarrow 0$
as $t\rightarrow\infty$.

(iii) Consider the function 
$F:{\mathbb R}^{2}\rightarrow{\mathbb R}$
given by $F(s,t)=f(s)+f(t)$ where $f$ is the function
defined in (i). Show that $F$ is everywhere differentiable
but $F_{,1}$ and $F_{,2}$ are not continuous at
$(0,0)$.

\end{question}
\begin{question}
Let $\mathbf{u}$ and $\mathbf{v}$ be linearly
independent vectors in ${\mathbb R}^{2}$.
Suppose that $F:{\mathbb R}^{2}\rightarrow{\mathbb R}$
has the property that
\[\frac{F({\mathbf x}+h{\mathbf u})-F({\mathbf x})}{h}
\rightarrow a({\mathbf x}),\ \text{and}
\ \frac{F({\mathbf x}+h{\mathbf v})-F({\mathbf x})}{h}
\rightarrow b({\mathbf x})\]
as $h\rightarrow 0$ for all ${\mathbf x}\in{\mathbb R}^{2}$.
Suppose further that $a:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is continuous. Show that $F$ is everywhere differentiable.

\end{question}
\begin{question}
Show that Definition~\ref{Definition operator norm}
can be modified to give the definition of an operator
norm $\|\alpha\|$ for a linear map 
$\alpha:{\mathbb C}^{m}\rightarrow{\mathbb C}^{p}$
and that the results of Lemma~\ref{Operator norm}
continue
hold \emph{mutatis mutandis}\footnote{Changing those
things that need to be changed.}.

If $\alpha$ has matrix $(a_{ij})$ with respect to
the standard bases show that
\[\max_{i,j}|a_{ij}|\leq \|\alpha\|\leq mp\max_{i,j}|a_{ij}|.\]

For the rest of this question we take $m=p$ (so we
are dealing with \emph{endomorphisms} of the
vector space ${\mathbb C}^{m}$).

(i) If $\alpha$ has a lower triangular matrix $(a_{ij})$
with respect to the standard basis show that the
set of its eigenvalues is precisely the set
of the diagonal entries $a_{ii}$. (Pure algebra.)

(ii) If $\alpha$ has a lower triangular matrix $(a_{ij})$
and $\epsilon>0$ show that we can find $\beta$ such that
$\beta$ has $m$ distinct non-zero eigenvalues and
$\|\beta-\alpha\|<\epsilon$.

(iii) In algebra you show that given any endomorphism
$\alpha$ we can find an invertible endomorphism
$\theta$ such that $\theta\alpha\theta^{-1}$
has a lower triangular matrix
with respect to the standard basis.
(N.B. This is true for complex vector spaces but not
for real ones.) Use this fact to show that
given any endomorphism
$\alpha$ and any $\eta>0$ we can an endomorphism $\gamma$
such that $\|\gamma-\alpha\|<\eta$ and 
$\gamma$ has $m$ distinct non-zero eigenvalues.

(iv) Write $P_{\alpha}(t)=\det(t\iota-\alpha)$
(where $\iota$ is the identity endomorphism). 
Show that if $\alpha$ has $m$ distinct eigenvalues
then $P_{\alpha}(\alpha)=0$. (Pure algebra.)

(v) Let $\alpha$ be any endomorphism. By considering
a sequence $\alpha_{n}$ of endomorphisms with
$m$ distinct eigenvalues such that 
$\|\alpha_{n}-\alpha\|\rightarrow 0$ as $n\rightarrow \infty$
show that $P_{\alpha}(\alpha)=0$.

\noindent
[This procedure is substantially longer than that used to
prove the Cayley-Hamilton theorem in your algebra
course but it is an instructive way of looking at things.]  
\end{question}    


\section{Third Sheet of Exercises}
I have tried to
produce 12 rather routine questions for each sheet.
Any remaining questions are for interest
only. Ambitious
students and their supervisors should look at Tripos
questions to supplement this meagre fare.

\vspace{1\baselineskip}

\begin{question}
At the beginning of the 19th century it was
hoped that the calculus could be reduced to 
\emph{mere algebra} by using Taylor's theorem.
The following example due to Cauchy shows why
such hopes were misplaced.

Let $E:{\mathbb R}\rightarrow{\mathbb R}$ be defined
by $E(t)=\exp(-1/t^{2})$ for $t\neq 0$ and $E(0)=0$.
Use induction to show that $E$ is infinitely differentiable
with
\begin{align*}
E^{(n)}(t)&=Q_{n}(1/t)E(t)\qquad\text{for $t\neq 0$}\\
E^{(n)}(0)&=0.
\end{align*}
where $Q_{n}$ is a polynomial.
For which values of $t$ is it true that
\[E(t)=\sum_{n=0}^{\infty}\frac{E^{n}(0)t^{n}}{n!}?\] 

Explain why the behaviour of $E$ is consistent
with Lemma~\ref{local} (taking $m=1$).

\end{question}
\begin{question}
In the first year you saw how the find
stationary points for a well behaved
function $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
and how to decide (except in
certain specified cases) whether they
are maxima, minima or saddle-points.
Use Theorem~\ref{2 Taylor} to state and
prove your results rigorously.

In the course on quadratic mathematics you
will see that given any real symmetric $m\times m$
matrix $A$ you can find a special orthogonal
matrix $P$ such that $PAP^{T}$ is diagonal.
Explain the relevance of this result to
the problem of deciding (except in certain
\emph{specified} cases) when a stationary
point of a well behaved function
$f:{\mathbb R}^{m}\rightarrow{\mathbb R}$
is a maximum, a minimum or a saddle point.

\end{question}
\begin{question}
Define $f:[0,1]\rightarrow{\mathbb R}$ by
$f(p/q)=1/q$ when $p$ and $q$ are coprime integers
with $1\leq p <q$ and $f(x)=0$ otherwise.

(i) Show that $f$ is Riemann integrable and
find $\int_{0}^{1}f(x)\,dx$.

(ii) At which points is $f$ continuous? Prove
your answer.

\end{question}
\begin{question}
We say that a function $f:[0,1]\rightarrow{\mathbb R}$
is of bounded variation if there exists a constant $K$
such that whenever
$0\leq x_{0}\leq x_{1}\leq x_{2}\dots\leq x_{n}\leq 1$
we have
\[\sum_{j=1}^{n}|f(x_{j-1})-f(x_{j})|\leq K.\]

(i) Show that if $g:[0,1]\rightarrow{\mathbb R}$ is
increasing then $g$ is of bounded variation. Show that
if $h_{1},h_{2}:[0,1]\rightarrow{\mathbb R}$ are
increasing then $h=h_{1}-h_{2}$ is of bounded variation.

(ii) Suppose that $f$ is of bounded variation. Show
that
\[f_{1}(x)=\sup\{
{\textstyle \sum_{j=1}^{n}f(x_{j})-f(y_{j})}
:0\leq y_{1}\leq x_{1}\leq y_{2}\leq x_{2}\leq
\dots\leq y_{n}\leq x_{n}\leq x\}\]
is a well defined increasing function. Show
that $f_{1}-f$ is also an increasing function
and so $f$ is the difference of two increasing functions.

(iii) Show that if we set $f(x)=x\sin x^{-1}$ for
$x\neq 0$ and $f(0)=0$ then $f:[0,1]\rightarrow{\mathbb R}$
is a continuous function which is not of bounded
variation.

\noindent
[Remember diagrams may not be rigorous but they certainly help
you understand what you are talking about.]

\end{question}
\begin{question}
If $f:[0,1]\rightarrow{\mathbb R}$ is increasing
show that if $x_{n}$ is an increasing sequence
then $f(x_{n})$ tends to limit as $n\rightarrow\infty$.
Deduce carefully that if $0<y\leq 1$ then
$f(x)$ tends to a limit $f(y-)$ as $x\rightarrow y$
through values of $x$ with $x<y$. (We shall set $f(0-)=f(0)$.)
Define $f(y+)$ similarly.

Let us call $f(y+)-f(y-)$ the jump $J(y)$ at $y$. Show
that if $y_{1}$, $y_{2}$, \dots $y_{n}$ are distinct
points of $(0,1)$ then
\[\sum_{j=1}^{n}J(y_{j})\leq f(1)-f(0).\]
Show that $f$ can have only finitely many jumps of
size $1/n$ or more. Deduce that $f$ has only
countably many jumps
and that $f$ must be continuous at some point.

\end{question}
\begin{question}
Let ${\mathcal K}$ be the set of functions
$f:[0,1]\rightarrow{\mathbb R}$ such that $f$ is continuous
on $(0,1]$ and $f$ has an improper Riemann integral
\[{\mathcal I}f=\int_{0}^{1}f(x)\,dx.\]

Show that if $f,g\in \mathcal K$ and $\lambda,\mu\in{\mathbb R}$
then $\lambda f+\mu g\in \mathcal K$ and
\[{\mathcal I}(\lambda f+\mu g)=\lambda {\mathcal I}f
+\mu {\mathcal I}g.\]

By giving a proof or counter-example establish whether
it is always true that if $f,g\in {\mathcal K}$
then $fg\in {\mathcal K}$.

\end{question}
\begin{question}
Suppose that $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
is continuous. By using results on the differentiation
of integrals which should be quoted exactly show that
\[\frac{d\ }{dt}\int_{a}^{t}
\left(\int_{c}^{d}f(u,v)\,dv\right)\,du
=\frac{d\ }{dt}\int_{c}^{d}
\left(\int_{a}^{t}f(u,v)\,du\right)\,dv.\]
Deduce that
\[\int_{a}^{b}
\left(\int_{c}^{d}f(u,v)\,dv\right)\,du
=\int_{c}^{d}
\left(\int_{a}^{b}f(u,v)\,du\right)\,dv,\]
or, more informally
\[\int_{a}^{b}
\int_{c}^{d}f(u,v)\,dv\,du
=\int_{c}^{d}
\int_{a}^{b}f(u,v)\,du\,dv.\]

\end{question}
\begin{question}\label{First whinge}
Prove all the results of Section~\ref{section metric}.
(A long but easy and useful exercise. In order to keep the
noise level from whinging students down to tolerable
levels this counts as 2 questions.)
\end{question}
\begin{question}
See Question~\ref{First whinge}.
\end{question}
\begin{question}\label{3.10}
Consider ${\mathcal E}$ the space of linear
maps $\alpha:{\mathbb R}^{u}\rightarrow{\mathbb R}^{u}$.
We wish to show that $d(\alpha,\beta)=\|\alpha -\beta\|$
(with $\|\alpha\|$ the usual `operator norm') defines
a complete metric. 

To this end consider a Cauchy sequence $\alpha_{n}$ in
${\mathcal E}$. Show that if $\mathbf{x}\in{\mathbb R}^{u}$
then $\alpha_{n}(\mathbf{x})$ tends to limit, call it
$\alpha(\mathbf{x})$. Now show that 
$\alpha:{\mathbb R}^{u}\rightarrow{\mathbb R}^{u}$
is linear. By using the inequality
\begin{align*}
\|\alpha(\mathbf{x})-\alpha_{n}(\mathbf{x})\|&
\leq\|\alpha_{m}(\mathbf{x})-\alpha_{n}(\mathbf{x})\|
+\|\alpha_{m}(\mathbf{x})-\alpha(\mathbf{x})\|\\
&\leq\sup_{p,q\geq n}\|\alpha_{p}(\mathbf{x})-\alpha_{q}(\mathbf{x})\|
+\|\alpha_{m}(\mathbf{x})-\alpha(\mathbf{x})\|,
\end{align*}
and allowing $m\rightarrow\infty$ show that
\[\|\alpha-\alpha_{n}\|\leq\sup_{p,q\geq n}\|\alpha_{p}-\alpha_{q}\|,\]
and deduce that $\|\alpha-\alpha_{n}\|\rightarrow\infty$
as $n\rightarrow\infty$.

\end{question}
\begin{question}\label{3.11}
In Question~\ref{3.10}
we showed that $\mathcal{E}$ with
the appropriate metric is complete. In this question
we make use of the result.

(i) Suppose $\alpha\in\mathcal{E}$ and $\|\alpha\|<1$.
Let 
\[\sigma_{n}=\iota+\alpha+\alpha^{2}+\dots+\alpha^{n}.\]
Show that $\sigma_{n}$ converges to a limit $\sigma$,
say.

Compute $(\iota-\alpha)\sigma_{n}$ and show, carefully
that $(\iota-\alpha)\sigma=\iota$.

(iii) Deduce that if $\|\iota-\gamma\|<1$ then $\gamma$
is invertible.

(iv) Show that if $\beta\in\mathcal{E}$ is invertible
and $\|\beta-\tau\|<\|\beta^{-1}\|^{-1}$ then $\tau$
is invertible.

(v) Conclude that the invertible elements of ${\mathcal E}$
form an open set.
\end{question}
\begin{question}  
Consider $C([-1,1])$ the set of continuous
functions $f:[-1,1]\rightarrow{\mathbb R}$. Show that,
if we set
\[d(f,g)=\|f-g\|_{2}=
\left(\int_{-1}^{1}|f(x)-g(x)|^{2}\,dx\right)^{1/2}\]
then $d$ is a metric but $d$ is not complete.

Show that if $\|f-g\|_{1}$ is defined as in
Example~\ref{one norm} then $2^{1/2}\|f\|_{2}\geq \|f\|_{1}$
for all $f\in C([-1,1])$ but that, given any
constant $K>0$ we can find a $g\in C([-1,1])$
such that $\|g\|_{2}\geq K\|g\|_{1}$.
\end{question}

\vspace{1\baselineskip}

The remaining question is included for general interest
and not for relevance to the syllabus or to passing Tripos exams.
\vspace{1\baselineskip}

\begin{question}
We use the notation of Questions~\ref{3.10} and~\ref{3.11}.

(i) Show that if $\alpha\in\mathcal{E}$ we can find
$e^{\alpha}\in\mathcal{E}$ such that
\[\left\|\sum_{r=0}^{n}\frac{\alpha^{r}}{r!}
-e^{\alpha}\right\|\rightarrow 0\]
as $n\rightarrow\infty$.

(ii) Show carefully that if $\alpha$ and $\beta$ commute
\[e^{\alpha}e^{\beta}=e^{\alpha+\beta}.\]

(iii) Show that if $\alpha$ and $\beta$ are general
(not necessarily commuting) elements of ${\mathcal E}$
then
\[\left\|h^{-2}(e^{h\alpha}e^{h\beta}-e^{h\beta}e^{h\alpha})
-(\alpha\beta-\beta\alpha)\right\|\rightarrow 0\]
as the real number $h\rightarrow 0$.

Conclude that, in general, $e^{\alpha}e^{\beta}$
and $e^{\alpha+\beta}$ need not be equal.
\end{question}
\newpage
\section{Fourth Sheet of Exercises}
I have tried to
produce 12 rather routine questions for each sheet.
The remaining questions are for interest
only. Ambitious
students and their supervisors should look at Tripos
questions to supplement this meagre fare.

\vspace{1\baselineskip}

\begin{question}\label{Odd railway}
Consider the two railway metrics
$d_{1}$ and $d_{2}$ on ${\mathbb R}^{2}$ given in
Examples~\ref{Railway 1} and~\ref{Railway 2}.
For each of the two metrics give a reasonably
simple description of the open balls
of radius $r$ and centre ${\mathbf x}$
when  ${\mathbf x}={\mathbf 0}$
and when  ${\mathbf x}\neq{\mathbf 0}$
and (a)~$r\leq\|{\mathbf x}\|$, (b)~$r<\|{\mathbf x}\|\leq 2r$,
(c)~$2r<\|{\mathbf x}\|$.

Explain the statement `when finding open sets only
balls of small radius are important so cases
(b) and (c) are irrelevant'.

Give the simplest description you can find
of open sets in the two metrics.
\end{question}

\begin{question}
Let $f:{\mathbb R}^{2}\rightarrow{\mathbb R}$
be differentiable  and let $g(x)=f(x,c-x)$ where $c$
constant. Show that $g:{\mathbb R}\rightarrow{\mathbb R}$
is differentiable and find its derivative

(i) directly from the definition of differentiability

\noindent
and also

(ii) by using the chain  rule.

\noindent
Deduce that if $f_{,1}=f_{,2}$ throughout ${\mathbb R}$
then $f(x,y)=h(x+y)$ for some differentiable
function $h$.

\end{question}
\begin{question}
[Traditional] Consider the functions
$f_{n}:[0,1]\rightarrow{\mathbb R}$ defined by
$f_{n}(x)=n^{p}x\exp(-n^{q}x)$ where $p,q>0$.

(i) Show that $f_{n}$ converges pointwise on $[0,1]$.

(ii) Show that if $p<q$ then $f_{n}$ converges uniformly
on $[0,1]$.

(iii) Show that if $p\geq q$ then $f_{n}$ does not converge
uniformly on $[0,1]$. Does $f_{n}$ converge
uniformly on $[0,1-\epsilon]$? 
Does $f_{n}$ converge
uniformly on $[\epsilon,1]$?
(Here $0<\epsilon<1$, you should justify your answers.)

\end{question}
\begin{question}
Let $(X,d)$ be a metric space. We say
that $E$ is dense in $X$ if whenever $x\in X$
we can find $e_{n}\in E$ with $e_{n}\rightarrow x$.

(i) Show that the rationals are dense in the reals
(i.e. $\mathbb Q$ is dense in $\mathbb R$ if we give
$\mathbb R$ the usual metric). Show that the
irrationals are dense in the reals.

\noindent
[You should make explicit any use you make of the axiom
of Archimedes.]

(ii) Consider the space $C([0,1])$ of continuous
functions with the uniform norm. Show that the
piecewise linear functions are dense in $C([0,1])$.

\end{question}
\begin{question}\label{4.3}
Recall from your courses
in mathematical methods that, if 
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a 
continuous periodic function with period $2\pi$
then we  write
\[\hat{f}(n)=\frac{1}{2\pi}
\int_{-\pi}^{\pi}f(t)\exp(-int)\,dt.\]

Show, quoting carefully any results that you use,
that, if $a_{n}\in{\mathbb C}$ and 
$\sum_{n=-N}^{N} |a_{n}|$ converges,
then $\sum_{n=-N}^{N} a_{n}\exp(int)$ converges
uniformly to a continuous periodic function
$f(t)$, say, with period $2\pi$.
Show that $\hat{f}(n)=a_{n}$. for each $n$.

\noindent
[This chain of thought is continued in Question~\ref{4.13}.]
\end{question}
\begin{question}
Recall that $\|\ \|$ is said to be a norm
on a real vector space $V$ if the following conditions
hold.

(1) $\|\mathbf{x}\|\geq 0$ for all $\mathbf{x}\in V$.

(2) $\|\mathbf{x}\|=0$ implies $\mathbf{x}=\mathbf{0}$.
     
(3) $\|\lambda\mathbf{x}\|=|\lambda|\|\mathbf{x}\|$
for all $\lambda\in{\mathbb R}$ and all $\mathbf{x}\in V$.

(4) $\|\mathbf{x}+\mathbf{y}\|\leq \|\mathbf{x}\|+
\|\mathbf{y}\|$ for all $\mathbf{x},\mathbf{y}\in{\mathbb R}$.

(i) Let $\|\ \|_{2}$ be the usual Euclidean norm
on ${\mathbb R}^{m}$ and let $\|\ \|$ be another
norm on ${\mathbb R}^{m}$. Let 
$I:({\mathbb R}^{m},\|\ \|_{2})\rightarrow ({\mathbb R}^{m},\|\ \|)$
be the identity map, given by $I\mathbf{x}=\mathbf{x}$.
By considering basis vectors or 
otherwise that there exists a constant $K$ 
such that $\|\mathbf{x}\|\leq K\|\mathbf{x}\|_{2}$
for all $\|\mathbf{x}\|\in {\mathbb R}^{m}$. Deduce
that $I$ is continuous. 

By applying a theorem on continuous functions on closed
bounded sets show that there exists a $k>0$ such that
$\|\mathbf{x}\|\geq k$ for all $\mathbf{x}$ with
$\|\mathbf{x}\|_{2}=1$. Conclude that  
\[k\|\mathbf{x}\|_{2}\leq
\|\mathbf{x}\|\leq K\|\mathbf{x}\|_{2}\]
for all $\|\mathbf{x}\|\in {\mathbb R}^{m}$.

\noindent
[Thus all norms on a finite dimensional space are
essentially the same.]


(ii) Consider the real vector space $\mathbb{R}^{\mathbb{N}}$
of all real sequences $\mathbf{x}=(x_{1},x_{2},\dots)$
with the usual vector addition and multiplication
by scalars.
Let $V$ be the set of all sequences 
$\mathbf{x}=(x_{1},x_{2},\dots)$ such that $x_{j}\neq 0$ for
only finitely many $j$. Show that $V$ is a subspace of
$\mathbb{R}^{\mathbb{N}}$ and so a vector space in its own right.
By considering norms of the form 
$\|\mathbf{x}\|=\max_{j}\kappa_{j}|x_{j}|$, or otherwise,
find two norms $\|\ \|_{A}$ and $\|\ \|_{B}$ such that
\[\sup_{\|\mathbf{x}\|_{A}=1}\|\mathbf{x}\|_{B}
=\sup_{\|\mathbf{x}\|_{B}=1}\|\mathbf{x}\|_{A}=\infty.\]

\end{question}
\begin{question} (i) Suppose that $f:{\mathbb R}\rightarrow{\mathbb R}$
is a twice differentiable function such that
\[\left|\frac{f(x)f''(x)}{f'(x)^{2}}\right|
\leq\lambda\]
for all $x$ and some $|\lambda|<1$
Show that the mapping
\[Tx=x-\frac{f(x)}{f'(x)}\]
is a contraction mapping and deduce that $f$ has
a unique root $y$.

(ii) Suppose that $F:{\mathbb R}\rightarrow{\mathbb R}$
is a twice differentiable function such that
\[\left|\frac{F(x)F''(x)}{F'(x)^{2}}\right|
\leq\lambda\]
for all $|x|\leq a$ and some $|\lambda|<1$
and that $F(0)=0$. 
Consider the mapping
\[Tx=x-\frac{F(x)}{F'(x)}.\]
Show that $T^{n}x\rightarrow 0$.

Suppose that
\[\frac{\sup_{|t|\leq a}|f'(t)|\sup_{|t|\leq a}|f''(t)|}
{\inf_{|t|\leq a}|f'(t)|^{2}}=M.\]
By using the mean value theorem twice,
show that if $|x|\leq a$ then
\[|Tx|\leq Mx^{2}.\]

(iii) If you know what the Newton--Raphson method is,
comment on the relevance of the results of (i) and (ii)
to that method.


\end{question}
\begin{question}
We work in ${\mathbb R}^{m}$
with the usual `Euclidean norm'.
The following line of thought is
extremely important in later work. Suppose
we want the solution ${\mathbf x}_{0}$  of
\[{\mathbf x}+{\boldsymbol\epsilon}({\mathbf x})=
{\mathbf y}\]
where ${\boldsymbol\epsilon}({\mathbf x})$ is 
`a small error term' or `of first order compared
to ${\mathbf x}$. The following method
of solution is traditional (for the excellent
reason that it usually works like a Spanish charm).
Start by ignoring the small ${\boldsymbol\epsilon}$
term and guess ${\mathbf x}_{0}\approx
{\mathbf x}_{1}={\mathbf y}$.
Since this initial guess is good we estimate
the small ${\boldsymbol\epsilon}$
term as
${\boldsymbol\epsilon}({\mathbf x}_{1})$.
Feeding this estimate back into the equation
we now guess ${\mathbf x}_{0}\approx
{\mathbf x}_{2}={\mathbf y}-{\boldsymbol\epsilon}({\mathbf x}_{1})$.
Repeating this process gives us the rule for
successive approximations
\[\mathbf{x}_{n+1}=\mathbf{y}-{\boldsymbol\epsilon}(\mathbf{x}_{n}).\]
We now try to justify this in certain cases.

(i) Suppose that ${\boldsymbol\epsilon}({\mathbf 0})={\mathbf 0}$
and 
\[\|{\boldsymbol\epsilon}({\mathbf a})-
{\boldsymbol\epsilon}({\mathbf b})\|
<\|{\mathbf a}-{\mathbf b}\|/2\]
for all $\|\mathbf{a}\|,\|\mathbf{b}\|\leq\delta$ where $\delta>0$.
(Note that these are rather stronger conditions
than might have been expected. However, this is not
to say that the general idea might not work under
weaker conditions.)
We shall show that the method works for $\|{\mathbf y}\|\leq\delta/2$.

Consider the closed ball 
$B=\{{\mathbf x}:\|{\mathbf x}\|\leq \delta\}$.
Show that the equation
\[T({\mathbf x})=\mathbf{y}-{\boldsymbol\epsilon}({\mathbf x})\]
defines a map $T:B\rightarrow B$. Now use the contraction
mapping theorem to show that
\[{\mathbf x}+{\boldsymbol\epsilon}({\mathbf x})=
{\mathbf y}\]
has a unique solution ${\mathbf x}(\mathbf{y})$ with
$\|{\mathbf x}(\mathbf{y})\|\leq\delta$ and that
$T^{n}({\mathbf y})\rightarrow {\mathbf x}(\mathbf{y})$
as $n\rightarrow\infty$.

(ii) Suppose that $f:{\mathbb R}^{m}\rightarrow{\mathbb R}^{m}$
is a differentiable function with derivative continuous
at ${\mathbf 0}$, $f({\mathbf 0})={\mathbf 0}$ 
and $Df({\mathbf 0})=I$. Show that
we can find a $\delta>0$ such that whenever
$\|{\mathbf y}\|\leq\delta/2$ the equation
\[f({\mathbf x})={\mathbf y}\]
has a unique solution ${\mathbf x}(\mathbf{y})$ with
$\|{\mathbf x}(\mathbf{y})\|\leq\delta$.

(iii) Suppose that $f:{\mathbb R}^{m}\rightarrow{\mathbb R}^{m}$
is a differentiable function
with derivative continuous
at ${\mathbf 0}$, $f({\mathbf 0})={\mathbf 0}$ 
and $Df({\mathbf 0})$ invertible. Show that
we can find a $\delta_{1},\delta_{2}>0$ such that whenever
$\|{\mathbf y}\|\leq\delta_{1}$ the equation
\[f({\mathbf x})={\mathbf y}\]
has a unique solution ${\mathbf x}(\mathbf{y})$ with
$\|{\mathbf x}(\mathbf{y})\|\leq\delta_{2}$.


\end{question}
\begin{question} (A method of Abel) (i) Suppose that
$a_{j}$ and $b_{j}$ are sequences of complex numbers
and that $S_{n}=\sum_{j=1}^{n}a_{j}$ for $n\geq 1$
and $S_{0}=0$. Show that, if $1\leq u\leq v$ then
\[\sum_{j=u}^{v}a_{j}b_{j}=\sum_{j=u}^{v}S_{j}(b_{j}-b_{j+1})
-S_{u-1}b_{u}+S_{v}b_{v+1}.\]
(This is known as partial summation, for obvious reasons.)

(ii) Suppose now that, in addition, the $b_{j}$
form a decreasing sequence of positive terms
and that $|S_{n}|\leq K$ for all $n$.
Show that
\[\left|\sum_{j=u}^{v}a_{j}b_{j}\right|
\leq 2Kb_{u}.\]
Deduce that if $b_{j}\rightarrow 0$ as $j\rightarrow\infty$
then $\sum_{j=1}^{\infty}a_{j}b_{j}$ converges.

Deduce the alternating series test.

(iii) If $b_{j}$ is a decreasing sequence of positive terms
with $b_{j}\rightarrow 0$ as $j\rightarrow\infty$ show
that  $\sum_{j=1}^{\infty}b_{j}z^{j}$ converges uniformly
in the region given by $|z|\leq 1$ and $|z-1|\geq \epsilon$
for all $\epsilon>0$.


\end{question}
\begin{question}\label{4.9}
We work in ${\mathbb R}^{m}$ with the usual distance.

(i) Show that if $A_{1}$, $A_{2}$, are non-empty,
closed and bounded with $A_{1}\supseteq A_{2}\supseteq\dots$
then $\bigcap_{j=1}^{\infty}A_{j}$ is non empty.
Is this result true if we merely assume $A_{j}$ closed and non-empty?
Give reasons.

(ii) If $A$ is non-empty, closed and bounded show that we can find
${\mathbf a}',{\mathbf b}'\in A$ such that
\[\|{\mathbf a}'-{\mathbf b}'\|\geq\|{\mathbf a}-{\mathbf b}\|\]
for all ${\mathbf a},{\mathbf b}\in A$.
Is this result true if we merely assume $A$ bounded and non-empty?
Give reasons.


\end{question}
\begin{question}
We work in ${\mathbb R}^{m}$ with the usual distance.
Let $E$ be a closed non-empty subset of ${\mathbb R}^{m}$
and let $T$ be a map $T:E\rightarrow E$.

(i) Suppose
$\|T({\mathbf a})-T({\mathbf b})\|<\|{\mathbf a}-{\mathbf b}\|$
for all ${\mathbf a},{\mathbf b}\in E$
with ${\mathbf a}\neq{\mathbf b}$. We saw in
Example~\ref{no fixed} that $T$ need not have a fixed point.
Show that, if $T$ has a fixed point, it is unique.

(ii) Suppose
$\|T({\mathbf a})-T({\mathbf b})\|>\|{\mathbf a}-{\mathbf b}\|$
for all ${\mathbf a},{\mathbf b}\in E$
with  ${\mathbf a}\neq{\mathbf b}$.
In Question~\ref{4.16} it is shown that $T$ need not have a fixed
point. Show
that, if $T$ has a fixed point, it is unique.

(iii) Suppose
$\|T({\mathbf a})-T({\mathbf b})\|=\|{\mathbf a}-{\mathbf b}\|$
for all ${\mathbf a},{\mathbf b}\in E$.
Show that $T$ need not have a fixed point.
and that, if $T$ has a fixed point, it need not be unique.

(iv) Suppose now that $E$ is non-empty, closed and bounded and
\[\|T({\mathbf a})-T({\mathbf b})\|<\|{\mathbf a}-{\mathbf b}\|\]
for all ${\mathbf a},{\mathbf b}\in E$
with ${\mathbf a}\neq{\mathbf b}$.
By considering $\inf_{{mathbf x}\in E}\|{\mathbf x}-T({mathbf x})\|$,
or otherwise
show that $T$ has a fixed point.


\end{question}
\begin{question}
From time to time numerical analysts mention
the \emph{spectral radius}. It forms no part of any
of non-optional 1B courses but the reader may be interested
to see what it is.

(i) Give an example of a linear map
$\beta:{\mathbb R}^{m}\rightarrow{\mathbb R}^{m}$
such that $\beta^{m-1}\neq {\mathbf 0}$ but
$\beta^{m}={\mathbf 0}$.

(ii) Let $\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{m}$
be linear.
If $n=jk+r$ explain why
\[\|\alpha^{jk+r}\|\leq \|\alpha^{j}\|^{k}\|\alpha\|^{r}.\]

(iii) Continuing with the hypotheses of (ii), show that
$\Delta=\inf_{n}\|\alpha^{n}\|^{1/n}$ is well defined
and, by using the result of (ii), or otherwise, that
\[\|\alpha^{n}\|^{1/n}\rightarrow \Delta.\]
We call $\Delta$ the spectral radius of $\alpha$
and write $\rho(\alpha)=\Delta$.

(iv) If $\alpha:{\mathbb R}^{m}\rightarrow{\mathbb R}^{m}$
is diagonalisable show that
\[\rho(\alpha)=\max\{|\lambda|:\lambda\ \text{an eigenvalue
of $\alpha$}\}.\]

(v) Give an example of linear maps
$\alpha,\beta:{\mathbb R}^{2}\rightarrow{\mathbb R}^{2}$
such that $\rho(\alpha)=\rho(\beta)=0$ but
$\rho(\alpha+\beta)=1$.

(vi) Recalling Question~\ref{3.11} of the third sheet, show that
if $\rho(\iota-\gamma)<1$ then $\gamma$ is invertible.


\end{question}

\vspace{1\baselineskip}

The remaining questions are included for general interest
and not for relevance to the syllabus or to passing Tripos exams.

\begin{question}\label{4.12}
We work in ${\mathbb R}^{m}$, 
$\|{\mathbf x}-{\mathbf y}\|$ will represent the
usual Euclidean distance between ${\mathbf x}$ and
${\mathbf y}$.

(i) If $K$ is a closed non-empty bounded set and ${\mathbf x}$
is any point, show that there exists a point
${\mathbf k}'\in K$ such that  
\[\|{\mathbf x}-{\mathbf k}\|\geq \|{\mathbf x}-{\mathbf k}'\|\]
for all ${\mathbf k}\in K$.  Is ${\mathbf k}'$ 
necessarily unique?

(ii) If $E$ is a non-empty closed set and ${\mathbf x}$
is any point, show that there exists a point
${\mathbf e}'\in E$ such that  
\[\|{\mathbf x}-{\mathbf e}\|\geq \|{\mathbf x}-{\mathbf e}'\|\]
for all ${\mathbf e}\in E$. 
We write $d({\mathbf x},E)=\|{\mathbf x}-{\mathbf e}'\|.$

(iii) With the notation of (ii) show that
\[d({\mathbf x},E)+\|{\mathbf x}-{\mathbf y}\|
\geq d({\mathbf y},E).\]
Show that $d(\ ,E):{\mathbb R}^{m}\rightarrow{\mathbb R}$
is continuous.

(iv) If $E$ is closed and non-empty and $K$ closed,
bounded
and non-empty show that
there exist ${\mathbf e}'\in E$ and ${\mathbf k}'\in K$
such that
\[\|{\mathbf e}-{\mathbf k}\|\geq \|{\mathbf e}'-{\mathbf k}'\|.\]
Would this result be true if we only assumed $E$ and $K$
closed and non-empty. 

\end{question}

\begin{question}\label{4.13}
(i)  Show that if
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuous periodic function with period $2\pi$
such that $\hat{f}(n)=0$ for all $n$
then
\[\int_{-\pi}^{\pi}(1-\epsilon_{1}+\epsilon_{2}\cos t)^{N}f(t)\,dt=0\]
for all $N$.

(ii) Show that, given $\delta>0$ we can find 
$\epsilon_{1},\ \epsilon_{2}>0$
and an $\eta>0$ such that
\begin{align*}
(1-\epsilon_{1}+\epsilon_{2}\cos t)^{N}\rightarrow\infty&\ \text{uniformly
for $|t|\leq\eta$}\\
(1-\epsilon_{1}+\epsilon_{2}\cos t)^{N}\rightarrow 0&\ \text{uniformly
for $\delta\leq |t|\leq\pi$}
\end{align*}
as $N\rightarrow\infty$.

(iii) Show that if $f:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuous periodic function with period $2\pi$
such that $f(0)$ is real and $f(0)>0$ 
we can find $\epsilon_{1},\ \epsilon_{2}>0$ such that
\[\Re\int_{-\pi}^{\pi}(1-\epsilon_{1}+\epsilon_{2}\cos t)^{N}f(t)\,dt
\rightarrow\infty\]
as $N\rightarrow\infty$.

(iv) Show that if
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuous periodic function with period $2\pi$
such that $\hat{f}(n)=0$ for all $n$
then $f(t)=0$ for all $t$.

(v) By using Question~\ref{4.3}, or otherwise, show that if
if
$F:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuous periodic function with period $2\pi$
such that $\sum |\hat{F}(n)|$ converges then
\[F(t)=\sum_{n=-\infty}^{\infty}\hat{F}(n)\exp (int).\]

\end{question}
\begin{question}
(i)  Suppose that
$g:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuous periodic function with period $2\pi$.
Show that
\[\frac{1}{2\pi}\int_{0}^{2\pi}
\left|g(t)-\sum_{r=-N}^{N}\hat{g}(r)\exp(irt)\right|^{2}
\,dt
=\frac{1}{2\pi}\int_{0}^{2\pi}|g(t)|^{2}\,dt
-\sum_{r=-N}^{N}|\hat{g}(r)|^{2}.\]
Deduce that
\[\sum_{r=-N}^{N}|\hat{g}(r)|^{2}
\leq\frac{1}{2\pi}\int_{0}^{2\pi}|g(t)|^{2}\,dt.\]
(This is a version of Bessel's inequality.)

(ii) Now suppose that
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuously differentiable
periodic function with period $2\pi$.
If $f$ has derivative $g$, obtain a simple
relation between $\hat{f}(n)$ and $\hat{g}(n)$.
By applying the Cauchy-Schwarz inequality
to $\sum_{n\neq 0,\ |n|\leq N} |n||\hat{g}(n)|$ show that
\[\sum _{n\neq 0,\ |n|\leq N} |\hat{f}(n)|
\leq A \frac{1}{2\pi}\int_{0}^{2\pi}|g(t)|^{2}\,dt,\]
for some constant $A$.
Conclude that  $\sum |\hat{f}(n)|$ converges. 

(iii) Deduce, using Question~\ref{4.13}, that if
$f:{\mathbb R}\rightarrow{\mathbb C}$ is a
continuously differentiable
periodic function with period $2\pi$
then
\[f(t)=\sum_{n=-\infty}^{\infty}\hat{f}(n)\exp (int).\]

\end{question}
\begin{question}
We continue with the ideas of Question~\ref{4.12}.
From now on all sets will belong to the collection
$\mathcal{K}$
of closed bounded non-empty subsets of ${\mathbb R}^{m}$.
Define
\[d(E,F)=\sup_{e\in E}d(e,F)+\sup_{f\in F}d(f,E).\]
Show that $d$ is a metric on $\mathcal{K}$.

Show also that $d$ is a complete metric.

\end{question}
\begin{question}\label{4.16} If $(X,d)$ is a complete metric
space and $T:X\rightarrow X$ is a surjective
map such that
\[d(Tx,Ty)\geq Kd(x,y)\]
for all $x,y\in X$ and some $K>1$ show that
$T$ has a unique fixed point.


By considering the map 
$T:{\mathbb R}\rightarrow{\mathbb R}$ defined by
$T(x)=1+4n+2x$ for $0\leq x<1$ and $n$ an integer,
or otherwise show that the condition $T$ surjective
can not be dropped.
\end{question}
\begin{question}[A continuous nowhere differentiable function]
The following construction is due to Van der Waerden.

(i) Sketch the graph of the function $g$ given by the condition
\[g(x+k)=|x|\ \ \ \ \mbox{if $k$ is any integer and
$-1/2<x\leq 1/2$.}\]

(ii) Let $g_{n}(t)=2^{-n}g(8^{n}t)$. Sketch the graphs of
$g_{1}$, $g_{2}$ and $g_{3}$.

(iii) Let $f_{n}(t)=g_{1}(t)+g_{2}(t)+\ldots+g_{n}(t)$. Sketch the
graphs of $f_{1}$, $f_{2}$ and $f_{3}$.

(iv) Show that $f_{n}$ converges uniformly to a continuous function 
$f$.

(v) If $n$ is a positive integer and $r$ an odd integer give
an expression for $f(r2^{-n})$. Show that when $n$ is large
then $2^{n}|f((r+2)2^{-n})-f(r2^{-n})|$ is large.
Conclude that
if $r2^{-n}<t<(r+2)2^{-n}$ then
\[\max\left(\left|\frac{f(t)-f(r2^{-n})}{t-r2^{-n}}\right|,
\left|\frac{f((r+2)2^{-n})-f(t)}{(r+2)2^{-n}-t}\right|\right).\]

(vi) By developing the ideas of part (v) show that $f$ is
nowhere differentiable.
\end{question}
\begin{center}
{\bf Final Note To Supervisors}
\end{center}
Let me reiterate my request for corrections and improvements
\emph{particularly to the exercises}. The easiest
path for me is e-mail. My e-mail address is \verb+twk@dpmms+.
I have to express my gratitude to Drs Barden and Pinch
for finding a multitude of errors but I am sure that once the first
veil of error has been lifted a further veil of deeper errors
will appear.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercises}

\begin{exercise}
Which of the following sequences of functions converge uniformly on $X$?
\ben
\item [(a)] $f_n(x) = x^n$ on $X = \bb{0, \frac 12}$;
\item [(b)] $f_n(x) = \sin(n^2x)/ \log n$ on $X = \R$;
\item [(c)] $f_n(x) = x^n$ on $X = (0, 1)$;
\item [(d)] $f_n(x) = x^n - x^{2n}$ on $X = [0, 1]$;
\item [(e)] $f_n(x) = xe^{-nx}$ on $X = [0,\infty)$;
\item [(f)] $f_n(x) = e^{-x^2}\sin(x/n)$ on $X = \R$.
\een
\end{exercise}

Solution. \ben
\item [(a)] Yes. $\forall \ve >0$, take $N>\log_2\frac 1{\ve}$, then $\forall n\geq N$, $\forall x \in \bb{0,\tfrac 12}$, we have $f(x)=0$
\be
\abs{f_n(x)-f(x)} = \abs{x^n-0} = x^n < \frac 1{2^n} < \frac 1{2^N} < \ve.
\ee

\item [(b)] Yes. $\forall \ve >0$, take $N>e^{\frac 1{\ve}}$, then $\forall n\geq N$, $\forall x \in \R$, we have $f(x)=0$
\be
\abs{f_n(x)-f(x)} = \abs{\frac{\sin(n^2x)}{\log n}-0} \leq \frac 1{\log n} < \frac 1{\log N} < \ve.
\ee

\item [(c)] No. $\forall x\in(0,1)$, $\lim_{n\to\infty}f_n(x) = 0$, but $\forall n\in \N$, 
\be
f_n\bb{2^{-\frac 1n}} =\frac 12,
\ee
so the convergence is not uniform.

\item [(d)] No. $\forall x\in(0,1)$, $\lim_{n\to\infty}f_n(x) = 0$, but $\forall n\in \N$, 
\be
f_n\bb{2^{-\frac 1n}} =\frac 14,
\ee
so the convergence is not uniform. 

\item [(e)] Yes. Each $f_n$ is differentiable, with 
\be
f_n'(x) = e^{-nx} - nxe^{-nx} = (1-nx)e^{-nx},
\ee
so the maximum value occurs at $\frac 1n$ and $f_n\bb{\tfrac 1n} = \tfrac 1{ne}$, thus, the maximum value tends to 0 as $n\to \infty$.

\item [(f)] Yes. $\forall \ve >0$, choose $X>0$ such that $e^{-X^2}<\ve$, and take $N> \tfrac X{\ve}$, then $\forall n\geq N$, $\forall x \in \R$, if $\abs{x} > X$ we have 
\be
\abs{f_n(x)} = \abs{e^{-x^2}\sin(x/n)} \leq \abs{e^{-x^2}} < \abs{e^{-X^2}} < \ve,
\ee
while if $\abs{x} \leq X$, we have
\be
\abs{f_n(x)} = \abs{e^{-x^2}\sin(x/n)} \leq \abs{\sin \frac xn} < \sin \frac Xn < \frac Xn \leq \frac XN < \ve.
\ee

\een

\begin{exercise}
Suppose that $f : [0, 1] \mapsto \R$ is continuous. Show that the sequence $(x^nf(x))$ is uniformly convergent on $[0, 1]$ if and only if $f(1) = 0$.
\end{exercise}

Solution. "$\Longrightarrow$". Since a uniform limit of continuous functions is continuous, and the limit as $n\to \infty$ of $x^nf(x)$ is 0 if $x<1$ and $f(1)$ if $x=1$, for $x^n f(x)$ to converge uniformly requires $f(1)=0$.

"$\Longleftarrow$". Conversely, suppose $f(1)=0$, as $f$ is continuous on a closed bounded interval, it is bounded. So $\exists M$ s.t. $\abs{f(x)} \leq M$ for all $x\in [0,1]$. Given $\ve >0$, $\exists\delta>0$ s.t.
\be
\abs{x-1}<\delta, \ \abs{f(x)-f(1)} = \abs{f(x)} < \ve \quad(\text{definition of continuity at 1}).
\ee

Choose $N$ with 
\be
\bb{1-\frac 12 \delta}^N < \frac {\ve}M,
\ee

then $\forall n\geq N$, $\forall x\in [0,1]$, if $x< 1 -\tfrac 12 \delta $ we have
\be
\abs{x^nf(x)} < \bb{1-\frac 12 \delta}^nM \leq \bb{1-\frac 12 \delta}^NM < \ve,
\ee
while if $x\geq 1 -\tfrac 12 \delta$ we have
\be
\abs{x^nf(x)} \leq \abs{f(x)} < \ve.
\ee

\begin{exercise}\label{ques:x^2_uniform_continuous} 
Let $f$ and $g$ be uniformly continuous real-valued functions on a set $E \subseteq \R$. Show that the pointwise sum $f + g$ is uniformly continuous on $E$, and so is $\lm f$ for each real constant $\lm$. Give an example showing that the (pointwise) product $fg$ need not be uniformly continuous on $E$. Is
it possible to find such an example with $f$ bounded?
\end{exercise}

Solution. Since $f$ ang $g$ are uniformly continuous, $\forall \ve >0$, then $\exists \delta_1>0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta_1$,
\be
\abs{f(x)-f(y)} < \frac {\ve}2,
\ee
$\exists \delta_2>0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta_2$,
\be
\abs{g(x)-g(y)} < \frac {\ve}2.
\ee

Take $\delta = \min\{\delta_1,\delta_2\}$, we have $\forall \ve>0$, $\exists \delta>0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta$,
\be
\abs{f(x)+g(x)-f(y)-g(y)} \leq \abs{f(x)-f(y)} + \abs{g(x)-g(y)} < \frac {\ve}2 + \frac {\ve}2 = \ve.
\ee

so $f+g$ is uniformly continuous.

Now given $\lm \in \R$, if $\lm = 0$ the result is trivial, so assume $\lm \neq 0$. $\forall \ve >0$, $\exists \delta >0$ s.t. $\forall x,y \in E$ and $\abs{x-y}<\delta$,
\be
\abs{f(x)-f(y)} < \frac {\ve}{\abs{\lm}} \ \ra \ \abs{(\lm f)(x)-(\lm f)(y)} = \abs{\lm} \cdot \abs{f(x)-f(y)} < \abs{\lm}\cdot \frac {\ve}{\abs{\lm}} = \ve.
\ee
so $\lm f$ is uniformly continuous.

Let $E = \R$ and $f(x)=g(x)=x$, then $f$ and $g$ are both uniformly continuous (given $\ve>0$, we may let $\delta = \ve$), but we have
\be
fg(x) = x^2,\quad x\in \R.
\ee

Take $\ve = 1$, then $\forall \delta >0$, $\exists x > \frac 1{\delta}$ and set $y = x+ \frac {\delta}2$, then $\abs{x-y} = \frac {\delta}2 < \delta$ but
\be
\abs{fg(x)-fg(y)} = \abs{x^2 - \bb{x+\tfrac {\delta}2}^2} = \delta \abs{x+\tfrac{\delta}4} > \delta \abs{x} > \delta \frac 1{\delta} = 1 = \ve.
\ee
so $fg$ need not be uniformly continuous.

Now consider $E\in \R$ and $f(x)=\sin x$, $g(x) = x$ (note that $f$ is periodic with period $2\pi$, and on $[0,2\pi]$ (closed and bounded), it is continuous and hence uniformly continuous). Take $\ve =1$, then $\forall \delta >0$, $\exists$
\be
n > \frac 1{2\pi \abs{\sin \frac{\delta}2}},\ n \in \N, \quad x = 2n\pi,\quad y = 2n\pi + \tfrac {\delta}2,
\ee
then $\abs{x-y}=\frac{\delta}2 < \delta$ but
\beast
\abs{fg(x)-fg(y)} & = & \abs{x\sin x - \bb{x+\tfrac {\delta}2}\sin \bb{x+\tfrac {\delta}2}} = \abs{\bb{x+\tfrac {\delta}2}\sin \bb{x+\tfrac {\delta}2}} \\
& = & \abs{\bb{2n\pi +\tfrac {\delta}2}\sin \bb{2n\pi+\tfrac {\delta}2}} = \abs{\bb{2n\pi +\tfrac {\delta}2}\sin \tfrac {\delta}2} = \abs{\bb{2n\pi +\tfrac {\delta}2}}\abs{\sin \tfrac {\delta}2} > \delta \abs{x} > 1 = \ve.
\eeast
so $fg$ need nto be uniformly continuous if $f$ is bounded but $g$ is not.

\begin{exercise}
Let $(f_n)$ be a sequence of continuous real-valued functions on a closed, bounded interval $[a, b]$, and suppose that $f_n$ converges pointwise to a continuous function $f$.

Show that if $f_n \to f$ uniformly on $[a, b]$ and $(x_n)$ is a sequence of points in $[a, b]$ with $x_n \to x$, then $f_n(x_n) \to f(x)$. [Careful -- this is not quite as easy as it looks!]

On the other hand, show that if $f_n$ does not converge uniformly to $f$, then we can find a convergent sequence $x_m \to x$ in $[a, b]$ such that $f_n(x_n)$ does not converge to $f(x)$. 

[Hint: Bolzano-Weierstrass.]
\end{exercise}

Solution. Since $f$ is continuous and $x_n \to x$, we have $f(x_n) \to f(x)$ i.e. $\forall \ve>0$, $\exists N$ s.t. $n\geq N$,
\be
\abs{f(x_n)-f(x)} < \frac {\ve}2.
\ee

Also, if $f_n \to f$ uniformly on $[a, b]$, we have given $\ve>0$, $\exists N'$ s.t. 
\be
\abs{f_n(y) - f(y)} < \frac {\ve}2,
\ee
for all $y\in [a,b]$. Thus, $\forall \ve>0$, we take $n \geq \max\{N,N'\} $,
\be
\abs{f_n(x_n)-f(x)} \leq \abs{f(x_n)-f(x)} + \abs{f_n(x_n) - f(x_n)} < \frac {\ve}2 + \frac {\ve}2 = \ve.
\ee
so $f_n(x_n) \to f(x)$.

If $f_n$ does not converge uniformly to $f$, for each $n\in \N$ the continuous function $f_n-f$ is bounded on $[a,b]$ and attains its bounds, so we take $y_n$ such that $\abs{f_n(y_n) - f(y_n)}$ is maximal. Then
\be
f_n(y_n) - f(y_n) \nrightarrow 0 \quad \text{as }n\to \infty, \quad (\text{otherwise it converges uniformly})
\ee
so $\exists \ve>0$ s.t. $\forall N \in\N$, $\exists n \geq N$ with 
\be
\abs{f_n(y_n) - f(y_n)} > \ve.
\ee

Thus, there exists a sequence $n_1<n_2< \dots$ such that $\forall k \in \N$ we have
\be
\abs{f_{n_k}(y_{n_k}) - f(y_{n_k})} > \ve.
\ee

With Bolzano-Weierstrass Theorem, the bounded sequence $(y_{n_k})$ has a convergent subsequence $(y_{n_{k_i}})$ converging to $x$, say. For $m\in \N$, define
\be
x_m = \left\{\ba{ll}
y_{n_{k_i}} \quad\quad & m= n_{k_i}\\
x & m \neq \{ n_{k_i}:i\in \N\}
\ea\right.
\ee
then $x_m \to x$, so as $f$ is continuous $f(x_m)\to f(x)$, and thus given $\ve>0$, $\exists N'\in \N$ such that $\forall m \geq N'$,
\be
\abs{f(x_m) -f(x)} < \frac{\ve}2.
\ee

Thus, $\forall N$, there exist $\ve>0$ and $N'$, and then $\exists i \in\N$ s.t. $n_{k_i}\geq \max\{N,N'\}$ with
\be
\abs{f_{n_{k_i}}(y_{n_{k_i}}) - f(y_{n_k})} > \ve,\quad\quad \abs{f(y_{n_{k_i}}) -f(x)} < \frac{\ve}2.
\ee

Hence, 
\beast
\abs{f_{n_{k_i}}(y_{n_{k_i}}) - f(x)} & = & \abs{f_{n_{k_i}}(y_{n_{k_i}}) - f(y_{n_k}) + f(y_{n_k}) - f(x)} \\
& \geq & \abs{f_n(y_{n_{k_i}}) - f(y_{n_k})} - \abs{f(y_{n_{k_i}}) -f(x)} > \ve - \frac{\ve}2 =  \frac{\ve}2.
\eeast

so we have $f_n(x_n)\nrightarrow f(x)$.

\begin{exercise}
Suppose that $f :\ [0,\infty) \mapsto \to \R$ is continuous and that $f(x)$ tends to a (finite) limit as $x \to \infty$. Is $f$ necessarily uniformly continuous on $[0,\infty)$? Give a proof or a counter-example as appropriate.
\end{exercise}

Solution. Let $f(x)\to \ell$ as $x\to \infty$. Given $\ve>0$, $\exists M >0$ s.t. $x > M$ with
\be
\abs{f(x) - \ell} < \frac{\ve}2.
\ee

On $[0,M+1]$ (closed and bounded) $f$ is continuous and hence uniformly continuous, so $\exists \delta >0$ (we may assume $\delta <1$) such that if $0\leq x<y \leq M+1$ then
\be
\abs{y-x} \ \ra \ \abs{f(y)-f(x)} < \ve.
\ee

If $y>M+1$, then $\abs{y-x}<\delta$ implies $x > M$, thus,
\be
\abs{f(y)-f(x)} \leq \abs{f(y)-\ell} + \abs{f(x)-\ell} < \frac{\ve}2 + \frac{\ve}2 = \ve.
\ee

Thus, $f$ is uniformly continuous on $[0,\infty)$.

\begin{exercise}
Which of the following functions $f$ on $[0,\infty)$ are (a) uniformly continuous, (b) bounded?
\ben
\item [(i)] $f(x) = \sin x^2$;
\item [(ii)] $f(x) = \inf\{\abs{x - n^2}:\ n \in \N\}$;
\item [(iii)] $f(x) = (\sin x^3)/(x + 1)$.
\een
\end{exercise}

Solution. \ben
\item [(i)] (a) Take $\ve = \frac 12$, then $\forall \delta >0$ (we may assume that $\delta < \frac{\sqrt{\pi}}2$) and choose
\be
N > \frac{\sqrt{\pi}}{4\delta \sqrt{2}},\ N\in \N \quad\quad \delta' = \frac{\sqrt{\pi}}{4N\sqrt{2}}<\delta
\ee

Take $x = N\sqrt{2\pi}$ and $y = x + \delta'$ we have $\abs{x-y}< \delta$, but
\beast
\abs{f(x)-f(y)} & = & \abs{\sin y^2} = \abs{\sin (2\pi N^2 + 2\delta' \sqrt{2\pi} N + \delta'^2)} = \abs{\sin (\tfrac {\pi}2 + \delta'^2)}\\
& > & \sin \tfrac {3\pi}4 = \frac 1{\sqrt{2}} > \frac 12 = \ve. \quad\quad (\text{since $\delta'^2 < \delta^2 < \frac {\pi}4$})
\eeast
So it is not uniformly continuous. (b) It is bounded by $\pm 1$.

\item [(ii)] (a) $f$ is piecewise linear with linear sections of gradient $\pm 1$, so $\forall \ve >0$ we may take $\delta = \ve$ to prove that $f$ is uniformly continuous. (b) $\forall n\in\N$, we have $f(n^2 +n) = n$, which implies that $f$ is unbounded.

\item [(iii)] (a) Since $f:\ [0,\infty) \mapsto \to \R$ is continuous and that $f(x)$ tends to a limit 0 as $x \to \infty$, we can see that it is uniformly continuous by previous question. (b) Obviously, it is bounded by $\pm 1$.

\een

\begin{exercise}
Show that if $(f_n)$ is a sequence of uniformly continuous functions on $\R$, and $f_n \to f$ uniformly on $\R$, then $f$ is uniformly continuous. Give an example of a sequence of uniformly continuous functions $f_n$ on $\R$, such that $f_n$ converges pointwise to a continuous function $f$, but $f$ is not uniformly continuous.

[Hint for the last part: choose the limit function $f$ first.]
\end{exercise}

Solution. $\forall \ve >0$, by uniform convergence of $f_n$, we have $\exists N \in \N$ s.t. $\forall n\geq N$, $\forall x \in\R$, we have
\be
\abs{f_n(x)-f(x)} < \frac{\ve}3.
\ee

Then by uniform continuity of $f_N$, given $\ve$, $\exists \delta >0$ s.t. $\abs{x-y}<\delta $ with
\be
\abs{f_N(x) - f_N(y)} < \frac{\ve}3.
\ee

Thus, $\forall \ve>0$, $\exists \delta >0$ s.t. $\abs{x-y}<\delta$ with
\be
\abs{f(x)-f(y)} \leq \abs{f(x)-f_N(x)} + \abs{f_N(x) - f_N(y)} + \abs{f(y)-f_N(y)} < \frac{\ve}3 + \frac{\ve}3 + \frac{\ve}3 = \ve.
\ee

Hence, $f$ is uniform continuous.

Now consider function
\be
f_n(x) = \left\{\ba{ll}
x^2 \quad\quad & \abs{x} \leq n\\
n^2 & \abs{x} >n
\ea\right.,
\ee
then $f_n$ is uniformly continuous on $\R$, as its restriction to $[-n,n]$ is continuous and hence uniformly continuous, and it is constant outside $[-n,n]$. 

Let $f(x)=x^2$, then $f_n \to f$ pointwise, so the limit is continuous but not uniformly continuous by previous question (question \ref{ques:x^2_uniform_continuous}).

\begin{exercise}
Let $f_n(x) = n^\alpha x^n(1 - x)$, where $\alpha$ is a real constant.
\ben
\item [(i)] For which values of $\alpha$ does $f_n(x) \to 0$ pointwise on $[0, 1]$?
\item [(ii)] For which values of $\alpha$ does $f_n(x) \to 0$ uniformly on $[0, 1]$?
\item [(iii)] For which values of $\alpha$ does $\int^1_0 f_n(x)dx \to 0$?
\item [(iv)] For which values of $\alpha$ does $f'_n(x) \to 0$ pointwise on $[0, 1]$?
\item [(v)] For which values of $\alpha$ does $f'_n(x) \to 0$ uniformly on $[0, 1]$?
\een
\end{exercise}

Solution. \ben
\item [(i)] First $f_n(0)=f_n(1)=0$ for all $n\in\N$. Now for $x\in (0,1)$ we have
\be
\frac{f_{n+1}(x)}{f_n(x)} = \bb{\frac {n+1}n}^\alpha x = \bb{1+\frac 1n}^\alpha x,
\ee
so for large enough $n$ we have 
\be
\frac{f_{n+1}(x)}{f_n(x)} < \sqrt{x} < 1,
\ee
and thus $f_n(x)\to 0$. Thus $f_n(x) \to 0$ pointwise on $[0,1]$ for all $\alpha \in \R$.

\item [(ii)] Since 
\be
f_n'(x) = n^\alpha \bb{nx^{n-1} - (n+1)x^n},
\ee
the maximum value take by $f_n$ occurs at $\frac n{n+1}$ and is 
\be
f_n\bb{\tfrac n{n+1}} = n^\alpha \bb{\frac n{n+1}}^n\bb{1-\frac n{n+1}} = \frac{n^{n+\alpha}}{(n+1)^{n+1}}.
\ee

Thus, if $\alpha <1$ then given $\ve >0$ take $N\in \N$ s.t. $N^{\alpha -1} < \ve$, then we have $\forall n\geq N$ and $\forall x\in [0,1]$
\be
\abs{f_n(x)} \leq \frac{n^{n+\alpha}}{(n+1)^{n+1}} < n^{\alpha -1} < N^{\alpha -1} < \ve.
\ee
as required for uniform convergence. However, if $\alpha \geq 1$ then
\be
f_n\bb{\tfrac n{n+1}} = \frac{n^{n+\alpha}}{(n+1)^{n+1}} \geq \bb{\frac n{n+1}}^{n+1} = \bb{1+ \frac 1n}^{-n} \cdot \frac 1{1+ \frac 1n} \to \frac 1e \quad\text{as }n\to \infty,
\ee
so we do not have uniform convergence. Thus, $f_n \to 0$ uniformly on $[0,1]$ for $\alpha < 1$.

\item [(iii)] We have
\be
\int^1_0 f_n(x)dx = \int^1_0 n^\alpha x^n(1 - x) dx = \frac{n^\alpha}{(n+1)(n+2)},
\ee
so $\int^1_0 f_n(x)dx\to 0$ for $\alpha < 2$.

\item [(iv)] We already have
\be
f_n'(x) = n^\alpha \bb{nx^{n-1} - (n+1)x^n} = n^{\alpha+1} x^{n-1}\bb{1 - \frac {n+1}nx},
\ee
for $x \in [0,1)$, for large enough $n$ we have $\frac{n+1}n x <1$ so that $1-\frac {n+1}nx >0$, and then
\be
0\leq f_n'(x)\leq n^{\alpha+1} x^{n-1}.
\ee

So $f_n'(x) \to 0$ for any $\alpha\in \R$. However, $f_n'(1) = -n^{\alpha}$, so for $f_n'(1) \to 0$ we need $\alpha <0$. Thus $f_n'(x)\to 0$ pointwise on $[0,1]$ for $\alpha <0$.


\item [(v)] The minimum value taken by $f_n'$ is $f_n'(1)=-n^\alpha$, the maximum value occurs when $f_n''(x) = 0$, i.e., 
\be
n^\alpha \bb{n(n-1)x^{n-2} - n(n+1)x^{n-1}} = n^{\alpha+1}(n-1)x^{n-2}\bb{1 - \frac{n+1}{n-1}x} = 0 \ \ra \ x = \frac {n-1}{n+1}.
\ee
So the maximum value is 
\be
f_n'\bb{\tfrac {n-1}{n+1}} = n^{\alpha+1} \bb{\frac {n-1}{n+1}}^{n-1}\bb{1 - \frac {n+1}n \frac {n-1}{n+1}} = n^{\alpha} \bb{\frac {n-1}{n+1}}^{n-1}.
\ee

Thus, if $\alpha <0 $ then given $\ve >0$ take $N\in \N$ s.t. $N^{\alpha} < \ve$, then we have $\forall n\geq N$ and $\forall x\in [0,1]$
\be
\abs{f_n'(x)} \leq n^{\alpha} \bb{\frac {n-1}{n+1}}^{n-1} < n^{\alpha} < N^{\alpha} < \ve.
\ee
as required for uniform convergence. However, if $\alpha \geq 0$ then
\be
f_n'\bb{\tfrac {n-1}{n+1}} = n^{\alpha} \bb{\frac {n-1}{n+1}}^{n-1} \geq \bb{\frac {n-1}{n+1}}^{n-1}  = \bb{1+ \frac 2{n-1}}^{-(n-1)} \to e^{-2} \quad\text{as }n\to \infty,
\ee
so we do not have uniform convergence. Thus, $f_n' \to 0$ uniformly on $[0,1]$ for $\alpha < 0$.

\een

\begin{exercise}
Consider the sequence of functions $f_n :\ \R\backslash \Z \to \R$ defined by $f_n(x) = \sum^n_{m=-n}(x-m)^{-2}$. Show that $f_n$ converges pointwise on $\R\backslash \Z$ to a function $f$. Show that $f_n$ does not converge uniformly on $\R \backslash \Z$. Why can we nevertheless conclude that the limit function $f$ is continuous, and indeed differentiable, on $\R \backslash \Z$?
\end{exercise}

Solution. Given $x\in \R\bs\Z$ and $\ve>0$, choose $M\in\N$ s.t. 
\be
\sum^\infty_{k=M+1} \frac 1{k^2} < \frac{\ve}2
\ee
and take $N > \abs{x} + M, N\in\N$. Then if $\abs{m} = N+\ell$ with $\ell \in \N$, we have
\be
\abs{m-x} \geq \abs{m}-\abs{x} = N+\ell -\abs{x} > M + \ell,
\ee
so that if $n' > n>N$, we have
\beast
\abs{f_{n'}(x)-f_n(x)} & = & \sum^{-(n+1)}_{m=-n'} (x-m)^{-2} + \sum^{n'}_{m=n+1} (x-m)^{-2} <  \sum^{-(N+1)}_{m=-n'} (x-m)^{-2} + \sum^{n'}_{m=N+1} (x-m)^{-2}\\
& \leq & 2\sum^{n'-N}_{\ell=1} (M+\ell)^{-2} < 2\sum^{\infty}_{k=M+1} k^{-2} < \ve,
\eeast
so the sequence $\bb{f_n(x)}$ is Cauchy and hence converges. Thus, $f_n$ converges pointwise on $\R\bs\Z$ to function $f$.

Now we prove that the sequence $(f_n)$ does not converge uniformly to $f$ on $\R\bs\Z$. Take $\ve =1$, then $\forall N\in\N$ and take $x=N+\frac 12$, we have
\be
\abs{f_N(x)-f(x)} = \sum_{\abs{m}>N}(x-m)^{-2} > \bb{\frac 12}^{-2} = 4 > \ve.
\ee
Thus, $f_n$ does not converge to a function $f$. 

However, $f$ is differentiable and therefore continuous: take $x\in \R\bs\Z$ and choose $N>\abs{x}+2$, then as $f_N$ is a finite sum of differentiable functions, it suffices to show $f-f_N$ is differentiable at $x$. Take $y\in \R\bs\Z$ with $\abs{y-x}<1$, then we have
\beast
& & \frac{(f-f_N)(y)-(f-f_N)(x)}{y-x} + 2\sum_{\abs{m}>N}(x-m)^{-3} \\
& = & \frac 1{y-x}\sum_{\abs{m}>N}\bb{(y-m)^{-2}-(x-m)^{-2}} + 2\sum_{\abs{m}>N}(x-m)^{-3} \\
& = & \sum_{\abs{m}>N}\frac {(x-m)^3- (y-m)^2(x-m) + 2 (y-x)(y-m)^2 }{(y-x)(y-m)^2(x-m)^3} \\
& = & \sum_{\abs{m}>N}\frac {(x-m)\bb{(x-m)^2- (y-m)^2} + 2 (y-x)(y-m)^2 }{(y-x)(y-m)^2(x-m)^3} \\
& = & \sum_{\abs{m}>N}\frac { 2(y-m)^2 - (x-m)(x+y-2m)}{(y-m)^2(x-m)^3} = \sum_{\abs{m}>N}\frac { (2(y-m)+ (x-m))((y-m)-(x-m))}{(y-m)^2(x-m)^3}\\
& = & (y-x)\sum_{\abs{m}>N}\frac {2(y-m)+ (x-m)}{(y-m)^2(x-m)^3} = (y-x)\sum_{\abs{m}>N}\frac {2}{(y-m)(x-m)^3} + \frac {1}{(y-m)^2(x-m)^2}.
\eeast

since $\abs{x-m}>2$ and $\abs{y-m} \geq \abs{x-m}-\abs{x-y} > 1$ if $\abs{m}>N$, we have
\be
\sum_{\abs{m}>N}\frac {2}{(y-m)(x-m)^3} + \frac {1}{(y-m)^2(x-m)^2} < 2 \sum_{\abs{m}>N}\frac 1{(x-m)^2}
\ee
which converges, so as $y\to x$ we have 
\be
\frac{(f-f_N)(y)-(f-f_N)(x)}{y-x} + 2\sum_{\abs{m}>N}(x-m)^{-3} \to 0 \ \ra \ \frac{(f-f_N)(y)-(f-f_N)(x)}{y-x} \to - 2\sum_{\abs{m}>N}(x-m)^{-3}.
\ee
Thus, $f-f_N$ is differentiable at $x$ as required.

\begin{exercise}
Let $f$ be a differentiable, real-valued function on a (bounded or unbounded) interval $E \subseteq \R$, and suppose that $f'$ is bounded on $E$. Show that $f$ is uniformly continuous on $E$.

Let $g :\ [-1, 1] \mapsto \R$ be the function defined by $g(x) = x^2 \sin(1/x^2)$, for $x \neq 0$ and $g(0) = 0$. Show that $g$ is differentiable, but its derivative is unbounded. Is $g$ uniformly continuous on $[-1, 1]$?
\end{exercise}

Solution. Suppose $\forall x\in\R$, $\abs{f'(x)}<M$, given $\ve>0$ set $\delta = \frac{\ve}M$, then if $\abs{y-x}<\delta$ and with Mean Value Theorem, we have
\be
\abs{f(y)-f(x)} = \abs{y-x}\cdot \abs{f'(z)}
\ee
for some $z\in (x,y)$, so 
\be
\abs{f(y)-f(x)} < \delta M = \ve.
\ee
Thus, $f$ is uniformly continuous.

Now consider function $g$,
\be
\lim_{h\to 0} \frac{g(h)-g(0)}{h} = \lim_{h\to 0} \frac{h^2\sin \tfrac 1{h^2}}{h} = 0,
\ee
we see that $g$ is differentiable at 0. At $x\neq 0$, $g$ is clearly differentiable with 
\be
g'(x) = 2x\sin \tfrac 1x - \frac 2x \cos \tfrac 1x.
\ee

In particular,
\be
g'\bb{\tfrac 1{\sqrt{2n\pi}}}= -2\sqrt{2n\pi},
\ee
so $g'$ is unbounded. However, as $g$ is continuous on a closed bounded interval it is uniformly continuous.

\begin{exercise}
Suppose that a function $f$ has a continuous derivative on $(a, b) \subseteq \R$ and
\be
f_n(x) = n\bb{f\bb{x +\tfrac 1n} - f(x)}.
\ee
Show that $f_n$ converges uniformly to $f'$ on each interval $[\alpha , \beta] \subset (a, b)$.
\end{exercise}

Solution. Take an interval $[\alpha , \beta] \subset (a, b)$. Let
\be
c = \frac 12 (a + \alpha),\quad\quad d = \frac 12 (b+\beta)
\ee
so that $a<c<\alpha$ and $\beta <d<b$. Since $f'$ is continuous on $[c,d]$, it is uniformly continuous there. Thus, $\forall \ve>0$, $\exists \delta >0$ (and we may assume $\delta < \min(a-c,d-\beta)$) such that if $y,x\in [c,d]$, then
\be
\abs{y-x} < \delta \ \ra \ \abs{f'(y)-f'(x)}< \ve.
\ee

Take $N > \frac 1{\delta}$, then for all $x\in[\alpha,\beta]$, $\forall n> N$ we may write (since $f$ is continuous and differentiable on $(c,d)$),
\be
f_n(x) = n\bb{f\bb{x +\tfrac 1n} - f(x)} = \frac{f\bb{x +\tfrac 1n} - f(x)}{x+\frac 1n -x} = f'(y)
\ee
for some $y\in (x,x+\tfrac 1n)$ by M.V.T. (where $x+ \frac 1n \in (\alpha -\delta, \beta + \delta)\subseteq (c,d)$). Then as $\abs{y-x} < \frac 1n < \delta$, we have $y\in [c,d]$ and so
\be
\abs{f_n(x)-f'(x)} = \abs{f'(y)-f(x)} < \ve. 
\ee

Thus, $f_n$ converges uniformly to $f'$ on $[\alpha, \beta]$.

\begin{exercise}
Let $\sum^\infty_{n=1} a_n$ be an absolutely convergent series of real numbers. Define a sequence $(f_n)$ of functions on $[-\pi, \pi]$ by 
\be
f_n(x) = \sum^n_{m=1} a_m \sin mx
\ee
and show that each $f_n$ is differentiable with 
\be
f'_n(x) = \sum^n_{m=1} m a_m \cos mx.
\ee
Show further that 
\be
f(x) = \sum^\infty_{m=1} a_m \sin mx
\ee
defines a continuous function on $[-\pi, \pi]$, but that the series 
\be
\sum^\infty_{m=1} m a_m \cos mx
\ee
need not converge.
\end{exercise}

Solution. As $x\mapsto \sin mx$ is a differentiable function on $[-\pi,\pi]$ with derivative $x\mapsto m\cos mx$, it follows that $f_n$, being a finite sum of such functions, is differentiable with 
\be
f_n'(x) = \sum^n_{m=1} ma_m \cos mx.
\ee

As $\sum^\infty_{n=1}a_n$ is absolutely convergent, $\forall \ve>0$, $\exists N\in \N$ s.t. 
\be
\sum_{m>N}\abs{a_m} < \ve.
\ee

Thus, $\forall n\geq N$, $\forall x\in [-\pi,\pi]$,
\be
\abs{f_n(x)-f(x)} = \abs{\sum_{m>n}a_m \sin mx} \leq \sum_{m>n}\abs{a_m \sin mx} \leq \sum_{m>n} \abs{a_m} \leq \sum_{m>N}\abs{a_m} < \ve
\ee
so $f_n$ converges uniformly to $f$, thus $f$ is a uniform limit of continuous functions, hence is continuous. However, if we take $a_m = \frac 1{m^2}$, then $\sum a_m$ is absolutely convergent but 
\be
\sum m a_m \cos mx = \sum \frac 1m \cos mx,
\ee
which does not converge at 0.

\begin{exercise}
Let $f$ be a bounded function defined on a set $E \subseteq \R$, and for each positive integer $n$ let $g_n$ be a function defined on $E$ by
\be
g_n(x) = \sup\{\abs{f(y) - f(x)} :\ y \in E, \abs{y - x} < 1/n\}.
\ee

Show that $f$ is uniformly continuous on $E$ if and only if $g_n \to 0$ uniformly on $E$ as $n \to \infty$.
\end{exercise}

Solution. "$\Longrightarrow$". Suppose $f$ is uniformly continuous on $E$ and take $\ve>0$. Then $\exists \delta >0$ such that for all $x,y\in E$, $\abs{y-x}< \delta$, with $\abs{f(y)-f(x)}< \ve$. So $\forall \ve >0$, $\exists \delta >0$ and $n> \frac 1{\delta} \ \ra \ \abs{y-x}<\frac 1n < \delta$, we have $g_n(x) < \ve$, which means that $g_n \to 0$ uniformly on $E$.

"$\Longleftarrow$". Suppose $g_n \to 0$ uniformly on $E$ and take $\ve>0$. Then there exists $N\in \N$ s.t. $\forall x\in E$ and $\forall n \geq N$, we have
\be
\abs{g_n(x)} < \ve \ \ra g_n(x) < \ve.
\ee

So we set $\delta = \frac 1{N+1}$, then for $\forall x,y \in E$ with $\abs{x-y} < \delta = \frac 1{N+1} < \frac 1N$,
\be
\abs{f(y)-f(x)} \leq g_N(x) < \ve.
\ee

Thus, $f$ is uniformly continuous on $E$.

\begin{exercise}[Dini's theorem]
Let $f_n :\ [0, 1] \mapsto \R$ be a sequence of continuous functions converging pointwise to a continuous function $f :\ [0, 1] \mapsto \R$. Suppose that $f_n(x)$ is a decreasing sequence $f_n(x) \geq f_{n+1}(x)$ for each $x \in [0, 1]$. Show that $f_n \to f$ uniformly on $[0, 1]$.

[If you have done Metric and Topological Spaces then you may prefer to find a topological proof.]
\end{exercise}

Solution. Here we use two approaches.

\vspace{2mm}

{\bf Non-topological proof.} 

By replacing each $f_n$ by the continuous function $g_n = f_n -f$, we may assume $g=0$. Then $\forall n\in \N$ and $x\in [0,1]$ we have $g_n$ is decreasing sequence and $g_n(x) \geq 0$.

If $g_n$ does not converge to 0 uniformly, $\forall N\in \N$, $\exists \ve >0$ and $\exists n\geq N, \exists x_n\in [0,1]$ s.t.
\be
\abs{g_n(x_n)} \geq \ve.
\ee  
Then the bounded sequence $(x_n)$ has a convergent subsequence $(x_{n_i})$ with limit $\ell \in [0,1]$. 

But with the above argument, $\forall \delta >0$, $\exists \ve >0, i\in \N$ with
\be
\abs{x_{n_i}-\ell}<\delta,\quad\quad \abs{g_{n_i}(x_{n_i})} \geq \ve. \quad\quad (*)
\ee

Since $g_n \to 0$ pointwise, there exists $N\in \N$, $\forall n\geq N$ with $g_n(\ell) < \frac{\ve}2$. 

Since $g_n$ is continuous, there exists $\delta > 0$ with $\abs{y-\ell} < \delta \ \ra \ \abs{g_n(y)-g_n(\ell)} < \frac {\ve}2$. Thus,
\be
\abs{g_n(y)} \leq \abs{g_n(y) - g_n(\ell)} + \abs{g_n{\ell}} <  \frac{\ve}2 +  \frac{\ve}2 = \ve.
\ee

So $\forall m \geq n$ we have 
\be
\abs{y-\ell}< \delta \ \ra \ g_m(y) < \ve.
\ee

i.e., $\forall \ve >0$, $\exists \delta >0, N\in\N$, $\forall m \geq N$ with $\abs{y-\ell}< \delta$,
\be
g_m(y) < \ve.
\ee

This is a contradiction to $(*)$. Thus $g_n\to 0$ uniformly.

\vspace{4mm}

{\bf Topological proof.} 

Given $\ve>0$, let $E_n= \{x\in [0,1]: f_n(x)-f(x) < \ve\}$. For all $n\in\N$, since $f_n-f$ is continuous $E_n$ is open, and as $f_n(x)\geq f_{n+1}(x)$ for all $x$ we have $E_n\subseteq E_{n+1}$. Since $f_n\to f$, for all $x$ there exists $n\in \N$ with $x\in E_n$, so the $E_n$ form an open cover of the compact set $[0,1]$, and thus there is a finite subcover, and so there exists $N\in \N$ with $E_N=[0,1]$. Then $n> N$, $\forall x\in [0,1]$ we have
\be
\abs{f_n(x)-f(x)} = f_n(x)-f(x) < \ve.
\ee
So $f_n \to f$ uniformly on $[0,1]$.

\begin{exercise}[Abel's test]
Let $a_n$ and $b_n$ be real-valued functions on $E \subseteq \R$. Suppose that $\sum^\infty_{n=0} a_n(x)$ is uniformly convergent on $E$. Suppose further that the $b_n(x)$ are uniformly bounded on $E$ (this means there is a constant $K$ with $\abs{b_n(x)} \leq K$ for all $n$ and all $x \in E$), and that $b_n(x) \geq b_{n+1}(x)$ for all $n$ and all $x \in E$. Show that the sum 
\be
\sum^\infty_{n=0} a_n(x)b_n(x)
\ee
is uniformly convergent on $E$. 

[Hint: show first that 
\be
\sum^m_{k=n} a_k b_k = \sum^{m-1}_{k=n} (b_k-b_{k+1}) A_k + b_mA_m - b_nA_{n-1},
\ee
where $A_n = \sum^n_{k=0} a_k$.]

Deduce that if $a_n$ are real constants and $\sum^\infty_{n=0} a_n$ is convergent, then $\sum^\infty_{n=0} a_nx^n$ is uniformly convergent on $[0, 1]$. (But note that $\sum^\infty_{n=0} a_nx^n$ need not be convergent at $x = -1$; you almost certainly know a counterexample!)
\end{exercise}

Solution. Set 
\be
A_n =\sum^n_{k=0} a_k \ \ra \ a_n = A_n - A_{n-1}.
\ee

then if $m>n$, we have
\beast
& & \sum^{m-1}_{k=n}(b_k-b_{k+1}) A_k + b_mA_m - b_nA_{n-1} \\
& = & -b_nA_{n-1} + (b_n - b_{n+1})A_n + (b_{n+1}-b_{n+2})A_{n+1} + \dots + (b_{m-1}-b_m)A_{m-1} + b_mA_m \\
& = & b_n(A_n - A_{n-1}) + b_{n+1} (A_{n+1}-A_n) + \dots + b_m(A_m - A_{m-1}) = \sum^m_{k=n} a_k b_k.
\eeast

Thus,
\beast
\sum^m_{k=n} a_k b_k & = & \sum^{m-1}_{k=n}(b_k-b_{k+1}) A_k + b_mA_m - b_nA_{n-1} \\
& = & \sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + \sum^{m-1}_{k=n}(b_k-b_{k+1}) A_m + b_mA_m - b_nA_{n-1}\\
& = & \sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + (b_n-b_m) A_m + b_mA_m - b_nA_{n-1}\\
& = & \sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + b_n(A_m - A_{n-1}).
\eeast

Now take $\ve >0$, as $\sum^\infty_{n=0} a_n(x)$ uniformly convergent on $E$, there exists $N\in \N$ s.t. if $m>n\geq N$, then for all $x\in E$ we have
\be
\abs{A_m(x)-A_n(x)} < \frac {\ve}{3K}.
\ee

Then, if $m>n\geq N+1$, we have
\beast
\abs{\sum^m_{k=n} a_k b_k} & = & \abs{\sum^{m-1}_{k=n}(b_k-b_{k+1})(A_k - A_m) + b_n(A_m - A_{n-1})}\\
& \leq & \sum^{m-1}_{k=n}(b_k-b_{k+1})\abs{A_k - A_m} + \abs{b_n(A_m - A_{n-1})}\\
& \leq & \sum^{m-1}_{k=n}(b_k-b_{k+1})\frac{\ve}{3K} + \abs{b_n}\abs{A_m - A_{n-1})}\\
& \leq & 2K \frac{\ve}{3K} + K \frac{\ve}{3K} = \ve.
\eeast
Thus, $\sum^\infty_{n=0} a_n(x)b_n(x)$ is uniformly convergent on $E$. 

Set $E =[0,1]$, $a_n(x)=a_n$ and $b_n(x) = x^n$. Since $a_n$ are real constants and $\sum^\infty_{n=0} a_n$ is convergent, then it is uniformly convergent. Thus, applying the Abel's test, we have $\sum^\infty_{n=0} a_nx^n$ is uniformly convergent on $[0,1]$.

\begin{exercise}
Define $\varphi(x) = |x|$ for $x\in [-1, 1]$ and extend the definition of $\varphi(x)$ to all real $x$ by requiring that
\be
\varphi(x + 2) = \varphi(x).
\ee
\ben
\item [(i)] Show that $\abs{\varphi(s) - \varphi(t)} \leq \abs{s - t}$ for all $s$ and $t$.
\item [(ii)] Define $f(x) = \sum^\infty_{n=0} \bb{\frac 34}^n \varphi(4^nx)$. Prove that $f$ is well-defined and continuous.
\item [(iii)] Fix a real number $x$ and positive integer $m$. Put $\delta_m = \pm \frac 12 4^{-m}$, where the sign is so chosen that no integer lies between $4^m x$ and $4^m(x + \delta_m)$. Prove that
\be
\abs{\frac {f(x + \delta_m) - f(x)}{\delta_m} } \geq \frac 12 (3^m + 1).
\ee
\item [(iv)] Conclude that $f$ is not differentiable at $x$. Hence there exists a real continuous function on the real line which is nowhere differentiable.
\een
\end{exercise}

\scutline

Solution. \ben
\item [(i)] We may assume $s>t$. Since $\varphi$ has period 2 and takes values in $[0,1]$, it suffices to consider $s,t$ s.t. 
\be
\abs{t} \leq 1,\quad\quad \abs{s-t} \leq 1. \quad (\text{the inequality is obvious when }\abs{s-t}> 1)
\ee
If $\abs{s} \leq 1$, then
\be
\abs{\varphi(s)-\varphi(t)} = \abs{\abs{s}-\abs{t}} \leq \abs{s-t} \quad\quad (\text{by triangle inequality}).
\ee

If $\abs{s} >1$, then $s\in (1,2]$ and hence $t\in (0,1]$. Hence,
\be
\abs{\varphi(s)-\varphi(t)} = \abs{(2-s) - t} \leq \abs{s-1}+\abs{t-1} = s-1 + 1-t = s-t = \abs{s-t}.
\ee

Thus for all $s$ and $t$, the inequality holds.

\item [(ii)] For all $x$, the partial sum of $\sum^\infty_{n=N+1} \bb{\frac 34}^n \varphi(4^nx)$ are bounded by 
\be
\sum^\infty_{n=0}\bb{\tfrac 34}^n = \frac 1{1-\frac 34} = 4.
\ee
so as the terms are all non-negative, the sum converges, thus $f$ is well-defined. For $m\in\N$, set
\be
f_m(x) = \sum^m_{n=0}\bb{\tfrac 34}^n \varphi(4^nx).
\ee

Given $\ve>0$, choose $N\in\N$ s.t. $\sum^\infty_{n=N+1}\bb{\frac 34}^n < \frac {\ve}3$, then $f_N$ is a finite sum of continuous functions, so is continuous. Thus given $x$ there exists $\delta>0$ s.t. 
\be
\abs{y-x}< \delta,\quad\quad\abs{f_N(y)-f_N(x)}< \frac {\ve}3.
\ee
Thus,
\beast
\abs{f(y)-f(x)} & \leq & \abs{f(x)- f_N(x)} + \abs{f(y)- f_N(y)} + \abs{f_N(y)-f_N(x)} \\
& \leq & \sum^\infty_{n=N+1}\bb{\tfrac 34}^n \bb{\varphi(4^nx)+\varphi(4^ny)} + \abs{f_N(y)-f_N(x)} \\
& \leq & 2 \sum^\infty_{n=N+1}\bb{\tfrac 34}^n + \abs{f_N(y)-f_N(x)} < \frac {2\ve}3 + \frac {\ve}3 = \ve.
\eeast

Thus, $f$ is continuous.

\item [(iii)] If $n>m$, then $4^n(x+\delta_m) = 4^nx \pm 2\cdot 4^{n-m-1}$, so that
\be
\varphi(4^nx) = \varphi(4^n(x+\delta_m)) \ \ra \ (f-f_m)(x+\delta_m) = (f-f_m)(x) \ \ra \ f(x+\delta_m)-f(x) = f_m(x+\delta_m) - f_m(x).
\ee

If $n\leq m$, as no integer lies between $4^nx$ and $4^n(x+\delta_m)$, the function $y\mapsto \bb{\tfrac 34}^n \varphi(4^ny)$ is linear between $x$ and $x+\delta_m$, and its gradient is $\pm \bb{\frac 34}^n 4^n = 3^n$. Thus $f_m$ is linear between $x$ and $x+\delta_m$ and the absolute value of its gradient is at least 
\be
3^m - 3^{m-1} - \dots - 3 -1 = 3^m - \frac 12 (3^m-1) = \frac 12\bb{3^m + 1}
\ee

Thus, we have
\be
\abs{\frac {f(x + \delta_m) - f(x)}{\delta_m} } = \abs{\frac {f_m(x + \delta_m) - f_m(x)}{\delta_m} } \geq \frac 12 (3^m + 1).
\ee

\item [(iv)] If $f$ is differentiable at $x$ with $f'(x) = M$, there exists $\delta>0$ s.t. $\abs{h}<\delta$, 
\be
\abs{\frac{f(x+h)-f(x)}{h}-M} < 1 \ \ra \ \abs{\frac{f(x+h)-f(x)}{h}} < \abs{M}+1,
\ee
but we may choose $m$ s.t. $\abs{\delta_m}< \delta$ and $\frac 12 (3^m+1)>\abs{M}+1$, and we have a contradiction. Thus $f$ is not differentiable at $x$. Since $x$ is arbitrary, $f$ is a real continuous function which is nowhere differentiable.

\een

\qcutline

Unless stated otherwise, the norm on $\R^n$ may be taken to be the Euclidean norm $\|x\|_2 = \sqrt{\sum^n_{i=1} x_i^2}$, and the spaces $\ell_0$ and $\ell_\infty$ may be assumed to have the sup-norm $\|x\|_\infty = \sup_i |x_i|$. ($\ell_0$ denotes the space of real sequences $(x_n)^\infty_{n=1}$ such that all but finitely many $x_n$ are zero.) 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{exercise}
Let $(x^{(m)})$ and $(y^{(m)})$ be sequences in $\R^n$ converging to $x$ and $y$ respectively. Show that $x^{(m)}\cdot y^{(m)}$ converges to $x \cdot y$. Deduce that if $f :\ \R^n \to \R^p$ and $g :\ \R^n \to \R^p$ are continuous at $x \in \R^n$, then so is the pointwise scalar product function $f \cdot g:\ \R^n \to \R$.
\end{exercise}

Solution. First observe that if $a,b\in \R^n$ then 
\be
\abs{a\cdot b}^2 = \bb{\sum a_i b_i}^2 \leq \bb{\sum a_i^2}\cdot \bb{\sum b_i^2} = \dabs{a}^2 \cdot\dabs{b}^2 \ \ra \ \abs{a\cdot b} \leq \dabs{a}\cdot \dabs{b}.
\ee 

Now given $\ve>0$, $\exists N_1 \in\N$ s.t. $m>N_1$ 
\be
\dabs{x^{(m)}-x} < \frac {\ve}{2\bb{\dabs{y}+1}}
\ee
and $\exists N_2 \in\N$ s.t. $m>N_2$ 
\be
\dabs{y^{(m)}-y} < \frac {\ve}{2\bb{\dabs{x}+\frac{\ve}{2\bb{\dabs{y}+1}}}}.
\ee

Thus, $m> \max\{N_1,N_2\}$, we have
\beast
\abs{x^{(m)}\cdot y^{(m)} - x\cdot y} & \leq & \abs{x^{(m)}\cdot y^{(m)} - x^{(m)}\cdot y} + \abs{x^{(m)}\cdot y - x\cdot y}\\
 & \leq & \dabs{x^{(m)}}\dabs{y^{(m)} - y} + \dabs{y}\dabs{x^{(m)} - x}\\
 & < & \bb{\dabs{x} + \frac {\ve}{2\bb{\dabs{y}+1}}}\dabs{y^{(m)} - y} + \dabs{y}\frac {\ve}{2\bb{\dabs{y}+1}} \\
& < & \frac {\ve}2 + \frac {\ve}2 = \ve.
\eeast

Thus, $x^{(m)}\cdot y^{(m)}$ converges to $x \cdot y$. 

Now if $f$, $g$ are continuous at $x$, take any sequence $x^{(m)}$ converging to $x$, then by continuity we have
\be
f(x^{(m)}) \to f(x),\quad \quad g(x^{(m)}) \to g(x) \quad  \ra\quad  (f\cdot g)(x^{(m)}) = f(x^{(m)}) \cdot g(x^{(m)}) \to f(x)\cdot g(x) = f\cdot g(x) 
\ee
by the previous result. Thus, $f\cdot g$ is continuous at $x$.

\begin{exercise}
Show that $\dabs{x}_1 = \sum^n_{i=1} \abs{x_i}$ defines a norm on $\R^n$. Show directly that it is Lipschitz equivalent to the Euclidean norm.
\end{exercise}

Solution. For $\dabs{x}_1 = \sum^n_{i=1} \abs{x_i}$, we check \ben
\item [(i)] Clearly $\dabs{\cdot}_1:\R^n \to \R$ takes non-negative values,
\item [(ii)] $\dabs{x}_1 = 0 \ \ra\ \sum^n_{i=1} \abs{x_i} = 0 \ \ra \ \forall i,\ x_i = 0 \ \ra \ x = (0,\dots,0)^T$,
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm x}_1 = \sum^n_{i=1} \abs{\lm x_i} = \abs{\lm} \sum^n_{i=1} \abs{x_i} = \abs{\lm}\cdot \dabs{x}_1$,
\item [(iv)] and
\be
\dabs{x+y}_1 = \sum^n_{i=1} \abs{(x+y)_i} = \sum^n_{i=1} \abs{x_i+y_i} \leq \sum^n_{i=1} \abs{x_i}+\abs{y_i} = \sum^n_{i=1} \abs{x_i} +\sum^n_{i=1} \abs{y_i} = \dabs{x}_1 + \dabs{y}_1.
\ee
\een
So $\dabs{\cdot}_1$ is a norm. 

We have
\be
\dabs{x}^2 = \sum^n_{i=1} x_i^2 \leq \bb{\sum^n_{i=1} \abs{x_i}}^2= \dabs{x}_1^2 \ \ra \ \dabs{x} \leq \dabs{x}_1.
\ee
and
\be
\dabs{x}_1^2 = \bb{\sum^n_{i=1} \abs{x_i}}^2 \leq \bb{\sum^n_{i=1} \abs{x_i}^2}\bb{\sum^n_{i=1} 1^2} = n\dabs{x}^2 \ \ra \ \dabs{x}_1 \leq \sqrt{n}\dabs{x}.
\ee
Thus, $\dabs{\cdot}_1$ is Lipschitz equivalent to the Euclidean norm $\dabs{\cdot}$.

\begin{exercise}\label{ques:norm_one} 
\ben
\item [(a)] Show that $\dabs{f}_1 = \int^1_0 \abs{f(x)} dx$ defines a norm on the space $C[0, 1]$. Is it Lipschitz equivalent to the uniform norm?
\item [(b)] Let $R[0, 1]$ denote the vector space of all integrable functions on $[0, 1]$. Does $\dabs{f} = \int^1_0 \abs{f(x)} dx$ define a norm on $R[0,1]$?
\een
\end{exercise}

Solution. \ben
\item [(a)] For $\dabs{f}_1 = \int^1_0 \abs{f(x)} dx$ on the space $C[0,1]$, we check \ben
\item [(i)] Clearly $\dabs{\cdot}_1:C[0,1] \to \R$ takes non-negative values,
\item [(ii)] $\dabs{f}_1 = 0 \ \ra\ \int^1_0 \abs{f} = 0 \ \ra \ f = 0$, (Analysis I Example Sheet 4)
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm f}_1 = \int^1_0 \abs{\lm f} = \abs{\lm } \int^1_0 \abs{f} = \abs{\lm}\cdot \dabs{f}_1$,
\item [(iv)] and
\be
\dabs{f+g}_1 = \int^1_0 \abs{f+g} \leq \int^1_0 \abs{f}+\abs{g} = \int^1_0 \abs{f} + \int^1_0 \abs{g} = \dabs{f}_1 + \dabs{g}_1.
\ee
\een
so $\dabs{\cdot}_1$ is a norm. Given $M>0$, choose $n\in \N$ with $n>M+1$ and define $f \in C[0,1]$ by
\be
f(x) = \left\{\ba{ll}
nx & 0\leq x< \frac 1n\\
2-nx \quad\quad & \frac 1n \leq x < \frac 2n\\
0 & \frac 2n \leq x \leq 1
\ea\right..
\ee 
then
\be
\dabs{f}_1 = \int^1_0 \abs{f(x)}dx = \int^{\frac 1n}_0 nx dx + \int^{\frac 2n}_{\frac 1n}(2-nx)dx = \frac n2\frac 1{n^2} + 2\frac 1n - \frac n2 \bb{\frac 4{n^2}-\frac 1{n^2}} = \frac 1n.
\ee
\be
\dabs{f}_\infty = \sup_{x\in [0,1]}\abs{f(x)} = 1.
\ee
so $\dabs{f}_\infty = n\dabs{f}_1 > M\dabs{f}_1$, so $\dabs{\cdot}_1$ is not Lipschitz equivalent to $\dabs{\cdot}_\infty$.

\item [(b)] No. Define $f\in R[0,1]$ by 
\be
f(x) = \left\{\ba{ll}
1 & x=0\\
0 \quad\quad & x>0
\ea\right. \ \ra \ f\neq 0, \text{ but }\dabs{f} = \int^1_0 \abs{f(x)} dx = 0.
\ee 

\een

\begin{exercise}
Which of the following subsets of $\R^2$ are open? Which are closed? (And why?)
\ben
\item [(i)] $\{(x, 0) :\ 0 \leq x \leq 1\}$;
\item [(ii)] $\{(x, 0) :\ 0 < x < 1\}$;
\item [(iii)] $\{(x, y) :\ y \neq 0 \}$;
\item [(iv)] $\{(x, y) :\ x \in \Q \text{ or }y \in \Q\}$;
\item [(v)] $\{(x, y) :\ y = nx \text{ for some }n \in \N \} \cup \{(x, y) :\ x = 0\}$;
\item [(vi)] $\{(x, f(x)) :\ x \in \R\}$, where $f :\ \R \to \R$ is a continuous function.
\een
\end{exercise}

Solution. In each case we denote the subset concerned by $S$.
\ben
\item [(i)] Not open. $S$ contains no open ball about any point. 

Closed. If $(x,y)\notin S$ then either $y\neq 0$ or $y=0$ and $x<0$, or $y=0$ and $x>1$. The open ball about $(x,y)$ of radius $\frac {\abs{y}}2$ or $-\frac x2$, or $\frac{x-1}2$ as appropriate lies outside $S$.

\item [(ii)] Not open. as in (i). 

Not closed. $(0,0)\notin S$ but any open ball about $(0,0)$ meets $S$.

\item [(iii)] Open. If $(x,y)\in S$ the open ball about $(x,y)$ of radius $\frac{\abs{y}}2$ lies in $S$.

Not closed. as in (ii).

\item [(iv)] Not open. as in (i).

Not closed. If $(x,y)\notin S$ then any open ball about $(x,y)$ meets $S$.

\item [(v)] Not open. $(1,1)\in S$ but $S$ contains no open ball about $(1,1)$.

Closed. If $(x,y)\notin S$ then $x\neq 0$ and $\frac yx \notin \N$. If $\frac yx <1$ set
\be
S_1 = \{(x',0):x'\in \R\},\quad\quad S_2 = \{(x',x'):x'\in \R\},
\ee
while if $\frac yx >1$ choose $n\in\N$ such that $n< \frac yx < n+1$ and set
\be
S_1 = \{(x',nx'):x'\in \R\},\quad\quad S_2 = \{(x',(n+1)x'):x'\in \R\}.
\ee

As in (iii), $S_1$ and $S_2$ are closed, so $S_1\cup S_2$ is closed and thus $\R^2\bs (S_1\cup S_2)$ is open, and as this set contains $(x,y)$ it contains an open about $(x,y)$, which then lies outside $S$.


\item [(vi)] Not open. as in (i). 

Closed. If $(x,y)\not\in S$ then $f(x) \neq y$. Let $\ve=\frac 12 \abs{f(x)-y}$, then $\exists \delta >0$ such that
\be
\abs{z-x}< \delta \ \ra \ \abs{f(z)-f(x)} < \ve \ \ra \ \abs{y-f(z)} \geq \abs{y-f(x)}-\abs{f(x)-f(z)} > 2\ve - \ve = \ve,
\ee
so the open ball about $(x,y)$ of radius $\min\{\delta, \ve\}$ lies outside $S$.

\centertexdraw{
\drawdim in
\def\bdot {\fcir f:0 r:0.02 }

\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0

\move (-0.2 0) \avec(2.3 0)
\move (0 -0.2) \avec(0 1.8)

\move (2.8 0) \avec(5.3 0)
\move (3 -0.2) \avec(3 1.8)

\lpatt( )
\move (0 0) \lvec (1.8 1.8)
\move (0 0) \lvec (0.9 1.8)

\move (3.2 0.5) \clvec (3.5 0.8)(4 0)(4.5 1.2)
\move (4.5 1.2) \clvec (4.8 2)(4.9 1.5)(5 1.3)

\htext (0.1 1.9){$x=0$}
\htext (0.8 1.9){$y = 2x$}
\htext (1.4 1.9){$y = x$}


\htext (3.2 0.6){$f$}
\htext (1 -0.2){(v)}
\htext (4 -0.2){(vi)}

\lpatt(0.05 0.05)

\move (1.5 1.2) \lcir r:0.2
\move (1.5 1.2) \bdot

\move (4.5 1.2) \lcir r:0.2
\move (4.5 1.2) \bdot
\move (0 2.2)
}

\een

\begin{exercise}
Is the set $\{f : f(1/2) = 0\}$ closed in the space $C[0, 1]$ with the uniform norm? What about the set $\{f : \int^1_0 f(x)dx = 0\}$? In each case, does the answer change if we replace the uniform norm with the norm $\dabs{\cdot}_1$ defined in Question \ref{ques:norm_one}?
\end{exercise}

Solution. Let $S=\{f : f(1/2) = 0\}$. Given $f\in C[0,1]\bs S$, set $r = \frac 12 \abs{f(\tfrac 12)} \ (\neq 0)$. Given $g\in C[0,1]$, if we have
\be
\dabs{g-f}_\infty < r \ \ra \ \sup_{x\in [0,1]}\abs{g(x)-f(x)} < r \ \ra \ g(\tfrac 12) - f(\tfrac 12) < r.
\ee

Thus,
\be
\abs{g(\tfrac 12)}  \geq \abs{f(\tfrac 12)} - \abs{g(\tfrac 12) - f(\tfrac 12)} > 2r -r = r \ \ra \ g \notin S.
\ee
So $C[0,1]\bs S$ is open and thus $S$ is closed in $C[0,1]$ with the uniform norm.

Now define $f\in C[0,1]\bs S$ by $f(x)=1$ and $g_1, g_2, \dots \in C[0,1]$ by
\be
g_n(x) = \left\{\ba{ll}
n\abs{x-\frac 12} \quad\quad & \abs{x-\frac 12}\leq \frac 1n\\
1 & \text{otherwise}
\ea\right. \ \ra \ g_n \in S.
\ee
and $\dabs{h}_1 = \int^1_0 \abs{h(x)}dx$
\be
\dabs{g_n - f}_1 = \int^1_0 \abs{g_n - f} = \frac 2n - n\int^{\frac 12 + \frac 1n}_{\frac 12 - \frac 1n}\abs{x-\frac 12}dx  = \frac 2n - 2n\int^{\frac 1n}_0 x dx = \frac 1n.
\ee

So $S$ meets every open ball about $f$, and hence is not closed on $C[0,1]$ with norm $\dabs{\cdot}_1$.

Now consider $S'= \{f : \int^1_0 f(x)dx = 0\}$. Given $f\in C[0,1]\bs S'$, set $r = \frac 12 \abs{\int^1_0 f} \ (\neq 0)$. Given $g\in C[0,1]$, if we have
\be
\dabs{g-f}_\infty < r \ \ra \ \sup_{x\in [0,1]}\abs{g(x)-f(x)} < r \ \ra \ \int^1_0 \abs{g-f} < r.
\ee
Thus,
\be
\abs{\int^1_0 g} \geq \abs{\int^1_0 f} - \abs{\int^1_0 (g-f)} \geq 2r -\int^1_0  \abs{g-f} > 2r -r = r \ \ra \ g \notin S'.
\ee
So $C[0,1]\bs S'$ is open and thus $S'$ is closed in $C[0,1]$ with the uniform norm.

Similarly, if $\dabs{g-f}_1 < r$ then $\dabs{h}_1 = \int^1_0 \abs{h(x)}dx$
\be
\int^1_0 \abs{g-f} < r \ \ra \ \abs{\int^1_0 g} \geq r \ \ra \ g\notin S'.
\ee
So $C[0,1]\bs S'$ is open and thus $S'$ is closed in $C[0,1]$ with the norm $\dabs{\cdot}_1$.

\begin{exercise}
Which of the following functions $f$ are continuous?
\ben
\item [(i)] The linear map $f :\ \ell_\infty \to \R$ defined by $f(x) = \sum^\infty_{n=1} x_n/n^2$.
\item [(ii)] The identity map from the space $C[0, 1]$ with the uniform norm to the space $C[0, 1]$ with the norm $\dabs{\cdot}_1$ defined in Question \ref{ques:norm_one}.
\item [(iii)] The identity map from $C[0, 1]$ with the norm $\dabs{\cdot}_1$ to $C[0, 1]$ with the uniform norm.
\item [(iv)] The linear map $f :\ \ell_0 \to \R$ defined by $f(x) = \sum^\infty_{i=1} x_i$.
\een
\end{exercise}

Solution. \ben
\item [(i)] Continuous. Given $x\in \ell_\infty$ and $\ve>0$, set $\delta = \frac {6\ve}{\pi^2}$, then 
\be
\dabs{y-x}_\infty < \delta \ \ra \ \forall n \in \N, \ \abs{y_n - x_n} < \frac {6\ve}{\pi^2}.
\ee

Thus,
\be
\abs{f(y)-f(x)} = \abs{\sum^\infty_{n=1} \frac{y_n-x_n}{n^2}} \leq \abs{y_n - x_n} \sum^\infty_{n=1} \frac 1{n^2} < \frac {6\ve}{\pi^2}\sum^\infty_{n=1} \frac 1{n^2} = \ve.
\ee

\item [(ii)] Continuous. Given $f\in C[0,1]$ and $\ve>0$, set $\delta = \ve$, then 
\be
\dabs{g-f}_\infty < \delta \ \ra \ \sup\abs{g(x)-f(x)} < \delta \ \ra \ \forall x \in [0,1], \ \abs{g(x)-f(x)} < \delta = \ve
\ee
Then
\be
\dabs{g-f}_1 = \int^1_0 \abs{g-f} < \int^1_0 \ve dx = \ve.
\ee

\item [(iii)] Not continuous. Take $f\in C[0,1]$ defined by $f(x)=1$ and set $\ve = \frac 12$. So $\forall \delta >0$ choose $n\in\N$ with $n> \frac 1{\delta}$, and define $g\in C[0,1]$ by 
\be
g(x) = \left\{\ba{ll}
n\abs{x-\frac 12} \quad\quad & \abs{x-\frac 12}\leq \frac 1n\\
1 & \text{otherwise}
\ea\right.
\ee
Then 
\be
\dabs{g-f}_1 = \int^1_0 \abs{g-f} = \frac 1n < \delta \quad (\text{by previous result})
\ee
but 
\be
\dabs{g-f}_\infty = \sup\abs{g(x)-f(x)} = 1 > \ve.
\ee

\item [(iv)] Not continuous. Take $x=0\in \ell_0$ and set $\ve= \frac 12$. Given $\delta>0$ choose $N>\frac 1{\delta}$ and take $y=(y_n)$ where 
\be
y_n = \left\{\ba{ll}
\frac 1N \quad\quad & n\leq N\\
0 & n>N
\ea\right. \ \ra \ \dabs{y-x} = \frac 1N< \delta, \quad \quad \text{but } \abs{f(y)-f(x)} = 1 > \ve.
\ee

\een

\begin{exercise}
If $A$ and $B$ are subsets of $\R^n$, we write $A + B$ for the set $\{a + b :\ a \in A,\ b \in B\}$. Show that if $A$ and $B$ are both closed and one of them is bounded then $A + B$ is closed. Give an example in $\R^1$ to show that the boundedness condition cannot be omitted. If $A$ and $B$ are both open, is $A + B$ necessarily open? Justify your answer.
\end{exercise}

Solution. If say $A$ is bounded, suppose $c\in \R^n$ is such that for all $\ve>0$, there exists $a\in A$, $b\in B$ with 
\be
\dabs{c-(a+b)} < \ve.
\ee

Taking $\ve = 1,\frac 12,\frac 13,\dots$, we obtain sequences $(a_m)$ in $A$ and $(b_m)$ in $B$ with $a_m+b_m\to c$. As the sequence $(a_m)$ is bounded, it has a convergent subsequence $(a_{m_i})$ (by Bolzano-Weierstrass Theorem) with limit $a$, and as $A$ is closed we have $a\in A$. Then
\be
\lim (a_{m_i} + b_{m_i}) = c \ \ra \ \lim b_{m_i} = c - \lim a_{m_i} = c-a, 
\ee
and as $B$ is closed we have $c-a \in B$, thus $c=a+(c-a) \in A+B$ and so $A+B$ is closed.

Let $A=\Z$ and $B=\{n+\frac 1n:n\geq 2,n\in \N\}$. Both sets are clearly closed. We have
\be
A+B = \{m+\tfrac 1n,\ m\in \Z,\ n\geq 2,n\in\N\}
\ee
So $0\notin A+B$ but $\forall n\geq 2$, $\frac 1n\in A+B$, so any open ball about 0 meets $A+B$, thus $A+B$ is not closed.

If $A,B\subset \R^n$ are open, $\forall c\in A+B$, then $\exists a\in A$, $b\in B$ with $c=a+b$. As $A$ is open, $\exists r>0$ s.t.
\be
\dabs{a'-a} < r \ra \ a' \in A, 
\ee
So 
\be
\dabs{c'-c} < r \ \ra \ \dabs{(c'-b)-a}<r \ \ra \ c'-b \in A \ \ra \ c'-b = a', a'\in A \ \ra \ c' = a'+b \in A+B.
\ee
Thus, $A+B$ is open.

\begin{exercise}
\ben
\item [(a)] Show that the space $\ell_\infty$ is complete. Show also that $c_0 = \{x\in \ell_\infty:\ x_n \to 0\}$, the vector subspace of $\ell_\infty$ consisting of all sequences converging to 0, is complete.
\item [(b)] Is the space $R[0, 1]$ of integrable functions on $[0, 1]$, equipped with the uniform norm, complete?
\een
\end{exercise}

Solution. \ben
\item [(a)] Take a Cauchy sequence $x^{(1)}, x^{(2)}, \dots, $ in $\ell_\infty$. Given $\ve>0$, $\exists N\in\N$ such that $n,m>N$,
\be
\dabs{x^{(n)}- x^{(m)}}_\infty < \ve \ \ra \ \forall t \in \N, \quad \abs{x^{(n)}_t - x^{(m)}_t} < \ve.
\ee
Thus, $\forall t\in \N$ the sequence $x^{(1)}_t, x^{(2)}_t,\dots$ is Cauchy and hence converges to some $x_t \in \R$. Set $x=(x_1, x_2,\dots)$. $\exists N\in \N$ such that $n>N$,
\be
\dabs{x^{(n)}- x^{(N)}} < 1, \quad \text{i.e., }\forall t\in\N,\quad \abs{x^{(n)}_t - x^{(N)}_t} < 1 \ \ra \ \abs{x_t-x^{(N)}_t} < 1 \ \ra\ \abs{x_t} < \abs{x^{(N)}_t}  + 1.
\ee

Thus, as $x^{(N)}$ is a bounded sequence, so is $x$, and $x\in \ell_\infty$. As above, $\forall \ve >0$, $\exists N\in \N$ such that $n,m>N$,
\be
\forall t\in \N,\quad \abs{x_t^{(n)}-x_t^{(m)}} < \ve,
\ee
taking the limit as $m\to\infty$ gives $\forall t\in \N$, 
\be
\abs{x_t^{(n)}-x_t} < \ve \ \ra \ \dabs{x^{(n)}-x}_\infty < \ve \ \ra\ x^{(n)}\to x. \ \ra \ \ell_\infty \text{ is complete.}
\ee

Now suppose $x^{(1)}, x^{(2)},\dots$ is a Cauchy sequence in $c_0$, then $x^{(n)}\to x$ for some $x\in \ell_\infty$. Given $\ve>0$, $\exists N\in\N$ such that $n,m>N$
\be
\forall t\in \N,\quad \abs{x^{(n)}_t -x^{(m)}_t} < \frac {\ve}2,
\ee
taking the limit as $m\to\infty$ gives 
\be
\forall t\in \N,\quad \abs{x^{(n)}_t -x_t} < \frac {\ve}2.
\ee
Then $\exists T\in\N$ such that $\forall t>T$,
\be
\abs{x^{(n)}_t} < \frac{\ve}2 \quad\quad (\text{by }x_n\to 0)
\ee
Thus,
\be
\abs{x_t} \leq \abs{x_t^{(n)}-x_t} + \abs{x_t^{(n)}} < \frac {\ve}2 + \frac {\ve}2 = \ve \ \ra \ x\in c_0 \ \ra \ c_0 \text{ is complete}.
\ee

\item [(b)] Take a Cauchy sequence $f_1,f_2,\dots$ in $R[0,1]$. Given $\ve >0$, $\exists N\in\N$ such that $n,m>N$
\be
\dabs{f_n - f_m}_\infty < \ve \ \ra \ \sup_{x\in [0,1]}\abs{f_n(x)-f_m(x)} < \ve.
\ee
Then $\forall x \in [0,1]$, the sequence $f_1(x),f_2(x),\dots$ is Cauchy and hence converges in $\R$. Define $f:[0,1]\to \R$ by $f(x)=\lim_{n\to\infty}f_n(x)$. Thus, given $\ve>0$, $\exists N\in\N$ such that 
\be
n>N,\ \forall x \in [0,1], \quad \abs{f_n(x)-f(x)} < \frac {\ve}3.
\ee
Choose $n>N$, then exists a dissection $\sD$ of $[0,1]$ such that 
\be
S_{\sD}(f_n) - s_{\sD}(f_n) < \frac {\ve}3.
\ee

On any interval $[a,b]\subseteq [0,1]$ we have
\be
\sup_{[a,b]}f \leq \sup_{[a,b]}f_n + \frac {\ve}3,\quad\quad \inf_{[a,b]}f \geq \inf_{[a,b]}f_n - \frac {\ve}3,
\ee
then
\be
S_{\sD}(f) \leq S_{\sD}(f_n) + \frac {\ve}3, \quad s_{\sD}(f) \geq s_{\sD}(f_n) - \frac {\ve}3 \ \ra \ S_{\sD}(f) - s_{\sD}(f) < \frac {\ve}3 + \frac {\ve}3 + \frac {\ve}3 = \ve \ \ra \ f\in R[0,1].
\ee

As above, $\forall \ve>0$, $\exists N\in\N$ such that $n,m > N$
\be
\forall x\in [0,1],\quad \abs{f_n(x)-f_m(x)} < \ve.
\ee
Taking the limit as $m\to \infty$ gives $\forall x\in [0,1]$, 
\be
\abs{f_n(x) -
 f(x)} < \ve \ \ra \ \dabs{f_n - f}_{\infty} < \ve \ \ra \ f_n \to f\ \ra \ R[0,1]\text{ is complete.}
\ee
\een

\begin{exercise}
Let $\alpha:\R^n \to \R^m$ be a linear map. Show that $\dabs{x}' = \dabs{x}+\dabs{\alpha x}$ defines a norm on $\R^n$. Using the fact that all norms on a finite-dimensional space are Lipschitz equivalent, deduce that $\alpha$ is continuous.
\end{exercise}

Solution. For $\dabs{x}' = \dabs{x}+\dabs{\alpha x}$ we check
\ben
\item [(i)] Clearly, $\dabs{\cdot}':\R^n\to\R$ takes non-negative values
\item [(ii)]
\be
\dabs{x}' = 0 \ \ra \ \dabs{x} = \dabs{\lm x} = 0 \ \ra \ x = 0,
\ee
\item [(iii)] For all $\lm \in\R$
\be
\dabs{\lm x}' = \dabs{\lm x} +\dabs{\alpha \lm x} = \abs{\lm} \dabs{x} + \abs{\lm \alpha}\dabs{x} = \abs{\lm}\dabs{x}',
\ee
\item [(iv)] and
\be
\dabs{x+y}' = \dabs{x+y}+\dabs{\alpha (x+y)} \leq \dabs{x} + \dabs{y} + \abs{\alpha} \dabs{x+y} = \dabs{x}' + \dabs{y}'.
\ee
\een
Thus, $\dabs{x}'$ defines a norm on $\R^n$. 

As $\dabs{\cdot}$ and $\dabs{\cdot}'$ are Lipschitz equivalent, $\exists M>0$ such that $\forall x\in\R^n$
\be
\dabs{x}' \leq M\dabs{x} \ \ra \ \dabs{x} + \dabs{\alpha x}\leq M\dabs{x} \ \ra \ \dabs{\alpha x} \leq (M-1)\dabs{x}
\ee

Given $\ve>0$, set $\delta = \frac {\ve}{M-1}$, then
\be
\dabs{y-x} < \delta \ \ra \ \dabs{\alpha(y)-\alpha(x)} = \abs{\alpha(y-x)} \leq (M-1)\dabs{y-x} < \ve,
\ee
thus, $\alpha$ is continuous.

\begin{exercise}
Which of the following vector spaces of functions, considered with the uniform norm, are complete? (Justify your answer.)
\ben
\item [(i)] The space $C_b(\R)$ of bounded continuous functions $f : \R \to \R$.
\item [(ii)] The space $C_0(\R)$ of continuous functions $f : \R \to \R$ such that $f(x) \to 0$ as $\abs{x} \to \infty$.
\item [(iii)] The space $C_c(\R)$ of continuous functions $f : \R \to \R$ such that $f(x) = 0$ for $\abs{x}$ sufficiently large.
\een
\end{exercise}

Solution. In each case, let $S$ be the space and $(f_n)$ be a Cauchy sequence in $S$. We know that the space of continuous function $\R\to\R$ is complete with respect to the uniform norm, so $f_n \to f$ for some continuous function $f$. It thus suffices to determine whether or not we must have $f\in S$.
\ben
\item [(i)] If $S=C_b(\R)$, there exists $N\in\N$ such that $\forall m>n\geq N$,
\be
\dabs{f_n-f_m}_\infty = \sup\{\abs{f_n - f_m}:x\in\R\} <1.
\ee
Since $f_N$ is bounded, there exists $K\in\R$ such that for all $x\in\R$, we have $\abs{f_N(x)}<K$, and then $\forall m>N$ and $x\in\R$, 
\be
\abs{f_m(x)} \leq \abs{f_m(x)-f_N(x)} + \abs{f_N(x)} < K+1 \ \ra \ \abs{f(x)}\leq \abs{f(x)-f_m(x)} + \abs{f_m(x)} 
\ee
we take $\ve = 1$, $\exists N'\in\N$ such that $\forall n > N, x\in\R$, $\abs{f_n(x) -f(x)} < 1$, then take $N'' = \max\{N,N'\}$, then $\forall n>N$
\be
\abs{f(x)}\leq \abs{f(x)-f_n(x)} + \abs{f_n(x)} < 1 + K+1 = K+2 \ \ra \ f \in C_b(\R).
\ee

\item [(ii)] If $S=C_0(\R)$, given $\ve>0$, there exists $N\in\N$ such that $\forall m>n\geq N,\ x\in S$,
\be
\sup\{\abs{f_n(x)-f_m(x)}:x\in\R\} < \frac {\ve}4, \ \ra \ \abs{f_N(x)-f_m(x)} < \frac {\ve}4,
\ee
and then by the definition of $C_0(\R)$ and $f_N\in C_0(\R)$, $\exists M>0$ such that $\abs{x}>M$, 
\be
\abs{f_N(x)} < \frac{\ve}4.
\ee
Take $x\in\R$ with $\abs{x}>M$, then 
\be
\abs{f_m(x)} \leq \abs{f_m(x)-f_N(x)} + \abs{f_N(x)} < \frac {\ve}4 + \frac {\ve}4 = \frac {\ve}2.
\ee
then since $f_m \to f$, $\exists N'\in\N$ such $\forall n \geq N'$, 
\be
\abs{f_m(x)-f(x)} < \frac {\ve}2.
\ee

Thus, $\forall \ve>0$, $\exists M>0$ such that $\abs{x}>M$ and $\exists N'' = \max\{N,N'\}$ such that $\forall n\geq N''$, 
\be
\abs{f(x)} \leq \abs{f_m(x)-f(x)} + \abs{f_m(x)} < \frac {\ve}2 + \frac {\ve}2 = \ve \ \ra \ f\in C_0(\R).
\ee

\item [(iii)] If $S=C_c(\R)$, define $f_n$ by 
\be
f_n(x)= \left\{\ba{ll}
1 & \abs{x}\leq 1 \\
\frac 1{\abs{x}} & 1<\abs{x}\leq n\\
\frac 1n\bb{n+1-\abs{x}} \quad\quad & n< \abs{x} \leq n+1\\
0 & n+1 < \abs{x}
\ea\right.
\ee

It is obvious that $f_n \in S$. Given $\ve>0$ choose $N > \frac 1{\ve}$, then if $m>n\geq N$ the function $f_m$ and $f_n$ agree on $[-n,n]$, while $\abs{x}>n$, we have
\be
\abs{f_m(x)-f_n(x)} \leq \frac 1n < \ve \ \ra \ (f_n) \text{ is a Cauchy sequence in }S.
\ee

Also, we know that $f_n \to f$ with 
\be
f(x) = \left\{\ba{ll}
1 & \abs{x}\leq 1 \\
\frac 1{\abs{x}} \quad\quad & \abs{x}>1
\ea\right. \quad \ra \ f\notin S \ \ra \ C_c(\R) \text{ is not complete.}
\ee

\een

\begin{exercise}
In lectures we proved that if $E$ is a closed and bounded set in $\R^n$, then any continuous function defined on $E$ has bounded image. Prove the converse: if every continuous real-valued function on $E \subseteq \R^n$ is bounded, then $E$ is closed and bounded.
\end{exercise}

Solution. Take $a\in E$ and define $f:E\to\R$ by $f(x) = \dabs{x-a}$, then $f$ is continuous and therefore bounded (by the assumption), there exists $r>0$ such that for all $x\in E$ we have $\abs{f(x)} \leq r$. So $E$ lies in the ball of radius $r$ about $a$, thus $E$ is bounded.

Given $b\in \R^n\bs E$ define $f:E\to \R$ by $f(x) = \frac 1{\dabs{x-b}},\ x\in E$, then $f$ is continuous therefore bounded (by the assumption), there exists $\ve>0$ such for all $x\in E$ we have 
\be
\abs{f(x)} < \frac 1{\ve} \ \ra \ \dabs{x-b} > \ve.
\ee
So the open ball of radius $\ve$ about $b$ does not meet $E$, thus $E$ is closed.

\begin{exercise}
Let $(x^{(m)})_{m\geq 1}$ be a bounded sequence in $\ell_\infty$. Show that there is a subsequence $(x^{(m_j)})_{j\geq 1}$ which converges in every coordinate; that is to say, the sequence $(x^{(m_j)}_i)_{j\geq 1}$ of real numbers converges for each $i$. Why does this not show that every bounded sequence in $\ell_\infty$ has a convergent subsequence?
\end{exercise}

Solution. Choose a subsequence $(x^{(m_i)})$ of $x^{(m)}$ as follow. The value $x_1^{(m)}$ form a bounded sequence, so they have a convergent subsequence $x_1^{(l_{1,m})}$. 
\begin{center}
\begin{tabular}{c|ccc}
 & \ bounded \  & \ bounded \  &  \\ 
 & \ sequence \  & \ sequence \  & $\quad$ \\ \hline
$x^{(1)}$ & $x_1^{(1)}$ & $x_2^{(1)}$ & $\dots$  \\ 
$x^{(2)}$ & $x_1^{(2)}$ & $x_2^{(2)}$ & $\dots$  \\ 
$\vdots$ & & &\\
$x^{(m)}$ & $x_1^{(m)}$ & $x_2^{(m)}$ & $\dots$  
\end{tabular} $\ \ra \ $
\begin{tabular}{c|ccc}
 & \ convergent \  & \ bounded \  &  \\ 
 & \ sequence \  & \ sequence \  & $\quad$ \\ \hline
$x^{(l_{1,1})}$ & $x_1^{(l_{1,1})}$ & $x_2^{(l_{1,1})}$ & $\dots$  \\ 
$x^{(l_{1,2})}$ & $x_1^{(l_{1,2})}$ & $x_2^{(l_{1,2})}$ & $\dots$  \\ 
$\vdots$ & & & \\
$x^{(l_{1,m})}$ & $x_1^{(l_{1,m})}$ & $x_2^{(l_{1,m})}$ & $\dots$  
\end{tabular} $\  \ra \ $
\begin{tabular}{c|ccc}
 & \ convergent \  & \ convergent \  &  \\ 
 & \ sequence \  & \ sequence \  & $\quad$ \\ \hline
$x^{(l_{2,1})}$ & $x_1^{(l_{2,1})}$ & $x_2^{(l_{2,1})}$ & $\dots$  \\ 
$x^{(l_{2,2})}$ & $x_1^{(l_{2,2})}$ & $x_2^{(l_{2,2})}$ & $\dots$  \\ 
$\vdots$ & & & \\
$x^{(l_{2,m})}$ & $x_1^{(l_{2,m})}$ & $x_2^{(l_{2,m})}$ & $\dots$  
\end{tabular}
\end{center}
Then for the bounded sequence $x_2^{(l_{1,m})}$, we have a convergent subsequence $x_2^{(l_{2,m})}$. Repeating these steps we can find a convergent subsequence $x_i^{(l_{i,m})}$ converges in the 1st, 2nd, $\dots,\ i$th co-ordinates. Thus, there is subsequence as required. 

This does not show that every bounded sequence in $l_{\infty}$ has a convergent subsequence. We have shown the statement
\be
\forall i \in \N,\ \forall \ve>0, \ \exists N\in\N \text{ such that }\forall m\geq N,\quad \abs{x_i^{(l_{i,m})}-\lim_{m\to\infty}x_i^{(l_{i,m})}}<\ve,
\ee
but this does not imply
\be
\forall \ve >0, \ \exists N\in\N \text{ such that }\forall m\geq N,\ \forall i\in \N,\quad \abs{x_i^{(l_{i,m})}-\lim_{m\to\infty}x_i^{(l_{i,m})}}<\ve.
\ee

For instance, take $\ve= \frac 12$,
\be
\left\{\ba{l}
x^{(m_1)} = \bb{1,1,1,\dots}\\
x^{(m_2)} = \bb{0,\frac 12,\frac 23,\dots}\\
\vdots\\
x^{(m_n)} = \bb{0,0,\dots, \frac 1n, \frac 2{n+1},\dots,}
\ea\right. \ \ra \ x^{(m_n)} = \bb{0,0,\dots, \frac 1n, \frac 2{n+1},\dots,\underbrace{\frac {n+1}{2n}}_{>\frac 12},\dots}.
\ee

\begin{exercise}
Show that $\dabs{x}_1 = \sum^\infty_{i=1} \abs{x_i}$ defines a norm on $\ell_0$ and that this norm is not Lipschitz equivalent to the uniform norm $\dabs{\cdot}$. Find a third norm on $\ell_0$ which is neither Lipschitz equivalent to $\dabs{\cdot}_1$, nor to $\dabs{\cdot}$. Is it possible to find uncountably many norms on $\ell_0$ such that no two are Lipschitz equivalent?
\end{exercise}

Solution. For $\dabs{x}_1 = \sum^\infty_{i=1} \abs{x_i}: \ell_0 \to\R$, we check \ben
\item [(i)] Clearly $\dabs{\cdot}_1:\ell_0 \to \R$ takes non-negative values,
\item [(ii)] $\dabs{x}_1 = 0 \ \ra\ \sum^\infty_{i=1} \abs{x_i} = 0 \ \ra \ \forall i,\ x_i = 0 \ \ra \ x = 0$,
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm x}_1 = \sum^\infty_{i=1} \abs{\lm x_i} = \abs{\lm} \sum^\infty_{i=1} \abs{x_i} = \abs{\lm}\cdot \dabs{x}_1$,
\item [(iv)] and
\be
\dabs{x+y}_1 = \sum^\infty_{i=1} \abs{(x+y)_i} = \sum^\infty_{i=1} \abs{x_i+y_i} \leq \sum^\infty_{i=1} \abs{x_i}+\abs{y_i} = \sum^\infty_{i=1} \abs{x_i} +\sum^\infty_{i=1} \abs{y_i} = \dabs{x}_1 + \dabs{y}_1.
\ee
\een
so $\dabs{\cdot}_1$ is a norm. 

For $n\in \N$ define $x^{(n)}\in \ell_0$ by 
\be
x_t^{(n)} = \left\{\ba{ll}
1 \quad\quad & t\leq n\\
0 & t>n
\ea\right. \ \ra \ \dabs{x^{(n)}}_\infty = 1,\quad \dabs{x^{(n)}}_1 = n.
\ee
Thus, $\forall M>0$, we may choose $n\in \N$ with $n>M$, and then
\be
\dabs{x^{(n)}}_1 > M \dabs{x^{(n)}}_\infty
\ee
So $\dabs{\cdot}_1$ is not Lipschitz equivalent to the uniform norm $\dabs{\cdot}_\infty$.

Note that if $x\in\ell_0$, then $\exists k\in\N$ such that 
\be
\sum_{i>k} \abs{x_i} < 1 \ \ra\ \forall i>k, \ \abs{x_i} <1 \ \ra \ \forall i>k,\ \abs{x_i}^2 < \abs{x_i} \ \ra \ \sum\abs{x_i} \text{ converges (increasing and bounded).}
\ee

We may therefore define the norm $\dabs{\cdot}_2: \ell_0 \to \R$ by
\be
\dabs{x}_2 = \bb{\sum^\infty_{i=1} x_i^2}^{\frac 12}.
\ee
We check \ben
\item [(i)] Clearly $\dabs{\cdot}_2:\ell_0 \to \R$ takes non-negative values,
\item [(ii)] $\dabs{x}_2 = 0 \ \ra\ \sum^\infty_{i=1} x_i^2 = 0 \ \ra \ \forall i,\ x_i = 0 \ \ra \ x = 0$,
\item [(iii)] For all $\lm \in\R$, 
\be
\dabs{\lm x}_2 = \bb{\sum^\infty_{i=1} (\lm x_i)^2}^{\frac 12} = \abs{\lm} \bb{\sum^\infty_{i=1} x_i^2}^{\frac 12} = \abs{\lm}\cdot \dabs{x}_2,
\ee
\item [(iv)] and
\beast
\dabs{x+y}_2 & = & \bb{\sum^\infty_{i=1} ((x+y)_i)^2}^{\frac 12} = \bb{\sum^\infty_{i=1} (x_i+y_i)^2}^{\frac 12} = \bb{\sum^\infty_{i=1} x_i^2 +y_i^2 + 2x_iy_i}^{\frac 12}\\
& \leq & \bb{\sum^\infty_{i=1} x_i^2 +y_i^2 + 2\sqrt{\sum^\infty_{i=1}x_i^2 \sum^\infty_{i=1}y_i^2}}^{\frac 12} \quad\quad (\text{by Cauchy-Schwarz inequality}) \\
& = & \bb{\bb{\sqrt{\sum^\infty_{i=1}x_i^2} + \sqrt{\sum^\infty_{i=1}y_i^2} }^2}^{\frac 12} = \sqrt{\sum^\infty_{i=1}x_i^2} + \sqrt{\sum^\infty_{i=1}y_i^2} = \dabs{x}_2 + \dabs{y}_2.
\eeast
\een
so $\dabs{\cdot}_2$ is a norm. Given $M>0$ we may choose $n\in\N$ with $n>M^2$, since $\dabs{x^{(n)}}_2 = \sqrt{n}$ we have 
\be
\dabs{x^{(n)}}_1 > M\dabs{x^{(n)}}_2,\quad\quad \dabs{x^{(n)}}_2 > M\dabs{x^{(n)}}_\infty
\ee
so $\dabs{\cdot}_2$ is Lipschitz equivalent to neither $\dabs{\cdot}_1$ nor $\dabs{\cdot}_\infty$.

In fact, for all $p\geq 1$ we observe as above that $\abs{x_i}^p$ converges (since $\abs{x_i}<1$), and hence we may define the map $\dabs{\cdot}_p:\ell_0\to\R$ by
\be
\dabs{x}_p = \bb{\sum^\infty_{i=1}\abs{x_i}^p}^{\frac 1p}.
\ee
We claim that $\dabs{\cdot}_p$ is a norm. Assuming this, whenever $p>q\geq 1$, Given $M>0$, we may choose $n\in\N$ with $n> M^{pq/(p-q)}$, then we have
\be
\dabs{x^{(n)}}_p = n^{\frac 1p}, \quad \dabs{x^{(n)}}_q = n^{\frac 1q} \ \ra \ \dabs{x^{(n)}}_q/\dabs{x^{(n)}}_p = n^{\frac 1q - \frac 1p} = n^{\frac {p-q}{pq}} > M \ \ra \ \dabs{x^{(n)}}_q > M \dabs{x^{(n)}}_p.
\ee
Thus $\dabs{\cdot}_p$ is not Lipschitz equivalent to $\dabs{\cdot}_q$. Thus the uncountably many norms $\dabs{\cdot}_p$ for $p\geq 1$ have the property that no two are Lipschitz equivalent.

It remains to check that $\dabs{\cdot}_p $ is a norm. The only property which is not obvious is the triangle inequality. Take $p$ and write $q=\frac p{p-1}$ so that $\frac 1p + \frac 1q = 1$. We first show the Young inequality:

Since the function $\log$ is concave, for all $a,b>0$ we have
\be
\log ab = \tfrac 1p \log a^p + \tfrac 1q \log b^q \leq \log \bb{\tfrac 1p a^p + \tfrac 1q b^q} \ \ra \ ab \leq \tfrac 1p a^p + \tfrac 1q b^q.
\ee

Next we use this to prove H\"older's inequality: given $x,y\in \ell_0$ we have
\be
\frac{\abs{\sum^\infty_{k=1} x_ky_k}}{\dabs{x}_p\dabs{y}_q} \leq \frac{\sum^\infty_{k=1} \abs{x_k}\cdot\abs{y_k}}{\dabs{x}_p\dabs{y}_q} \leq \sum^\infty_{k=1} \frac{\abs{x_k}}{\dabs{x}_p} \cdot \frac{\abs{y_k}}{\dabs{y}_q} \leq \frac 1p \sum^\infty_{k=1} \frac{\abs{x_k}^p}{\dabs{x}_p^p} + \frac 1q\frac{\abs{y_k}^q}{\dabs{y}_q^q} = \frac 1p + \frac 1q = 1.
\ee
Thus,
\be
\abs{\sum^\infty_{k=1}x_ky_k}\leq \dabs{x}_p\dabs{y}_q = \bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}\bb{\sum^\infty_{k=1}\abs{y_k}^q}^{\frac 1q}
\ee

Finally, given $x,y\in \ell_0$, for all $k\in\N$ we have
\be
\abs{x_k+y_k}^p = \abs{x_k + y_k} \cdot \abs{x_k + y_k}^{p-1} \leq \abs{x_k}\cdot \abs{x_k + y_k}^{p-1} + \abs{y_k}\cdot \abs{x_k + y_k}^{p-1}.
\ee

By H\"older's inequality, we have
\be
\sum^\infty_{k=1}\abs{x_k}\cdot \abs{x_k + y_k}^{p-1} \leq \bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p} \bb{\sum^\infty_{k=1}\abs{x_k+y_k}^{(p-1)q}}^{\frac 1q},
\ee
\be
\sum^\infty_{k=1}\abs{y_k}\cdot \abs{x_k + y_k}^{p-1} \leq \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p} \bb{\sum^\infty_{k=1}\abs{x_k+y_k}^{(p-1)q}}^{\frac 1q}.
\ee
Thus,
\beast
\sum^\infty_{k=1}\abs{x_k + y_k}^{p} & \leq & \bb{\bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}+ \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p}} \bb{\sum^\infty_{k=1}\abs{x_k+y_k}^p}^{\frac 1q}\\ 
\ra \ \bb{\sum^\infty_{k=1}\abs{x_k + y_k}^{p}}^{1-\frac 1q} & \leq & \bb{\bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}+ \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p}} \\
\ra \ \bb{\sum^\infty_{k=1}\abs{x_k + y_k}^{p}}^{\frac 1p} & \leq & \bb{\bb{\sum^\infty_{k=1}\abs{x_k}^p}^{\frac 1p}+ \bb{\sum^\infty_{k=1}\abs{y_k}^p}^{\frac 1p}} \\
\ra \ \dabs{x+y}_p & \leq & \dabs{x}_p + \dabs{y}_p
\eeast
as require.

\begin{exercise}
Let $V$ be a normed space in which every bounded sequence has a convergent subsequence.
\ben
\item [(a)] Show that $V$ must be complete.
\item [(b)] Show further that $V$ must be finite-dimensional.
\een
[Hint for (b): Show first that for every finite-dimensional subspace $V_0$ of $V$ there exists an $x \in V$ with $\dabs{x + y} > \dabs{x}/2$ for each $y \in V_0$.]
\end{exercise}

Solution. \ben
\item [(a)] Let $x_1,x_2,\dots$ be a Cauchy sequence in $V$. Take $\ve=1$, then $\exists N\in\N$ such that 

\be
\forall n,m>N, \quad \dabs{x_n-x_m} <1 \ \ra\ \forall n >N,\quad \dabs{x_n} \leq \dabs{x_n-x_{N+1}} + \dabs{x_{N+1}} < \dabs{x_{N+1}} + 1.
\ee
Thus, the sequence $(x_n)$ is bounded by $\max\{\dabs{x_1},\dots,\dabs{x_N},\dabs{x_{N+1}}+1\}$, so it has a convergent subsequenct $(x_{n_i})$ with limit $x\in V$. 

Now given $\ve>0$, $\exists M\in \N$ such that 
\be
\forall n,m>M,\quad \dabs{x_n-x_m}< \frac {\ve}2.
\ee
As $x_{n_i}\to x$, we may choose $n_i > M$ such that 
\be
\dabs{x_{n_i}-x} < \frac {\ve}2.
\ee
Then $\forall m>M$,
\be
\dabs{x_m-x} \leq \dabs{x_m - x_{n_i}} + \dabs{x_{n_i}-x} < \frac {\ve}2 + \frac{\ve}2 = \ve \ \ra \ x_m \to x \ \ra\ V \text{ is complete.}
\ee

\item [(b)] Let $V$ be an infinite-dimensional normed space. Suppose we have chosen $x_1,\dots,x_k\in V$ with $\dabs{x_i} = 1$ for all $i$ and $x_i$ has the bases
\be
\bb{1,0,0,\dots},\quad \bb{0,1,0,\dots}, \quad \dots, \quad (0,0,\dots,0,\underbrace{1}_{i\text{th}},0,\dots).
\ee
Let $S$ be the subspace spanned by $x_1,\dots,x_k$, and choose $y\in V\bs S$. Let
\be
r=\inf\{\dabs{y-s}:s\in S\},
\ee
then as $S$ is closed subspace (any finite dimensional subspace of a normed vector space is closed), we have $r>0$. Choose $s\in S$ with $r\leq \dabs{y-s}< 2r$ and set 
\be
x_{k+1} = \frac {y-s}{\dabs{y-s}} \ \ra \ \dabs{x_{k+1}} = 1.
\ee

Then, for all $i\leq k$, since $x_i \in S$ we have
\be
s+ \dabs{y-s}x_i \in S \quad \quad (\text{since $S$ is spanned by $x_1,\dots,x_k$}).
\ee
so that 
\be
\dabs{x_{k+1}-x_i} = \dabs{\frac {y-s}{\dabs{y-s}}-x_i} = \frac 1{\dabs{y-s}} \dabs{y- (s+\dabs{y-s}x_i)} \leq \frac 1{\dabs{y-s}}r > \frac 12.
\ee

Thus the sequence $(x_n)$ bounded, but for all $i<j$ we have $\dabs{x_i - x_j} > \frac 12$, so there can be no convergent subsequence. Therefore any normed space in which every bounded sequence has a convergent subsequence must be finite-dimensional.

\een

\begin{exercise}
Recall from the lectures the normed space $\ell_2$. The Hilbert cube is the subset of $\ell_2$ consisting of all the sequences $(x_n)^\infty_{n=1}$ such that for each $n$, $\abs{x_n} \leq 1/n$. Show that the Hilbert cube is closed in $\ell_2$, and that it has the Bolzano-Weierstrass property, that is, any sequence in the Hilbert cube has a convergent subsequence. (So the Hilbert cube is \emph{compact}.)
\end{exercise}

Solution. Let $H$ be the Hilbert cube, and take $(x_n)\in \ell_2\bs H$. Then there exists $k\in\N$ with $\abs{x_k}>\frac 1k$. Let 
\be
\ve = \frac 12 \bb{\abs{x_k}-\frac 1k},
\ee
then if $(y_n)\in \ell_2$ with $\abs{(y_n)-(x_n)}_2 < \ve$, then
\be
\ve^2 > \sum^\infty_{k=1}\abs{y_k -x_k}^2 \geq \abs{y_k - x_k}^2 \ \ra \ \abs{y_k - x_k} < \ve.
\ee
Thus,
\be
\abs{y_k}\geq \abs{x_k} - \abs{y_k - x_k} > \abs{x_k} -\ve = \frac 12 \bb{\abs{x_k}+\frac 1k} > \frac 1k \ \ra \ (y_n) \in \ell_2\bs H.
\ee
Therefore, $\ell_2\bs H$ is open, and hence $H$ is closed in $\ell_2$.

Let $(x_n^{(m)})$ be any sequence in $H$ (bounded). As previous question we may obtain a subsequence $(x_n^{m_j})$ which converges in every co-ordinate. For each $n\in \N$, let 
\be
\lim_{j\to \infty} x_n^{(m_j)} = x_n,
\ee
then as $\abs{x_n^{(m_j)}} \leq \frac 1n$ for all $j$, we have $\abs{x_n}\leq \frac 1n$. Thus $(x_n)\in H$.

Given $\ve>0$, take $N\in\N$ such that 
\be
\sum_{n>N} \frac 1{n^2} < \frac {\ve^2}8.
\ee

For each $n\leq N$ choose $M_n\in \N$ such that
\be
j>M_n \ \ra \ \abs{x_n^{(m_j)}-x_n} < \frac {\ve}{\sqrt{2N}},
\ee
and let $M = \max\{M_1,M_2,\dots, M_N\}$. Then if $j>M$, we have
\beast
\bb{\dabs{(x_n^{(m_j)})-(x_n)}}^2 & = & \sum^N_{n=1}\bb{x_n^{(m_j)}-x_n}^2 + \sum_{n>N}\bb{x_n^{(m_j)}-x_n}^2 \\
& \leq & \sum^N_{n=1}\bb{x_n^{(m_j)}-x_n}^2 + \sum_{n>N}\bb{\abs{x_n^{(m_j)}}+\abs{x_n}}^2 \\
& < & N\cdot \bb{\frac{\ve}{\sqrt{2N}}}^2 + \sum_{n>N} \bb{\frac 2{n}}^2 < \frac {\ve^2}2 + \frac {\ve^2}2 = \ve^2.
\eeast
so
\be
\dabs{(x_n^{(m_j)})-(x_n)} < \ve \ \ra \ (x_n^{(m_j)}) \to (x_n).
\ee
Hence $H$ has the Bolzano-Weierstrass property.

\begin{exercise}\label{ques:norm_derivative} 
Let $\dabs{\cdot}$ denote the usual Euclidean norm on $\R^n$. Show that the map sending $x$ to $\dabs{x}^2$ is differentiable everywhere. What is its derivative? Where is the map sending $x$ to $\dabs{x}$ differentiable and what is its derivative?
\end{exercise}

Solution. Define $f:\R^n\to \R$ by $f(x) = \dabs{x}^2$. Given $x,h\in\R$ we have
\be
f(x+h)-f(x) = \sum^n_{i=1}(x_i + h_i)^2 - \sum^n_{i=1}x_i^2 = \sum^n_{i=1} 2x_ih_i + \sum^n_{i=1}h_i^2.
\ee
Define $\alpha: \R^n\to \R$ by $\alpha(h) = 2x\cdot h$, then $\alpha$ is linear and 
\be
\abs{\frac {f(x+h)-f(x)-\alpha(h)}{\dabs{x+h-x}}} = \frac{\dabs{h}^2}{\dabs{h}} = \dabs{h} \to 0 \quad \text{as }h\to 0.
\ee

Thus, $f$ is differentiable at $x$ and the derivative is $D_xf(h) = 2x\cdot h$.

Now define $g:\R^n\to\R$ by $g(x) = \dabs{x}$. Then $g = j\circ f$ where $j:\R\to\R$ is defines by $j(r) = \sqrt{\abs{r}}$. We know that if $r>0$ then $j$ is differentiable at $r$ with $Dj(r)= \frac 1{2\sqrt{r}}$. Thus if $x\in\R^n\bs\{0,\dots,0\}$, by the chain rule $g$ is differentiable with
\be
D_xg(h) = D_{f(x)}j \cdot \bb{D_xf(h)} = \bb{D_{\dabs{x}^2}j} \cdot \bb{D_xf(h)} = \frac 1{2\dabs{x}} 2x\cdot h = \frac {x\cdot h}{\dabs{x}}
\ee

However, $g$ is not differentiable at $(0,\dots,0)$. If it had derivative $\alpha:\R^n\to \R$ with $\alpha(h) = \sum^n_{i=1} \lm_i \frac{h_i}{\dabs{h}}$, choosing $h= (\delta,0,\dots,0)$ gives
\be
\abs{\frac{g(h)-g(0)- \alpha(h)}{\dabs{h-0}}} = \abs{1-\lm_1 \frac{\delta}{\abs{\delta}}},
\ee
and for this to tend to 0 as $\delta \to 0^+$ we need $\lm_1 = 1$, but then it does not tend to 0 as $\delta \to 0^-$.

\begin{exercise}
At which points of $\R^2$ are the following functions $\R^2 \mapsto \R$ differentiable?
\ben
\item [(i)] $f(x, y) = \left\{\ba{ll} x/y \quad\quad & y \neq 0,\\ 0 & y = 0.\ea\right.$
\item [(ii)] $f(x, y) = \abs{x}\abs{y}$.
\item [(iii)] $f(x, y) = xy \abs{x - y}$.
\item [(iv)] $f(x, y) = \left\{\ba{ll} xy / \sqrt{x^2 + y^2}\quad\quad & (x, y) \neq (0, 0),\\ 0 & (x, y) = (0, 0). \ea\right.$
\item [(v)] $f(x, y) = \left\{\ba{ll} xy \sin(1/x) \quad\quad & x \neq 0, \\ 0 & x = 0.\ea\right.$
\een
\end{exercise}


Solution. \ben
\item [(i)] At $(x,y)$ with $y\neq 0$, $f$ is clearly differentiable, at $(x,0)$ with $x\neq 0$, $f(x,h) = \frac xh \nrightarrow 0$ as $h\to 0$, so $f$ is not even countinuous. At $(0,0)$, $f(h,h)=1 \nrightarrow 0$ as $h\to 0$, so again $f$ is not even continuous.
\item [(ii)] At $(x,y)$ with $x,y\neq 0$, write $\abs{x} = ex$, $\abs{y} = e'y$ with $e,e'\in\{\pm 1\}$, then in a neighourhood of $(x,y)$ we have $f(x,y) = ee'xy$ so $f$ is clearly differentiable. At $(0,y)$ with $y\neq 0$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to\R$ where $\alpha(h,k) = \lm h + \mu k$, then
\be
f(h,y) - f(0,y) = \abs{h}\abs{y}= \lm h + \epsilon(h,0)\abs{h} \ \ra\ \abs{y}-\lm \frac{h}{\abs{h}} = \epsilon(h,0).
\ee
If $h>0$, this forces $\lm = \abs{y}$ while if $h<0$, it forces $\lm = -\abs{y}$, a contradiction. Similarly, at $(x,0)$ with $x\neq 0$, $f$ is not differentiable. At $(0,0)$ it is differentiable with derivative 0, since 
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{f(h,k)}{\dabs{(h,k)}}} = \frac{\abs{h}\abs{k}}{\dabs{(h,k)}} = \frac{\abs{h}\abs{k}}{\sqrt{h^2+k^2}} \leq \frac 12 \bb{\frac{\abs{h}\abs{k}}{\sqrt{h^2}} + \frac{\abs{h}\abs{k}}{\sqrt{k^2}}} = \frac 12\bb{\abs{h}+\abs{k}} \to 0 
\ee
as $(h,k)\to (0,0)$.
\item [(iii)] At $(x,y)$ with $a \neq b$, there is neighbourhood on which $f(x,y) = exy(x-y)$ for some $e\in\{\pm 1\}$, so $f$ is clearly differentiable. At $(x,x)$ with $x\neq 0$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to\R$ where $\alpha (h,k) = \lm h + \mu k$, then
\be
f(x+h,x)-f(x,x) = x(x+h)\abs{x+h-x} - 0 = x(x+h)\abs{h} = \lm h + \epsilon(h,0)\abs{h}.
\ee
So
\be
x^2 + xh - \lm \frac {h}{\abs{h}} = \epsilon(h,0).
\ee
If $h>0$ this forces $\lm = x^2$ while if $h<0$ it forces $\lm = -x^2$, a contradiction. At $(0,0)$ it is differentiable with derivative 0, since
\beast
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} & = & \abs{\frac{hk\abs{h-k}}{\dabs{(h,k)}}} = \frac{\abs{h}\abs{k}\abs{h-k}}{\dabs{(h,k)}} = \frac{\abs{h}\abs{k}\abs{h-k}}{\sqrt{h^2+k^2}} \leq \frac 12 \abs{h-k} \bb{\frac{\abs{h}\abs{k}}{\sqrt{h^2}} + \frac{\abs{h}\abs{k}}{\sqrt{k^2}}} \\
& = & \frac 12\bb{\abs{h}+\abs{k}}\abs{h-k} \to 0 \quad \quad \text{as }(h,k)\to (0,0).
\eeast

\item [(iv)] At $(x,y)\neq (0,0)$, $f$ is clearly differentiable. At $(0,0)$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to \R$ where $\alpha(h,k)=\lm h + \mu k$, then
\be
f(h,0) - f(0,0) = 0 = \lm h + \epsilon(h,0)\abs{h}
\ee
forces $\lm = 0$, and similarly we must have $\mu = 0$, so that $\alpha(h,k)=0$, but
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{hk/\sqrt{h^2+k^2}}{\dabs{(h,k)}}} = \frac{\abs{h}\abs{k}}{h^2+k^2}.
\ee
Without loss of generality, we have $\abs{h}= M\abs{k}$. Thus,
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \frac{\abs{h}\abs{k}}{h^2+k^2} = \frac{M\abs{k}^2}{(1+M^2)k^2} = \frac M{1+M^2} \nrightarrow 0=\alpha(h,k).
\ee
\item [(v)] At $(x,y)$ with $x\neq 0$, $f$ is clearly differentiable. At $(0,y)$ with $y \neq 0$, $f$ is not differentiable, for if it had derivative $\alpha:\R^2\to\R$ at $(0,y)$, where $\alpha(h,k)= \lm h + \mu k$, then
\be
f(h,y)-f(0,y) = hy\sin \tfrac 1h = \lm h + \epsilon(h,0)\abs{h} \ \ra \ \bb{y\sin \tfrac 1h - \lm}\frac{h}{\abs{h}} = \epsilon(h,0) \ \ra \ \lm = y\sin\tfrac 1h
\ee
which is a contradiction. At $(0,0)$ it is differentiable with derivative 0, since
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{hk\sin \tfrac 1h}{\dabs{(h,k)}}} \leq \frac{\abs{h}\abs{k}}{\dabs{(h,k)}} = \frac{\abs{h}\abs{k}}{\sqrt{h^2+k^2}} \leq \frac 12 \bb{\frac{\abs{h}\abs{k}}{\sqrt{h^2}} + \frac{\abs{h}\abs{k}}{\sqrt{k^2}}} = \frac 12\bb{\abs{h}+\abs{k}} \to 0 .
\ee
\een

\begin{exercise}
Let $f(x, y) = x^2y/(x^2 + y^2)$ for $(x, y) \neq (0, 0)$, and $f(0, 0) = 0$. Show that $f$ is continuous at $(0, 0)$ and that it has directional derivatives in all directions there (i.e. for any fixed $\alpha$, the function $t \mapsto f(t \cos \alpha, t \sin \alpha )$ is differentiable at $t = 0$). Is $f$ differentiable at $(0, 0)$?
\end{exercise}

Solution. Given $\ve>0$, take $\delta = \ve$, then
\be
\dabs{(x,y)-(0,0)} < \delta \ \ra \ \abs{y}<\ve \ \ra \ \abs{f(x,y)-f(0,0)}= \abs{\frac{x^2y}{x^2 + y^2}} \leq \abs{\frac{x^2y}{x^2}} = \abs{y}< \ve.
\ee
so $f$ is continuous at $(0,0)$. For fixed $\alpha$, let $f_\alpha:\R\to\R$ be the function defined by 
\be
f_\alpha(t) = f(t\cos\alpha,t\sin \alpha) = \frac{t^3\cos^2\alpha\sin\alpha}{t^2} = t\cos^2\alpha\sin\alpha
\ee
then $f_\alpha$ is linear and thus differentiable at $t=0$, i.e., $f$ has directional derivatives in all directions at $(0,0)$. Suppose $f$ had derivative $\alpha:\R^2\to \R$ at $(0,0)$ where $\alpha(h,k) = \lm h + \mu k$, then
\be
f(h,0)-f(0,0) = 0 = \lm h + \epsilon(h,0)\abs{h}
\ee
forces $\lm = 0$, and similarly we must have $\mu = 0$, thus $\alpha = 0$, but (ithout loss of generality, we have $\abs{h}= M\abs{k}$)
\be
\abs{\frac{f(h,k)-f(0,0)}{\dabs{(h,k)-(0,0)}}} = \abs{\frac{f(h,k)}{\dabs{(h,k)}}} = \abs{\frac{h^2k/(h^2+k^2)}{\dabs{(h,k)}}} = \abs{\frac{M^2}{(1+M^2)^{\frac 32}}} \nrightarrow 0 \quad\text{as }(h,k)\to (0,0).
\ee
So $f$ is not differentiable at $(0,0)$.

\begin{exercise}
We work in $\R^3$ with the usual inner product. Consider the map $f :\R^3 \mapsto \R^3$ given by $f(x) = x/\dabs{x}$ for $x \neq 0$ and $f(0) = 0$. Show that $f$ is differentiable except at 0 and 
\be
D_xf(h) = \frac h{\dabs{x}} - \frac{x(x \cdot h)}{\dabs{x}^3},\quad\quad x \neq 0, \ h \in \R^3
\ee

Verify that $D_xf(h)$ is orthogonal to $x$ and explain geometrically why this is the case.
\end{exercise}


Solution. Since $\dabs{f(x)} = 1$ for all $x\neq 0$ but $f(0)=0$, $f$ is not continuous at 0, so it is certianly not differentiable there. Define $g:\R^3\to \R^3$ by $g(x) = \dabs{x}$, $i:\R^3\to\R^3$ by $i(x)=x$, and $k:\R\bs\{0\}$ by $k(r) = \frac 1r$. Then by question \ref{ques:norm_derivative}, $g$ is differentiable on $\R^3\bs\{0\}$ with $D_xg(h) = \frac {x\cdot h}{\dabs{x}}$, and we know that $i$ and $k$ are differentiable at $x$ with
\be
D_xi(h) = h,\quad \quad D k(r) = -\frac 1{r^2}.
\ee

Since on $\R^3\to \{0\}$ we have $f=i \cdot (k\circ g)$, by the chain and product rules $f$ is differentiable at $x$ with
\beast
D_xf(h) & = & D_xi(h) \cdot (k\circ g)(x) + i(x) \cdot D_{g(x)}k \cdot D_xg(h)\\
& = & h \frac 1{\dabs{x}} + x \bb{-\frac 1{\dabs{x}^2} \frac {x\cdot h}{\dabs{x}}} = \frac h{\dabs{x}} - \frac{x(x \cdot h)}{\dabs{x}^3}.
\eeast

Thus,
\be
x\cdot D_xf(h) = x\cdot \bb{\frac h{\dabs{x}} - \frac{x(x \cdot h)}{\dabs{x}^3}} = \frac {x\cdot h}{\dabs{x}} - \frac{\dabs{x}^2(x \cdot h)}{\dabs{x}^3} =0.
\ee
so $D_xf(h)$ is orthogonal to $x$. The geometrical reason is that for fixed $x$ the value of $f$ is constant in the direction of $x$.

\begin{exercise}
\ben
\item [(i)] Suppose that $f : \R^2 \mapsto \R$ is such that $D_1 f= \partial f/\partial x$ is continuous in some open ball around $(a, b)$, and $D_2 f= \partial f/\partial y$ exists at $(a, b)$. Show that $f$ is differentiable at $(a, b)$.
\item [(ii)] Suppose that $f : \R^2 \mapsto \R$ is such that $\partial f/\partial x$ exists and is bounded near $(a, b)$, and that for a fixed, $f(a, y)$ is continuous as a function of $y$. Show that $f$ is continuous at $(a, b)$.
\een
\end{exercise}

Solution. \ben
\item [(i)] Take $r>0$ such that $\partial f/\partial x$ exists and is continuous at $(a+h,b+k)$ whenever $\dabs{(h,k)}<r$. Given $\ve>0$, $\exists \delta >0$ with $\delta<r$ such that
\be
\dabs{(h',k')} < \delta \ \ra \ \abs{D_1 f(a+h',b+k')-D_1 f(a,b)} < \ve. \quad (\text{by continuity of }\partial f/\partial x)
\ee
We may write 
\be
f(a,b+k) = f(a,b) + kD_2 f(a,b) + \epsilon_1(k)\abs{k}
\ee
where $\epsilon_1(k)\to 0$ as $k\to 0$. Take $(h,k)$ with $\dabs{(h,k)}<r$, then by the MVT $\exists \theta\in(0,1)$ with
\be
f(a+h,b+k) - f(a,b+k) = hD_1 f(a+\theta h,b+k).
\ee

Thus provided $\dabs{(h,k)}< \delta$ we have
\beast
& & \abs{f(a+h,b+k) -f(a,b) - hD_1 f(a,b) - kD_2 f(a,b) } \\
& = & \abs{\bb{f(a+h,b+k) -f(a,b+k) - hD_1 f(a,b)} + \bb{ f(a,b+k) - f(a,b) -kD_2 f(a,b) }}\\
& = & \abs{\bb{hD_1 f(a+\theta h,b+k) - hD_1 f(a,b)} + \bb{ f(a,b+k) - f(a,b) -kD_2 f(a,b) }}\\
& \leq & \abs{hD_1 f(a+\theta h,b+k) - hD_1 f(a,b)} + \abs{ f(a,b+k) - f(a,b) -kD_2 f(a,b)}\\
& < & \ve \abs{h} + \epsilon_1(k)\abs{k}
\eeast
Since we can make $\ve$ arbitrarily small, we have 
\be
\abs{f(a+h,b+k) -f(a,b) - hD_1 f(a,b) - kD_2 f(a,b) }/\dabs{(h,k)} \to 0
\ee
as $(h,k)\to (0,0)$. Thus $f$ is differetiable at $(a,b)$ with
\be
D_{(a,b)} f(h,k) = hD_1f(a,b) + kD_2f(a,b). 
\ee

\item [(ii)] Take $r>0$ such that $D_1f$ exists and is bounded by $M$ at $(a+h,b+k)$ whenever $\dabs{h,k}<r$. By MVT $\theta \in (0,1)$,
\be
f(a+h,b+k) - f(a,b+k) = h D_1 f(a+\theta h,b+k).
\ee

Given $\ve>0$, $\delta >0$ with $\delta<r$ such that $\abs{k}<\delta$
\be
\abs{f(a,b+k)-f(a,b)} < \frac {\ve}2. \quad (\text{by the continuity as a function of }y)
\ee

Then take $\delta < \frac {\ve}{2M}$, then we have $\abs{(h,k)} < \delta  \ra \ \abs{h},\abs{k}<\delta$, 
\beast
\abs{f(a+h,b+k)-f(a,b)} & \leq & \abs{f(a+h,b+k)-f(a,b+k)} + \abs{f(a,b+k)-f(a,b)} \\
& < & \abs{h D_1 f(a+\theta h,b+k)} + \frac {\ve}2\\
& \leq & \abs{h} M + \frac {\ve}2 < \delta M + \frac {\ve}2 < \frac {\ve}2 + \frac {\ve}2 = \ve.
\eeast
Thus $f$ is continuous at $(a,b)$.
\een

\begin{exercise}
Let $M_n = M_n(\R)$ be the space of $n \times n$ real matrices (it can be identified with $\R^{n^2}$). Show that the function $f : M_n \mapsto M_n$ defined by $f(X) = X^2$ is differentiable everywhere in $M_n$. Is it true that $D_Af = 2A$? If not, what is the derivative of $f$?
\end{exercise}

Solution. Given $A,H\in M_n$ we have 
\be
f(A+H)-f(A) = (A+H)^2 -A^2 = AH + HA + H^2. 
\ee
Define $\alpha (H) = AH + HA$, then $\alpha$ is linear and
\be
\dabs{\frac{f(A+H)-f(A)-\alpha(H)}{\dabs{H}}} = \dabs{\frac{H^2}{\dabs{H}}} = \dabs{H}\to 0\quad \text{as }H\to 0.
\ee
Thus, $f$ is differentiable at $A$ and 
\be
D_Af(H)= AH + HA.
\ee
(It would not even make sense to say that $D_A f = 2A$, since $D_af$ is linear map $M_n\to M_n$.)

\begin{exercise}
Let $A : \R^n \to \R^m$ be a linear map. Recall that the operator norm of $A$ is
\beast
\dabs{A}' = \sup \left\{ \dabs{Ax} : x\in \R^n,\ \dabs{x} \leq 1\right\} & = & \sup \left\{ \frac{\dabs{Ax}}{\dabs{x}}: 0 \neq x \in \R^n\right\} = \inf\{k\in\R: k \text{ is a Lipschitz constant for }A\}.
\eeast

Complete the proof that this defines a norm on the vector space $L(\R^n,\R^m)$ of all linear maps $\R^n \mapsto \R^m$.

Now assume $m = n$ and identify $L(\R^n,\R^n)$ with $M_n(\R)$, the space of $n\times n$ real matrices. Show that if the operator norm of $A \in M_n$ satisfies $\dabs{A}' < 1$, then the sequence $B_k = I +A+A^2+\dots + A^{k-1}$ converges (here $I$ is the identity matrix), and deduce that $I - A$ is then invertible. Deduce that the set $GL_n(\R)$ of all invertible $n\times n$ real matrices is an open subset of $M_n(\R)$.
\end{exercise}

Solution. Given $A$, set
\be
S=\left\{ \dabs{Ax} : x\in \R^n,\ \dabs{x} \leq 1\right\}, \quad\quad R = \left\{ \frac{\dabs{Ax}}{\dabs{x}}: 0 \neq x \in \R^n\right\},
\ee
\be
T = \{k\in\R:k \text{ is a Lipschitz constant for }A\}.
\ee

Given $k\in T$, for all $x\in\R^n$ we have $\dabs{Ax} \leq k\dabs{x}$, so 
\be
\dabs{x}\leq 1 \ \ra \ \dabs{Ax}\leq k \ \ra \ \forall s\in S,\ s\leq k \ \ra \ \sup S\leq k
\ee
and 
\be
x\neq 0 \ \ra \ \dabs{A\frac{x}{\dabs{x}}}\leq k \ \ra \ \forall r\in R,\ r\leq k \ \ra \ \sup R\leq k.
\ee
Since this is true for all $k\in T$ and $\sup S\leq \inf T$ and $\sup R \leq \inf T$. 

On the other hand, for all $x\in \R^n\bs\{0\}$ we have $\dabs{\frac{x}{\dabs{x}}}=1$, so that $\dabs{A\frac{x}{\dabs{x}}}\in S$ and hence 
\be
\dabs{A\frac{x}{\dabs{x}}} \leq \sup S \ \ra \ \dabs{Ax} \leq (\sup S)\dabs{x},
\ee
so $\sup S$ is a Lipschitz constant for A, i.e., $\sup S\in T$, and so $\sup S \geq \inf T$. 

Similarly, $\sup R \in T$ and $\sup R\geq \inf T$. 

Thus $\sup S = \sup R = \inf T$ as required.

If we set
Define $\dabs{\cdot}': L(\R^n,\R^m)\to \R$ by $\dabs{A}' = \sup S$.
\ben
\item [(i)] Clearly $\dabs{\cdot}'$ takes non-negative values,
\item [(ii)] \beast
\dabs{A}' = 0 & \ra & \sup\{\dabs{Ax}:\dabs{x}\leq 1\}= 0 \ \ra \ \forall x\in\R^n\bs\{0\}, \ \dabs{A\bb{\frac{x}{\dabs{x}}}}=0\\
& \ra & \forall x\in\R^n, \ \dabs{Ax}=0 \ \ra \ \forall x\in\R^n,\ Ax = 0 \quad(\text{since $\dabs{\cdot}$ is a norm}) \ \ra \ A = 0.
\eeast
\item [(iii)] For all $\lm \in\R$, $\dabs{\lm A}' = \sup\{\dabs{\lm A x}:\dabs{x}\leq 1\} = \abs{\lm} \sup\{\dabs{A x}:\dabs{x}\leq 1\} = \dabs{A}'$,
\item [(iv)] and
\beast
\dabs{A+B}' & = & \sup\{\dabs{(A+B)x}:\dabs{x}\leq 1\} \leq \sup\{\dabs{Ax} + \dabs{Bx}:\dabs{x}\leq 1\}  \\
& \leq & \sup\{\dabs{Ax}:\dabs{x}\leq 1\} + \sup\{\dabs{Bx}:\dabs{x}\leq 1\}  = \dabs{A}' + \dabs{B}'.
\eeast
\een

Now given $A,B\in M_n$, for all $x\in \R^n$, $\dabs{x}\leq 1$ we have
\beast
\dabs{(AB)x} & = & \dabs{A(Bx)} \leq \dabs{A}'\dabs{Bx} \quad\quad (\text{by the set }R) \\
& \leq & \dabs{A}' \dabs{B}' \quad\quad\quad\quad\quad\quad (\text{by the set }S)
\eeast

Thus, we have
\be
\dabs{AB}' \leq \dabs{A}' \dabs{B}'.
\ee

Hence, for all $k\in \N$, we have $\dabs{A^k}' \leq \dabs{A}'^k$. So if $\dabs{A}'<1$, then the sequence
\be
B_k = I + A + A^2 + \dots + A^{k-1} 
\ee
is a Cauchy squence, because given $\ve>0$ we may choose $N\in\N$ with $\dabs{A}'^N < \ve(1-\dabs{A}')$, and then if $m>n\geq N$ we have
\be
\dabs{B_m - B_n}' = \dabs{A^{m-1} + A^{m-1} + \dots + A^n}' \leq \sum^{m-1}_{k=n} \dabs{A^k}'  \leq \sum^{m-1}_{k=n} \dabs{A}'^k < \sum^{\infty}_{k=N} \dabs{A}'^k = \frac{\dabs{A}'^N}{1-\dabs{A}'}< \ve.
\ee

Since $M_n$ is complete it follows that $B_k$ converges and let the limit be $B$. Since 
\be
\dabs{\lim_{k\to \infty} A^k}' = \lim_{k\to\infty} \dabs{A^k}' \leq \lim_{k\to\infty}\dabs{A}'^k = 0,
\ee
we have $\lim_{k\to\infty} A^k = 0$, whence 
\be
(I-A)B = (I-A)\lim_{k\to\infty}B_k = \lim_{k\to\infty} (I-A)B_k = \lim_{k\to\infty} I - A^k = I - \lim_{k\to\infty} A^k = I.
\ee
So $I-A$ is invertible. Thus there is an open ball about $I$ in $GL_n(\R)$. Given $X\in GL_n(\R)$, left multiplication by $X^{-1}$ is a linear and hence continuous map from $M_n$ to itself. 

Thus the preimage of the open ball $\{I-A : \dabs{A}'<1\}$ is an open set, i.e.,, $\{X(I-A):\dabs{A}'<1\}$ is an open set and it contains $X$, and lies in $GL_n(\R)$. Thus each element of $GL_n(\R)$ lies in an open ball in $GL_n(\R)$, so $GL_n(\R)$ is an open subset of $M_n$.

\begin{exercise}
We regard $GL_n(\R)$ as an open subset of $M_n(\R) \simeq \R^{n^2}$ (cf. the previous question). Define $g : GL_n(\R) \to M_n(\R)$ by $g(X) = X^{-1}$ for $X \in GL_n(\R)$. Show that $g$ is differentiable at the identity matrix $I \in GL_n(\R)$, and that its derivative there is the map $D_I g(H) = -H$.

Let $A \in GL_n(\R)$. By writing $(A + H)^{-1} = A^{-1}\bb{I + HA^{-1}}^{-1}$, or otherwise, show that $g$ is differentiable at $X = A$. What is $D_Ag$?

Show further that $g$ is twice differentiable at $I$, and find $D^2_I g$ as a bilinear map $M_n \times M_n \to M_n$.
\end{exercise}

Solution. From the previous question, we know that $\dabs{H}' < 1 \ \ra \ I+H \text{ is invertible}$, then
\be
(I+H)^{-1}-I^{-1} + H = (I+H)^{-1}\bb{I-I-H+H+H^2}= \bb{I+H}^{-1} H^2 = \epsilon(H)\dabs{H}'
\ee
where $\epsilon(H)\to 0$ as $H\to 0$, so
\be
g(I+H)=g(I)-H + \epsilon(H)\dabs{H}'
\ee
and hence $g$ is differentiable at $I$ with derivative given by $D_I g(H) = -H$.

Now given $A\in GL_n(\R)$, take $\delta >0$ such that $\dabs{H}<\delta \ \ra \ A+H\in GL_n(\R)$ (since $GL_n(\R)$ is an open set), then
\beast
(A+H)^{-1} & = & A^{-1}(I+HA^{-1})^{-1} = A^{-1}\bb{I+HA^{-1}}^{-1}\bb{I-(HA^{-1})^2 + (HA^{-1})^2}\\
& = & A^{-1}(I-HA^{-1} + (I+HA^{-1})^{-1}(HA^{-1})^2) \\
& = & A^{-1} - A^{-1}HA^{-1} + A^{-1}\bb{I + HA^{-1}}^{-1} (HA^{-1})^2\\
& = & A^{-1} - A^{-1}HA^{-1} + \bb{A + H}^{-1} (HA^{-1})^2
\eeast

Since 
\be
\frac 1{\dabs{H}'} \bb{A + H}^{-1} (HA^{-1})^2 \to 0 \quad \text{as }H\to 0,
\ee

$g$ is differentiable at A with derivative 
\be
D_Ag(H) = -A^{-1}HA^{-1}.
\ee

For all $H,K \in M_n$ with $\dabs{H}'<1$ (then $g$ is differentiable at $I+H$), we have
\beast
D_{I+H}g(K) - D_I g(K) & = & -(I+H)^{-1}K(I+H)^{-1} + K = (I+H)^{-1}\bb{-K + (I+H)K(I+H)}(I+H)^{-1} \\
& = &  (I+H)^{-1}\bb{ HK + KH + HKH}(I+H)^{-1}
\eeast

Set $\psi(H,K) = HK + KH$, then
\beast
& & \frac1{\dabs{H}'}\bb{D_{I+H}g(K) - D_I g(K)-\psi(H,K)}\\
& = & \frac1{\dabs{H}'}\bb{(I+H)^{-1}\bb{ HK + KH + HKH}(I+H)^{-1} - (HK+KH)}\\
& = & \frac1{\dabs{H}'}(I+H)^{-1}\bb{ HK + KH + HKH - (I+H)(HK+KH)(I+H)}(I+H)^{-1} \\
& = & \frac1{\dabs{H}'}(I+H)^{-1}\bb{ HK + KH + HKH - (HK + H^2K + KH + HKH )(I+H)}(I+H)^{-1} \\
& = & -\frac1{\dabs{H}'}(I+H)^{-1}\bb{ H^2K + HKH + H^2KH + KH^2 + HKH^2}(I+H)^{-1} \to 0 \quad \text{as }H\to 0.
\eeast

Thus $g$ is twice differentiable at $I$ and $D^2_Ig(H,K) = HK + KH$.

\begin{exercise}
\ben
\item [(i)] Define $f : M_n \to M_n$ by $f(X) = X^3$. Find the Taylor series of $f(A + H)$ about $A$.
\item [(ii)] Let again $g : GL_n(\R) \to GL_n(\R)$ be defined by $g(X) = X^{-1}$. Find the Taylor series of $g(I + H)$ about $I$.
\een
\end{exercise}


Solution. \ben
\item [(i)] Given $A,H\in M_n$ we have
\be
f(A+H)-f(A) = (A^2H + AHA + HA^2) + (AH^2 + HAH + H^2A + H^3).
\ee
Define $\alpha:M_n\to M_n$ by
\be
\alpha (H) = A^2H + AHA + HA^2,
\ee
then $\alpha$ is linear and 
\be
\frac 1{\dabs{H}'}\bb{f(A+H)-f(A)-\alpha (H)} \to 0 \quad \text{as }H\to 0.
\ee
Thus, $f$ is differentiable at $A$ and $D_Af = \alpha$.

Now given $A,H,K\in M_n$ we have
\beast
D_{A+H}f(K) - D_Af(K) & = & (A+H)^2K + (A+H)K(A+H) + K(A+H)^2 - \bb{A^2K + AKA + KA^2}\\
 & = & AHK + HAK + H^2K + HKA + AKH + HKH + KAH + KHA + KH^2 \\
 & = & \bb{AHK + HAK + HKA + AKH + KAH + KHA} + \bb{H^2K + KH^2 + HKH}
\eeast

Define $\psi(H,K) = AHK + HAK + HKA + AKH + KAH + KHA$, then $\psi$ is bilinear and 
\be
\frac 1{\dabs{H}'}\bb{D_{A+H}f(K) - D_Af(K)-\psi(H,K)} = \frac 1{\dabs{H}'}\bb{H^2K + KH^2 + HKH} \to 0 \quad \text{as }H\to 0.
\ee
Thus, $f$ is differentiable at $A$ and $D^2_Af = \psi$.

Finally, given $A,H,K,L\in M_n$ we have
\beast
& & D^2_{A+H}f(K,L) - D_Af(K,L) \\
& = & (A+H)KL + K(A+H)L + KL(A+H) + (A+H)LK + L(A+H)K + LK(A+H)\\ 
& & \quad -\bb{AKL + KAL + KLA + ALK + LAK + LKA }\\
 & = & HKL + KHL + KLH + HLK + LHK + LKH .
\eeast

Define $\chi(H,K,L) = HKL + KHL + KLH + HLK + LHK + LKH$, then $\chi$ is trilinear and 
\be
\frac 1{\dabs{H}'}\bb{D^2_{A+H}f(K,L) - D^2_Af(K,L)-\chi(H,K,L)} = 0.
\ee
Thus, $f$ is thrice differentiable at $A$ and $D^3_Af = \chi$. Hence the Taylor series of $(A+H)^3$ about $A$ is
\beast
f(A+H) & = & f(A) + D_Af(H) + \frac 12 D^2_Af(H,H) + \frac 16 D^3_A f(H,H,H)\\
& = & f(A) + A^2H + AHA + HA^2 + AH^2 + HAH + H^2A + H^3.
\eeast

\item [(ii)] For each $k\in\N$ we define a $k$-linear map $\psi_k:M_n\times M_n\times \dots\times M_n \to M_n$ by 
\beast
\psi_k(H_1,H_2,\dots,H_k) & = & (-1)^k \sum_{\sigma\in S_k}A^{-1}H_{\sigma(1)}A^{-1}H_{\sigma(2)}\dots A^{-1}H_{\sigma(k)}A^{-1} \\
& = & (-1)^k \sum_{\sigma\in S_k}A^{-1} \prod^k_{i=1} \bb{H_{\sigma(i)}A^{-1}}
\eeast
where $\sigma(k)$ is a permutation of $S_k$. We claim that $D^k_Ag = \psi_k$ for all $k\in\N$. By previous question, we know that this is true for $k=1$. For convenience we write $B=A^{-1}$. Note that
\beast
(A+H)^{-1} & = & A^{-1}(I+HA^{-1})^{-1} = A^{-1}\bb{I+HA^{-1}}^{-1}\bb{I-(HA^{-1})^2 + (HA^{-1})^2}\\
& = & A^{-1}(I-HA^{-1} + (I+HA^{-1})^{-1}(HA^{-1})^2) \\
& = & A^{-1} - A^{-1}HA^{-1} + A^{-1}\bb{I + HA^{-1}}^{-1} (HA^{-1})^2\\
& = & A^{-1} - A^{-1}HA^{-1} + \bb{A + H}^{-1} (HA^{-1})^2 \\
& = & B - BHB + (A+H)^{-1}(HB)^2.
\eeast

Thus, ignoring terms involving at least two factors $H_{k+1}$ we have
\beast
& & D^k_{A+H_{k+1}}g(H_1,\dots,H_k) - D^k_{A}g(H_1,\dots,H_k) - \psi_{k+1}(H_1,\dots,H_k) \\
& = & (-1)^k\bb{\sum_{\sigma\in S_k}\bb{(A+H_{k+1})^{-1} \prod^k_{i=1} \bb{H_{\sigma(i)}(A+H_{k+1})^{-1}} - B \prod^k_{i=1} \bb{H_{\sigma(i)}B}} + \sum_{\tau\in S_{k+1}}B \prod^{k+1}_{i=1} \bb{H_{\tau(i)}B}}\\
& =& (-1)^k\bb{\sum_{\sigma\in S_k}\bb{(B - BH_{k+1}B)\prod^k_{i=1} \bb{H_{\sigma(i)}(B - BH_{k+1}B)} - B \prod^k_{i=1} \bb{H_{\sigma(i)}B}} + \sum_{\tau\in S_{k+1}}B \prod^{k+1}_{i=1} \bb{H_{\tau(i)}B}}.
\eeast
The final step is made since the remainders are at least two factors $H_{k+1}$. If we multiply out the products in the first sum, each first term is of the form $B \prod^k_{i=1} \bb{H_{\sigma(i)}B}$ and thus is cancelled, each term involving one factor $BH_{k+1}B$ and $k$ factors $B$ cancels with one in the final sum, and all remaining terms involve at least two factors $H_{k+1}$, so
\be
\frac 1{\dabs{H_{k+1}}'} \bb{D^k_{A+H_{k+1}}g(H_1,\dots,H_k) - D^k_{A}g(H_1,\dots,H_k) - \psi_{k+1}(H_1,\dots,H_k)} \to 0 \quad \text{as }H_{k+1} \to 0.
\ee
Thus, $D^{k+1}_A g = \psi_{k+1}$ as required. Thus the Taylor series of $g(I+H)$ about $I$ is
\beast
\sum^\infty_{k=0}\frac 1{k!}D^k_I g(\underbrace{H,\dots,H}_{k \text{ terms}}) & = & \sum^\infty_{k=0}\frac 1{k!}(-1)^k \sum_{\sigma\in S_k}I^{-1} \prod^k_{i=1} \bb{HI^{-1}} = \sum^\infty_{k=0}\frac 1{k!}(-1)^k k! H^k\\
& = & I - H+ H^2 -H^3 + \dots.
\eeast

\een

\begin{exercise}
Show that $\det : M_n \to \R$ is differentiable at the identity matrix $I$ with $(D_I \det)(H) = \tr(H)$. 

Deduce that $\det$ is differentiable at any invertible matrix $A$ with $(D_A \det)(H) = \det(A) \tr(A^{-1}H)$. 

Show further that $\det$ is twice differentiable at $I$ and find $D^2_I \det$ as a bilinear map.
\end{exercise}

Solution. Given $H\in M_n$, each term of $\det(I+H)$ other than 1 is either $h_{ii}$ for some $i$ or a product of two or more entries $h_{ij}$, 
\beast
& & \det\bepm
1+h_{11} & h_{12} & \dots & h_{1n}\\
h_{21} & 1+h_{22} & \dots & h_{2n}\\
\vdots & \vdots & \ddots &\\
h_{n1} & h_{n2} & \dots & 1+ h_{nn} 
\eepm \\
& = & (1+h_{11}) \det\bepm
1+h_{22} & h_{23} & \dots & h_{2n}\\
h_{32} & 1+h_{33} & \dots & h_{3n}\\
\vdots & \vdots & \ddots &\\
h_{n2} & h_{n3} & \dots & 1+ h_{nn} 
\eepm + \underbrace{\sum^n_{i=2} (-1)^{1+i}h_{1i}\det\bepm
h_{21} & \dots\\
\vdots & \dots\\
h_{n1} & \dots
\eepm}_{\text{two or more entries of $h_{ij}$, $(i\neq j)$}}\\
& = & (1+h_{11})(1+h_{22}) \det\bepm
1+h_{33} & h_{34} & \dots & h_{3n}\\
h_{43} & 1+h_{44} & \dots & h_{4n}\\
\vdots & \vdots & \ddots &\\
h_{n3} & h_{n4} & \dots & 1+ h_{nn} 
\eepm + \underbrace{\text{two or more entries of $h_{ij}$}}_{i\neq j}\\
& = & (1+h_{11})(1+h_{22}) \dots (1+h_{nn}) + \underbrace{\text{two or more entries of $h_{ij}$}}_{i\neq j}\\
& = & 1+\underbrace{\bb{h_{11}+h_{22} \dots h_{nn}}}_{\tr H} + \underbrace{\text{two or more entries of $h_{ij}$}}_{i\neq j}
\eeast
thus,
\be
\frac 1{\dabs{H}'}\bb{\det(I+H)-\det I-\tr H} \to 0 \quad\text{as }H\to 0,
\ee
so as $H\to \tr H$ is a linear map we see that $\det$ is differentiable at $I$ with derivative $D_I\det(H) = \tr H$.

Thus, if $A\in GL_n(\R)$ we have
\be
\frac 1{\dabs{H}'} \bb{\det (A+H) -\det A - \det (A) \tr(A^{-1}H)} = \frac 1{\dabs{H}'} \det A \bb{\det (I+A^{-1}H) - \det I - \tr(A^{-1}H)} \to 0 
\ee
as $H\to 0$. So $\det$ is differentiable at $A$ with derivative $D_A \det (H) = \det(A) \tr(A^{-1}H) $.

Now given $H,K\in M_n$ such that $\dabs{H}'<1 \ \ra \ I+H\in GL_n(\R)$ (since $GL_n(\R)$ is open), we have
\beast
D_{I+H}\det(K) - D_I \det(K) & = & \det(I+H)\tr((I+H)^{-1}K) - \tr K \\
& = & \tr(\det(I+H)(I+H)^{-1}K) - \tr K \\
& = & \tr((I+H)^*K) - \tr K \\
& = & \tr\bb{\bb{(I+H)^* - I}K}
\eeast
where $X^*$ is the adjugate matrix of $X$. Write $H=(h_{ij})$, $K = (k_{ij})$ and work modulo terms involving 2 or more $h_{ab}$. Then if $i\neq j$, without loss of generality, we assume that $i<j$, the element corresponding to $C_{ij} = (-1)^{i+j}M$ (where $M$ is determinant of the $(n-1)\times(n-1)$ matrix that results from deleting row $i$ and column $j$ of $H$) is 
\be
(-1)^{i+j} \det\bepm
\underline{1+h_{11}} & h_{12} & \dots & \dots & \dots & \dots & h_{1,j-1} & h_{1,j+1} & \dots & h_{1n}\\
h_{21} & \underline{1+h_{22}} & \dots & \dots & \dots & \dots & h_{2,j-1} & h_{2,j+1} & \dots & h_{2n}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots &\\
h_{i-1,1} & h_{i-1,2} & \dots & \underline{1+ h_{i-1,i-1}} & \dots & \dots & h_{i-1,j-1} & h_{i-1,j+1} & \dots & h_{i-1,n}\\
h_{i+1,1} & h_{i+1,2} & \dots & h_{i+1,i-1}  & \underline{h_{i+1,i}} & 1 + h_{i+1,i+1} &  h_{i+1,j-1} &  h_{i+1,j+1} & \dots & h_{i-1,n}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots &\\
h_{j-1,1} & h_{j-1,2} & \dots & h_{j-1,i-1} &  \dots & \dots & 1+h_{j-1,j-1} & h_{j-1,j+1} & \dots & h_{j-1,n}\\
h_{j,1} & h_{j,2} & \dots & h_{j,i-1} &  h_{ji}  & \dots & \underline{h_{j,j-1}} & h_{j,j+1} & \dots & h_{j,n}\\
h_{j+1,1} & h_{j+1,2} & \dots & h_{j+1,i-1}  & h_{i+1,i} &  \dots & h_{j+1,j-1} &  \underline{1 + h_{j+1,j+1}} & \dots & h_{j-1,n}\\
\vdots & \vdots & \ddots &\ddots &\ddots &\ddots &\ddots &\ddots &\ddots &\\
h_{n1} & h_{n2} & \dots & \dots & \dots &  \dots & \dots & \dots &\dots & \underline{1+ h_{nn}}
\eepm 
\ee
where the underlined terms are the diagonal elements of matrix. Thus, its value is 
\be
(-1)^{i+j}(1+h_{11})\dots (1+ h_{i-1,i-1}) (1 + h_{j+1,j+1})\dots (1+ h_{nn})
\det\bepm
\underline{h_{i+1,i}} & 1 + h_{i+1,i+1} &  \dots & h_{i-1,n}\\
\vdots & \vdots & \vdots & \vdots \\
h_{j-1,i} & \dots & \dots & 1+h_{j-1,j-1} \\
h_{ji}  & \dots & \dots & \underline{h_{j,j-1}}
\eepm 
\ee
which is
\be
(-1)^{i+j} (-1)^{(j-i)+1} h_{ji} = -h_{ji}.
\ee

If $i = j$ we have
\be
C_{ii} = 1 + (h_{11}+h_{nn}) = 1 + \tr H - h_ii
\ee

so $(I+H)^* = C^T$ and
\be
\bb{(I+H)^* - I}_{ij} = \left\{\ba{ll}
\tr H - h_{ii} \quad\quad & i = j\\
-h_{ij} & i\neq j
\ea\right.
\ee

Then we have
\beast
\tr\bb{\bb{(I+H)^* - I}K} & = & \sum_{i,j} \bb{(I+H)^*-I)_{ij}k_{ji}} =  \sum_i \bb{\tr H -h_{ii}}k_{ii} - \sum_{j\neq i} h_{ij}k_{ji}\\
& = & \tr H \sum_i k_{ii} - \sum_i h_{ii}k_{ii} - \sum_{j\neq i} h_{ij}k_{ji}\\
& = & \tr H \tr K - \sum_{ij} h_{ij}k_{ji} = \tr H \tr K - \tr HK.
\eeast

Therefore, define $\psi: M_n\times M_n\to M_n$ by 
\be
\psi(H,K) =  \tr H \tr K - \tr HK,
\ee
then $\psi$ is bilinear and 
\be
\frac1{\dabs{H}'}\bb{D_{I+H}\det(K) - D_I \det(K) - \psi(H,K)} \to 0 \quad\text{as }H\to 0.
\ee

Hence, $\det$ is twice differentiable at $I$ and $D^2_I\det(H,K) = \tr H \tr K - \tr HK$.

\begin{exercise}
Show that there is a continuous square-root function on some neighbourhood of $I$ in $M_n$; that is, show that there is an open ball $B(I; r) \subset M_n$ for some $r > 0$ and a continuous function $g : B(I; r) \to M_n$ such that $g(X)^2 = X$ for all $X \in B(I; r)$.

Is it possible to define a square-root function on all of $M_n$? What about a cube-root function?
\end{exercise}

Solution. Define $f:M_n \to M_n$ by $f(A) = A^2$. By previous question, we have 
\be
D_Af(H) = AH + HA.
\ee

Thus, the map sending $A$ to $D_Af$ is linear and hence continuous. Since $D_If(H) = 2H$, the derivative $D_If$ is an isomorphism $M_n\to M_n$. Thus, by the inverse function theorem $f$ is a local $C^1$-diffeomorphism at $I$, i.e., there exists an open set $U$ in $M_n$ containing $I$ such that $f:U\to f(U)$ is bijective, $f(U)$ is open an both $f:U\to f(U)$ and $f^{-1}:f(U)\to U$ are continuously differentiable.

Since $f(U)$ is open and $f(I) = I$, $I\in U$, $\exists r>0$ such that $B(I;r) \subseteq f(U)$. Let $g: B(I,r)\to M_n$ be the restriction of $f^{-1}$, then $g$ is certainly continuous and for all $A\in B(I,r)$ we have
\be
f(g(A)) = f(f^{-1}A) = A \ \ra \ g(A)^2 = A.
\ee

It is not possible to define either a square-root or a cube root function on the whole of $M_n$, since not all matrices have square or cube root. To see this, let
\be
A = \bepm
0 & 1 & 0 & \dots & 0 & 0\\
0 & 0 & 1 & \dots & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 1\\
0 & 0 & 0 & \dots & 0 & 0
\eepm \ \ra \ A^{n-1} = \bepm
0 & 0 & 0 & \dots & 0 & 1\\
0 & 0 & 0 & \dots & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 0\\
0 & 0 & 0 & \dots & 0 & 0
\eepm \neq 0, \quad A^n = 0.
\ee

Thus if $B^r = A$ for $r\in\{2,3\}$, then $B^{rn} = A^n=0$, so that if we work over $\C$ then all eigenvalues of $B$ are 0, whence its Jordan normal form consists of blocks with diagonal entries 0. But raising such a matrix to the $n$th power then gives 0, so as $r(n-1)\geq n$ we have $0=B^{r(n-1)} = A^{n-1}$, a contradiction.

\begin{exercise}
Define $f : \R^2 \to \R^2$ by $f(x, y) = (x, x^3+y^3-3xy)$ and let $C = \{(x, y) \in \R^2 : x^3+y^3-3xy = 0\}$. Show that $f$ is locally invertible around each point of $C$ except $(0, 0)$ and $(2^{\frac 23}, 2^{\frac 13})$; that is, show that if $(x_0,y_0) \in C \left\backslash \left\{(0, 0), (2^{\frac 23}, 2^{\frac 13})\right\}\right.$ then there are open sets $U$ containing $(x_0, y_0)$ and $V$ containing $f(x_0, y_0)$ such that $f$ maps $U$ bijectively to $V$. What is the derivative of the local inverse function? Deduce that for each point $(x_0,y_0) \in C$ other than $(0, 0)$ and $(2^{\frac 23}, 2^{\frac 13})$ there exist open intervals $I$ containing $x_0$ and $J$ containing $y_0$ such that for each $x \in I$ there is a unique $y \in J$ with $(x, y) \in C$.
\end{exercise}

Solution. By computing partial derivatives we have
\be
D_{(x,y)}f(h,k) = (h,3hx^2-3hy + 3ky^2 - 3kx) \ \ra \ D_{(x,y)}f = \bepm
1 & 0 \\
3x^2 - 3y & 3y^2 - 3x
\eepm.
\ee

Therefore 
\be
D_{(x,y)}f \text{ is invertible } \ \lra \ 3y^2 -3x \neq 0 \ \lra \ x \neq y^2.
\ee

Thus if $(x,y)\in C$ and $D_{(x,y)}f$ is not invertible, then 
\be
x^3 + y^3 = 3xy, \quad x = y^2 \ \ra \ y^6 + y^3 = 3y^3 \ \ra \ y^3(y^3-2) = 0 \ \ra \ y =0,\ 2^{\frac 13} \ \ra \ (x,y) = \bb{0,0},\bb{2^{\frac 23},2^{\frac 13}}.
\ee

Therefore by the inverse function theorem if $(x_0,y_0)\in C \left\backslash \left\{(0, 0), (2^{\frac 23}, 2^{\frac 13})\right\}\right.$ then exist open sets $U$ containing $(x_0,y_0)$ and $V$ containing $f(x_0,y_0)$ such that $f:U\to V$ is a bijection with continuously differentiable inverse.

By the chain rule, 
\be
Df^{-1}(f(x,y))= (D_{(x,y)}f)^{-1} = \frac 1{3y^2 - 3x} \bepm
3y^2 - 3x & 0\\
3y - 3x^2 & 1
\eepm.
\ee

Suppose $(x_0,y_0)\in C \left\backslash \left\{(0, 0), (2^{\frac 23}, 2^{\frac 13})\right\}\right.$ and take open sets $U$ and $V$ as above. Then $\exists r>0$ such that $B((x,y);r)\subseteq U$. Let
\be
I' = \bb{x_0-\frac r2,x_0+\frac r2},\quad\quad J = \bb{y_0-\frac r2,y_0+\frac r2}
\ee  
then
\be
(x_0,y_0) \in I' \times J \subseteq U.
\ee

The set $f(I'\times J)\subseteq V$ is open and contains $f(x_0,y_0) = (x_0,0)$, so $\exists r'>0$ such that 
\be
B((x_0,0),r') \subseteq f(I'\times J).
\ee
Let
\be
I = \bb{x_0-\frac {r'}2,x_0+\frac {r'}2}, \quad\quad K = \bb{-\frac {r'}2,\frac {r'}2}
\ee
then
\be
(x_0,0) \in I\times K \subseteq f(I'\times J).
\ee

We know that $f: I\times J \to f(I\times J)$ is a bijection and $I\times K \subseteq f(I\times J)$ (since $f$ does not change first co-ordinates). Thus, $\forall x\in I$ we have
\be
(x,0)\in I\times K \subseteq f(I\times J)
\ee
so there is a unique $y\in J$ such that $f(x,y) = (x,0)$, i.e., we have $(x,y)\in C$.

\begin{exercise}
Let $f : \R^2 \to \R$ be a differentiable function and let $g(x) = f(x, c- x)$ where $c$ is a constant. Show that $g : \R \to \R$ is differentiable and find its derivative
\ben
\item [(i)] directly from the definition of differentiability

and also
\item [(ii)] by using the chain rule.
\een

Deduce that if $\partial f/\partial x = \partial f/\partial y$ holds throughout $\R^2$, then $f(x, y) = h(x + y)$ for some differentiable function $h$.
\end{exercise}

Solution. \ben
\item [(i)] Write 
\be
f(x+h,y+k) = f(x,y) + D_{1,(x,y)}f h + D_{2,(x,y)}f k + \epsilon(h,k)\dabs{(h,k)},
\ee
where $\epsilon(h,k)\to 0$ as $(h,k)\to (0,0)$. Then
\beast
g(x+h)-g(x) & = & f(x+h,c-x-h) - f(x,c-x) = D_{1,(x,c-x)}f h + D_{2,(x,c-x)}f(-h) + \epsilon(h,-h)\abs{h}\\
& = & \bb{D_{1,(x,c-x)}f - D_{2,(x,c-x)}f}h + \epsilon(h,-h)\abs{h},
\eeast
so as $\epsilon(h,-h) \to 0$ as $h\to 0$ we see that $g$ is differentiable and its derivative at $x$ is $D_{1,(x,c-x)}f - D_{2,(x,c-x)}f$.

\item [(ii)] Define $j:R\to \R^2$ by $j(x) = (x,c-x)$, so that $g= f\circ i$. Then $j$  is linear so it is differentiable with derivative given by 
\be
D_xj(h) = (h,-h).
\ee

Thus, $g$ is differentiable, and we have
\beast
D_xg(h) & = & D_{(x,c-x)}f D_xj(h) = D_{(x,c-x)}f(h,-h) \\
& = & D_{1,(x,c-x)}fh + D_{2,(x,c-x)}f(-h) \\
& = & \bb{D_{1,(x,c-x)}f - D_{2,(x,c-x)}f}h.
\eeast
Thus $D_xg = D_{1,(x,c-x)}f - D_{2,(x,c-x)}f$.
\een

Thus, if $D_1f = D_2f$ holds throughtout $\R^2$, then $g$ has derivative 0 and hence is constant (for each value of $c$). Define $h:\R\to\R$ by $h(t) = f(0,t)$, then $h$ is differentiable. Given $(x,y)\in\R^2$, set $c=x+y$ and let $g$ be as given then
\be
f(x,y) = f(x,c-x) = g(x) = g(0) = f(0,c) = f(0,x+y) = h(x+y).
\ee

\begin{exercise}
Let $U \subset \R^2$ be an open set that contains a rectangle $[a, b] \times [c, d]$. Suppose that $g : U \to \R$ is continuous and that the partial derivative $\partial g/\partial y$ exists and is continuous. Set $G(y) = \int^b_a g(x, y)dx$. Show that $G$ is differentiable on $(c, d)$ with derivative
\be
G'(y) = \int^b_a (\partial g/\partial y)(x, y)dx.
\ee

Show further that 
\be
H(y) = \int^y_a g(x, y)dx
\ee
is differentiable. What is its derivative $H'(y)$?

[Hint: consider a function $F(y, z) = \int^z_a g(x, y)dx$ before dealing with $H$.]
\end{exercise}

Solution. Given $x\in[a,b]$ and $y\in(c,d)$, choose $\delta >0$ such that $(y-\delta ,y+\delta) \subseteq (c,d)$. Then if $\abs{k}<\delta$ we have
\be
g(x,y+k) -g(x,y) = D_{2,(x,y)}g k + \epsilon(k)\abs{k}
\ee
where $\epsilon(k)\to 0$ as $k\to 0$. So 
\be
\frac{G(y+k)-G(y)}{k} = \frac 1{k}\bb{\int^b_a g(x,y+k)dx - \int^b_a g(x,y)dx} = \int^b_a \bb{D_2g(x,y) + \frac{\epsilon(k)\abs{k}}{k}}dx.
\ee

Thus 
\be
\lim_{k\to 0} \frac{G(y+k)-G(y)}{k} = \int^b_a D_2g(x,y) dx = \int^b_a \fp{}{y} g(x, y)dx,
\ee
i.e., $G$ is differentiable at $y$ with derivative $G'(y) = \int^b_a \fp{}{y} g(x, y)dx$.

Now assume that $[c,d]\subseteq [a,b]$ and define $F:[c,d]\times [c,d]\to \R$ by $F(y,z) = \int^z_a g(x,y)dx$. Then by above
\be
D_1F(y,z) = \int^z D_2g(x,y)dx \ \ra \ D_2F(y,z) = g(z,y) \quad\quad (\text{fundamental theorem of calculus})
\ee

Since both partial derivatives exist and are continuous on $[c,d]\times [c,d]$, it follows that $F$ is differentiable on $(c,d)\times (c,d)$.

Now define $j:\R\to \R^2$ by $j(y) = (y,y)$, so that $H=F\circ j$. Then $j$ is linear, so it is differentiable with derivative given by $D_yj(h) = (h,h)$. Thus $H$ is differentiable on $(c,d)$ and we have
\be
D_yH(h) = D_{(y,y)}F (D_y j(h)) = D_{(y,y)}F(h,h) = D_1 F(y,y)h + D_2 F(y,y)h = \bb{\int^y_a D_2g(x,y)dx + g(y,y)}h.
\ee
Thus, $H'(y) = \int^y_a \fp{}{y}g(x,y)dx + g(y,y)$.

\begin{exercise}
\ben
\item [(i)] For each of the following metric spaces $Y$
\be
\text{(a)}\ Y = \R,\quad\quad \text{(b)}\ Y = [0, 2],\quad\quad \text{(c)}\ Y = (1, 3),\quad\quad \text{(d)}\ Y = (1, 2] \cup (3, 4],
\ee
with metric $d(x, y) = \abs{x - y}$, is the set $(1, 2]$ an open subset of $Y$? Is it closed?

\item [(ii)] Suppose that $X$ is a metric space and $A_1,A_2$ are two closed balls in $X$ with radius respectively $r_1, r_2$, such that $r_1 > r_2 > 0$. Can $A_1$ be a proper subset of $A_2$ (i.e. $A_1 \subset A_2$ and $A_1 \neq A_2$)?
\een
\end{exercise}

Solution. \ben
\item [(i)] \ben 
\item [(a)] Not open. It contains 2 but no open ball about 2.

Not close. The complement contains 1 but no open ball about 1.

\item [(b)] Open. Given $x\in(1,2]$, $B(x;x-1)$ lies in the set.

Not closed. As in (a).

\item [(c)] Not open. As in (a).

Closed. The complement is (2,3), and given $x\in (2,3)$, $B(x;x-2)$ lies in it.

\item [(d)] Open. Given $x\in (1,2]$, $B(x;x-1)$ lies in the set. 

Closed. The complement is $(3,4]$, and given $x\in(3,4]$, $B(x;x-3)$ lies in it.
\een
\item [(ii)] Take $X=[0,\infty)$ with the usual metric. Let $A_1$ be the closed ball of radius 1 centred at 0, and $A_2$ be the closed ball of radius $\tfrac 34$ centred at $\frac 12$, $A_1=[0,1]$ is proper subset of $A_2 = [0,\frac 54]$.
\een

\begin{exercise}\label{ques:open_ball} 
For each of the following sets $X$, determine whether or not the given function d defines a metric on $X$. In each case where the function does define a metric, describe the open ball $B(x;r)$ for each $x \in X$ and $r > 0$ small.
\ben
\item [(i)] $X = \R^n$; $d(x, y) = \min\{\abs{x_1 -y_1}, \abs{x_2 - y_2},\dots,\abs{x_n - y_n}\}$.
\item [(ii)] $X = \Z$; $d(x, x) = 0$ and for $x \neq y$, $d(x, y) = 2^n$, where $x - y = 2^n a$ with $n$ a non-negative integer and $a$ an odd integer.
\item [(iii)] $X = \Q$; $d(x, x) = 0$ and for $x \neq y$, $d(x, y) = e^{-n}$, where $x - y = 3^{-n}a/b$ for $n, a, b \in \Z$ with both $a$ and $b$ not divisible by 3.
\item [(iv)] $X$ is the set of functions from $\N$ to $\N$; $d(f, f) = 0$ and for $f \neq g$, $d(f, g) = 2^{-n}$ for the least $n$ such that $f(n) \neq g(n)$.
\item [(v)] $X = \C$; $d(z, z) = 0$ and for $z \neq w$, $d(z,w) = \abs{z} + \abs{w}$.
\item [(vi)] $X = \C$; $d(z,w) = \abs{z - w}$ if $z$ and $w$ lie on the same straight line through the origin, $d(z,w) = \abs{z} + \abs{w}$ otherwise.
\een
\end{exercise}

Solution. \ben
\item [(i)] No. If $x=\bb{1,0,\dots,0}$, $y=\bb{0,0,\dots,0}$ then $x\neq y$ but $d(x,y) = 0$.
\item [(ii)] No. Take $n=0,a=1$, $d(0,1) + d(1,4) = 1 + 1 = 2 < 4 = d(0,4)$.
\item [(iii)] No. Take $x-y = 3^{-2}$ and $x-z = 3^{-1}$, then $y-z = 2\cdot 3^{-2}$. Thus
\be
d(x,z) = e^{-1} > e^{-1} \frac 2{e} = 2e^{-2} = d(x,y) + d(y,z).
\ee
\item [(iv)] Yes. We check
\ben
\item [(a)] $d(f,g)\geq 0$ for all $f,g\in X$. 
\item [(b)] If $d(f,g)=0\ \lra f=g$.
\item [(c)] The symmetry is obvious.
\item [(d)] Given $f,g,h$ the triangle inequality is clear if two of them are equal, while if $d(f,g) = 2^{-n}$ and $d(g,h) = 2^{-m}$ then $f$ and $g$ agree at $1,2,\dots \min(n,m)-1$ so 
\be
d(f,h) \leq 2^{-\min(n,m)} < 2^{-n} + 2^{-m} = d(f,g) + d(g,h).
\ee
\een
We have 
\be
B(f;r) = \{g:g(n)=f(n), \forall n \leq \log_2 \tfrac 1r\}.
\ee
\item [(v)] Yes. We check \ben
\item [(a)] $d(z,w)\geq 0$ for all $z,w\in X$. 
\item [(b)] If $d(z,w)=0\ \lra z=w$.
\item [(c)] The symmetry is obvious.
\item [(d)] Given $z,w,v$ the triangle inequality is clear if two of them are equal, while if $d(z,w) = \abs{z}+\abs{w}$ and $d(w,v) = \abs{w} + \abs{v}$ then \be
d(z,v) = \abs{z} + \abs{v} \leq \abs{z}+\abs{w}+\abs{w} + \abs{v} = d(z,w) + d(w,v).
\ee
\een
We have 
\be
B(0;r) = \{w:\abs{w}<r\}
\ee
while if $z\neq 0$ and $r<\abs{z}$ then $B(z;r) = \{z\}$.
\item [(vi)] Yes. We check
\ben
\item [(a)] $d(z,w)\geq 0$ for all $z,w\in X$. 
\item [(b)] If $d(z,w)=0\ \lra z=w$.
\item [(c)] The symmetry is obvious.
\item [(d)] Given $z,w,v$ the triangle inequality is clear if two of them are equal, or if all three lies on the same straight line through the orgin (by the standard triangle inequality). If $z,w$ do but not $v$ then
\be
d(z,v) = \abs{z} + \abs{v} \leq \abs{z-w} +\abs{w} + \abs{v} = d(z,w) + d(w,v).
\ee
If $v,w$ do but not $z$ the case is similar. If $z,v$ do but not $w$ then
\be
d(z,v) = \abs{z-v} \leq \abs{z}+\abs{v} \leq \abs{z}+\abs{w}+\abs{w} + \abs{v} = d(z,w) + d(w,v).
\ee
\een
If no two do the argument is as in previous question. We have 
\be
B(0;r) = \{w:\abs{w}<r\}
\ee
while if $z\neq 0$ and $r<\abs{z}$ then $B(z;r) = \left\{\lm z: \abs{\lm -1} < \frac r{\abs{z}}\right\}$.

\een

\begin{exercise}
Let $d$ and $d'$ denote the usual and discrete metrics respectively on $\R$. Show that all functions $f$ from $\R$ with metric $d'$ to $\R$ with metric $d$ are continuous. What are the continuous functions from $\R$ with metric $d$ to $\R$ with metric $d'$?
\end{exercise}

Solution. Given any $f:(\R,d') \to (\R,d)$, $\forall x\in\R$, $\forall \ve >0$ choose $\delta=\frac 12$, then
\be
d'(y,x) < \delta \ \ra \ y = x \ \ra \ d(f(y),f(x)) = 0< \ve,
\ee
so $f$ is continuous. 

Given $f:(\R,d) \to (\R,d')$, continous, if $f$ is not constant, $\exists a,b\in\R$ with 
\be
a<b,\quad f(a) \neq f(b).
\ee
Set $S = \{x>a:f(x)\neq f(a)\}$, then $b\in S$ and $a$ is a lower bound for $S$, $exists s = \inf S$. By continuity, $\exists \delta >0$ such that
\be
d(x,s) < \delta \ \ra \ d'(f(x),f(s)) < \frac 12 \ \ra \ f(x) = f(s).
\ee

If $s\in S$ then
\be
f(s-\tfrac{\delta}2) = f(s) \neq f(a)
\ee
which is contrary to $s$ being a lower bound for $S$, while if $s\notin S$, then $f(s) = f(a)$, so $\forall t\in [0,\delta)$ we have
\be
f(s+t) = f(a) \ \ra \ s+t\notin S \ \ra \ s+\delta \text{ is a lower bound for }S,
\ee
contrary to $s$ being the greatest such. Thus, $f$ must be constant. As constant functions are clearly continuous, the continuous functions $(\R,d)\to (\R,d')$ are the constant functions.

\begin{exercise}
\ben
\item [(a)] Show that the intersection of an arbitrary collection of closed subsets of a metric space must be closed.
\item [(b)] We define the \emph{closure} of subset $Y$ of a metric space $X$ to be the smallest closed set $cl(Y)$ containing $Y$. Why does the result of (a) tell us that this definition makes sense?
%if $Y$ is a subset of a metric space $X$, there is a unique closed subset $Z$ of $X$ such that $Z$ contains $Y$ and any closed subset of $X$ containing $Y$ also contains $Z$. The set $Z$ is called the closure of $Y$ in $X$, denoted $\bar{Y}$ or $cl(Y)$.
\item [(c)] Show that
\be
cl(Y) = \{x \in X : x_n \to x \text{ for some sequence $(x_n)$ in }Y \}.
\ee
\een
\end{exercise}

Solution. \ben
\item [(a)] Let $C_i$ for $i\in I$ be closed sets, then their complements $C_i^c$ are open. Given $x\in \bigcup_{i\in I}C_i^c$, take $i\in I$ such that $x \in C_i^c$, then $\exists r>0$ such that 
\be
B(x;r)\subseteq C_i^c \ \ra \ B(x;r)\subseteq \cup_{i\in I}C_i^c \ \ra \ \bigcup_{i\in I}C_i^c \text{ is open} \ \ra \ \bigcap_{i\in I}C_i = \bb{\bigcup_{i\in I}C_i^c}^c \text{ is closed}. 
\ee
\item [(b)] Given $Y$, let $C_i$ for $i\in I$ be the closed sets containing $Y$, then $\bigcap_{i\in I}C_i$ is closed by (a), contains $Y$, and lies in any closed set containing $Y$, so it is the smallest such.
\item [(c)] Let $S =\{x \in X : x_n \to x \text{ for some sequence $(x_n)$ in }Y \}$. Take $x\in X$. If there exists a sequence $(x_n)$ in $Y$ with $x_n\to x$, given $ve>0$, $\exists N\in\N$ such that
\be
n>N \ \ra \ d(x_n, x) < \ve \ \ra \ x_n \in B(x;\ve),
\ee
so that meets $Y$ and hence $cl(Y)$. Thus the open set $cl(Y)^c$ contains no open ball about $x$, so we must have $x\notin cl(Y)^c$, whence $x\in cl(Y)$ and $S\subseteq cl(Y)$.

Conversely, if $x\in cl(Y)$, then $\forall n \in\N$ the closed set $B(x;\frac 1n)^c$ does not contain $cl(Y)$, so cannot contain $Y$. So $\exists x_n \in B(x;\frac 1n)\cap Y$, and then $(x_n)$ is a sequence in $Y$ with $x_n\to x$. This gives $cl(Y) \subseteq S$.

\een

\begin{exercise}
Let $V$ be a normed space, $x \in V$ and $r > 0$. Prove that the closure of the open ball $B(x; r)$ is the closed ball $\{y \in V : \dabs{x - y} \leq r\}$. Give an example to show that, in a general metric space $(X, d)$, the closure of the open ball $B(x; r)$ need not be the closed ball $\{y \in X : d(x, y) \leq r\}$.
\end{exercise}

Solution. Let $A_r(x) = \{y \in V : \dabs{x - y} \leq r\}$. To show $A_r(x) \subseteq cl(B(x;r))$. Take $y\in V$. If $\dabs{x-y}\leq r$, define a sequence $(y_n)$ by 
\be
y_n = \frac 1n x + \frac {n-1}n y \ \ra \ \dabs{x-y_n} = \dabs{\frac{n-1}n(x-y)} = \frac {n-1}n \dabs{x-y} \leq \frac {n-1}n r<r \ \ra \ y_n \in B(x;r).
\ee
and
\be
\dabs{y_n -y} = \dabs{\frac 1n(x-y)} = \frac 1n \dabs{x-y} \to 0 \quad \text{as }n\to \infty \ \ra \ y_n \to y.
\ee
Thus by previous question we have $y\in cl(B(x;r))$. 

Now we need to show $cl(B(x;r)) \subseteq A_r(x) $. If $\dabs{x-y}>r$, let $r' = \dabs{x-y}-r$, then
\be
B(y;r')\cap B(x;r) = \emptyset \quad (\text{because if $\dabs{z-y}\leq r'$, then }\dabs{z-x}\geq \dabs{x-y}-\dabs{z-y} \geq \dabs{x-y} - r' = r)
\ee
so $B(y;r')^c$ is a closed set containing $B(x;r)$, so it contains $cl(B(x;r))$, and hence $y \notin cl(B(x;r))$. Thus,
\be
cl(B(x;r)) = \{y \in V : \dabs{x - y} \leq r\} = A_r(x).
\ee

Let $V=\Z$ with the discrete metric. If $r=1$ then $B(x;r)=\{x\}$ which is closed, so $cl(B(x;r))=\{x\}$, while $A_r(x) = V$.

\begin{exercise}
Show that the space of real sequences $a = (a_n)$, such that all but finitely many of the $a_n$ are zero, is not complete in the norm defined by $\dabs{a}_1 = \sum^\infty_{n=1} \abs{a_n}$. Is there an obvious 'completion'?
\end{exercise}


Solution. Let $\ell_0$ be the space of real sequence with finitely many non-zero terms, with the norm 
\be
\dabs{a}_1 = \sum^\infty_{n=1}\abs{a_n}.
\ee

Define
\be
a_n^{(k)} = \left\{\ba{ll}
\frac 1{2^n} \quad\quad & n\leq k\\
0 & n > k
\ea\right.
\ee
then each sequence $a^{(k)}$ lies in $\ell_0$. Given $\ve>0$, take $M\in \N$ with $\frac 1{2^M} < \ve$, then if $k>l>M$ we have
\be
\dabs{a^{(k)}-a^{(l)}} = \sum^\infty_{n=1} \abs{a_n^{(k)}-a_n^{(l)}} = \sum^k_{n=l+1} \frac 1{2^n} < \sum^\infty_{n=M+1} \frac 1{2^n} = \ve.
\ee

Thus $a^{(k)}$ is a Cauchy sequence in $\ell_0$. Given $a\in X$, take $N\in\N$ with $a_n = 0$ for all $n>N$, and let $\ve = \frac 1{2^{n+2}}$. Then if $k>N$ we have
\be
\dabs{a^{(k)}-a}_1 = \sum^\infty_{n=1}\abs{a_n^{(k)}-a_n} \geq \abs{a_{N+1}^{(k)} - a_{N+1}} = \frac 1{2^{N+1}} > \ve.
\ee

Thus $a$ is not the limit of the sequence $a^{(k)}$, so this Cauchy sequence has no limit in $\ell_0$, whence $\ell_0$ is not complete.

Set 
\be
X = \left\{(a_n): \sum^\infty_{n=1} \abs{a_n} <  \infty \right\},
\ee
so that $X$ contains $\ell_0$. We claim that $X$ is complete. Take a Cauchy sequence $a^{(k)}$ in $X$. For all $n\in \N$ the sequence $a_n^{(k)}$ is then Cauchy, so converges to some $a_n\in \R$. Let $a$ be the sequence whose $n$th term is $a_n$. 

We claim that $a\in X$ and that $a$ is the limit of the sequence $a^{(k)}$.

Take $M\in\N$ such that $k>l\geq M$, then $\sum^\infty_{n=1}\abs{a_n^{(k)}-a_n^{(l)}} < 1$. Let $\sum^\infty_{n=1} \abs{a_n^{(M)}}=R<\infty$. Take $N\in\N$, then for all $n\leq N$ there exists $K_n\in\N$ such that 
\be
k>K_n \ \ra \ \abs{a_n-a_n^{(k)}} < \frac 1N.
\ee

Let $K = \max\{K_1,\dots,K_N,M\} + 1$, then 
\be
\sum^N_{n=1} \abs{a_n - a_n^{(K)}} < N \cdot \frac 1N = 1, \quad\quad \sum^\infty_{n=1}\abs{a_n^{(K)}-a_n^{(M)}} < 1,
\ee
so
\be
\sum^N_{n=1} \abs{a_n} \leq \sum^N_{n=1} \abs{a_n - a_n^{(K)}} + \sum^N_{n=1} \abs{a_n^{(K)}-a_n^{(M)}} + \sum^N_{n=1} \abs{a_n^{(M)}} < 1+ 1+R = R+2.
\ee

Thus, $\sum^\infty_{n=1}\abs{a_n}$ is bounded by $R+2$, and so $a\in X$.

Now choose $M\in\N$ such that 
\be
\sum^\infty_{n=M+1} \abs{a_n} < \frac {\ve}4,\quad\quad \sum^\infty_{n=M+1} \abs{a_n^{(k)}} < \frac {\ve}4. \quad\quad (\text{since }a,a^{(k)}\in X)
\ee

For all $n\leq M$ take $L_n\in \N$ such that $l\geq L_n$, we have
\be
\abs{a_n - a_n^{(l)}} <  \frac {\ve}{4M}.
\ee

Let $l = \max\{L_1,\dots,L_M,K\}+1$, then
\beast
\sum^\infty_{n=1} \abs{a_n - a_n^{(k)}} & = & \sum^M_{n=1} \abs{a_n - a_n^{(k)}} +\sum^\infty_{n=M+1} \abs{a_n - a_n^{(k)}} \\
& \leq & \sum^M_{n=1} \abs{a_n - a_n^{(l)}} + \sum^M_{n=1} \abs{a_n^{(l)} - a_n^{(k)}} + \sum^\infty_{n=M+1} \abs{a_n} + \sum^\infty_{n=M+1} \abs{a_n^{(k)}} \\
& \leq & \sum^M_{n=1} \abs{a_n - a_n^{(l)}} + \sum^\infty_{n=1} \underbrace{\abs{a_n^{(l)} - a_n^{(k)}}}_{\text{Cauchy sequence}} + \sum^\infty_{n=M+1} \abs{a_n} + \sum^\infty_{n=M+1} \abs{a_n^{(k)}} \\
& < & M\cdot \frac{\ve}{4M} + \frac {\ve}4 + \frac {\ve}4 + \frac {\ve}4 = \ve.
\eeast
Thus $a$ is the limit of the sequence $a^{(k)}$ as required.

\begin{exercise}
Use the Contraction Mapping Theorem to show that the equation $\cos x = x$ has a unique real solution. Find this solution to some reasonable accuracy using a pocket calculator or the calculator on your computer (remember to work in radians!), and justify the claimed accuracy of your approximation.
\end{exercise}

Solution. Since $\abs{\cos x}\leq 1$, $\forall x\in\R$, any solution to $\cos x = x$ must lies in $[-1,1]$. Let $f(x) = \cos x$, then by the MVT, $\forall x,y \in [-1,1]$, $\exists z \in (x,y)$ such that
\be
\abs{f(y)-f(x)} = \abs{f'(z)}\cdot\abs{y-x} = \abs{\sin z}\cdot \abs{y-x} = (\sin 1) \abs{y-x}.
\ee
So $f$ is a contraction on $[-1,1]$ with constant $K =\sin 1 \simeq 0.841$, and hence $f$ has a unique fixed point. Take $x_0 =1$, 
\be
x_n = f(x_{n-1}) \quad \text{for }n\in\N \ \ra\ x_1 = 0.540 \dots \ \ra \ d(x_0,x_1) = 0.46.
\ee
Thus $m>n$, we have
\be
d(x_n,x_m) < d(x_0,x_1) \frac {K^n}{1-K} < 0.46 \frac{0.842^n}{0.158} < 3(0.842)^n,
\ee
so for 3 decimal place accuracy take $n > \frac{\log\bb{\frac{0.001}3}}{\log 0.842} = 46.55\dots$. Thus if we take $x_{47} = 0.739\dots$, this is correct to 3 decimal places. 

\begin{exercise}
Let $I = [0,R]$ be an interval and let $C(I)$ be the space of continuous functions on $I$. Show that, for any $\alpha \in \R$, we may define a norm by $\dabs{f}_\alpha = \sup_{x\in I} \abs{f(x)e^{-\alpha x}}$, and that the norm $\dabs{\cdot}_\alpha$ is Lipschitz equivalent to the uniform norm $\dabs{f} = \sup_{x\in I}\abs{f(x)}$.

Now suppose that $\varphi: \R^2 \to \R$ is continuous, and Lipschitz in the second variable $\abs{\varphi(t,x) - \varphi(t, y)} \leq K\abs{x - y}$, for all $t,x,y \in \R$. Consider the map $T$ from $C(I)$ to itself sending $f$ to $y_0+ \int^x_0 \varphi(t,f(t))dt$. Give an example to show that $T$ need not be a contraction under the uniform norm. Show, however, that $T$ is a contraction under the norm $\dabs{\cdot}_\alpha$ for some $\alpha$, and deduce that the differential equation $f' = \varphi(x,f(x))$ has a unique solution on $I$ satisfying $f(0) = y_0$.
\end{exercise}

Solution. First, we check
\ben
\item [(i)] Clearly, $\dabs{\cdot}_\alpha$ takes non-negative values.
\item [(ii)] If $\dabs{f}_\alpha = 0 \ \ra \ f=0$.
\item [(iii)] For $\lm \in\R$, 
\be
\dabs{\lm f}_\alpha = \sup_{x\in I} \abs{\lm f(x)e^{-\alpha x}} = \abs{\lm }\sup_{x\in I} \abs{f(x)e^{-\alpha x}} = \abs{\lm}\dabs{f}_\alpha.
\ee
\item [(iv)]
\be
\dabs{f+g}_\alpha = \sup_{x\in I} \abs{(f(x)+g(x))e^{-\alpha x}} \leq \sup_{x\in I} \abs{f(x)e^{-\alpha x}} + \sup_{x\in I} \abs{g(x)e^{-\alpha x}} = \dabs{f}_\alpha + \dabs{g}_\alpha.
\ee
\een
so $\dabs{\cdot}_\alpha$ is a norm.

$\forall x\in I$, we have
\be
\abs{f(x)e^{-\alpha x}} \leq \abs{f(x)}e^{\abs{\alpha} R} \leq e^{\abs{\alpha} R} \dabs{f} \ \ra \ \dabs{f}_\alpha = \sup_{x\in I} \abs{f(x)e^{-\alpha x}} \leq e^{\abs{\alpha} R}\dabs{f}.
\ee
Conversely, $\forall x\in I$, we have
\be
e^{-\abs{\alpha}R}\abs{f(x)} \leq \abs{f(x)e^{-\alpha x}} \leq \dabs{f}_\alpha \ \ra \ e^{-\abs{\alpha}R}\dabs{f} \leq \sup_{x\in I} e^{-\abs{\alpha}R} \abs{f(x)} \leq \dabs{f}_\alpha.
\ee
Thus 
\be
e^{-\abs{\alpha}R}\dabs{f} \leq \dabs{f}_\alpha \leq e^{\abs{\alpha}R}\dabs{f} \ \ra \ \dabs{\cdot}_\alpha \text{ is Lipschitz equivalent to }\dabs{\cdot}.
\ee

Take $R=1$, $\varphi(t,s) = 2s$ (which is continuous and Lipschitz in the second variable), $f(x)=1$, $g(x)=0$ for all $x$. Then $\dabs{f-g} =1$ while 
\be
\dabs{T(f)-T(g)} = \sup_{x\in[0,1]} \abs{\int^x_0 \bb{\varphi(t,f(t)) - \varphi(t,g(t))}dt} = \sup_{x\in[0,1]} 2 \abs{\int^x_0 \bb{f(t) - g(t)}dt} = 2 = 2\dabs{f-g},
\ee
so that $T$ is not a contraction under norm $\dabs{\cdot}$. Now take $\alpha >0$. Given $\varphi$ continuous and Lipschitz in the second variable with Lipschitz constant $K$, $\forall f,g\in C(I)$ we have
\beast
\dabs{T(f)-T(g)}_\alpha & = & \sup_{x\in I} \abs{e^{-\alpha x}\bb{\int^x_0(\varphi(t,f(t))- \varphi(t,g(t))dt}}\\
& \leq & \sup_{x\in I} \bb{\int^x_0 e^{-\alpha x}\abs{\varphi(t,f(t))- \varphi(t,g(t)}dt}\\
& \leq & \sup_{x\in I} \bb{\int^x_0 e^{-\alpha x}K\abs{f(t)- g(t)}dt} \leq \sup_{x\in I} \bb{\int^x_0 e^{-\alpha x}K\dabs{f-g}_\alpha e^{\alpha t}dt}\\
& \leq & K\dabs{f-g}_\alpha \sup_{x\in I} e^{-\alpha x} \bb{\int^x_0  e^{\alpha t}dt} = \frac K{\alpha}\dabs{f-g}_\alpha \sup_{x\in I} \bb{1-e^{-\alpha x}}\\
& < & \frac K{\alpha}\dabs{f-g}_\alpha.
\eeast
then $T$ is a contraction under the norm $\dabs{\cdot}_\alpha$. If $f$ is the fixed point of $T$ then
\be
f(x) = (T(f))(x) = y_0 + \int^x_0 \varphi(t,f(t))dt.
\ee

Differentiating gives 
\be
f'(x) = \varphi(x,f(x)), \quad\quad f(0)= y_0.
\ee

Conversely, if $f$ is a solution of $f'(x) = \varphi(x,f(x))$ and $f(0) = y_0$, then the fundamental theorem of calculus gives 
\be
f(x) = y_0 + \int^x_0 f'(t)dt = y_0 + \int^x_0 \varphi(t,f(t))dt.
\ee
So $f$ is a fixed point of $T$. Thus the differential equation $f'(x) = \varphi(x,f(x))$ has a unique solution on $I$ satisfying $f(0)=y_0$.

\begin{exercise}
Let $(X,d)$ be a non-empty complete metric space. Suppose $f : X \to X$ is a contraction and $g : X \to X$ is a function which commutes with $f$, i.e. such that $f(g(x)) = g(f(x))$ for all $x \in X$.

Show that $g$ has a fixed point. Must this fixed point be unique?
\end{exercise}

Solution. Let $x$ be the unique fixed point of $f$, then $f(g(x)) = g(f(x)) = g(x)$, so $g(x)$ is fixed point of $f$, by uniqueness we must have
\be
g(x) = x,
\ee
i.e., $x$ is a fixed point of $g$. It need not be unique, e.g. $g$ might be the identity map on $X$.

\begin{exercise}
Give an example of a non-empty complete metric space $(X,d)$ and a function $f : X \to X$ satisfying $d(f(x), f(y)) < d(x, y)$ for all $x, y \in X$ with $x \neq y$, but such that $f$ has no fixed point.

Suppose now that $X$ is a non-empty closed bounded subset of $\R^n$ with the Euclidean metric. Show that in this case $f$ must have a fixed point. If $g : X \to X$ satisfies $d(g(x), g(y)) \leq d(x, y)$ for all $x, y \in X$, must $g$ have a fixed point?
\end{exercise}

Solution. Take $X=[1,\infty)$ with usual metric $d$, and define $f:X\to X$ by 
\be
f(x) = x + \frac 1x.
\ee
Then
\be
d(f(x),f(y)) = \abs{x+\frac 1x - y - \frac 1y} = \abs{x-y - \frac{x-y}{xy}} = \abs{x-y}\bb{1-\frac 1{xy}} < \abs{x-y} = d(x,y),
\ee
but $f$ has no fixed point.

If $X$ is a closed bounded set in $\R^n$, define $h:X\to \R$ by $h(x) = d(x,f(x))$. Given $\ve>0$, if $d(y,x) < \frac {\ve}2$ then
\beast
d(f(y),f(x)) < \frac {\ve}2 \ \ra \ d(y,f(y)) & \leq & d(y,x) + d(x,f(x)) + d(f(x),f(y)) \\
& < & \frac {\ve}2 + d(x,f(x)) + \frac {\ve}2 = d(x,f(x)) + \ve.
\eeast

Similarly, $d(x,f(x)) < d(y,f(y)) + \ve$, so that
\be
\abs{h(y)-h(x)} < \ve.
\ee
Thus $h$ is continuous, so it is bounded and attains its bounds, and hence $\exists x\in X$ such that $\forall y\in X$ we have $h(y)\geq h(x)$. Now if $h(x)>0$ then
\be
h(f(x)) = d(f(x),f(f(x))) < d(x,f(x)) = h(x),
\ee
a contradiction (since $f(x)\in X$), so we must have $h(x) = 0 \ \ra \ d(x,f(x)) = 0 \ \ra\ x=f(x)$, and thus $x$ is a fixed point of $f$. 

Take $X= [-2,-1]\cup [1,2]$ and define $g:X\to X$ by $g(x)=-x$. Then $\forall x,y \in X$ we have
\be
d(g(x),g(y)) = d(x,y),
\ee
but $g$ has no fixed point.

\begin{exercise}
\ben
\item [(i)] Suppose that $(X, d)$ is a non-empty complete metric space, and $f : X \to X$ a continuous map such that, for any $x,y \in X$, the sum $\sum^\infty_{n=1} d(f^n(x),f^n(y))$ converges. ($f^n$ denotes the function $f$ applied $n$ times.) Show that $f$ has a unique fixed point.
\item [(ii)] By considering the function $x \mapsto \max \{x - 1, 0\}$ on the interval $[0,1) \subset \R$, show that a function satisfying the hypotheses of (i) need not be a contraction mapping.
\item [(iii)] Give an example to show that the result of (i) need not be true if $f$ is not assumed to be continuous.
\een
\end{exercise}

Solution. \ben
\item [(i)] Take $x_0 \in X$ and set $x_n = f^n(x_0)$ for $n\in\N$. Take $\ve>0$, since 
\be
\sum^\infty_{n=1}d(x_n,x_{n+1}) = \sum^\infty_{n=1}d(f^n(x_0),f^n(x_1)) 
\ee
converges, there exists $N\in\N$ such that 
\be
\sum_{n>N} d(x_n,x_{n+1}) < \ve.
\ee

Thus if $k>l>N$, we have 
\be
d(x_k,x_l) \leq \sum^{l-1}_{n=k}d(x_n,x_{n+1})\leq \sum_{n>N} d(x_n,x_{n+1}) < \ve \ \ra \ \text{the sequence $(x_n)$ is Cauchy.}
\ee

As $X$ is complete it therefore converges to some $x\in X$. Since $f$ is continuous we have 

\be 
f(x) = \lim_{n\to \infty} f(x_n) = \lim_{n\to\infty} x_{n+1} = x,
\ee
so that $x$ is a fixed point. If $y \neq x$ were any other fixed point then for all $n\in\N$, we would have
\be
d(f^n(x),f^n(y)) = d(x,y) \ \ra \ \sum^\infty_{n=1} d(f^n(x),f^n(y)) = \sum^\infty_{n=1} d(x,y) ,
\ee
which would not converge, so that $f$ has a unique fixed point.

\item [(ii)] Set $X=[0,\infty)$ with standard metric, and define $f:X\to X$ by $f(x)= \max\{x-1,0\}$. Given $x,y\in X$, we have
\be
d(f(x),f(y)) \leq d(x,y)
\ee
so $f$ is continuous, and if we choose $M\in\N$ with $M>\max{x,y}$, then
\be
f^M(x) = 0 = f^M(y) \ \ra\  \sum^\infty_{n=1}d(f^n(x),f^n(y)) \text{ is finite sum and hence converges.}
\ee
However, $f$ is not a contraction since 
\be
d(f(1),f(2)) = d(0,1) = 1 = d(1,2).
\ee

\item [(iii)] Let $X=[-1,1]$ with standard metric, and define $f: X\to X$ by 
\be
f(x) = \left\{\ba{ll}
\frac x2 \quad\quad x\neq 0\\
1 & x = 0
\ea\right.
\ee
then clearly, $f$ has no fixed point. However, given $x,y\in X$, for all $m\in \N$, we have $f^m(x), f^m(y)\neq 0$ whence ($f(x) \neq 0$)
\be
d\bb{f^{m+1}(x),f^{m+1}(y)} = \abs{f^{m+1}(x)-f^{m+1}(y)} = \abs{f^m(f(x))- f^m(f(y))} = \frac 12 \abs{f^m(x)- f^m(y)}.
\ee

Thus,
\be
\sum^\infty_{n=1} d\bb{f^n(x),f^n(y)} = d(f(x),f(y)) \sum^\infty_{n=0}\frac 1{2^n} = 2d(f(x),f(y)) \leq 3.
\ee
\een

\begin{exercise}
\ben
\item [(i)] Let $(X, d)$ be a metric space. For a nonempty subset $Y \subset X$ and $x \in X$ define
\be
d(x, Y) = \inf_{y\in Y} d(x, y).
\ee
Show that for fixed $Y$, the function $x \mapsto d(x, Y)$ defines a continuous function on $X$, and determine the subset of $X$ on which it vanishes.

\item [(ii)] For $Y, Z \subset X$ nonempty, define
\be
d(Y,Z) = \inf_{y\in Y} d(y,Z).
\ee

Show that if $Y$ and $Z$ are closed subsets of $\R^n$, and at least one of $Y$, $Z$ is bounded, then $d(Y,Z) > 0$ iff $Y$ and $Z$ are disjoint. Show that this conclusion can fail if the boundedness condition is removed.
\een
\end{exercise}

Solution. 

\begin{exercise}
A metric $d$ on a set $X$ is called an ultrametric if it satisfies the following stronger form of the triangle inequality:
\be
d(x, z) \leq \max \{d(x, y), d(y, z)\} \text{ for all }x, y, z \in X.
\ee

Which of the metrics in question \ref{ques:open_ball} are ultrametrics? Show that in an ultrametric space 'every triangle is isosceles' (that is, at least two of $d(x, z)$, $d(y, z)$ and $d(x, y)$ must be equal), and deduce that every open ball in an ultrametric space is a closed set. Does it follow that every open set must be closed?
\end{exercise}

Solution. The metric in question \ref{ques:open_ball}.(iv) is an ultrametric: in the notation of the answer above we have
\be
d(f,h) \leq 2^{-\min(n,m)} = \max\{2^{-n}, 2^{-m}\} = \max\{d(f,g), d(g,h)\}.
\ee

Those in question \ref{ques:open_ball}.(v) and (vi) are not ultrametric: in both cases 
\be
d(-1,0) = d(0,1) = 1, \quad d(-1,1) = 2.
\ee

Suppose $X$ is an ultrametric space. Take $x,y,z\in X$ and assume wlog that 
\be
d(x,y) \leq d(y,z) \ \ra \ d(x,z) \leq \max\{d(x,y),d(y,z)\} = d(y,z),
\ee
and $d(y,z) \leq \max\{d(y,x),d(x,z)\}$, we must have $d(y,z) \leq d(x,z)$, so $d(y,z) = d(x,z)$. Thus 'every triangle is isosceles', and indeed in any triangle the two longest sides have the same length. 

Now take $x\in X$ and $r>0$. If $y \notin B(x;r)$ then $d(x,y)\geq r$, then given $z\in B(y;r)$ we have
\be
d(z,y)< r\leq d(x,y) \ \ra \ d(x,z) = d(x,y) \geq r  \ \ra \ z\notin B(x;r).
\ee

Hence every point in $B(x;r)^c$ lies in an open ball contained in $B(x;r)^c$, so $B(x;r)^c$ is open, and thus $B(x;r)$ is closed.

It does not follow that every open set must be closed: take $X$ as in question \ref{ques:open_ball}.(iv) and choose $x\in X$, then $\{x\}^c$ is open since if $y\in \{x\}^c$ then $B(y;d(x,y)/2) \subseteq \{x\}^c$. But $\{x\}$ is not open since it contains no open ball, i.e., $\{x\}^c$ is not closed.

\begin{exercise}
There is (rumoured to be) a persistent 'urban myth' about the mathematics research student who spent three years writing a thesis about properties of 'antimetric spaces', where an antimetric on a set $X$ is a function $d : X \times X \to \R$ satisfying the same axioms as a metric except that the triangle inequality is reversed (i.e. $d(x, z) \geq d(x, y) + d(y, z)$ for all $x,y,z$). Why would such a thesis be unlikely to be considered worth a Ph.D.?
\end{exercise}

Solution. If $a,b\in X$ with $a\neq b$, then
\be
0 = d(a,a) \geq d(a,b) + d(b,a) =  2d(a,b) > 0,
\ee
a contradiction. So $\abs{X} \leq 1$.

\begin{exercise}
Let $X$ be the space of bounded real sequences. Is there a metric on $X$ such that a sequence $(x^{(k)})$ in $X$ converges to $x$ in this metric if and only if $(x^{(k)})$ converges to $x$ in every coordinate (i.e. $x^{(k)}_n \to x_n$ in $\R$ for every $n$)? Is there a norm with this property?
\end{exercise}

Solution. First, note that if $0\leq p\leq q$, then
\be
p(1+q) = p + pq \leq q + pq = q(1+p) \ \ra \ \frac p{1+p} \leq \frac q{1+q}.
\ee

Then if $0\leq r \leq p+q$ we have
\be
\frac r{1+r} \leq \frac{p+q}{1+p+q} = \frac p{1+p+q} + \frac q{1+p+q} \leq \frac p{1+p} + \frac q{1+q}.
\ee

Thus if we define $\delta:\R^2\to\R$ by 
\be
\delta(a,b) = \frac{\abs{a-b}}{1+\abs{a-b}},
\ee
then $\delta$ satisfies the triangle inequality, and hence $\delta$ is a metric on $\R$ and clearly $\forall a,b\in \R$ we have $\delta(a,b)<1$.

Thus if we define $d:X^2\to\R$ by 
\be
d(x,y) = \sum^\infty_{n=1} \frac 1{2^n}\delta(x_n,y_n),
\ee
it immediately follows that $d$ is metric on $X$. Suppose $x^{(k)}\to x$ in this metric. $\forall n\in\N$, $\forall \ve>0$,  $\exists K\in\N$ such that $\forall k>K$
\be
d(x^{(k)},x) < \frac 1{2^n} \frac {\ve}{1+\ve} \ \ra \  \frac 1{2^n} \frac {\ve}{1+\ve} > \sum^\infty_{m=1}\frac 1{2^m}\delta(x_m^{(k)},x_m) \geq \frac 1{2^n}\delta(x_n^{(k)},x_n) = \frac 1{2^n} \frac{\abs{x_n^{(k)}-x_n}}{1+\abs{x_n^{(k)}-x_n}},
\ee
giving 

\be
\frac {\ve}{1+\ve} > \frac{\abs{x_n^{(k)}-x_n}}{1+\abs{x_n^{(k)}-x_n}} \ \ra \ \abs{x_n^{(k)}-x_n} < \ve.
\ee
Thus, for all $n\in\N$, we have $x_n^{(k)}\to x_n$.

Conversely, assume that for all $n\in\N$ we have $x_n^{(k)}\to x_n$, and $\forall \ve>0$, we can choose $N\in\N$ such that $\frac 1{2^N} < \frac{\ve}2$. For all $n\leq N$, choose $K_n$ such that 
\be
\forall k > K \ \ra \ \abs{x_n^{(k)}-x_n} < \frac {\ve}2,
\ee
and set $K=\max\{K_1,\dots,K_N\}$. Then if $k>N$ we have
\beast
d(x^{(k)},x) & = & \sum^\infty_{n=1} \frac 1{2^n}\delta(x_n^{(k)},x_n) = \sum^N_{n=1}\frac 1{2^n}\delta(x_n^{(k)},x_n) + \sum^\infty_{n=N+1} \frac 1{2^n}\delta(x_n^{(k)},x_n) \\
& < & \sum^N_{n=1}\frac 1{2^n}\abs{x_n^{(k)}-x_n)} + \sum^\infty_{n=N+1} \frac 1{2^n} < \frac {\ve}2 \frac {\frac 12-\frac 1{2^N}}{1-\frac 12} + \frac 1{2^N} < \frac {\ve}2 + \frac {\ve}2 = \ve.
\eeast
Thus, $x^{(k)}\to x$ in the metric $d$.

However, if $\dabs{\cdot}'$ is any norm on $X$, for each $k\in\N$ let $y^{(k)}$ be the sequence with $y_n^{(k)} = \delta_{nk}$ and set $x^{(k)} = \frac{y^{(k)}}{\dabs{y^{(k)}}'}$ so that $\dabs{x^{(k)}}' = 1$. Let $x$ be the zero sequence, then for all $n\in\N$, as $k\to \infty$, we have $x_n^{k} \to 0 = x_n$, but for all $k\in\N$ we have $\dabs{x^{(k)}-x}' = \dabs{x^{(k)}}' = 1$ so that $x^{(k)}\nrightarrow x$. Thus there is no norm on $X$ with the requied property. 

\begin{exercise}
Metrics $d$, $d'$ on $X$ are said to be uniformly equivalent if the identity maps $(X, d) \to (X, d')$ and $(X, d') \to (X, d)$ are both uniformly continuous. Give an example of a pair of metrics on $\R$ which are uniformly equivalent but not Lipschitz equivalent. Show that for every metric space $d$
on a set $X$ there exists a metric $d'$ which is uniformly equivalent to $d$ and which is bounded.
\end{exercise}

Solution. 

\begin{exercise}
Let $(X, d)$ be a non-empty complete metric space and let $f : X \to X$ be a function such that for each positive integer $n$ we have
\ben
\item [(i)] if $d(x, y) < n + 1$ then $d(f(x), f(y)) < n$; and
\item [(ii)] if $d(x, y) < 1/n$ then $d(f(x), f(y)) < 1/(n + 1)$.
\een
Must $f$ have a fixed point?
\end{exercise}


Solution. Take $x\in X$ and define $x_0 = x$, $x_n = f(x_{n-1})$ for $n\in\N$. Take $n\in\N$ such that 
\be
d(x_0,x_1) < n+1 \ \ra \ d(f(x_0),f(x_1))=d(x_1, x_2) < n \ \ra \ \dots \ \ra \ d(x_n,x_{n+1}) < 1. \quad (\text{by (i)})
\ee
Write $y_i=x_{n+i}$ for $i\geq 0$, so $d(y_0,y_1)<1$. Given $k\in\N$, by (ii) we have
\be
d(y_k,y_{k+1})< \frac 1{k+1} < 1.
\ee
Assume $d(y_k,y_{k+n}) <1$ for some $n\in\N$, then
\be
d(y_k,y_{k+n+1}) \leq d(y_k,y_{k+1}) + d(y_{k+1},y_{k+n+1}) < \frac 1{k+1} + d(f(y_k),f(y_{k+n})) < \frac 1{k+1} + \frac 12 \leq 1.
\ee

Thus by induction $\forall n\in\N$, $d(y_k,y_{k+n}) < 1$, and this is true $\forall k\in\N$ (since $\frac 1{k+1} + \frac 12 \leq 1$).

Now $\forall \ve>0$, $\exists N > \frac 1{\ve} -1$, then $m,n>N$
\be
d(y_{m-N},y_{n-N}) < 1 \ \ra \ d(y_m,y_n) = d\bb{f^N(y_{m-N}),f^N(y_{n-N})} < \frac 1{N+1} < \ve \ \ra \ (y_n) \text{ is a Cauchy sequence}.
\ee

So as $X$ is complete we have $y_n \to y$ for some $y\in X$. As $f$ is Lipschitz with Lipschitz constant 1, it is continuous, so
\be
y_{n+1} = f(y_n) \to f(y) (\text{by continuity})
\ee
Thus by uniqueness of limits we must have $f(y)=y$, i.e., $y$ is fixed point of $f$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

