\chapter{Basic Probability (draft)}

\section{Basic Concepts}

\subsection{Sample spaces}

\begin{definition}
We will deal with a experiment which has a "random" outcome, the possible outcomes being, say, $\omega_1,\omega_2,\dots$. The set of all possible outcomes $\Omega­ = \{\omega_1,\omega_2,\dots\}$ is the sample space\index{sample space} of the experiment. A particular point $\omega \in \Omega$­ is an observation\index{observation}.
\end{definition}

\begin{example}
We have the following experiments
\ben
\item [(i)] Tossing a normal six-faced die, $\Omega­ = \{1, 2,\dots, 6\}$.
\item [(ii)] Picking a card from a standard pack, ­$\Omega$ is the pack of 52 cards.
\item [(iii)] Administer a drug to 20 patients. For simplicity assume there are only two possible outcomes for each patient, $R$ if the patient recovers, $D$ otherwise. Then 
\be
\Omega­ = \{(i_1,\dots,i_{20}): i_j = R \text{ or }D\}
\ee
where $i_j$ is $R$ or $D$ according as patient $j$ recovers or not, so ­ has $2^{20}$ points.
\item [(iv)] Pick a point from the unit interval, $[0,1]$, then ­$\Omega = [0,1]$.
\een
\end{example}

All but (iv) are examples of discrete sample spaces.

\begin{definition}
For the moment ­$\Omega$ will be assumed to be discrete\index{discrete}, i.e., it is a finite or countable set. A subset A of $\Omega$­ will be called an event. If the experiment is performed with outcome $\omega$ and $\omega \in A$, the event A is said to occur\index{occur}.
\end{definition}

In Example (i) A might be the event that "the outcome is even", so that $A = \{2,4,6\}$, in which case $A$ occurs if one of 2,4,6 is shown on the die.

Certain set-theoretic notions have special interpretations in probability.

\begin{definition}
The complement\index{complement} in $\Omega$­ of the event $A$, $A^c$ is the event "not $A$", and occurs if and only if $A$ does not occur. 
\end{definition}

\begin{definition}
The union\index{union} $A \cup B$ of two events $A$ and $B$ is the event "at least one of $A$ or $B$ occurs". 
\end{definition}

\begin{definition}
The intersection\index{intersection} $A \cap B$ is the event "both $A$ and $B$ occur".
\end{definition}

\begin{definition}
The inclusion relation\index{inclusion} $A \subseteq B$ means "the occurrence of $A$ implies the occurrence of $B$". 
\end{definition}

\begin{definition}
Events $A$ and $B$ are said to be mutually exclusive\index{mutually exclusive} if they are disjoint, $A \cap B = \emptyset$, and so both cannot occur together.
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Classical probability}

Classical probability was concerned with modelling situations where there are just a finite number of possible outcomes of the experiment and each of these outcomes is "equally likely". For example, tossing a fair coin or an unloaded die, or picking a card from a standard well-shu²ed pack. Here $\Omega$­ is a finite set with $N$ points, say, $\Omega­ = \{\omega_1,\dots,\omega_N\}$, and attached to each event $A \subseteq \Omega$­ is a measure of how "likely" the event $A$ is to occur.

Here we take $\pro(A)$, the probability of the event $A$ to be
\be
\pro(A) = \frac{N_A}N = \frac{\text{number of points in }A}{\text{number of points in ­}\Omega}
\ee
where $N_A$ is the number of points in $A$. Thus the probability $\pro(\cdot)$ is just a real-valued function defined on subsets $A$ of ­$\Omega$.

Properties of $\pro(\cdot)$
\ben
\item [1.] $0 \leq \pro(A) \leq 1$.
\item [2.] $\pro(\Omega­) = 1$.
\item [3.] If $A \cap B = \emptyset$, then $\pro(A \cup B) = \pro(A) + \pro(B)$.
\een

In a while we will take properties 1, 2 and an extension of 3 as the basis of an axiomatic development of probability in a more general context. The following properties follow immediately from the definition of $\pro(\cdot)$ above, but they may be deduced easily from 1, 2 and 3 without reference to the definition.
\ben
\item [4.] $\pro(\emptyset) = 0$.
\item [5.] $\pro(A^c) = 1 - \pro(A)$.
\item [6.] If $A \subseteq B$ then $\pro(A) \leq \pro(B)$.
\item [7.] $\pro(A \cup B) = \pro(A) + \pro(B) - \pro(A \cap B)$.
\item [8.] $\pro(A \cup B) \leq \pro(A) + \pro(B)$.
\een

\subsection{Combinatorial analysis}

Fundamental rule: There are $r$ multiple choices to be made in sequence: there are $m_1$ possibilities for the first choice, after making the first choice there are $m_2$ possibilities for the second choice, after making the first two choices there are $m_3$ possibilities for the third choice, and so on until after making the first $r-1$ choices there are $m_r$ possibilities for the $r$th choice. Then the total number of different possibilities for the set of choices is
\be
m_1m_2 \dots m_r.
\ee

\begin{example}
A restaurant menu has 6 starters, 7 main courses and 6 puddings. The total number of different three-course meals that may be served is $6 \times 7 \times 6 = 252$.
\end{example}

\begin{flushleft}Sampling models\end{flushleft}

Many of the standard calculations that arise in classical probability where outcomes need to be counted may be put in a standard framework of sampling\index{sampling}. Think of drawing $m$ balls from an urn which initially contains n distinguishable balls (they are numbered 1 to $n$, say). This may be done in a number of ways:
\ben
\item [1.] Sampling with replacement and with ordering. The balls are replaced between successive draws and the order in which balls are drawn is noted. Then the Fundamental Rule shows that there are $n^m$ possible ways.
\item [2.] Sampling without replacement and with ordering ($m \leq n$). The balls are not replaced after drawing and the order is noted. Then the number of ways is
\be
n (n- 1) \dots (n - m + 1) = \frac{n!}{(n - m)!} = P^n_m.
\ee
The symbol $P^n_m$ represents the number of permutations of $n$ objects $m$ at a time. An important special case occurs when $n = m$. We get the number of permutations of $n$ distinguishable objects (that is the number of distinguishable ways they may be laid out in a line, say) is $n!$.

\item [3.] Sampling without replacement and without ordering. If we take $m$ balls from $n$ and they were ordered there would be $n(n - 1) \dots (n - m + 1) = n!/(n - m)!$ ways but each unordered selection may be permuted in $m!$ different ways (or give $m!$ ordered arrangements) so that the total number of unordered ways is
\be
\frac{n!}{(n - m)!m!} = \binom{n}{m} = C^n_m.
\ee

The symbol $\binom{n}{m}$ is the binomial coe±cient, usually read "$n$ choose $m$", and represents the number of ways of picking $m$ objects from $n$, note that $\binom{n}{m}$ is the coe±cient of $x^m$ in the expansion of $(1 + x)^n$ using the binomial theorem. 

Suppose that, of the $n$ balls, $m_1$ are of colour 1, $m_2$ are of colour 2 and so on up to $m_k$ balls of colour $k$, where $n = m_1 + \dots + m_k$. Consider the number of permutations of the $n$ balls when the balls are distinguishable only by colour. For example, if $n = 4$ and there are two black balls and two white balls then there are 6 possible arrangements:
\be
BBWW\quad BWBW\quad BWWB\quad WBBW\quad WBWB\quad WWBB
\ee

If the balls are distinguishable then there are $n!$ permutations, but within colour $i$ there are $m_i!$ ways of permuting the balls so the Fundamental Rule gives that for each distinguishable arrangement there are $\prod^k_{i=1} (m_i!)$ ways of permuting the balls leaving the arrangement the same if the balls are distinguishable only by colour. Thus the number of arrangements distinguishable only by colour is
\be
\frac{n!}{m_1!m_2! \dots m_k!} = \binom{n}{m_1 \dots m_k},
\ee
which is the multinomial coefficient (the coefficient of $x^{m_1}_1 x^{m_2}_2 \dots x^{m_k}_k$ in the expansion of $(x_1 + \dots + x_k)^n$). An alternative way of seeing this is to think of first choosing the positions for the balls of colour 1, which can be done in $\binom{n}{m_1}$ ways, then choose the positions for the balls of colour 2 in $\binom{n- m_1}{m_2}$ ways and so on to see that the total number of ways, using the Fundamental Rule, is
\be
\binom{n}{m_1}\binom{n -m_1}{m_2} \binom{n -m_1 -m_2}{m_3} \cdots \binom{n - m_1 - \dots - m_{k-1}}{m_k} = \binom{n}{m_1 \dots m_k}.
\ee

\item [4.] Sampling with replacement and without ordering. Draw $m$ balls one after another, each time noting the number of the ball and replacing it before the next draw,
\begin{center}
\begin{tabular}{lcc c|c|c|c|c|c}
Ball No. & & & 1 & 2 & 3 & 4 & \dots & n\\
Times drawn & & & $\surd$ & $\surd\surd$ & $\surd$ & $\surd\surd$ & \dots & $\surd$\\
\end{tabular}
\end{center}

so that the number of $\surd$ is the number of times the ball is drawn. The number of ways is then the number of ways that $n - 1$ vertical lines may be put between $m$ $\surd$ which is
\be
\binom{n + m -1}{n - 1}.
\ee
think of choosing $n-1$ slots for the vertical lines out of $n+m-1$ slots and then the checks go in the remainder.
\een

Allocation models
An alternative way of thinking of these counting schemes is to think about allocating $m$ tokens (labelled $1,\dots,m)$ to $n$ boxes (labelled $1,\dots, n$). The four cases considered previously correspond to:
\ben
\item [1.] Each box may contain any number of tokens and the labels on the tokens are observed.
\item [2.] No box may contain more than one token and the labels on the tokens are observed.
\item [3.] No box may contain more than one token and the labels on the the tokens are not observed.
\item [4.] Each box may contain any number of tokens and the labels on the tokens are not observed.
\een
In the next examples in which each outcome in the probability space is equally likely we will calculate the probability of the event $A$ by computing the number of points $N$ in the sample space ­$\Omega$ and then computing the number $N_A$ of points in $A$. In each case the probability will then be
\be
\pro(A) = \frac{N_A}N
\ee

\begin{example}
What is the probability that a Poker hand shows five different face values? (A poker hand contains 5 cards.) One way to do this is to think of ­ consisting of all possible poker hands, that is, unordered sets of five cards chosen from a standard pack of 52. We have $N = \binom{52}{5}$ and $A$ consists of those hands showing five different face values so that
\be
N_A = \binom{13}{5} \times 4^5
\ee

since we may think of first choosing the unordered sets of 5 face values and then the different suits for each value, and use the Fundamental Rule. An alternative way in this problem, and in other situations involving sampling without replacement, is to think of ­comprising all ordered sets of 5 cards (imagine the cards being dealt in sequence). Then $N = 52\times 51\times 50\times 49 \times 48$ and $N_A = 52\times 48\times 44\times 40\times 36$, thinking of the possible choices for the first card, second card in the hand and so on to give 5 different face values. You should check that these two different approaches give the same probabilities. Either approach is fine, but remember to be consistent - if your sample space has unordered (respectively,
ordered) points then the points in A must be unordered (respectively, ordered). 
\end{example}


\begin{example} What is the probability that a Bridge hand (13 cards) contains 5 spades ($\spadesuit$), 3 hearts ($\heartsuit$), 4 diamonds ($\diamondsuit$) and 1 club ($\clubsuit$)? Take $\Omega$­ to be the set of unordered Bridge hands, so we have $N = \binom{52}{13}$ while
\be
N_A = \binom{13}{5} \times \binom{13}{3} \times \binom{13}{4} \times \binom{13}{1}.
\ee
since we may think of choosing in sequence the spades, hearts, diamonds and club and then use the Fundamental Rule to get the total number of ways.
\end{example}

\begin{example}
If there are $r$ people in a room, what is the probability that at least two have the same birthday? In this case the number of points in the sample space ­$\Omega$ is $N = (365)^r$, since the sample space consists of all possible $r$-tuples of birthdays. If $A$ is the required event, this is a case where it is easier to calculate the number of points in $A^c$, the complement of $A$, since $A^c$ is just the event that no two of the r people share a birthday. We then have
\be
N_{A^c} = 365 \times 364 \times 363 \times \dots \times (365 -r + 1),
\ee
and so
\be
\pro(A) = 1 - P(A^c) = 1 - \frac{365 \times 364 \times 363 \times \dots \times (365 - r + 1)}{(365)^r} = p_r.
\ee

It is interesting to note the following values
\begin{center}
\begin{tabular}{c|ccccccccccc}
$r$ & 10 & 15 & 20 & 21 & 22 & 23 & 24 & 25 & 30 & 40 & 55\\
\hline
$p_r$ & 0.12 & 0.25 & 0.41 & 0.44 & 0.48 & 0.51 & 0.54 & 0.57 & 0.71 & 0.89 & 0.99\\
\end{tabular}
\end{center}
which shows the well known fact that if there are 23 or more people in a room there is higher than evens chances that at least 2 share a birthday. 
\end{example}

\begin{theorem}[Stirling's Formula\index{Stirling's Formula}]\label{thm:stirling_formula}
As $n \to \infty$,
\be
\log\bb{\frac{n! e^n}{n^{n+\frac 12}} } = \log\bb{\sqrt{2\pi}} + O(1/n).
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Let $c_n = \log (n!) + n - (n + \tfrac 12)\log n$, then
\be\label{equ:stirling_difference}
c_n - c_{n+1} = (n + \tfrac 12 ) \log (1 + 1/n) - 1.
\ee
For $0 < x < 1$, if we subtract the two expressions
\bea
\log(1 + x) - x & = & -\frac {x^2}2 + \frac{x^3}3 - \frac{x^4}4 + \dots\label{equ:stirling_log_plus}\\
\log(1 - x) + x & = & -\frac {x^2}2 - \frac{x^3}3 - \frac{x^4}4 - \dots\label{equ:stirling_log_minus}
\eea
and divide by $2x$, we obtain
\beast
\frac 1{2x} \log\bb{\frac{1 + x}{1 - x}} - 1 & = & \frac{x^2}3 + \frac{x^4}5 + \frac{x^6}7 + \dots \\
& \leq & \frac{x^2}3 + \frac{x^4}3 + \frac{x^6}3 + \dots = \frac 13 \frac{x^2}{1 - x^2}.
\eeast
Now put $x = 1/(2n + 1)$ and we see from (\ref{equ:stirling_difference}) that the left-hand side of (\ref{equ:stirling_log_plus}) is then $c_n - c_{n+1}$, and furthermore from (\ref{equ:stirling_log_plus}) that $c_n - c_{n+1} \geq 0$ and from (\ref{equ:stirling_log_minus}) that
\be
c_n - c_{n+1} \leq \frac 1{12} \bb{\frac 1n - \frac 1{n + 1}}.
\ee
This shows that $c_n$ is monotone non-increasing in $n$ and $c_n - 1/(12n)$ is monotone non-decreasing so that $c_n$ is bounded below and hence converges $c_n \to c$, for some $c$.

To determine the value of $c$, define $I_r = \int^{\pi/2}_0 \sin^r \theta d\theta$, for $r \geq 0$, so that $I_0 = \pi/2$ and $I_1 = 1$. Integrating by parts for $r > 1$, we have 
\be
I_r = -[-\sin^{r-1} \theta \cos \theta ]^{\pi/2}_0 + (r - 1) \int^{\pi/2}_0 \sin^{r-2} \theta \cos^2 \theta d\theta = (r - 1) (I_{r-2} - I_r),
\ee
so that $rI_r = (r - 1)I_{r-2}$. It is immediate that $I_{2n+1} \leq I_{2n} \leq I_{2n-1}$, hence
\be
1 \leq \frac{I_{2n}}{I_{2n+1}} \leq \frac{I_{2n-1}}{I_{2n+1}} = \frac{2n + 1}{2n} \to 1,\quad\text{as }n \to \infty.
\ee
Calculate
\be
I_{2n} = \frac{2n-1}{2n} I_{2n-2} = \frac{(2n-1)(2n - 3) \dots 1}{(2n)(2n - 2) \dots 2}I_0 = \frac{(2n)!}{(2^n n!)^2}\frac{\pi}2.
\ee
\be
I_{2n+1} = \frac{2n}{2n + 1} I_{2n-1} = \frac{(2n)(2n - 2) \dots 2}{(2n + 1)(2n - 1) \dots 1}I_1 = \frac{(2^nn!)^2}{(2n + 1)!}.
\ee
Dividing these we see that, as $n \to \infty$,
\be
\frac{I_{2n}}{I_{2n+1}} = \frac{(2n + 1)((2n)!)^2}{ (2^nn!)^4} \frac{\pi}2 \to 1,\quad \text{ or } \quad \frac{(2^nn!)^2}{(2n)!} \frac 1{\sqrt{n}} \to \sqrt{\pi}.
\ee
Now note that
\be
2c_n - c_{2n} = \log\sqrt{2} + \log\bb{\frac{(2^nn!)^2}{(2n)!} \frac 1{\sqrt{n}}} \to \log \sqrt{2} + \log\sqrt{\pi} = \log{2\pi},
\ee
so that $c = \lim_{n\to \infty} (2c_n - c_{2n}) = \log\sqrt{2\pi}$, as required.
\end{proof}

The most common statement of Stirling's formula is given as a corollary.

\begin{corollary} 
As $n \to \infty$, $n! \sim \sqrt{2\pi} e^{-n} n^{n+\frac 12}$, where, in this context, $\sim$ indicates that the ratio of the two sides tends to 1.
\end{corollary}

\begin{example}
Suppose that $4n$ balls, of which $2n$ are red and $2n$ are black, are put at random into two urns, so that each urn contains $2n$ balls. What is the probability that each urn contains $n$ red balls and $n$ black balls? The probability is
\beast
p_n & = & \binom{2n}{n} \binom{2n}{n} \left/\binom{4n}{2n}\right. = \bb{\frac{(2n)!}{n!}}^4 \frac 1{(4n)!}\\
& \sim & \bb{\frac{\sqrt{2\pi} e^{-2n} (2n)^{2n+\frac 12}}{\sqrt{2\pi} e^{-n} n^{n+\frac12}}}^4 \frac 1{\sqrt{2\pi} e^{-4n}(4n)^{4n+\frac 12}} = \sqrt{\frac{2}{\pi n}} = a_n.
\eeast
We may calculate the exact probability $p_n$, its approximant $a_n$ from Stirling's formula and their ratio for different values of $n$.
\begin{center}
\begin{tabular}{c|ccccccc}
$n$ & 1 & 10 & 13 & 20 & 40 & 60 & 100\\
\hline
$p_n$ & 0.667 & 0.248 & 0.218 & 0.177 & 0.126 & 0.103 & 0.080\\
$a_n$ & 0.798 & 0.252 & 0.221 & 0.178 & 0.126 & 0.103 & 0.080\\
$p_n/a_n$ & 0.836 & 0.981 & 0.986 & 0.991 & 0.995 & 0.997 & 0.998\\
\end{tabular}
\end{center}
The case $n = 13$ corresponds to dividing a standard pack of cards in two and obtaining equal numbers of red and black cards in each half. 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Axiomatic Probability}

\subsection{The axioms}
The formulation for classical probability in which all outcomes or points in the sample space are equally likely is too restrictive to develop a useful theory of probability. In this section we give a general definition. For the moment we will assume that the sample space of outcomes $\Omega$­ is either a finite or countable set.

\begin{axiom}
A probability distribution\index{probability distribution} $\pro$ on $\Omega$­ is a real-valued function de¯ned on subsets (events) of ­$\Omega$ satisfying the following three conditions:
\ben
\item [1.] $0\leq \pro(A) \leq 1$, for all $A \subseteq \Omega$­.
\item [2.] $\pro(\Omega­) = 1$.
\item [3.] For a finite or in¯nite collection of disjoint\index{disjoint} events $\{A_i\}_i$, that is $A_i \bs A_j = \emptyset$ for $i \neq j$, we have
\be
\pro\bb{\bigcup_i A_i} = \sum_i \pro(A_i).
\ee
First observe the following consequence of Axiom 3.
\item [4.] $\pro(\emptyset) = 0$.
\een
\end{axiom}

This follows because if we take $A_i = \emptyset$, for each $i$ in Axiom 3 then we get a contradiction unless $\pro(\emptyset) = 0$. Note that Axiom 3 here implies the property 3. that we had in Chapter 1 if we take $A_i = \empty$ for $i > 2$, $A_1 = A$ and $A_2 = B$. This of course extends to $n$ disjoint
events $A_1,\dots,A_n$ for which $\pro\bb{\bigcup^n_{i=1} A_i} = \sum^n_{i=1} \pro(A_i)$. An important consequence of this axiom is a continuity property of probabilities in that for events $B_1 \subseteq B_2 \subseteq B_3 \subseteq \dots$ we have
\be\label{equ:disjoint_probability}
\pro\bb{\bigcup^\infty_{i=1} B_i} = \lim_{i\to\infty} \pro(B_i).
\ee
This follows by taking $A_1 = B_1$ and for $i > 1$, $A_i = B_i \cap B^c_{i-1}$, then the $\{A_i\}$ are disjoint with $\bigcup^i_{j=1}A_j = B_i$ and $\bigcup^\infty_{i=1} A_i = \bigcup^\infty_{i=1} B_i$ so that $\pro(B_i) = \sum^i_{j=1} \pro(A_j)$, and
\be
\pro\bb{\bigcup^\infty_{i=1}B_i} = \pro\bb{\bigcup^\infty_{i=1} A_i} = \sum^\infty_{i=1} \pro(A_i) = \lim_{i\to \infty} \sum^i_{j=1} \pro(A_j) = \lim_{i\to \infty} \pro(B_i).
\ee

As in Chapter 1 we may deduce that
\ben
\item [5.] $\pro(A^c) = 1 - \pro(A)$.
\item [6.] If $A \subseteq B$ then $\pro(A) \leq \pro(B)$.
\item [7.] $\pro(A\cup B) = \pro(A) + \pro(B) - \pro(A \cap B)$.
\een

Using (\ref{equ:disjoint_probability}) and Property 5. we may see a corresponding continuity property for probabilities on decreasing events, in that for events $B_1 \supseteq B_2 \supseteq B_3 \supseteq \dots$, we have
\be
\pro\bb{\bigcap^\infty_{i=1} B_i} = \lim_{i\to\infty} \pro(B_i).
\ee

Boole's Inequality. We may deduce from Property 7, that for any events $A_1,\dots,A_n$,
\be
\pro\bb{\bigcup^n_{i=1} A_i} \leq \sum^n_{i=1} \pro(A_i).
\ee
The case $n = 2$ follows from Property 7 (and Axiom 1) immediately and the case for general $n$ by induction. We may use (\ref{equ:disjoint_probability}) to extend Boole's Inequality to the case of countably many events $A_1,A_2,\dots$ so that
\be
\pro\bb{\bigcup^\infty_{i=1} A_i} \leq \sum^\infty_{i=1} \pro(A_i).
\ee

\subsection{Inclusion-exclusion formula}

The inclusion-exclusion formula gives a method of calculating that at least one of a number of events occurs.

\begin{theorem}[Inclusion-exclusion] For any events $A_1,A_2,\dots,A_n$,
\be
\pro\bb{\bigcup^n_{i=1}A_i} = \sum_i \pro(A_i) - \sum_{i_1<i_2} \pro(A_{i_1} \cap A_{i_2}) + \sum_{i_1<i_2<i_3} \pro(A_{i_1} \cap A_{i_2} \cap A_{i_3}) - \dots + (-1)^{n-1} \pro\bb{\bigcap^n_{i=1} A_i}.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
The proof is by induction on $n$. The case $n = 2$ is just Property 7. Assume the result holds for any $n$ events and consider $A_1,\dots,A_{n+1}$. Now see from Property 7. again that
\bea
\pro\bb{\bigcup^{n+1}_{i=1} A_i} & = & \pro\bb{\bigcup^{n}_{i=1} A_i} + \pro(A_{n+1}) - \pro\bb{\bb{\bigcup^{n}_{i=1} A_i}\cap A_{n+1}}\nonumber\\
& = & \pro\bb{\bigcup^{n}_{i=1} A_i} + \pro(A_{n+1}) - \pro\bb{\bigcup^{n}_{i=1} (A_i\cap A_{n+1})},\label{equ:inclusion_exclusion}
\eea
and then apply the inductive hypothesis to the first and the third terms to complete the induction and obtain the conclusion.
\end{proof}


\begin{example}
What is the probability that in a round of Bridge at least one player holds exactly two aces and two kings? Let ­$\Omega$ be the sample space of all possible (unordered) hands for 4 players, then the number of points in $\Omega$­ is $N = \binom{52}{13\ 13\ 13\ 13}$. For each player $i$, $i = 1, 2,3,4$, let $A_i$ be the event that player $i$ holds exactly two aces and two kings. Then
\be
\pro (A_i) = \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9} \binom{39}{13\ 13\ 13}}{\binom{52}{13\ 13\ 13\ 13}} = \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9}}{\binom{52}{13}}.
\ee
think of choosing the aces for player $i$, then the kings and then 9 other cards and finally distributing the remaining 39 cards among the other three players. If $i \neq j$, then 
\be
\pro (A_i \cap A_j) = \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9\ 9\ 26} \binom{26}{13}}{\binom{52}{13\ 13\ 13\ 13}} = \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9\ 9\ 26}}{\binom{52}{13\ 13\ 26}},
\ee
as before, think of picking the aces and kings for player $i$ with the remaining aces and kings going to player $j$, and then picking 9 other cards for each of $i$ and $j$. If $i, j, k$ are unequal then $A_i \cap A_j \cap A_k = \emptyset$, so that by inclusion exclusion we have the required probability is
\be
\pro \bb{\bigcup^4_{i=1} A_i} = 4\times \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9} }{\binom{52}{13}} - \binom{4}{2}\times \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9\ 9\ 26}}{\binom{52}{13\ 13\ 26}}.
\ee
\end{example}

\begin{example}
Suppose that $n$ students leave their $n$ coats outside a lecture room and when they leave they pick up their coats at random. What is the probability that at least one student has his own coat? Let ­ consist of all permutations of $1,\dots,n$, so that if $(i_1,\dots,i_n) \in \Omega$­ then $i_j$ is the index of the coat got by student $j$. Denote by $A_i$ the event that student $i$ gets his own coat. Then for $i_1 < i_2 < \dots < i_r$, the probability that all of
the students $i_1,\dots, i_r$ get their own coats is
\be
\pro\bb{\bigcap^r_{k=1} A_{ik}} = \frac{(n - r)!}{n!},
\ee
since the numerator is the number of ways that the remaining $n-r$ coats may be permuted among the remaining $n- r$ students. Then by inclusion-exclusion we have
\beast
\pro\bb{\bigcup^n_{i=1} A_i} & = & \sum^n_{r=1} \left[(-1)^{r-1} \sum_{i_1<\dots<i_r} \pro\bb{\bigcap^r_{k=1} A_{ik}}\right]\\
& = & \sum^n_{r=1} (-1)^{r-1} \binom{n}{r} \frac{(n - r)!}{n !} = \sum^n_{r=1} (-1)^{r-1} \frac 1{r!},
\eeast
so that for large $n$ the probability is approximately equal to
\be
1 - \frac 1{2 !} + \frac 1{3 !} - \frac 1{4 !} + \dots = 1 - e^{-1} \approx 0.632.
\ee
\end{example}

\begin{corollary}[Bonferroni's Inequalities\index{Bonferroni's Inequalities}] 
For any events $A_1,A_2,\dots,A_n$ and for any $r$, $1 \leq r \leq n$,
\be
\pro\bb{\bigcup^n_{i=1} A_i} \ba{c}
\leq \\
\text{ or }\\
\geq
\ea \sum_i \pro(A_i) - \sum_{i_1<i_2} \pro(A_{i_1} \cap A_{i_2}) + \sum_{i_1<i_2<i_3} \pro(A_{i_1} \cap A_{i_2} \cap A_{i_3}) - \dots + (-1)^{r-1} \sum_{i_1<\dots<i_r} \pro(A_{i_1} \cap \dots \cap A_{i_r}),
\ee
according as $r$ is odd or even. That is, if the sum in the inclusion-exclusion formula is truncated after $r$ terms it overestimates or underestimates the probability of the union of the $n$ events according as $r$ is odd or even.
\end{corollary}

\begin{proof}[\bf Proof]
The proof proceeds by induction on $n$. Assume it is true for $n$, then for $n + 1$ events it is true when $r = n + 1$, by the inclusion-exclusion formula, and for $r \leq n$, apply the inductive hypothesis to probability of the two unions of $n$ events in relation (\ref{equ:inclusion_exclusion}) to get the result.
\end{proof}

\subsection{Conditional probability}

\begin{definition}
Suppose that $B \subseteq \Omega$­ is an event with $\pro(B) > 0$. For any event $A \subseteq \Omega$­, the conditional probability\index{conditional probability} of $A$ given $B$ is
\be
\pro(A | B) = \frac{\pro(A \cap B)}{\pro(B)}.
\ee
it is the probability that the event $A$ occurs, that is that the outcome of the 'experiment' is in $A$, if it is known that the outcome is in the event $B$, i.e., that $B$ occurs.
\end{definition}

\begin{example}
What is the probability that a Bridge hand contains the ace of hearts given that it contains exactly 5 hearts? Let $A$ be the event that the hand contains the ace of hearts and $B$ the event that the hand contains exactly 5 hearts. Then 
\be
\pro(B) = \frac{\binom{13}{5}\binom{39}{8}}{\binom{52}{13}}\quad\quad\text{and}\quad \pro(A \cap B) = \frac{\binom{12}{4}\binom{39}{8}}{\binom{52}{13}},
\ee
whence $\pro(A|B) = \binom{12}{4}\left/\binom{13}{5}\right. = \frac{5}{13}$.
\end{example}

The first thing to observe is that $\pro (\cdot|B)$ is a probability distribution on the sample space $B$, because it satis¯es the axioms for a probability distribution as follows:
\ben
\item [1.] For $C \subseteq B$, $\pro(C | B) = \pro(C)/\pro(B)$, so that $0 \leq \pro(C | B) \leq 1$.
\item [2.] $\pro(B | B) = \pro(B)/\pro(B) = 1$.
\item [3.] For disjoint events $C_1,C2,\dots$ in $B$.
\een
\be
\pro\bb{\left.\bigcup^\infty_{i=1} C_i\right|B} = \frac{\pro\bb{\bigcup^\infty_{i=1} C_i \cap B}}{\pro(B)} = \frac{\pro\bb{\bigcup^\infty_{i=1} C_i}}{\pro(B)} = \frac{\sum^\infty_{i=1} \pro(C_i)}{\pro(B)} = \sum^\infty_{i=1} \pro (C_i | B).
\ee
Multiplication rule. The next thing to notice is the multiplication rule that
\be
\pro(A \cap B) = \pro(A | B)P(B),
\ee
so that the probability of two events occurring can be broken up into calculating successive probabilities-firstly the probability that $B$ occurs and then given that $B$ has occurred the probability that $A$ occurs. This is one of two central procedures for calculating probabilities (the second is the Law of Total Probability introduced below).

\begin{example}
Suppose that two students are selected, without replacement, from a class of 5 women and 13 men. What is the probability that the first student selected is a man and the second is a woman? Let $B$ be the event that the first is a man and $A$ the event that the second is a woman, then $\pro(A \cap B) = \pro(A | B)\pro(B) = \frac 5{17} \times \frac{13}{18}$.
\end{example}

More generally we can write down the multiplication rule for events $A_1,\dots,A_n$, 
\be
\pro(A_1 \cap A_2 \dots \cap A_n) = \pro(A_1)\pro(A_2 |A_1)\pro(A_3 |A_1 \cap A_2) \dots \pro(A_n | A_1 \cap \dots\cap A_{n-1}).
\ee

\begin{example}
In drawing three cards without replacement from a pack what is the probability of three successive aces? Here $A_i$ would be the event that an ace is obtained on draw $i$, $i = 1,2,3$, then
\be
\pro(A_1 \cap A_2 \cap A_3) = \pro(A_1)\pro(A_2 | A_1)\pro(A_3 | A_1 \cap A_2) =  \frac 4{52} \times \frac 3{51} \times \frac 2{50}.
\ee
\end{example}


\begin{theorem}[Law of Total Probability\index{Law of Total Probability}]
A collection $\{B_i\}^infty_{i=1}$ of disjoint events for which $\bigcup^\infty_{i=1} B_i = ­\Omega$ is said to be a partition\index{partition} of the sample space ­$\Omega$. For any partition of the sample space, $\{B_i\}$, and for any event $A$, we may write
\be\label{equ:law_of_total_probability}
\pro(A) = \sum^\infty_{i=1} \pro(A \cap B_i) = \sum_i \pro (A | B_i) \pro(B_i),
\ee
where the second summation extends only over those events $B_i$ in the partition for which $\pro(B_i) > 0$. The identity in (\ref{equ:law_of_total_probability}) is known as the Law of Total Probability. It follows immediately from Axiom 3 and the multiplication rule, because the event A may be represented as $A = \bigcup^\infty_{i=1}(A \cap B_i)$.
\end{theorem}

\begin{example}
An urn contains $b$ black balls and $r$ red balls from which two balls are drawn without replacement. What is the probability that the second ball drawn is black? Let $A$ represent the event that the second ball is black and $B$ the event that the first ball is black. Then $B$ and $B^c$ form a partition of the sample space (think of the other events in the partition as being $\emptyset$). Then
\be
\pro(A) = \pro (A | B) \pro(B) + \pro (A | B^c) \pro(B^c) = \bb{\frac{b - 1}{b + r - 1}} \bb{\frac{b}{b + r}} + \bb{\frac{b}{b + r - 1}}\bb{\frac r{b + r}}
= \frac b{b + r}.
\ee
This probability may be seen to be the same as the probability that the second ball is black when sampling with replacement.
\end{example}

\begin{example}\label{exa:us_voter}
A survey of US voters shows the following figures for proportions of voters registered with the main parties and the proportions of voters registered for each of the parties who declare an intention to vote for Dubya.
\begin{center}
\begin{tabular}{lcc}
& \quad Registered\quad & \quad Proportion for Bush\quad\\
\hline
Democratic & 45\% & 10\%\\
Republican & 35\% & 60\%\\
Not affiliated\quad & 20\% & 40\%
\end{tabular}
\end{center}
Suppose that a voter is chosen at random, what is the probability (s)he is a Bush voter? Here the partition of the sample space is $D$ (Democrat), $R$ (Republican) and $NA$ (No affiliation), and if $B$ is the event the voter is a Bush supporter then
\be
\pro(B) = \pro(B| D)\pro(D) + \pro(B | R)\pro(R) + \pro(B | NA)\pro(NA) = 0.1 \times 0.45 + 0.6 \times 0.35 + 0.4 \times 0.2 = 0.335.
\ee
\end{example}

\begin{theorem}[Bayes' Theorem\index{Bayes' Theorem}]\label{thm:bayes_theorem_two}
For any events $A$ and $B$, for which $\pro(A) > 0$ and $\pro(B) > 0$, we have
\be
\pro (B |A) = \frac{\pro (A | B) \pro(B)}{\pro(A)}.
\ee
\end{theorem}

\begin{example}
In Example \ref{exa:us_voter}, given that the voter chosen is a Bush supporter, what is the probability that (s)he is a Republican?
\be
\pro(R | B) = \frac{\pro (B | R) \pro(R)}{\pro(B)} = 0.6 \times 0.35/0.335 \approx 0.63.
\ee
\end{example}

Combining the Law of Total Probability with the statement in (\ref{thm:bayes_theorem_two}), gives the general statement of Bayes' Theorem:

\begin{theorem}[Bayes' Theorem\index{Bayes' Theorem}]\label{thm:bayes_theorem}
Suppose that $\{B_i\}_i$ is a partition of the sample space and that $A$ is an event for which $\pro(A) > 0$. Then for any event, $B_i$, in the partition with $\pro(B_i) > 0$, we have
\be
\pro(B_i| A) = \frac{\pro (A | B_i) \pro (B_i)}{\sum_j \pro (A | B_j) \pro (B_j)},
\ee
where the summation in the denominator extends over all $j$ for which $\pro(B_j) > 0$.
\end{theorem}

\begin{example}
Consider a diagnostic test for some disease for which the outcome of the test is either positive, +, or negative, -, and which is 99\% accurate, so that if $D$ represents the event that the patient has the disease then
\be
\pro (+ | D) = 0.99 = \pro (- | D^c).
\ee
Suppose that 0.1\% of patients have the disease. A patient is chosen at random and tests positive, what is the probability that (s)he has the disease? Then from Bayes' Theorem
\be
\pro(D | +) = \frac{\pro (+ | D) \pro(D)}{\pro (+ | D) \pro(D) + \pro (+ | D^c) \pro(D^c)} =\frac{0.99 \times 0.001}{0.99 \times 0.001 + 0.01 \times 0.999} \approx 0.09.
\ee
At first sight this conclusion, that the probability that someone testing positive has a somewhat low probability of having the disease, may be counter intuitive. It arises because the probability of a 'false positive', $\pro (+ | D^c)$, is large compared to the probability of the disease in the population, $\pro(D)$. Notice that
\be
\pro(D | +) = \frac 1{1 + (\pro(+ | D^c) \pro(D^c)/\pro (+ | D) \pro(D))}.
\ee
typically $\pro(D^c)$ and $\pro(+ | D)$ will be close to 1 so that $\pro(D | +)$ will be large or small according as the ratio $\pro(+| D^c) =\pro (D)$ is small or large. This becomes less mysterious when you consider a population of 1,000 patients with just one person suffering from the disease. Of the 999 not suffering from the disease there will be about 10 who will test positive so there will be about 11 in the population who will test positive, given that
17 the person selected has tested positive then there is about a 1 in 11 chance that the person is the one suffering from the disease. A similar calculation to the above shows that $\pro(D |-) \approx 0.000001$.
\end{example}

\begin{example}
{\em Simpson's paradox}. One example of conditional probability that appears counter-intuitive when first seen is the following situation which can arise frequently. Consider one individual chosen at random from 50 men and 50 women applicants to a particular College. Figures on the 100 applicants are given in the following table indicating whether they were educated at a state school or at an independent school and whether they were admitted or rejected.
\begin{center}
\begin{tabular}{cccc}
\quad All applicants\quad & \quad Admitted\quad & \quad Rejected \quad &\quad \% Admitted \quad\\
\hline
State & 25 & 25 & 50\% \\
Independent & 28 & 22 & 56\%
\end{tabular}
\end{center}

Note that overall the probability that an applicant is admitted is 0.53, but conditional on the candidate being from an independent school the probability is 0.56 while conditional on being from a state school the probability is lower at 0.50. Suppose that when we break down the figures for men and women we have the following figures.
\begin{center}
\begin{tabular}{cccc}
\quad Men only\quad & \quad Admitted\quad & \quad Rejected \quad &\quad \% Admitted \quad\\
\hline
State & 15 & 22 & 41\%\\
Independent & 5 & 8 & 38\%\\
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{cccc}
\quad Women only \quad & \quad Admitted\quad & \quad Rejected \quad &\quad \% Admitted \quad\\
\hline
State & 10 & 3 & 77\%\\
Independent & 23 & 14 & 62\%\\
\end{tabular}
\end{center}
It may now be seen that now for both men and women the conditional probability of being admitted is higher for state school applicants, at 0.41 and 0.77, respectively. This may seem to be a surprising result in that while the aggregate data suggests that independent school applicants have a better chance of being admitted, the individual tables for both men and women show that in both cases state school applicants have a higher acceptance rate. A 18 result of this type in tables of this sort (called contingency tables in statistics) is known as Simpson's paradox. Strictly, it is not a paradox in that it has an easy explanation. Note that overall women have a much higher acceptance rate (66\%) than men (40\%) whereas the proportion of men from state schools is 74\% with 26\% from independent schools while those proportions are reversed for women. This situation is known in statistics as confounding, it occurs when figures for two separate and different populations are aggregated to give misleading conclusions. It may be difficult or impossible to determine whether data such as that presented in the first table arise from two disparate populations and so confounding is present.

The example shows that if $A, B, C$ are three events it is possible to have the three inequalities
\be\label{equ:conditional_inequality}
\pro (A | B \cap C) > \pro (A | B \cap C^c),\quad\quad \pro (A | B^c \cap C) > \pro (A | B^c \cap C^c),\quad\quad \pro (A | C^c) > \pro (A | C),
\ee
holding simultaneously. In this example, A would be the event 'being admitted', $B$ the event 'being a man' (with $B^c$ being a woman) and $C$ being 'state school' (with $C^c$ being independent school). One situation where the three inequalities in (\ref{equ:conditional_inequality}) cannot hold simultaneously is when
\be\label{equ:conditional_equality}
\pro (B| C) = \pro (B | C^c)
\ee
To see this, suppose that (\ref{equ:conditional_inequality}) and (\ref{equ:conditional_equality}) hold, then we see that 
\be
\pro(A \cap B | C) > \pro (A \cap B | C^c),\quad\quad  \pro(A \cap B^c | C) > \pro (A \cap B^c | C^c)
\ee
and then adding and observing that, for example,
\be
\pro (A \cap B | C) + \pro (A \cap B^c | C) = \pro(A | C),
\ee
we obtain $\pro (A | C) > \pro (A | C^c)$, which contradicts (\ref{equ:conditional_inequality}). In this example it would not be possible to engineer that (\ref{equ:conditional_equality}) holds, but consider the situation where a clinical trial of a new drug for some illness is being conducted to test its e®ectiveness against a standard drug. Then $A$ would be the event that a patient recovers from the illness and $C$ would be the event that the patient receives the new drug ($C^c$ the event that (s)he receives the standard drug), again $B$ or $B^c$ would correspond to the patient being a man or woman, 
respectively. Then to avoid the sort of situation described in this example, (\ref{equ:conditional_equality}) would require that the trial be designed so that the relative proportions of men and women that receive the new drug are the same as the relative proportions that receive the standard drug. 
\end{example}

\subsection{Independence}
\begin{definition}
We say that two events $A$ and $B$ are independent\index{independent} if
\be
\pro(A \cap B) = \pro(A)\pro(B).
\ee
\end{definition}

\begin{example}
Roll a die twice and let $A$ be the event that a 1 is obtained on the first roll and $B$ be the event that a 5 is obtained on the second roll. The sample space $\Omega=\{(i, j) : 1 \leq i \leq  6, 1 \leq j \leq 6\}$ has 36 points, all of which have the same probability $\frac 1{36}$. Then $A \cap B$ is just the outcome $(1, 5)$ and $\pro(A) = \frac 16 = \pro(B)$ so we can conclude that $A$ and $B$ are independent.
\end{example}

Notice that if $\pro(B) > 0$ then $A$ and $B$ are independent if and only if $\pro(A | B) = \pro(A)$. Furthermore, if $A$ and $B$ are independent then
\ben
\item [(i)] $A$ and $B^c$ are independent,
\item [(ii)] $A^c$ and $B^c$ are independent, and
\item [(iii)] $A^c$ and $B$ are independent.
\een

For (i),
\be
\pro(A \cap B^c) = \pro(A) - \pro(A \cap B) = \pro(A) - \pro(A)\pro(B) = \pro(A)(1 - \pro(B)) = \pro(A)\pro(B^c),
\ee
and (ii) and (iii) follow from (i).

We need to generalize the notion of independence to more than two events. Events $A_1,A_2,\dots$ are independent if for all choices of $i_1 < i_2 < \dots < i_r$, we have
\be\label{equ:independent}
\pro(A_{i_1} \cap A_{i_2} \cap \dots \cap A_{i_r}) = \pro (A_{i_1}) \pro (A_{i_2}) \dots \pro (A_{i_r}).
\ee
The definition in (\ref{equ:independent}) implies that if we take any two of the events $A_i$ and $A_j$ ($i$ distinct from $j$) then they are independent, so the events are said to be pairwise independent\index{pairwise independent}. However, it should be noted that pairwise independence does not imply independence as the next example shows.

\begin{example}
Suppose that a fair coin is tossed twice (by a fair coin we mean the 4 possible outcomes $HH$, $HT$, $TH$ and $TT$ are equally likely and equal to $\frac 14$, so that a Head or a Tail on either toss has probability $\frac 12$). Let $A_1$ be the event that a head is obtained on the first toss, $A_2$ the event that there is a head on the second toss and $A_3$ the event that exactly 1 head is obtained. Then $\pro(A_i) = \frac 12$ for $i = 1, 2, 3$. It may be seen that the events are pairwise independent since, for example,
\be
\pro(A_1 \cap A_3) = \pro(A_1 \cap A^c_2) = \frac 14 = \pro(A_1)\pro(A_3),
\ee
and similarly for $A_2$ and $A_3$ (and $A_1$ and $A_2$). But
\be
\pro(A_1 \cap A_2 \cap A_3) = 0 \neq \pro(A_1)\pro(A_2)\pro(A_3) = \frac 18,
\ee
so the three events are not independent. 
\end{example}

We need to see how to model the notion of independent experiments where the outcome of one experiment does not influence the outcome of the other and see how it relates to this definition of independence. Suppose that ­$\Omega_1$ and $\Omega_2­$ are the sample spaces for two experiments with probability distributions $\pro_1$ and $\pro_2$ respectively. 

Let $\Omega­ = \Omega_1\times \Omega_2$ be the sample space corresponding to both experiments being performed, so that $\Omega­ = \{(\omega_1,\omega_2) : \omega_1 \in \Omega_1­, \omega_2 \in \Omega_­\}$. We can define the probability distribution $\pro$ on ­$\Omega$ by specifying $\pro$ for any point (singleton) of $\Omega$­ (see next section), by letting 
\be
\pro (\{(\omega_1,\omega_2)\}) = \pro_1(\{\omega_1\})\pro_2(\{\omega_2\}).
\ee

If now $A_i \subseteq \Omega_­i$, $i = 1, 2,$ and both experiments are performed we can identify the event $A_1$ with the event $A_1 \times \Omega_2$ in ­ and the event $A_2$ with the event $\Omega_1 \times A_2$ in $\Omega$­, and the intersection $A_1 \cap A_2$ with $A_1 \times A_2$, so that
\beast
\pro(A_1 \cap A_2) & = & \sum_{\omega_1\in A_1} \sum_{\omega_2\in A_2} \pro (\{\omega_1,\omega_2)\}) = \sum_{\omega_1\in A_1} \sum_{\omega_2\in A_2} \pro_1 (\{\omega_1\}) \pro_2(\{\omega_2\})\\
& = & \sum_{\omega_1\in A_1} \pro_1 (\{\omega_1\}) \sum_{\omega_2\in A_2}\pro_2(\{\omega_2\}) = \pro_1(A_1)\pro_2(A_2),
\eeast
but we then have $\pro(A_1 \cap \Omega_2) = P_1(A_1)$, and $\pro(\Omega_1 \cap A_2) = \pro_2(A_2)$, whence we can interpret this as $\pro(A_1 \cap A_2) = \pro(A_1)\pro(A_2)$ which ties in with the definition of independence given above. This extends to $n$ independent experiments in the obvious way.

\subsection{Distributions}

We are considering a finite or countable sample space $\Omega = \{\omega_i\}_i$, and for each point $\omega_i \in \Omega$­ let $p_i = \pro(\{\omega_i\})$ be the probability of the event consisting of the single point $\omega_i$. Then the sequence $\{p_i\}_i$ satisfy
\be\label{equ:probability_distribution}
p_i \geq 0,\quad\quad \text{for all }i,\quad\quad \text{ and}\quad \sum_i p_i = 1.
\ee
because ­$\Omega = \bigcup_i \{\omega_i\}$. There is a one-to-one correspondence between sequences $\{p_i\}_i$ satisfying (\ref{equ:probability_distribution}) and probability distributions as defined in this chapter through the relation $\pro(A) = \sum_{i:\omega_i \in A} p_i$. Because of this correspondence the term probability distribution\index{probability distribution} is also applied to sequences satisfying (\ref{equ:probability_distribution}). We consider a number of particular distributions:

\begin{example}[Bernoulli distribution\index{Bernoulli distribution}]
Consider a sample space with just two points $\Omega =\{H,T\}$, where the two points may be thought of as $H$ = "heads", and $T$ = "tails", so we are modelling a coin toss where we will take $p$ as the probability of heads and $1- p$ as the probability of tails. Then $p = \pro(H)$ and $1 - p = \pro (T)$, with $0 \leq p \leq 1$.
\end{example}


\begin{example}[Binomial distribution\index{Binomial distribution}]
This models the number of heads obtained in $n$ successive tosses of the coin in the previous example. Then $\Omega­ = \{0, 1, 2,\dots, n\}$ and the probability of $k$ heads is
\be\label{equ:binomial_distribution}
p_k = \pro(k) = \binom{n}{k} p^k(1 - p)^{n-k},\quad  0 \leq k \leq n.
\ee
Notice that this defines a probability distribution since, by the Binomial Theorem
\be
\sum^n_{k=0} \binom{n}{k} p^k(1 - p)^{n-k} = [p + (1 - p)]^n = 1.
\ee
To see why this corresponds to the probability of $k$ heads in $n$ tosses of the coin-let $\Omega' = \{(i_1,\dots,i_n) : i_j = H \text{ or }T\}$ be the sample space where an outcome records whether a head or a tail is obtained on each toss. Then, if the sequence $\underline{i} = (i_1,\dots i_n)$ represents
an outcome in $\Omega'$, let $N(\underline{i})$ represent the number of indices $j$ with $i_j = H$, that is, $N(\underline{i})$ is the number of heads in $\underline{i}$. By independence the probability of the outcome $\underline{i}$ is 
\be
\pro(\{\underline{i}\}) = p^{N(\ul{i})}(1 - p)^{n-N(\ul{i})}.
\ee
Now the number of sequences $\ul{i} \in \Omega'$ for which $N(\ul{i}) = k$, that is the number of sequences for which there are exactly $k$ heads, is $\binom{n}{k}$ (think of choosing the $k$ positions for the heads from the $n$ possible positions) and hence we get (\ref{equ:binomial_distribution}).
\end{example}

\begin{example}[Poisson distribution\index{Poisson distribution}]
This distribution is often used to model the number of occurrences of some event in a specified period of time, such as the number of accidents on a particular stretch of road, for example, or the number of customers who enter a particular shop. Here the probability space is $\Omega­ = \{0,1, 2,\dots\}$, the non-negative integers, and the probability of the point $k$ is
\be
p_k = \pro(k) = e^{-\lm} \frac{\lm^k}{k!},\quad k = 0,1,2,\dots
\ee
for some fixed $\lm> 0$. Check that
\be
\sum^\infty_{k=0} p_k = e^{-\lm}\sum^\infty_{k=0} \frac{\lm^k}{k!} = e^{-\lm} e^\lm = 1.
\ee
\end{example}

Suppose that we consider customers entering a shop during an hour-long period, $(0,1]$. Think of dividing the period into $n$ segments, $((i - 1)/n, i/n]$, for $i = 1,\dots, n$, and suppose that 1 customer enters the shop in each segment with probability $p$, $0 < p < 1$. Then the probability that $k$ customers enter in the hour is the binomial probability
\be
\binom{n}{k} p^k (1- p)^{n-k} = \frac{n!}{k!(n - k)!} p^k (1 - p)^{n-k}.
\ee

Now suppose that $n \to \infty$ and $p \to 0$ in such a way that $np \to \lm$, then we have, for each fixed $k$,
\be
\lim_{n\to\infty} \bsb{\frac{n!}{k!(n - k)!} p^k (1 - p)^{n-k}} = \lim_{n\to \infty} \bsb{\bb{1-\frac{\lm}n}^{n-k} \frac{(np)^k}{k!} \frac{n!}{n^k(n- k)!}}
= e^{-\lm} \frac{\lm^k}{k!},
\ee
because
\be
\lim_{n\to \infty} \bsb{\frac{n!}{n^k(n- k)!}} = \lim_{n\to \infty}\bsb{1\bb{1-\frac 1n} \dots \bb{1-\frac{k - 1}n}} = 1.
\ee
This has proved the following result.

\begin{theorem}[Poisson approximation to the binomial]
Suppose that $n \to \infty$ and $p \to 0$ so that $np \to \lm$, then
\be
\binom{n}{k} p^k (1 - p)^{n-k} \to e^{-\lm} \frac{\lm^k}{k!},\quad\quad k = 0, 1, 2,\dots
\ee
\end{theorem}

\begin{example}[Geometric distribution\index{Geometric distribution}]
This is a distribution which models the number of tosses of a coin required to obtain the first occurrence of a head, when $p$, $0 < p < 1$, is the probability of a head on each toss, for the probability space $\Omega = \{1, 2,\dots\}$ we have 
\be
p_k = \pro(k) = p (1 - p)^{k-1},\quad\quad k = 1, 2,\dots
\ee
Check that $\sum^\infty_{k=1} p_k = p/ (1 - (1 - p)) = 1$. Note that the term geometric distribution is also applied to the distribution on the probability space $\Omega­ = \{0,1,2,\dots\}$ with $p_k = p(1 - p)^k$, for $k \geq 0$, this would be modelling the number of tails before first obtaining a head, but
no confusion should arise between the slightly different usage of the term.
\end{example}

\begin{example}[Hypergeometric distribution\index{Hypergeometric distribution}]
Consider an urn with $n_1$ red balls and $n_2$ black balls of which $n$ are drawn without replacement, $n \leq n_1 + n_2$. The probability that there are exactly $k$ red balls drawn is
\be
p_k = \frac{\binom{n_1}{k} \binom{n_2}{n- k}}{\binom{n_1 + n_2}{n}},\quad\quad \text{ for }\max(0, n- n_2) \leq k \leq \min(n, n_1).
\ee
For example, the probability that there are exactly 5 hearts in a bridge hand is
\be
\frac{\binom{13}{5} \binom{39}{8}}{\binom{52}{13}}.
\ee
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discrete Random Variables}

\subsection{Introduction}

When an experiment is conducted there may be a number of quantities associated with the outcome $\omega\in\Omega$­ that may be of interest. Suppose that the experiment is choosing a male student at random from the audience of the IA Probability lecture - there are many different measurements, or attributes, of the person chosen that may be of interest: his height, his weight, his IQ, the colour of his eyes, etc. Rather than think of each of these as the outcome of a separate experiment it is more useful to view them as functions of the outcome $\omega$. This leads to the following definition which is a central notion of probability.

\begin{definition}
A random variable $X$, taking values in a set $S$, is a function $X : \Omega \to S$.
\end{definition}

Typically, $S$ may be a subset of the real numbers, $\R$, as would be the case if the height of the student was of interest, or it could be a subset of $\R^k$, if more than one measurement is made on the subject as would be the case, with $k = 2$, if height and weight are measured, or, $S$ could be some arbitrary set such as $S = \{\text{Blue,Green,Brown}\}$, say, if it is the colour of the subject's eyes that are to be recorded. The most frequent situation that we will encounter is the case when $S \subseteq \R$, and $X$ is then said to be a real-valued random variable. Denote by $\Omega_­X$ the range of $X$, so that ­$\Omega_X = \{X(\omega) : \omega\in\Omega\}$. In this chapter we will assume that the sample space ­$\Omega$ is either a finite or a countable set, so that $\Omega_X$ is finite or countable.

For $T \subseteq S$, we denote the event $\{\omega: X(\omega) \in T\}$ as $\{X \in T\}$, so that the dependence of $X$ on $\omega$ is suppressed in the notation. Suppose that we enumerate the points in ­$\Omega_X$ (equivalently the values taken on by $X$), so that $\Omega_X = \{x_j : j \in J\}$, then we write the event $\{\omega : X(\omega) = x_j\} = \{X = x_j\}$. If we let $p_j = \pro(X = x_j)$, $j \in J$, then $\{p_j : j \in J\}$ is a probability distribution on the space $­\Omega_X$, and is referred to as the probability distribution of the random variable $X$. Note that it is a probability distribution on the set $­$X, not on the underlying sample space $\Omega$.

\begin{example}
Suppose that two standard dice are rolled so that the sample space is $\Omega­ = \{(i, j) : 1 \leq i, j \leq 6\}$, and we are interested in the sum of the numbers shown so that the random variable $X : \Omega\to \R$ is given by $X(i, j) = i + j$. The probability of each point in $\Omega$­ is $\frac 1{36}$ with the set of possible values taken on by $X$ being $\Omega_X = \{2,3,\dots,12\}$ and, for example,
\be
\pro(X = 6) = \pro (\{(1, 5), (2, 4), (3, 3), (4, 2), (5, 1)\}) = \frac{5}{36}.
\ee
If we set $p_j = \pro(X = j)$, for $j = 2,\dots, 12$, then the table
\begin{center}
\begin{tabular}{ccccccccccccc}
$j$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12\\
\hline
$p_j$ & $\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ & $\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ & $\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$ \\
\end{tabular}
\end{center}
gives the full probability distribution of the random variable $X$.
\end{example}


Terminology. If the probability distribution of $X$ is a standard distribution such as the binomial distribution (or Poisson, or geometric), we say that $X$ is a binomial (respectively, Poisson, or geometric) random variable. We often write $X \sim \text{Bin}(n, p)$, for example, for the statement that $X$ is binomial distribution where the parameters are $n$ and $p$, or $X \sim \text{Poi}(\lm)$ for a Poisson random variable with parameter $\lm$.

\begin{example}
Suppose that a coin is tossed $n$ times and a 1 is recorded whenever a head occurs and a 0 is recorded for each tail. Then ­$\Omega = \{(i_1, i_2, \dots, i_n) : i_j = 1\text{ or }0\}$. If $p$ is the probability of a head and tosses are independent then the probability on ­$\Omega$ is specified by 
\be
\pro(i_1,i_2,\dots, i_n) = p^{i_1+\dots+i_n}(1 - p)^{n-i_1-\dots-i_n}.
\ee
Let $X$ denote the number of heads obtained, so that $X(i_1,\dots,i_n) = i_1 + \dots + i_n$, then $X$ is a binomial random variable since the distribution of $X$ is given by
\be
\pro(X = k) = \binom{n}{k} p^k(1 - p)^{n-k},\quad\quad  0 \leq k \leq n.
\ee
\end{example}

For a function $g : S \to T$, mapping from the set $S$ to the set $T$, then if $X$ is a random variable taking values in $S$, $g(X)$ is the random variable taking values in $T$, with $g(X) : \Omega \to T$ specified by $g(X)(\omega) = g(X(\omega))$. For subsets $C \subseteq T$ we have
\be
\pro(g(X) \in C) = \pro\bb{X\in g^{-1}(C)}
\ee
and the distribution of $g(X)$ may be obtained from that of $X$ by observing that
\be
\pro(g(X) = y) = \pro\bb{X\in g^{-1}(y)} = \sum_{x\in g^{-1}(y)} \pro(X = x).
\ee
A real-valued random variable which takes on just the two values 0 and 1 is known as an indicator random variable\index{indicator random variable}, suppose that the event on which it takes the value 1 is $A \subseteq \Omega$­ then the random variable is denoted by $I_A$, so that
\be
I_A(\omega) = \left\{ \ba{ll}
1 \quad\quad & \omega\in A,\\
0 & \omega \notin A,
\ea\right.
\ee
and $I_A$ is 1 or 0 according as the event $A$ occurs or does not occur. The following properties of indicator random variables should be noted for events $A$ and $B$:
\ben
\item [1.] $I_{A^c} = 1 - I_A$.
\item [2.] $I_{A\cap B} = I_AI_B$.
\item [3.] $I_{A\cup B} = 1- (1 - I_A) (1 - I_B)$.
\een
and, for events $A_1,A_2,\dots,A_n$, Properties 2 and 3 generalize to $I_{A_1\cap A_2\cap \dots\cap A_n} = \prod^n_{i=1} I_{A_i}$, and
\beast
I_{A_1\cup A_2\cup \dots \cup A_n} & = & 1 - \prod^n_{i=1} (1 - I_{A_i})\\
& = & \sum_i I_{A_i} - \sum_{i_1<i_2} I_{A_{i_1}} I_{A_{i_2}} + \sum_{i_1<i_2<i_3} I_{A_{i_1}} I_{A_{i_2}}I_{A_{i_3}} - \dots + (-1)^{n-1} I_{A_1}\dots I_{A_n} \\
& = & \sum_i I_{A_i} - \sum_{i_1<i_2} I_{A_{i_1}\cap A_{i_2}} + \sum_{i_1<i_2<i_3} I_{A_{i_1}\cap A_{i_2}\cap A_{i_3}} - \dots + (-1)^{n-1} I_{A_1 \cap \dots \cap A_n}.
\eeast
In the next section we see how this last relation provides an alternate proof of the inclusion-exclusion formula.

\subsection{Expectation, variance and covariance}

From now on, unless we indicate to the contrary, the random variables we will consider will take real values. For a non-negative random variable $X$, that is one for which $X(\omega) > 0$ for all $\omega \in\Omega$­, (usually just written as $X \geq 0$), we define the expectation (or expected value or mean value) of $X$ to be
\be
\E X = \sum_{\omega \in \Omega} X(\omega)\pro(\{\omega\}),
\ee
since all the terms in the sum are non-negative the sum is well defined (although it may take the value $+\infty$). Note that, since ­$\Omega = \bigcup_{x\in \Omega_­X}\{X = x\}$, we have a more useful form for the expectation given by
\be
\E X = \sum_{\omega \in \Omega} X(\omega)\pro(\{\omega\}) = \sum_{x\in \Omega_X} \sum_{\omega\in \{X=x\}} X(\omega)\pro(\{\omega\}) = \sum_{x\in \Omega_X} x\pro(X = x).
\ee
Thus the expectation is the average of the values taken on by the random variable, averaged with weights corresponding to the probabilities of the values.

\begin{example}
Suppose that $X \sim \text{Bin}(n, p)$, so that $P (X = k) = \binom{n}{k} p^k(1 - p)^{n-k}$, for $0 \leq k \leq n$, then
\beast
\E X & = & \sum^n_{k=0} k \binom{n}{k} p^k(1 - p)^{n-k} = p \sum^n_{k=1} \frac{n!}{(n- k)!(k - 1)!} p^{k-1}(1 - p)^{n-k}\\
& = & np \sum^n_{k=1} \binom{n - 1}{k - 1} p^{k-1} (1 -p)^{n-k} = np [p + (1 - p)]^{n-1} = np.
\eeast
\end{example}


\begin{example}
Suppose that $X \sim \text{Poi}(\lm)$, so that $\pro(X = k) = e^{-\lm} \frac{\lm^k}{k!}$, for $k = 0,1,2,\dots$, then
\be
\E X = \sum^\infty_{k=0} ke^{-\lm} \frac{\lm^k}{k!} = \lm e^{-\lm} \sum^\infty_{k=1} \frac{\lm^{k-1}}{(k - 1)!} = \lm e^{-\lm} e^\lm = \lm.
\ee
\end{example}

For any random variable $X$ denote by $X^+ = \max(X, 0)$, the positive part of $X$, and $X^- = \max(-X, 0)$, the negative part of $X$ are non-negative random variables, for which so that $X = X^+ - X^-$ and $\abs{X} = X^+ + X^-$. Provided not both $\E X^+ = \infty$ and $\E X^- = \infty$, we define the expectation of $X$ to be 
\be
\E X = \E X^+ - \E X^- = \sum_{x\in \Omega_X} x\pro(X = x),
\ee

if both $\E X^+$ and $\E X^-$ are infinite then the expectation of $X$ is not defined. In the following, when we write $\E X$ for a random variable $X$, it may be assumed that the expectation of $X$ is well defined.

Properties of $\E X$
\ben
\item [1.] If $X \geq 0$, then $\E X \geq 0$, and $\E X = 0$ implies that $P(X = 0) = 1$.
\item [2.] If $c$ is a constant then $\E (cX) = c\E X$, and $\E c = c$.
\item [3.] For random variables $X$ and $Y$, $\E (X + Y) = \E X + \E Y$.

Properties 2 and 3 show the important property that the operator $\E(\cdot)$ is a linear operator and they generalize, by induction, to the case of random variables $X_1,\dots,X_n$ and constants $c_1,\dots,c_n$ so that $\E\bb{\sum^n_{i=1} c_iX_i} = \sum^n_{i=1} c_i\E X_i$.

\item [4.] $\E g(X) = \sum_{x\in \Omega_X} g(x)\pro(X = x)$.

To see this, let $Y = g(X)$, then
\beast
\E g(X) = \E Y & = & \sum_{y\in \Omega_Y} y \pro(Y = y) = \sum_{y\in \Omega_Y} y\bb{\sum_{x\in g^{-1}(y)} \pro(X = x)} \\
& = & \sum_{y \in \Omega_Y} \sum_{x\in g^{-1}(y)} y\pro (X = x) = \sum_{y\in \Omega_Y} \sum_{x\in g^{-1}(y)} g(x)\pro(X = x)\\
& = & \sum_{x\in \Omega_X} g(x)\pro(X = x).
\eeast

\item [5.] For the indicator of any event $A \subseteq \Omega$­ we have $\E I_A = \pro(A)$.
\item [6.] If $X \geq 0$ and $X$ takes integer values, then $\E X =\sum^\infty_{n=1} \pro(X \geq n)$.

\begin{proof}[\bf Proof]
We have
\be
\E X = \sum^\infty_{k=1} k\pro(X = k) = \sum^\infty_{k=1} \sum^k_{n=1} \pro(X = k) = \sum^\infty_{n=1} \sum^\infty_{k=n} \pro(X = k) = \sum^\infty_{n=1} \pro(X \geq n),
\ee
after interchanging the order of the summations.
\end{proof}
\een

Terminology. For a random variable $X$, the expected values of powers of $X$ are known as moments of $X$, thus $\E (X^r)$ (assuming it is well defined) is the $r$th moment of $X$ and $\E (\abs{X}^r)$ is the $r$th absolute moment of $X$.

\begin{example}
Another proof of inclusion-exclusion. For events $A_1,\dots,A_n$, use the previous expression for the product of indicators to calculate
\beast
\pro(A_1 \cup \dots \cup A_n) & = & \E (I_{A_1\cup A_2\cup \dots \cup A_n}) = \E\bb{1 - \prod^n_{i=1} (1 - I_{A_i})}\\
& = & \E\bb{\sum_i I_{A_i} - \sum_{i_1<i_2} I_{A_{i_1}\cap A_{i_2}} + \sum_{i_1<i_2<i_3} I_{A_{i_1}\cap A_{i_2} \cap A_{i_3}} - \dots + (-1)^{n-1} I_{A_1\cap \dots\cap A_n}}
\eeast
then using the linearity of the expectation, this
\beast
& = & \sum_i \E (I_{A_i}) - \sum_{i_1<i_2} \E\bb{I_{A_{i_1}\cap A_{i_2}}} + \dots + (-1)^{n-1}\E \bb{I_{A_1\cap \dots \cap A_n}}\\
& = & \sum_i \pro(A_i) - \sum_{i_1<i_2} \pro\bb{A_{i_1} \cap A_{i_2}} + \dots + (-1)^{n-1} \pro(A_1 \cap \dots \cap A_n),
\eeast
which is the required expression for the inclusion-exclusion formula.
\end{example}

For any random variable, $X$ with finite mean, the variance is defined to be
\be
\var (X) = \E (X - \E X)^2,
\ee
and it is a measure of how much the distribution of $X$ is spread out around the mean, the smaller the distribution the more the distribution of $X$ is concentrated close to $\E X$. The quantity $\sqrt{\var (X)}$ is known as the standard deviation of $X$. When we use the notation $\var (X)$ we will assume implicitly that it is a finite quantity. 

Properties of $\var (X)$
\ben
\item [1.] $\var (X) = \E X^2 - (\E X)^2$.
\begin{proof}[\bf Proof]
We have, using Properties 2 and 3 of the expectation, 
\be
\E (X - \E X)^2 = \E \bb{X^2 - 2X\E X + (\E X)^2} = \E X^2 - 2\E X \E X + (\E X)^2 = \E X^2 - (\E X)^2.
\ee
\end{proof}
\item [2.] If $c$ is a constant, $\var (cX) = c^2 \var (X)$.
\item [3.] If $c$ is a constant, $\var (X + c) = \var (X)$.
\item [4.] $\var (X) \geq 0$, and $\var (X) = 0$ if and only if $\pro(X = c) = 1$, for some constant $c$.
\item [5.] The expression $\E (X - c)^2$ is minimized over constants $c$ when $c = \E X$, so that $\E (X - c)^2 \geq \var (X)$, for all $c$, with equality when $c = \E X$.
\begin{proof}[\bf Proof]
Expand out the expression
\be
\E (X - c)^2 = \E \bb{X^2 - 2cX + c^2} = \E X^2 - 2c\E X + c^2,
\ee
and minimize the right-hand side in $c$ to see that the minimum occurs at $c = \E X$.
\end{proof}
\een

\begin{example}
For $X \sim \text{Bin}(n,p)$, we have
\beast
\E (X(X - 1)) & = & \sum^n_{k=0} k(k -1) \binom{n}{k} p^k(1 - p)^{n-k} = \sum^n_{k=2} \frac{n!}{(k - 2)!(n - k)!}p^k(1 - p)^{n-k}\\
& = & n(n - 1)p^2\sum^{n-2}_{r=0}\binom{n- 2}{r} p^r(1- p)^{n-2-r} = n(n - 1)p^2,
\eeast
then it follows that
\be
\E X^2 = \E (X(X - 1)) + \E X = n(n - 1)p^2 + np,
\ee
since we had seen that $\E X = np$, hence $\var (X) = \E X^2 - (\E X)^2 = np(1 - p)$.
\end{example}

\begin{example}
Suppose that $X \sim \text{Poi}(\lm)$, then a similar calculation to that in the previous Example gives
\be
E (X(X - 1)) = \sum^\infty_{k=0} k(k- 1)e^{-\lm}\frac{\lm^k}{k!} = \sum^\infty_{k=2} e^{-\lm} \frac{\lm^k}{(k - 2)!} = \lm^2e^{-\lm} \sum^\infty_{r=0} \frac{\lm^r}{r!} = \lm^2,
\ee
recalling that in this case $\E X = \lm$, we have $\E X^2 = \lm^2 + \lm$, so that $\var (X) = \lm$, showing that for a Poisson random variable the mean is the same as the variance.
\end{example}


\begin{example}
Use of indicators Return to the situation, considered in Chapter 2, where $n$ students leave their $n$ coats outside the lecture room and when they leave they pick up their coats at random. Let $N$ be the number of students who get their own coat, then $N = \sum^n_{i=1} I_{A_i}$, where $A_i$ is the event that student $i$ obtains his own coat. It follows that
\be
\E N = \E\bb{\sum^n_{i=1} I_{A_i}} = \sum^n_{i=1} \E (I_{A_i}) = \sum^n_{i=1} \pro (A_i) = \sum^n_{i=1} \frac 1n = 1,
\ee
\be
\E N^2 = \E\bb{\sum^n_{i=1} I_{A_i}}^2 = \E\bb{\sum^n_{i=1} (I_{A_i})^2 + \sum_i\sum_{j\neq i} I_{A_i}I_{A_j}},
\ee
since $I_{A_i}I_{A_j} = I_{A_i\cap A_j}$ we have $(I_{A_i})^2 = I_{A_i}$, and we see that
\beast
\E N^2 & = & \E\bb{\sum^n_{i=1} I_{A_i} + \sum_i\sum_{j\neq i} I_{A_i\cap A_j}} = \sum^n_{i=1} \pro(A_i) + \sum_i\sum_{j\neq i} \pro(A_i \cap A_j)\\
& = & \sum^n_{i=1} \frac 1n + \sum_i\sum_{j\neq i} \frac 1{n(n -1)} = n \times \frac 1n + n(n - 1) \times \frac 1{n(n- 1)} = 1 + 1 = 2.
\eeast

That gives $\var (N) = \E N^2 - (\E N)^2 = 1$. The fact that the mean and the variance are both the same might suggest that the distribution of the random variable $N$ is close to being Poisson (with mean $\lm = 1$) as is indeed the case when $n$ is large. If we let $p_n = \pro(N = 0)$, the probability that when there are $n$ students none of them gets his own coat, then we have seen previously (using inclusion-exclusion) that
\be
p_n = 1 - \frac 1{1!} + \frac 1{2!} - \frac 1{3!} + \dots + (-1)^n \frac 1{n!} \to e^{-1},\quad\text{ as }n \to \infty,
\ee
take $p_0 = 1$. The probability that exactly $k$ students get their own coats is
\be
\pro(N = k) = \binom{n}{k} \frac 1{n!} ((n - k)!)p_{n-k} = \frac 1{k!}p_{n-k} \to \frac 1{k!} e^{-1},\quad \text{ as }n \to \infty,
\ee
showing that the distribution of $N$ is approximately Poisson. 
\end{example}

\begin{theorem}[Cauchy-Schwarz inequality\index{Cauchy-Schwarz inequality}] For any random variables $X$ and $Y$,
\be
(\E (XY ))^2 \leq \E \bb{X^2}\E\bb{Y^2},
\ee
if $\E(Y^2) > 0$, equality occurs if and only if $X = aY$ for some constant $a \in \R$.
\end{theorem}
\begin{proof}[\bf Proof]
For any $a \in \R$, observe that $\E (X - aY)^2 \geq 0$, so that
\be
0 \leq \E (X^2 - 2aXY + a^2Y^2) = \E(X^2) - 2a\E (XY) + a^2\E(Y^2),
\ee
showing that the quadratic in $a$ on the right-hand side has at most one real root, whence the discriminant 
\be
4\bb{(\E (XY ))^2 - \E(X^2) \E(Y^2)} \leq 0,
\ee
giving the inequality. There is clearly equality if $X = aY$ for some $a \in \R$, whereas if $\E(Y^2) > 0$ and the discriminant is 0 then the quadratic is 0 for $a = \E (XY ) / \E(Y^2)$, and for that value of $a$, $\E (X - aY)^2 = 0$ and so $X = aY$. Of course, if $\E(Y^2)= 0$ then $Y = 0$ and equality occurs.
\end{proof}

For two random variable $X$ and $Y$, we define the covariance between $X$ and $Y$ as
\be
\cov (X, Y ) = \E ((X - \E X) (Y - \E Y )) .
\ee
We shall see that this is a measure of the dependence between the random variables $X$ and $Y$.

Properties of $\cov (X, Y)$
\ben
\item [1.] $\cov (X,Y ) = \cov (Y,X)$.
\item [2.] $\cov (X,Y ) = \E (XY ) - (\E X) (\E Y)$.
\begin{proof}[\bf Proof]
We have
\beast
\cov (X,Y ) & = & \E (XY - X(\E Y) - Y (\E X) + (\E X)(\E Y ))\\
& = & \E(XY ) - (\E X)(\E Y ) - (\E X)(\E Y ) + (\E X)(\E Y )\\
& = & \E (XY ) - (\E X) (\E Y).
\eeast
\end{proof}

\item [3.] $\cov (X,X) = \var (X)$.
\item [4.] $\var (X + Y ) = \var (X) + \var (Y ) + 2\cov (X, Y )$.
\begin{proof}[\bf Proof]
We have
\beast
\var (X + Y ) & = & \E (X + Y - \E X - \E Y )^2 = \E ((X - \E X) + (Y - \E Y ))^2\\
& = & \E\bb{(X - \E X)^2 + (Y - \E Y )^2 + 2 (X - \E X) (Y - \E Y)}\\
& = & \E (X - \E X)^2 + \E (Y - \E Y )^2 + 2\E (X - \E X) (Y - \E Y).
\eeast
\end{proof}

\item [5.] If $c$ is a constant, $\cov (X, c) = 0$.
\item [6.] If $c$ is a constant, $\cov (X + c,Y ) = \cov (X, Y )$.
\item [7.] If $c$ is a constant, $\cov (cX, Y ) = c\cov (X, Y )$.
\item [8.] $\cov (X + Z, Y ) = \cov (X, Y ) + \cov (Z, Y )$.
These last two generalize to the case of random variables $X_1,\dots,X_n$ and $Y_1,\dots,Y_n$ and constants $c_1,\dots,c_n$ and $d_1,\dots,d_n$ to give, by induction,
\be
\cov \bb{\sum^n_{i=1}c_iX_i,\sum^n_{j=1} d_jY_j} = \sum^n_{i=1}\sum^n_{j=1} c_id_j\cov (X_i,Y_j).
\ee
Using the fact that $\var (X) = \cov (X,X)$, we see that a special case of this is 
\be\label{equ:var_cov}
\var\bb{\sum^n_{i=1} X_i} = \sum^n_{i=1} \var (X_i) + \sum^n_{i=1} \sum_{j\neq i} \cov (X_i,X_j),
\ee
for any random variables $X_1,\dots,X_n$.
\een

\begin{definition}
The correlation coefficient\index{correlation coefficient} (or just the correlation) between random variables $X$ and $Y$ with $\var (X) > 0$ and $\var (Y ) > 0$ is
\be
\corr (X,Y ) = \frac{\cov (X, Y )}{\sqrt{\var (X)\var (Y )}}.
\ee
\end{definition}

Notice that by the Cauchy-Schwarz inequality
\be
\abs{\corr (X, Y)} \leq 1,\quad \quad\text{for all }X\text{ and }Y,
\ee
this follows by applying the inequality to the random variables $\bar{X} = X - \E X$ and $\bar{Y} = Y - \E Y$. It may be further seen that $\abs{\corr (X, Y )} = 1$ if and only if $X = aY +b$ for some constants $a$ and $b$. One property of correlation that we should note is that for constants $a$, $b$, $c$ and $d$ with $ac \neq 0$, we have
\be
\corr (aX + b, cY + d) = \left\{\ba{ll}
\corr(X, Y )\quad\quad & ac > 0,\\
\corr(X, Y )& ac < 0.
\ea\right.
\ee

This follows easily from the definition of correlation and the properties of the covariance and variance, notice that when $ac = 0$, $\cov (aX + b, cY + d) = 0$, and the correlation is not defined because at least one of $\var (aX + b) = 0$ or $\var (cY + d) = 0$.

Notice that one consequence of this fact is that the correlation between two random variables is scale invariant - if we multiply the observation of $X$ and $Y$ by positive constants we do not alter the correlation.

\subsection{Independence}

\begin{definition}
Discrete random variables $X_1,X_2,\dots,X_n$ are independent\index{independent} if, for all choices of $x_i \in \Omega_{X_i}$, $1 \leq i \leq n$, we have
\be\label{equ:discrete_independent}
\pro (X_1 = x_1,X_2 = x_2,\dots,X_n = x_n) = \prod^n_{i=1}\pro(X_i = x_i).
\ee
\end{definition}

Notice that $X_1,X_2,\dots,X_n$ are independent if and only if, for all choices of subsets $S_i \subseteq \Omega_{­X_i}$, $1 \leq i \leq n$, we have
\be\label{equ:discrete_independent_s}
\pro(X_1 \in S_1,X_2 \in S_2,\dots,X_n \in S_n) = \prod^n_{i=1} \pro(X_i \in S_i).
\ee
To see this, if (\ref{equ:discrete_independent_s}) holds, take $S_i = \{x_i\}$ for each $i$ and we see that (\ref{equ:discrete_independent}) is true,
conversely, the left hand side of (\ref{equ:discrete_independent_s}) is 
\be
\sum_{x_1\in S_1} \sum_{x_2\in S_2} \dots \sum_{x_n\in S_n} \pro (X_1 = x_1,X_2 = x_2,\dots,X_n = x_n)
\ee
and we see that, if (\ref{equ:discrete_independent}) holds, then this is expression is
\be
\sum_{x_1\in S_1}\sum_{x_2\in S_2} \dots \sum_{x_n\in S_n} \prod^n_{i=1} \pro (X_i = x_i) = \prod^n_{i=1} \bb{\sum_{x_i\in S_i} \pro (X_i = x_i)} = \prod^n_{i=1} \pro(X_i \in S_i),
\ee
which gives (\ref{equ:discrete_independent_s}).

Notice that events $A_1,\dots,A_n$ are independent, as defined in the previous chapter, if and only if their indicator random variables $I_{A_1},\dots, I_{A_n}$ are independent random variables. Observe also that if random variables are independent then they are independent in pairs (this follows by taking $S_i = \Omega_{X_i}$ for all but two of the subsets $S_i$ in (\ref{equ:discrete_independent_s})) - they are said to be pairwise independent, a similar argument shows that if any collection of random variables is independent then any sub-collection of them is independent. By considering indicators, the example from the last chapter shows that pairwise independence of random variables does not imply independence in general.

Properties of independent random variables
\ben
\item [1.] If $X_1,\dots,X_n$ are independent random variables and $g_i : \R \to\R$, $1 \leq i \leq n$, are functions then $g_1(X_1),\dots, g_n(X_n)$ are independent random variables.
\begin{proof}[\bf Proof]
For $y_i \in \Omega_{g_i(X_i)}$, $1 \leq i \leq n$, we have
\beast
\pro\bb{g_1(X_1) = y1,\dots,g_n(X_n) = y_n} & = & \pro\bb{X_1\in g^{-1}_1(y_1),\dots,X_n \in g^{-1}_n (y_n)} \\
& = & \prod^n_{i=1} \pro\bb{X_i \in g^{-1}_i (y_i)} = \prod^n_{i=1} \pro\bb{g_i(X_i) = y_i}
\eeast
after using (\ref{equ:discrete_independent_s}), showing that the random variables $g_1(X_1),\dots,g_n(X_n)$ are independent.
\end{proof}

\item [2.] If $X_1,\dots,X_n$ are independent random variables, then
\be
\E \bb{\prod^n_{i=1} X_i} = \prod^n_{i=1} \E (X_i),
\ee
that is, the expectation of the product of independent random variables is the product of their expectations.

\begin{proof}[\bf Proof]
In a similar way to the previous proof, we may represent the event
\be
\bb{\prod^n_{i=1} X_i = y} = \bigcup \bb{X_1 = x_1, X_2 = x_2,\dots,X_n = x_n}
\ee
as a disjoint union of events over values of $x_1,\dots,x_n$ with $\prod_i x_i = y$. Then
\beast
\E\bb{\prod^n_{i=1} X_i} & = & \sum_y y\pro \bb{\prod^n_{i=1} X_i = y} = \sum_y y\sum_{x_i:\prod_i x_i=y} \pro (X_1 = x_1,\dots,X_n = x_n)\\
& = & \sum_y \sum_{x_i:\prod_i x_i=y} y \prod^n_{i=1} \pro (X_i = x_i), \quad\quad \text{ by independence}\\
& = & \sum_{x_1,\dots,x_n} \prod^n_{i=1} (x_i\pro (X_i = x_i)) = \prod^n_{i=1} \bb{\sum_{x_i} x_i\pro (X_i = x_i)} = \prod^n_{i=1} \E (X_i),
\eeast
as required.
\end{proof}

\item [3.] If $X$ and $Y$ are independent random variables then $\cov (X, Y ) = 0$ (and hence $\corr (X, Y ) = 0$). The converse is not true in general (see Example \ref{equ:cov_notto_independent} below): that is, $\cov (X,Y ) = 0$ does not imply that $X$ and $Y$ are independent.

\begin{proof}[\bf Proof]
Property 1 shows that $X - \E X$ and $Y - \E Y$ are independent random variables and then by Property 2,
\be
\cov (X, Y ) = \E ((X - \E X) (Y - \E Y )) = \E (X - \E X) \E (Y - \E Y ) = 0
\ee
since $E (X - \E X) = \E(X) - \E(X) = 0$ (and similarly $\E (Y - \E Y ) = 0$).
\end{proof}

\item [4.] If $X_1,\dots,X_n$ are independent random variables then
\be
\var\bb{\sum^n_{i=1} X_i} = \sum^n_{i=1} \var (X_i),
\ee
that is, the variance of the sum of independent random variables is the sum of their variances.

\begin{proof}[\bf Proof]
Use Property 3 to see that for $j \neq i$, $\cov (X_i,X_j) = 0$ and the result follows from the relation (\ref{equ:var_cov}).
\end{proof}

\item [5.] If $X_1,\dots,X_n$ are independent random variables then the conditional probability
\be
\pro(X_1 = x_1,\dots,X_{n-1} = x_{n-1}| X_n = x_n) = \pro (X_1= x_1,\dots, X_{n-1} = x_{n-1}),
\ee
for all choices of $x_i \in \Omega_{X_i}$ , $1 \leq i \leq n$.

\begin{proof}[\bf Proof]
We have the conditional probability on the left-hand side is
\be
\frac{\pro (X_1= x_1,\dots,X_n = x_n)}{\pro (X_n = x_n)} = \frac{\prod^n_{i=1} \pro (X_i = x_i)}{\pro(X_n = x_n)} = \prod^{n-1}_{i=1} \pro(X_i = x_i),
\ee
which equals the right-hand side, again by independence.
\end{proof}
\een

\begin{flushleft}Terminology\end{flushleft}Random variables with the same distribution are usually said to be identically distributed, and if they are also independent they are i.i.d. (independent and identically distributed\index{independent and identically distributed}). If $X_1,\dots,X_n$ are i.i.d. then, from Property 4,
\be
\var\bb{\frac{X_1 + \dots + X_n}n} = \frac{\var (X_1)}n.
\ee

\begin{example}\label{equ:cov_notto_independent} 
Covariance equal to 0 does not imply independence. Suppose that $X$ is a random variable with distribution determined by
\begin{center}
\begin{tabular}{ccccc}
$x$ & 2 & 1 & -1 & -2 \\
\hline
$\pro(X = x)$ & $\frac 14$ & $\frac 14$ & $\frac 14$ & $\frac 14$
\end{tabular}
\end{center}
and let $Y = X^2$. Then $\E X = 0$ and $\E(X^3) = 0$ so that $\cov (X, Y ) = \E(X^3) = 0$, but
\be
\pro(X = 2, Y = 4) = \frac 14 \neq \pro (X = 2) \pro (Y = 4) = \frac 14\times \frac 12,
\ee
so that $X$ and $Y$ are not independent.
\end{example}

\begin{example} Efron's dice An interesting example showing that odds are not transitive is given by a set of 4 dice with the following faces:

\centertexdraw{
    
    \def\bdot {\fcir f:0 r:0.025 }
    
    \drawdim in

    \arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
    \linewd 0.01 \setgray 0 
  
    \move (0 0) \lvec(1.2 0)\lvec(1.2 0.3) \lvec(0 0.3) \lvec(0 0)
    \move (0.6 0.6) \lvec(0.9 0.6) \lvec(0.9 -0.3)\lvec(0.6 -0.3)\lvec(0.6 0.6)
    \move (0.3 0)\lvec(0.3 0.3)
    \htext (0.1 0.1){4}
    \htext (0.4 0.1){4}
    \htext (0.7 0.1){0}
    \htext (1.0 0.1){0}
    \htext (0.7 0.4){4}
    \htext (0.7 -0.2){4}

\htext (0.5 0.7){A}

    \move (1.5 0) \lvec(2.7 0)\lvec(2.7 0.3) \lvec(1.5 0.3) \lvec(1.5 0)
    \move (2.1 0.6) \lvec(2.4 0.6) \lvec(2.4 -0.3)\lvec(2.1 -0.3)\lvec(2.1 0.6)
    \move (1.8 0)\lvec(1.8 0.3)
    \htext (1.6 0.1){3}
    \htext (1.9 0.1){3}
    \htext (2.2 0.1){3}
    \htext (2.5 0.1){3}
    \htext (2.2 0.4){3}
    \htext (2.2 -0.2){3}

\htext (2 0.7){B}

    \move (3 0) \lvec(4.2 0)\lvec(4.2 0.3) \lvec(3 0.3) \lvec(3 0)
    \move (3.6 0.6) \lvec(3.9 0.6) \lvec(3.9 -0.3)\lvec(3.6 -0.3)\lvec(3.6 0.6)
    \move (3.3 0)\lvec(3.3 0.3)
    \htext (3.1 0.1){6}
    \htext (3.4 0.1){6}
    \htext (3.7 0.1){2}
    \htext (4.0 0.1){2}
    \htext (3.7 0.4){2}
    \htext (3.7 -0.2){2}

\htext (3.5 0.7){C}

    \move (4.5 0) \lvec(5.7 0)\lvec(5.7 0.3) \lvec(4.5 0.3) \lvec(4.5 0)
    \move (5.1 0.6) \lvec(5.4 0.6) \lvec(5.4 -0.3)\lvec(5.1 -0.3)\lvec(5.1 0.6)
    \move (4.8 0)\lvec(4.8 0.3)
    \htext (4.6 0.1){5}
    \htext (4.9 0.1){5}
    \htext (5.2 0.1){1}
    \htext (5.5 0.1){5}
    \htext (5.2 0.4){1}
    \htext (5.2 -0.2){1}

\htext (5 0.7){D}


\move (0 0.9){}
}

If each of the dice is rolled with respective outcomes $A$, $B$, $C$ and $D$ then
\be
\pro(A > B) = \pro(B > C) = \pro(C > D) = \pro(D > A) = \frac 23.
\ee
\end{example}

\subsection{Probability generating functions}

Consider a random variable, $X$, taking values in the non-negative integers $0, 1, 2, \dots$ with distribution determined by $p_r = \pro (X = r)$, $r = 0, 1, 2,\dots$. The probability generating function (p.g.f.) of $X$ is defined to be
\be
p(z) = \E(z^X) = \sum^\infty_{r=0} p_rz^r,\quad 0 \leq z \leq 1.
\ee
Since the terms in the sum are all non-negative and $0 \leq \sum_r p_r z^r \leq \sum_r p_r = 1$, the probability generating function is well defined and takes values in $[0, 1]$. Its importance stems from the following result.

\begin{theorem}\label{thm:pgf}
The probability generating function of $X$, $p(z)$, $0 \leq z \leq 1$, determines the probability distribution of $X$ uniquely.
\end{theorem}
\begin{proof}[\bf Proof]
Suppose that $p(z) = \sum^\infty_{r=0} p_rz^r = \sum^\infty_{r=0} q_rz^r$, for all $0 \leq z \leq 1$, where $p_r \geq 0$, and $q_r \geq 0$ for each $r$, and
$\sum^\infty_{r=0} p_r = 1 = \sum^\infty_{r=0} q_r$. We will show by induction on $n$ that $p_n = q_n$ for all $n$. First see, by setting $z = 0$, that $p_0 = q_0$. Now assume that $p_i = q_i$ for $0 \leq i \leq n$, then for $0 < z \leq 1$
\be
\sum^\infty_{r=n+1} p_rz^r = \sum^\infty_{r=n+1} q)rz^r.
\ee

Divide through both sides by $z^{n+1}$ and let $z \downarrow 0$ to see that $p_{n+1} = q_{n+1}$ to complete the induction.
\end{proof}

In addition to determining the distribution uniquely, the probability generating function may be used to compute moments of the random variable by  evaluating derivatives of the function.

\begin{theorem}\label{thm:pgf_mean}
Let $X$ be a random variable with probability generating function $p(z)$, then the mean of $X$ is 
\be
\E X = \lim_{z\uparrow 1} p'(z) = p'(1-).
\ee
\end{theorem}

\begin{proof}[\bf Proof]
First assume that $\E X < \infty$. For $0 < z < 1$,
\be
p'(z) = \sum^\infty_{r=1} rp_rz^{r-1} \leq \sum^\infty_{r=1} rp_r = \E X.
\ee
We see that $p'(z)$ is non-decreasing in $z$ so that $\lim_{z\uparrow 1} p'(z) \leq \E X$. Take $\ve > 0$, and choose $N$ so that $\sum^N_{r=1} rp_r \geq \E X -\ve$. Then
\be
\lim_{z\uparrow 1} p'(z) \geq \lim_{z\uparrow 1} \sum^N_{r=1} rp_rz^{r-1} = \sum^N_{r=1} rp_r \geq \E X - \ve,
\ee
this is true for each $\ve > 0$, whence $\lim_{z\uparrow 1} p'(z) \geq \E X$ and it follows that $\lim_{z\uparrow 1} p'(z) = \E X$. If $\E X = \infty$, then for any $M > 0$ choose $N$ so that $\sum^N_{r=1} rp_r \geq M$, and, as above, see that
\be
\lim_{z\uparrow 1} p'(z) \geq \lim_{z\uparrow 1} \sum^N_{r=1} rp_rz^{r-1} = \sum^N_{r=1} rp_r \geq M,
\ee
this is true for any $M$, whence $\lim_{z\uparrow 1} p'(z) = \infty$. 
\end{proof}

Note. By considering the second derivative of $p(z)$, a similar argument to that of Theorem \ref{thm:pgf_mean} may be used to show that 
\be
p''(1-) = \lim_{z\ua 1} p''(z) = \lim_{z\ua 1} \sum^\infty_{r=1} r(r - 1)p_rz^{r-2} = \E (X(X - 1)) ,
\ee
and by considering the $k$th derivative, $k \geq 1$, we have
\be
p^{(k)}(1-) = \lim_{z\ua 1} p^{(k)}(z) = \lim_{z\ua 1} \sum^\infty_{r=1} r(r- 1) \dots (r - k + 1)p_rz^{r-2} = \E (X(X - 1) \dots (X - k + 1)).
\ee
In particular, $\var (X) = p''(1-) + p'(1-) - (p'(1-))^2$.

\begin{example}[Geometric distribution\index{Geometric distribution}]\label{exa:geometic_distribution}
Let $X$ be a random variable with probability distribution given by $\pro (X = r) = p(1-p)^r = pq^r$, $r = 0, 1, 2,\dots$, where $0 < p = 1-q < 1$. Then $X$ may be thought of as the number of tails obtained before getting the first head when successively tossing a coin with probability $p$ of heads on each toss. The probability generating function of $X$ is
\be
p(z) = \E \bb{z^X} = \sum^\infty_{r=0} p_rz^r = \sum^\infty_{r=0} pq^rz^r = \frac{p}{1 - qz}.
\ee
We have $p'(z) = pq/(1 - qz)^2$, so that $\E X = p'(1) = q/p$. Also, $p''(z) = 2pq^2/ (1 - qz)^3$, so that $\E (X(X -1)) = 2q^2/p^2$, from which we deduce that
\be
\E (X^2) = \frac{2q^2}{p^2} + \frac qp \quad\text{ and }\quad \var (X) = \E(X^2) - (\E X)^2 = \frac q{p^2}.
\ee
\end{example}

Note. The term geometric distribution is often also given to the situation where $\pro (X = r) = pq^{r-1}$, $r = 1, 2, \dots$ for $0 < p = 1 - q < 1$. Here, $X$ would be the number of tosses required to achieve the first head where the probability of heads is $p$. This just corresponds to replacing $X$ in Example \ref{exa:geometic_distribution} by $X+1$, so the probability generating function becomes $pz/(1 - qz)$, the mean is $1/p$ and the variance is  unchanged at $q/p^2$.


Another use for probability generating functions is that they provide an easy way of dealing with sums of independent random variables. Suppose that $X_1, \dots,X_n$ are independent random variables with probability generating functions $p_1(z), \dots , p_n(z)$ respectively. Then, since $z^{X_1}, \dots, z^{X_n}$ are independent, we have that the probability generating function of $X_1 + \dots+ X_n$ is
\be
\E \bb{z^{X_1+\dots+X_n}} = \prod^n_{i=1} \E\bb{z^{X_i}} = \prod^n_{i=1} p_i(z).
\ee

In the special case when $X_1,\dots,X_n$ are i.i.d. with common probability generating function $p(z)$ we have
\be
\E \bb{z^{X_1+\dots+X_n}} = (p(z))^n.
\ee

\begin{example}[Sums of Binomial random variables]
Consider independent random variables $X \sim \text{Bin}(n, p)$ and $Y \sim \text{Bin}(m, p)$, where $0 < p = 1 - q < 1$. The probability generating function of $X$ is
\be
\E \bb{z^X} = \sum^n_{r=0} \binom{n}{r} p^rq^{n-r}z^r = (pz + q)^n,
\ee
so that the probability generating function of $Y$ is $(pz + q)^m$. It follows that the probability generating function of $X + Y$ is the product of the two generating functions and is therefore $(pz + q)^{m+n}$. From Theorem \ref{thm:pgf} we conclude that $X +Y \sim \text{Bin}(n+m, p)$. The probabilistic interpretation is immediate, of course, $X$ is the number of heads in $n$ tosses of a coin with probability $p$ of heads and $Y$ is the number of tosses in $m$ (independent) tosses of the coin, so that $X + Y$ is the number of heads in $n + m$ tosses. This of course generalizes, by induction, to the case of independent random variables $X_1, \dots,X_k$ with $X_i \sim \text{Bin}(n_i, p)$, to give $X_1 + \dots + X_k \sim \text{Bin} \bb{\sum^k_{i=1} n_i, p}$.
\end{example}

\begin{example}[Sums of Poisson random variables]\label{exa:poisson_sum}
Consider independent random variables $X \sim \text{Poi}(\lm)$ and $Y \sim \text{Poi}(\mu)$, where $\lm > 0$ and $\mu> 0$. The probability generating function of $X$ is
\be
\E \bb{z^X} = \sum^\infty_{r=0} z^re^{-\lm} \frac{\lm^r}{r!} = e^{-\lm(1-z)}.
\ee
The probability generating function is the same expression with $\mu$ replacing $\lm$ and the probability generating function of $X + Y$ is
\be
e^{-\lm (1-z)} e^{-\mu(1-z)} = e^{-(\lm+\mu)(1-z)},
\ee
from Theorem \ref{thm:pgf} we conclude that $X + Y \sim \text{Poi}(\lm + \mu)$, for an alternative argument see Example \ref{exa:tow_independent_sum} below.
\end{example}


\begin{example}[Negative binomial distribution\index{Negative binomial distribution}] 
Consider a random variable $X$ which has distribution given by
\be
\pro (X = r) = \binom{r - 1}{n - 1} p^n(1 - p)^{r-n},\quad r = n, n + 1, \dots,
\ee
where $0 < p = 1 - q < 1$, and $n \geq 1$. Here, $X$ represents the number of tosses of a coin to get $n$ heads for the first time, where the probability of heads is $p$. The probability generating function of $X$ is
\be
\E \bb{z^X} = \sum^\infty_{r=n} z^r\binom{r - 1}{n - 1} p^nq^{r-n} = (pz)^n \sum^\infty_{r=n} \binom{r- 1}{n- 1} (qz)^{r-n} = (pz/(1 - qz))^n.
\ee
From the note following Example \ref{exa:geometic_distribution} we see that $X$ may be represented as the sum $X_1 + \dots + X_n$ of $n$ i.i.d. random variables each with the same geometric distribution 
\be
\pro(X_1 = r) = pq^{r-1},\quad r = 1, 2, \dots
\ee
The distribution of $X$ is usually referred to as the negative binomial distribution.
\end{example}

\subsection{Conditional distributions}

\begin{definition}
The joint distribution\index{joint distribution} of random variables $X_1,\dots,X_n$, is given by 
\be
\pro (X_1 = x_1, \dots,X_n = x_n) \quad \text{or}\quad x_1 \in \Omega_{X_1} ,\dots , x_n \in \Omega_{X_n},
\ee
and it is a probability distribution on ­$\Omega_{X_1} \times \dots\times ­\Omega_{X_n}$, and the marginal distribution\index{marginal distribution} of $X_i$ is 
\be
\pro (X_i = x_i) = \sum \pro (X_1 = x_1, \dots,X_n = x_n) ,
\ee
where the summation is over $x_1,\dots, x_{i-1}, x_{i+1},\dots, x_n$, this identity is a consequence of the law of total probability. Now consider the case $n = 2$ and (to avoid unnecessary subscripts) consider the random variables $X$ and $Y$. The conditional distribution\index{conditional distribution} of $X$, given $Y = y$, is a probability distribution on ­$X$ given by
\be
\pro (X = x | Y = y) \quad\text{for }x \in \Omega_X,
\ee
where, of course, $\pro(X = x | Y = y) = \pro (X = x, Y = y) /\pro (Y = y)$. Again, by the law of total probability
\be
\pro (X = x) = \sum_{y\in \Omega_Y} \pro(X = x, Y = y) = \sum_{y\in \Omega_Y} \pro(X = x |Y = y) \pro(Y = y).
\ee
\end{definition}

\begin{example}\label{exa:two_independent_sum}
Sum of two independent random variables Suppose that $X$ and $Y$ are independent random variables, then we may express the distribution of their sum as follows
\be
\pro (X + Y = z) = \sum_{y\in \Omega_Y} \pro(X + Y = z | Y = y) \pro (Y = y) = \sum_{y\in\Omega_Y} \pro(X = z | y) \pro (Y = y) = \sum_{x\in \Omega_X} \pro (X = x) \pro (Y = z | x),
\ee
where the last expression is obtained if we condition on $X$ initially instead of $Y$. This procedure gives the convolution\index{convolution} of the distributions of $X$ and $Y$. For example, if $X \sim \pd(\lm)$ and $Y \sim \pd(\mu)$,
\beast
P (X + Y = n) & = & \sum^\infty_{r=0} \pro (X = n - r) \pro (Y = r)\\
& = & \sum^n_{r=0} e^{-\lm}\frac{\lm^{n-r}}{r!} e^{-\mu} \frac{\mu^r}{(n -r)!}, \quad\quad (\pro(X = k) = 0,\ \text{ for }k < 0)\\
& = & \frac{e^{-(\lm+\mu)}}{n!} \sum^n_{r=0} \binom{n}{r} \lm^{n-r}\mu^r = e^{-(\lm+\mu)} \frac{(\lm + \mu)^n}{n!},
\eeast
so that $X + Y \sim \pd(\lm + \mu)$, as seen in Example \ref{exa:poisson_sum} previously using generating functions.
\end{example}

\begin{definition}
The conditional expectation\index{conditional expectation} of $X$ given $Y = y$ is
\be
\E (X | Y = y) = \sum_{x\in \Omega_X} x\pro (X = x | Y = y) = \sum_{\omega:Y (\omega)=y} X(\omega)\pro (\{\omega\}) / \pro (Y = y).
\ee
Note that $\E (X | Y = y)$ is a function of $y$, $g(y)$ say, then the random variable $g(Y)$ is known as the conditional expectation of $X$ given $Y$ and is written $\E (X | Y )$. It is important to emphasize that $\E (X | Y )$ is a random variable and it is a function of $Y$, in contrast to $\E (X | Y = y)$, which is a real number.
\end{definition}

\begin{example}
Consider tossing a coin $n$ times where the probability of a head is $p$, $0 < p = 1 - q < 1$, and let $X_i = 1$ if the $i$th toss produces a head and $X_i = 0$, otherwise. Let $Y = X_1 + \dots + X_n$ denote the total number of heads so that $Y \sim \bd (n, p)$. Then, for $r\geq 1$,
\beast
\pro(X_1 = 1 | Y = r) & = & \frac{\pro (X_1 = 1, Y = r)}{\pro (Y = r)} = \frac{\pro(X_1 = 1,X_1 + \dots+ X_n = r)}{\pro (Y = r)} \\
& = & \frac{\pro (X_1 = 1,X_2 + \dots + X_n = r - 1)}{\pro (Y = r)} ,
\eeast
then by independence and the fact that $X_2 + \dots + X_n \sim \bd (n - 1, p)$ this
\be
= \frac{\pro (X_1 = 1) \pro (X_2 + \dots+ X_n = r - 1)}{\pro (Y = r)} = \frac{p\binom{n - 1}{r - 1} p^{r-1}q^{n-r}} {\binom{n}{r} p^rq^{n-r}} = \frac rn,
\ee
we may see also that $pro(X_1 = 1 | Y = 0) = 0$. Then
\be
\E (X_1 | Y = r) = 1 \times \pro (X_1 = 1 | Y = r) + 0 \times \pro (X_1 = 0 | Y = r) = \frac rn, \quad 0 \leq r \leq n.
\ee
In this case we have $\E (X_1 | Y ) = Y/n$.
\end{example}


Properties of conditional expectation
\ben
\item [1.] For $c$, a constant, then $\E (cX | Y ) = c\E (X | Y )$ and $\E ( c | Y ) = c$.
\item [2.] For random variables $X_1, \dots,X_n$, $\E\bb{\sum_i X_i | Y} = \sum_i \E (X_i | Y )$.
\item [3.] $\E (\E (X | Y )) = \E (X)$.
\begin{proof}[\bf Proof]
We have
\beast
\E (\E (X | Y ))  & = & \sum_{y \in \Omega_Y} \bb{\sum_{x\in \Omega_X} x\pro (X = x | Y = y)} \pro (Y = y)\\
& = & \sum_{x\in \Omega_X} x\sum_{y\in \Omega_Y} \pro (X = x, Y = y) = \sum_{x\in \Omega_X} x\pro (X = x) = \E (X) .
\eeast
\end{proof}

\item [4.] When $X$ and $Y$ are independent, $\E (X | Y ) = \E (X)$.
\begin{proof}[\bf Proof]
For $y \in \Omega_Y$,
\be
\E (X | Y = y) = \sum_{x\in \Omega_X} x\pro (X = x | Y = y) = \sum_{x\in \Omega_X} x\pro (X = x) = \E (X).
\ee
\end{proof}

\item [5.] When $Y$ and $Z$ are independent, $\E (\E (X | Y ) | Z) = \E (X)$.
\begin{proof}[\bf Proof] 
Since $\E (X | Y )$ is a function of $Y$ it is independent of $Z$, so using Property 4 and then Property 3, we have
\be
\E (\E (X | Y )| Z) = \E (\E (X | Y )) = \E (X) .
\ee
\end{proof}

\item [6.] For any function $h : \R \to \R$, we have $\E (h(Y )X | Y ) = h(Y )\E (X | Y )$.
\begin{proof}[\bf Proof]
We have, for $y \in \Omega_Y$,
\be
\E (h(Y )X | Y = y) = \sum_{\omega:Y (\omega)=y} h(Y (\omega))X(\omega)\pro (\{\omega\}) /\pro (Y = y) = h(y)\E (X | Y = y).
\ee
A particular consequence of this and Property 1 is that $\E (\E (X | Y ) | Y ) = \E (X | Y )$.
\end{proof}

\item [7.] The conditional expectation $\E (X | Y )$ is that function $h(Y )$ of $Y$ which minimizes $\E (X - h(Y ))^2$ over all functions $h$.
\begin{proof}[\bf Proof]
Write
\be
\E (X - h(Y ))^2 = \E \bb{X - \E (X| Y ) + \E (X | Y ) - h(Y )}^2,
\ee
which may be expanded to
\be
\E \bb{X - \E (X | Y )}^2 + \E \bb{ \E (X | Y ) - h(Y )}^2 + 2\E\bb{\bb{X- \E (X | Y )}\bb{\E (X | Y ) - h(Y )}}.
\ee
Now consider half the cross-product term,
\be
\E \bb{\bb{X - \E (X | Y )}\bb{\E (X | Y ) - h(Y )}} = \E \bb{ \E\bb{\bb{X - \E (X | Y )}\bb{\E (X | Y ) - h(Y )}| Y}}
\ee
by using Property 3, and then, using Property 5, this
\be
= \E \bsb{\bb{\E (X | Y ) - h(Y )}\E\bb{\bb{X - \E (X | Y )}|Y}},
\ee
but $\E\bb{\bb{X - \E (X | Y )}|Y} = \E (X | Y ) - \E (X | Y ) = 0$, so that
\be
\E (X  h(Y ))^2 = \E \bb{X - \E (X | Y )}^2 + \E \bb{\E (X | Y ) - h(Y )}^2
\ee
from which the result follows, since the first term in this expression does not involve $h$ and the second term is minimized by $h(Y ) = \E (X | Y )$.
\end{proof}
\een

\begin{example}[Sum of a random number of random variables]
Let $X_1,X_2, \dots$ be independent and identically distributed random variables with common probability generating function $p(z)$. Let $N$ be a non-negative integer valued random variable independent of the $\{X_i\}$ and having probability generating function $q(z)$. We consider the p.g.f. of the
random variable $X_1 + \dots + X_N$, (here the sum is 0 if $N = 0$).
\be
r(z) = \E\bb{z^{X_1+\dots+X_N}} = \E \bb{\E\bb{z^{X_1+\dots +X_N}|N}} = \E \bb{\bb{\E z^{X_1}}^N} = \E\bb{(p(z))^N} = q(p(z)).
\ee
If at a first reading you find the second equality too cryptic, you might wish to spell out the argument as
\beast
\E \bb{z^{X_1+\dots+X_N}} & = & \sum^\infty_{n=0} \E \bb{z^{X_1+\dots+X_N}|N = n}\pro (N = n)\\
& = & \sum^\infty_{n=0} \E\bb{z^{X_1+\dots+X_n}|N = n} \pro (N = n)\\
& = & \sum^\infty_{n=0} \E\bb{z^{X_1+\dots+X_n}}\pro(N = n) = \sum^\infty_{n=0} (p(z))^n \pro (N = n) = q(p(z)).
\eeast

After some practice you should find the conditional expectation shorthand notation given first more helpful. It follows from the expression for $r(z)$ that $r'(z) = q'(p(z))p'(z)$, so that
\be
\E (X_1 + \dots + X_N) = q'(p(1-))p'(1-) = (\E N) (\E X_1) ,
\ee
since $p(1-) = 1$. Furthermore, since $r''(z) = q''(p(z) (p'(z))^2 + q'(p(z))p''(z)$, and the fact that $q''(1-) = \E (N)^2 - \E N$ and $p''(1-) = \E (X_1)^2 - \E X_1$, we may calculate that
\be
\var (X_1 + \dots + X_N) = r''(1-) + r'(1-) - (r'(1-))^2 = (\E N)\var (X_1) + (\E X_1)^2 \var (N).
\ee
Notice that the variance of $X_1+\dots+X_N$ is increased over what it would be if $N$ is constant, $N \equiv \E N = n$, say, by the amount $(\E X_1)^2 \var (N)$, if $\var (N) = 0$ and $N$ is constant we get the usual expression for the variance of a sum of $n$ i.i.d. random variables.
\end{example}

\subsection{Branching processes\index{Branching processes}}

As an example of conditional expectations and of generating functions we will consider a model of population growth and extinction known as the Bienaym\'e-Galton-Watson process. Consider a sequence of random variables $X_0,X_1,\dots$, where $X_n$ represents the number of individuals in the nth generation. We will assume that the population is initiated by one individual, take $X_0 \equiv 1$, and when he dies he is replaced by $k$ individuals with probability $g_k$, $k = 0, 1, 2, \dots$. These individuals behave independently and identically to the parent individual, as do those in subsequent generations. The number in the $(n+1)$st generation, $X_{n+1}$, depends on the number in the $n$th generation and is given by
\be
X_{n+1} =\left\{\ba{ll}
Y^n_1 + Y^n_2 + \dots + Y^n_{X_n} \quad\quad & X_n \geq 1,\\
0 & X_n = 0.
\ea\right.
\ee
Here $\{Y^n_j : n \geq 1, j \geq 1\}$ are independent, identically distributed random variables with $\pro(Y^n_j = k) = g_k$, for $k \geq 0$ and $Y^n_j$ represents the number of offspring of the $j$th individual in the nth generation, $j \leq X_n$.

Assumptions (i) $g_0 > 0$, and (ii) $g_0 + g_1 < 1$.

Assumption (i) means that the population can die out (extinction) since in each generation there is positive probability that all individuals have no offspring, assumption (ii) means that the population may grow, there is positive probability that the next generation has more individuals than the present one. Now let $G(z) = \sum^\infty_{k=0} g_kz^k = \E \bb{z^{X_1}}$ and set $G_n(z) = \E \bb{z^{X_n}}$, for $n \geq 1$, so that $G_1 = G$.

\begin{theorem}
For all $n \geq 1$, $G_{n+1}(z) = G_n (G(z)) = G(\dots (G(z)) \dots) = G(G_n(z))$.
\end{theorem}
\begin{proof}[\bf Proof]
Note that $Y^n_1, Y^n_2 , \dots$ are independent of $X_n$, so that
\beast
G_{n+1}(z) & = & \E\bb{z^{X_n+1}} = \sum^\infty_{k=0} \E \bb{z^{X_{n+1}} |X_n = k} \pro (X_n = k)\\
& = & \sum^\infty_{k=0} \E\bb{z^{Y^n_1 + \dots+Y^n_k}}\pro (X_n = k) = \sum^\infty_{k=0} (G(z))^k \pro (X_n = k)\\
& = & \E\bb{(G(z))^{X_n}} = G_n (G(z)).
\eeast
\end{proof}

\begin{corollary}
For $m = \E (X_1) = \sum^\infty_{k=1} kg_k$ and $\sigma^2 = \var (X_1) = \sum^\infty_{k=0} (k - m)^2 g_k$, then for $n \geq 1$, we have
\be
\E (X_n) = m^n,\quad\quad \var (X_n) = \left\{\ba{ll}
\frac{\sigma^2m^{n-1} (m^n - 1)}{m- 1} \quad\quad & m \neq 1,\\
n\sigma^2 & m=1.
\ea\right.
\ee
\end{corollary}

\begin{proof}[\bf Proof]
Differentiating $G_n(z) = G_{n-1}(G(z))$ to obtain $G_n'(z) = G'_{n-1}(G(z))G'(z)$ and letting $z \ua 1$, it follows that 
\be
\E (X_n) = m\E (X_{n-1}) = \dots = m^n\E (X_0) = m^n,
\ee
since $X_0 = 1$. Differentiating $G_n(z)$ a second time gives
\be
G''_n(z) = G''_{n-1} (G(z)) (G'(z))^2 + G'_{n-1} (G(z))G''(z),
\ee
and letting $z \ua 1$ again we have
\be
\E (X_n (X_n - 1)) = m^2 \E (X_{n-1} (X_{n-1} - 1)) + \bb{\sigma^2 + m^2 - m} \E (X_{n-1}).
\ee
We then have, using the fact that $\E X_n = m^n$,
\beast
\var (X_n) & = & \E (X_n(X_n - 1)) + \E (X_n) - (\E X_n)^2\\
& = & m^2\E (X_{n-1}(X_{n-1} - 1)) + \bb{\sigma^2 + m^2 - m}\E (X_{n-1}) + m^n - m^{2n} \\
& = & m^2\bb{\var (X_{n-1}) - \E (X_{n-1}) + (\E X_{n-1})^2} + \bb{\sigma^2 + m^2} m^{n-1} - m^{2n}\\
& = & m^2\var (X_{n-1}) + \sigma^2 m^{n-1}.
\eeast
Iterating this, we see that
\beast
\var (X_n) & = & m^2\var (X_{n-1}) + \sigma^2 m^{n-1} = m^4\var (X_{n-2}) + \sigma^2 \bb{m^{n-1} + m^n} = \dots \\
& = & m^{2n}\var (X_0) + \sigma^2 \bb{m^{n-1} + \dots + m^{2n-2}}\\
& = & \sigma^2 \bb{m^{n-1} + \dots + m^{2n-2}},\quad\quad \text{since $\var (X_0) = 0$ because $X_0 = 1$},
\eeast
and then the result may be obtained immediately.
\end{proof}

Probability of extinction Notice that $X_n = 0$ implies that $X_{n+1} = 0$ so that if we let $A_n = (X_n = 0)$, the event that the population is extinct at or before generation $n$, we have $A_n \subseteq A_{n+1}$ and $A = \bigcup^\infty_{n=1} A_n$ represents the event that extinction ever occurs. Notice
that $\pro(A_n) = G_n(0)$ and by the continuity property of probabilities on increasing events we see that the extinction probability, $q$, say, is
\be
q = \pro(A) = \lim_{n\to\infty} \pro (A_n) = \lim_{n\to\infty} G_n(0) = \lim_{n\to\infty} \pro (X_n = 0).
\ee

\begin{theorem}
The extinction probability $q$ is the smallest positive root of the equation $G(z) = z$. When $m$, the mean number of offspring per individual, satisfies $m \leq 1$ then $q = 1$, when $m > 1$ then $q < 1$.
\end{theorem}

\begin{proof}[\bf Proof]
The fact that the extinction probability $q$ is well defined follows from the above and since $G$ is continuous and $q = \lim_{n\to \infty}G_n(0)$ we have $G\bb{\lim_{n\to\infty} G_n(0)} = \lim_{n\to\infty}G_{n+1}(0)$, so that $G(q) = q$, that is $q$ is a root of $G(z) = z$, note that 1 is always a root since
$G(1) = \sum^\infty_{r=0} g_r = 1$. Let $\alpha > 0$ be any positive root of $G(z) = z$, so that because $G$ is increasing, $\alpha = G(\alpha) > G(0)$, and repeating $n$ times we have $\alpha > G_n(0)$, whence $\alpha \geq \lim_{n\to\infty} G_n(0) = q$, so that we must have $\alpha\geq q$, that is, $q$ is the smallest positive root of $G(z) = z$.

Now let $H(z) = G(z)-z$, then $H'' = \sum^\infty_{r=0} r(r-1)g_rz^{r-2} > 0$ for $0 < z < 1$ provided $g_0 + g_1 < 1$, so the derivative of $H$ is strictly increasing in the range $0 < z < 1$, hence $H$ can have at most one root different from 1 in $[0, 1]$ (Rolle's Theorem (Theorem \ref{thm:rolle})).

Firstly, suppose that $H$ has no root in $[0, 1)$ then, since $H(0) = g_0 > 0$ we must have $H(z) > 0$ for all $0 < z < 1$, so $H(1) - H(z) < H(1) = 0$ and so
\be
H'(1-) = \lim_{z\ua 1} \frac{H(1) - H(z)}{1 - z} \leq 0,\quad\quad \text{whence } m = G'(1-) \leq 1.
\ee
Next, suppose that H has a unique root $r$ in $[0, 1)$, then $H'$ must have a root in $[r, 1)$, that is $H'(z) = G'(z)-1 = 0$ for some $z$, $r \leq z < 1$. The function $G'$ is strictly increasing (since $g_0 + g_1 < 1$) so that $m = G'(1-) > G'(z) = 1$. Thus we see that $m \leq 1$, if and only if, $q = 1$.
\end{proof}


Note. The following two figures illustrate the two situations $m \geq 1$ and $m > 1$, the dotted lines illustrate the iteration $G_{n+1}(0) = G(G_n(0))$ tending to the smallest positive root, $q$.

\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.02 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0 

\move (-0.2 0) \avec(2 0) 
\move (0 -0.2) \avec(0 1.8) 

\move (0 0) \lvec (1.8 1.8) 
\move (0 0.6) \clvec (0.9 1)(1.3 1.3)(1.6 1.6) 

\htext (-0.1 -0.15){0}
\htext (1.6 -0.15){1}
\htext (0.55 -0.15){$q$}
\htext (1.9 -0.15){$z$}
\htext (-0.4 0.45){$G(0)$}

\lpatt (0.05 0.05)

\move (1.6 1.6) \lvec (1.6 0)

%%%%%%%%%%%%%%%%%%%%%%%%

\lpatt (1 0)

\move (2.8 0) \avec(5 0) 
\move (3 -0.2) \avec(3 1.8) 

\move (3 0) \lvec (4.8 1.8) 
\move (3 0.6) \clvec (4.2 0.9)(4.5 1.4)(4.6 1.6) 

\htext (2.9 -0.15){0}
\htext (4.6 -0.15){1}
\htext (3.55 -0.15){$q$}
\htext (4.9 -0.15){$z$}
\htext (2.6 0.45){$G(0)$}

\lpatt (0.05 0.05)
\move (4 1) \lvec (4 0)
\move (4.6 1.6) \lvec (4.6 0)

\move(0 2)
}


\subsection{Random walks}

\begin{definition}
Let $X_1,X_2,\dots$ be i.i.d. random variables and set $S_k = S_0 + X_1 + \dots + X_k$ for $k \geq 1$ where $S_0$ is a constant then $\{S_k, k \geq 0\}$ is known as a (one-dimensional) random walk\index{random walk}. When each $X_i$ just takes the two values +1 and -1 with probabilities $p$ and $q = 1 - p$, respectively, it is a simple random walk and further when $p = q = \frac 12$ it is a simple, symmetric random walk\index{symmetric random walk}. We will consider simple random walks.
\end{definition}

Recurrence relations. The problems we will look at for the simple random walk often reduce to the solution of recurrence relations (or difference equations). We consider the general solution of such equations in the simplest situations which have constant coefficients.
\ben
\item [1.] First-order equations: The general first-order equation is $x_{n+1} = ax_n + b$, for $n \geq 0$, where $a$ and $b$ are constants, the case $b = 0$ gives the general first-order homogeneous equation $x_{n+1} = ax_n$, which trivially may be solved as $x_n = a^nx_0$, if $y_n$ is any solution of the inhomogeneous equation, then the general solution of the inhomogeneous equation is of the form $x_n = Ca^n + y_n$ for some constant $C$ (because $x_n-y_n$ must be a solution of the homogeneous equation). The constant is determined by a boundary condition.

\item [2.] Second-order equations: $x_{n+1} = ax_n + bx_{n-1} + c$, for $n \geq 1$, where $a$, $b$ and $c$ are constants. First consider the homogeneous case where $c = 0$. Then write the relation in matrix form as follows:
\be
\bepm x_{n+1} \\ x_n \eepm = \bepm a & b\\ 1 & 0 \eepm \bepm x_n \\ x_{n-1} \eepm = A\bepm x_n\\ x_{n-1} \eepm,\quad\text{where }A = \bepm a & b\\ 1 & 0\eepm.
\ee
It follows that 
\be
\bepm x_{n+1} \\ x_n\eepm = A^n \bepm x_1\\ x_0\eepm,
\ee
find the eigenvalues of $A$, by solving
\be
\bevm a-\lm & b\\ 1 & \lm \eevm = 0, \quad \text{to give the equation }\lm^2 - a\lm - b = 0,
\ee
with roots $\lm_1$ and $\lm_2$, say. This equation is known as the auxiliary equation\index{auxiliary equation} of the recurrence relation, it corresponds to seeking a solution of the form $x_n = \lm^n$. If $\lm_1$ and $\lm_2$ are distinct then for some matrix $\Lambda$ we may write
\be
A = \Lambda^{-1} \bepm \lm_1 & 0\\ 0 & \lm_2 \eepm \Lambda\quad \text{and then}\quad A^n = \Lambda^{-1} \bepm \lm^n_1 & 0\\ 0 & \lm^n_2 \eepm \Lambda,
\ee
so that the general solution of the homogeneous equation may be seen to be of the form $x_n = C\lm^n_1 + D\lm^n_2$ for some constants $C$ and $D$. If the eigenvalues are not distinct, $\lm_1 = \lm_2 = \lm$, then
\be
A = \Lambda^{-1} \bepm \lm & 1 \\ 0 & \lm \eepm \quad \text{and then}\quad A^n = \Lambda^{-1} \bepm \lm^n & n\lm^{n-1} \\ 0 & \lm^n\eepm \Lambda,
\ee
and then the general solution of the homogeneous equation may be seen to be of the form $x_n = \lm^n(C + Dn)$ for some constants $C$ and $D$. As before, if $y_n$ is any particular solution of the inhomogeneous equation, the general solution is of the form $x_n + y_n$ where $x_n$ is the general solution of the homogeneous equation.
\een

\begin{example}[Gambler's ruin]
For the simple random walk, $\{S_k\}$ may represent the fortune of a gambler after $k$ plays of a game where on each play he either wins \pounds 1, with
probability $p$, or loses \pounds 1 with probability $q = 1-p$, his initial fortune is \pounds $S_0$ and a classical problem is to calculate the probability that his fortune achieves the level $a$, $a > S_0$ , before the time of ruin, that is the time that he goes bankrupt (his fortune hits the level 0). If $T_a$ denotes the first time that the random walk hits the level $a$ and $T_0$ the time the random walk first hits the level 0, we would wish to calculate $\pro (T_a < T_0)$, given that his fortune starts at $S_0 = r$, $0 < r < a$.

\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.03 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0 

\move (-0.2 0) \avec(5 0) 
\move (0 -0.2) \avec(0 1.8) 

\move (0 0.6) \bdot
\move (0.2 0.9) \bdot
\move (0.4 0.6) \bdot
\move (0.6 0.9) \bdot
\move (0.8 1.2) \bdot
\move (1 0.9) \bdot
\move (1.2 1.2) \bdot
\move (1.4 1.5) \bdot
\move (1.6 1.2) \bdot
\move (1.8 0.9) \bdot
\move (2 1.2) \bdot
\move (2.2 1.5) \bdot
\move (2.4 1.2) \bdot
\move (2.6 0.9) \bdot
\move (2.8 1.2) \bdot
\move (3 0.9) \bdot
\move (3.2 0.6) \bdot
\move (3.4 0.9) \bdot
\move (3.6 0.6) \bdot
\move (3.8 0.3) \bdot
\move (4 0) \bdot
\move (4.2 -0.3) \bdot
\move (4.4 0) \bdot
\move (4.6 0.3) \bdot
\move (4.8 0.6) \bdot



\htext (1.4 -0.15){$T_a$}
\htext (3.8 -0.15){$T_0$}
\htext (-0.15 1.45){$a$}
\htext (-0.2 0.5){$S_0$}
\htext (4.9 0.5){$S_k$}
\htext (4.8 -0.15){$k$}

\lpatt (0.05 0.05)

\move (0 1.5) \lvec(5 1.5)
\move (1.4 1.4) \lvec (1.4 0)

\move (0 0.6) \lvec (0.2 0.9) \lvec (0.4 0.6) \lvec (0.6 0.9) \lvec (0.8 1.2) \lvec (1 0.9) \lvec (1.2 1.2) \lvec (1.4 1.5) \lvec (1.6 1.2) \lvec (1.8 0.9) \lvec (2 1.2) \lvec (2.2 1.5) \lvec (2.4 1.2) \lvec (2.6 0.9) \lvec (2.8 1.2) \lvec (3 0.9) \lvec (3.2 0.6) \lvec (3.4 0.9) \lvec (3.6 0.6) \lvec (3.8 0.3) \lvec (4 0) \lvec (4.2 -0.3) \lvec (4.4 0) \lvec (4.6 0.3) \lvec (4.8 0.6) 

\move (0 2)

}


The figure illustrates a path of the random walk-although, in the case of the game, it finishes at the instant $T_0$, the time of bankruptcy! Let $x_r = \pro (T_a < T_0)$ when $S_0 = r$, for $0 \leq r \leq a$, so that we have the boundary conditions $x_a = 1$ and $x_0 = 0$. A general rule in problems of this type in probability may be summed up as 'condition on the first thing that happens', which here would be a shorthand for using the Law of Total  Probability to express the probability conditional on the outcome of the first play of the game, that is, whether $X_1 = 1$ or $X_1 = -1$, or equivalently, $S_1 = r+1$ or $S_1 = r-1$. Thus, for $0 < r < a$,
\be
x_r = \pro (T_a < T_0 | S_1 = r + 1) \pro(X_1 = 1) + \pro(T_a < T_0 | S_1 = r - 1) \pro (X_1 = -1) = px_{r+1} + qx_{r-1}.
\ee
The auxiliary equation for this recurrence relation is $p\lm^2 - \lm +q = 0$, and since $p+q = 1$, this may be factored as $(\lm - 1)(p\lm - q) = 0$ to give roots $\lm = 1$ and $\lm = q=p$.

Case $p \neq q$: the roots are distinct and the general solution is of the form $x_r = A+B (q/p)^r$ for some constants $A$ and $B$, the boundary conditions at $r = a$ and $r = 0$, fix $A$ and $B$ and we conclude that
\be
x_r = \pro (T_a < T_0) = \frac{1 - (q/p)^r}{1 - (q/p)^a},\quad\quad 0 \leq r \leq a.
\ee

Case $p = q = \frac 12$: here $\lm = 1$ is a repeated root of the auxiliary equation so that the general solution of the recurrence relation is $x_r = A+Br$, which, after using the boundary conditions, leads to the solution $x_r = r/a$, $0 \leq r \leq a$.

We do not know necessarily that at least one of $T_0$ and $T_a$ must be finite, but if we interchange $p$ and $q$ and replace $r$ by $a - r$, (or just calculate directly as above) we may obtain, for $S_0 = r$, $0 \leq r \leq a$, that
\be
\pro (T_0 < T_a) = \left\{\ba{ll}
\frac{(q/p)^r - (q/p)^a}{1 - (q/p)^a} \quad \quad & p \neq q,\\
1 - r/a & p = q = \frac 12.
\ea\right.
\ee

It follows, in both cases, that $\pro(T_a < T_0) + \pro (T_0 < T_a) = 1$, so that at least one of the the two barriers, 0 or $a$, must be reached with certainty.
\end{example}

\begin{example}
Probability of ruin From the previous calculation we may derive an expression for $\pro (T_0 < \infty)$ given $S_0 = r > 0$, which is the probability that ruin ever happens. We see that the event that ruin occurs may be written as
\be
(T_0 < \infty) = \bigcup^\infty_{a=r+1} (T_0 < T_a),
\ee
the events in the union are expanding as $a$ increases, so by the continuity of the probability on expanding events, we have
\be
\pro(T_0 < \infty) = \lim_{a\to \infty} \pro (T_0 < T_a) = \left\{ \ba{ll}
(q/p)^r \quad\quad & p > q,\\
1 & p \leq q,
\ea\right.
\ee
so that ruin is certain except in the case when the probability of winning a play is strictly larger than $\frac 12$.
\end{example}

\begin{example}
Expected duration of the game Suppose that the gambler plays either until his fortune reaches $a$ or until he goes bankrupt, whichever is sooner. That is the number of plays is $\min (T_0, T_a) = T_0 \land T_a$. We will derive the expected length of the game, $\E (T_0 \land T_a)$, given that $S_0 = r$, $0 \leq r \leq a$, which we will denote by $m_r$. We do not know whether $m_r$ is finite. Consider blocks of jumps of the random walk of length $a$, that is
\be
\ba{cccc}
X_1 & X_2 & \dots & X_a\\
X_{a+1} & X_{a+2} & \dots & X_{2a}\\
X_{2a+1} & X_{2a+2} & \dots & X_{3a}\\
\vdots & \vdots & \dots & \vdots 
\ea
\ee
and for $i \geq 1$ set $Y_i = 1$ if either $X_{(i-1)a+1} = X_{(i-1)a+2} = \dots = X_{ia} = 1$ or $X_{(i-1)a+1} = X_{(i-1)a+2} = \dots = X_{ia} = -1$, otherwise $Y_i = 0$. Thus $Y_i = 1$ if and only if the $i$th block of plays is a run of all wins or all losses, and $\pro(Y_i = 1) = 1 - P (Y_i = 0) = p^a + q^a = \theta$, say. If we let $Z$ be the first $i$ such that $Y_i = 1$, then $Z$ has a geometric distribution $\pro(Z = j) = (1-\theta)^{j-1}\theta$, $j \geq 1$, and so $\E (Z) = 1/\theta < \infty$. But it is clear that $T_0\land T_a \leq aZ$, hence we see that $\E (T_0\land T_a) \leq a\E (Z) < \infty$. To compute $m_r$, we again condition on the first thing to happen, that is whether the first play is a win or loss, to see that for $0 < r < a$, 
\be
m_r = p (1 + m_{r+1}) + q (1 + m_{r-1}) = 1 + pm_{r+1} + qm_{r-1}, \quad\quad \text{with }m_0 = m_a = 0,
\ee
here the 1 in the recurrence relation counts the initial play of the game. The solution of the homogeneous equation is again $m_r = A + B (q/p)^r$ when $p \neq q$ and $m_r = A + Br$ for the case $p = q = \frac 12$.

Case $p \neq q$: look for a particular solution of the inhomogeneous equation with $m_r = cr$, then
\be
cr = 1 + pc(r + 1) + qc(r - 1), \quad \quad\text{so that }c = 1/(q -p),
\ee
so that the general solution is $m_r = r/(q-p)+A+B (q/p)^r$, and after using the boundary conditions we have
\be
m_r = \frac r{q - p} - \bb{\frac a{q - p}} \frac{1 - (q/p)^r}{1 - (q/p)^a}.
\ee

Case $p = q = \frac 12$: a particular solution of the inhomogeneous equation is $-r^2$, so the general solution is $m_r = A + Br - r^2$ and after using the boundary conditions we have $m_r = r(a - r)$. 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Continuous Random Variables}

\subsection{Introduction}

\begin{definition}
Up to now we have restricted consideration to sample spaces ­ which are finite, or countable, we will now relax that assumption. We assume that we have a probability $\pro(\cdot)$ defined on subsets (events) of ­ satisfying the axioms given previously. We will be interested in random variables which may take on uncountably many values. Here if $X : \Omega\to \R$, define the distribution function (sometimes called the cumulative distribution function\index{cumulative distribution function}) of $X$ as
\be
F(x) = \pro (X \leq x),\quad -\infty < x < \infty,
\ee
so that $F : \R \to [0, 1]$. Note that $\pro (X > x) = 1 - F(x)$.
\end{definition}

Properties of the distribution function $F(x)$
\ben
\item [1.] $F(x)$ is non-decreasing in $x$, $-\infty < x < \infty$.
\begin{proof}[\bf Proof]
If $x \leq y$, then the event $(X \leq x) \subseteq (X \leq y)$, so that
\be
F(x) = \pro (X \leq x) \leq \pro (X \leq y) = F(y).
\ee
\end{proof}


\item [2.] For $a < b$, $pro (a < X \leq b) = F(b) - F(a)$.
\begin{proof}[\bf Proof]
We have
\beast
\pro(a < X \leq b) & = & \pro ((X \leq a)^c \cap (X \leq b))\\
& = & \pro ((X \leq a)^c) + \pro (X \leq b) - \pro ((X \leq b) \cup (X \leq a)^c)\\
& = & 1 - \pro (X \leq a) + \pro (X \leq b) - \pro(\Omega­) = F(b) - F(a).
\eeast
\end{proof}

\item [3.] $F(x)$ is right continuous in $x$, that is, when $y \da x$ we have $F(y) \da F(x)$, since $F$ is non-decreasing the limit from the left $\lim_{y\ua x} F(y) = F(x-) \leq F(x)$ always exists.
\begin{proof}[\bf Proof]
Fix $x$, then for $n \geq 1$, consider the event
\be
A_n = (x < X \leq x + 1/n) = (X \leq x + 1/n) \cap (X \leq x)^c,
\ee
then the $\{A_n\}$ are decreasing events $A_n \supseteq A_{n+1}$, and $\cap_n A_n = \emptyset$, so by the continuity property of probabilities $\lim_{n\to \infty} \pro (A_n) = 0$. But $\pro(A_n) = F (x + 1/n) - F (x)$, from which the conclusion follows.
\end{proof}

\item [4.] $\lim_{x\to -\infty} F(x) = 0$ and $\lim_{x\to \infty} F(x) = 1$.

We say that a random variable $X$ is continuous if its distribution function, $F$, is a continuous function. We have seen that a distribution function is necessarily right continuous, then if $X$ is a continuous random variable, $F$ must also be left continuous. This is equivalent to the statement that $\pro (X = x) = 0$ for all $x \in \R$, since as in the proof of Property 2, we will have $\pro(X = x) = \lim_{y\ua x} \pro(y < X \leq x) = \lim_{y\ua x} [F(x) - F(y)]$.

In discussing continuous random variables we will restrict consideration to the situation where $F$ is not only continuous but also differentiable, and we will set $f(x) = F'(x)$, $f(\cdot)$ is known as the probability density function\index{probability density function} (p.d.f) of the random variable $X$. A
probability density function satisfies the following two conditions:
\be
\text{(i)}\ f(x) > 0,\text{ for all }x \in \R,\quad\quad \text{(ii)} \ \int^\infty_{-\infty} f(x)dx = 1,
\ee
and then $F(x) = \int^x_{-\infty} f(y)dy$.

Note that for a discrete random variable the distribution function is a right-continuous step function as illustrated in Figure 1, with the heights of the steps being $\pro(X = x_i)$ for the possible values $x_i$, while for a continuous random variable the distribution function is a continuous non-decreasing function as in Figure 2.


\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.02 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0 

\move (-1.5 0) \avec(1.5 0) 
\move (0 0) \avec(0 1.5) 

\move (0.9 1.1) \lvec (1.5 1.1) 
\move (0.9 1.1) \bdot
\move (0.3 0.9) \lvec (0.9 0.9) 
\move (0.3 0.9) \bdot
\move (-0.3 0.6) \lvec (0.3 0.6) 
\move (-0.3 0.6) \bdot
\move (-0.9 0.3) \lvec (-0.3 0.3) 
\move (-0.9 0.3) \bdot

\htext (-0.9 -0.15){$x_1$}
\htext (-0.3 -0.15){$x_2$}
\htext (0.3 -0.15){$x_3$}
\htext (0.9 -0.15){$x_4$}
\htext (-0.15 1){1}
\htext (1.5 -0.15){$x$}
\htext (0.05 1.35){$F(x)$}

\lpatt (0.05 0.05)

\move (-1.5 1.2) \lvec (1.5 1.2)

%%%%%%%%%%%%%%%%%%%%%%%%

\lpatt (1 0)

\move (2 0) \avec(5 0) 
\move (3.5 0) \avec(3.5 1.5) 

\move (2 0.1) \clvec (3.1 0.2)(3.9 1)(5 1.1) 

\htext (3.35 1){1}
\htext (5 -0.15){$x$}
\htext (3.55 1.35){$F(x)$}

\lpatt (0.05 0.05)
\move (2 1.2) \lvec (5 1.2)

\move(0 1.6)
}


Note that there is not a straight split between discrete and continuous random variables, it is possible to have a random variable which is continuous over some ranges of values while at the same time taking certain values with positive probabilities, however, in this course we will deal with the two cases separately. 

The intuitive interpretation of the p.d.f. is that for small $\triangle x$,
\be
\pro(x < X \leq x + \triangle x) = F (x + \triangle x) - F(x) = \int^{x+\triangle x}_x f(y) \approx f(x)\triangle x,
\ee
so that while $f(x)$ does not represent a probability, the probability that $X$ lies in a small interval around $x$ is proportional to $f(x)$, and for this reason many intuitive arguments involving probabilities carry over to probability density functions. Note that areas under the probability density function represent probabilities as illustrated in the figure.

\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.02 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0 

\move (-1 0) \avec(3 0) 
\move (0 0) \avec(0 1.5) 

\move (-1 0.1) \clvec (0 0.3)(0.3 1.2)(1 1.2) 
\move (1 1.2) \clvec (1.2 1.2)(1.4 1)(1.5 0.9) 
\move (1.5 0.9) \clvec (1.6 0.8)(2 0.3)(3 0.1) 

\htext (0.95 -0.15){$a$}
\htext (1.45 -0.15){$b$}
\htext (0.05 1.35){$f(x)$}
\htext (1.55 1.1){$\pro(a < X \leq b)$}
\htext (3 -0.15){$x$}

\move (1 1.2) \clvec (1.2 1.2)(1.4 1)(1.5 0.9) \lvec (1.5 0) \lvec (1 0) \lvec (1 1.2) \lfill f:0.8

\move(0 1.6)
}

More generally, for a set $S \subseteq \Omega_X$, we have $\pro(X \in S) = \int_{x\in S} f(x)dx$.
\een

\subsection{Expectation, variance and standard distributions}

Consider a continuous random variable $X$ with distribution function $F$ and p.d.f.f. Then set
\be
\E (X^+) = \int^\infty_0 xf(x)dx,\quad\text{ and }\quad \E (X^-) = \int^0_{-\infty} (-x)f(x)dx,
\ee
and if not both $\E (X^+)$ and $\E (X^-)$ are in¯nite then de¯ne the expectation of $X$ to be
\be
\E (X) = \E (X^+) - \E (X^-) = \int^\infty_{-\infty} xf(x)dx,
\ee
otherwise, the expectation is not defined.

For a continuous non-negative random variable $X \geq 0$, we may write
\be
\E X = \int^\infty_0 (1- F(x)) dx,
\ee

\be
\E X = \int^\infty_0 yf(y)dy = \int^\infty_{y=0} \bb{\int^y_{x=0} dx} f(y)dy = \int^\infty_{x=0} \bb{\int^\infty_{y=x} f(y)dy} dx = \int^\infty_0 (1 - F(x)) dx,
\ee
by interchanging the order of integration. By considering $X^+$ and $X^-$, we may see that for any continuous random variables we may write
\be
\E X = \int^\infty_0 (1 - F(x)) dx - \int^0_{-\infty} F(x)dx.
\ee
Observe that the properties of expectation as set out for discrete random variables carry over to the situation here with one change, which is that for a function $g(\cdot)$,
\be
\E (g(X)) = \int^\infty_{-\infty} g(x)f(x)dx.
\ee
We may define the variance of a continuous random variable in exactly the same way, $\var (X) = \E (X - \E X)^2$, and its properties are exactly as before, in particular $\var (X) = \E\bb{X^2} - (\E X)^2$. The standard deviation of $X$ is again just $\sqrt{\var (X)}$.

\begin{example}[Exponential distribution\index{Exponential distribution}]
One of the two most important continuous distribution is the exponential distribution for which the random variable $X$ has the probability density function is $f(x) = \lm e^{-\lm x}$ for $x \geq 0$, with $f(x) = 0$ for $x < 0$, where $\lm > 0$ is a constant. We write $X \sim \sE(\lm)$. First note that
$\int^\infty_0 \lm e^{-\lm x} dx = 1$, so that $f$ is a genuine p.d.f.. Then, for $x > 0$,
\be
F(x) = \int^x_0 \lm e^{-\lm y}dy = 1 - e^{-\lm x} .
\ee
We may calculate
\be
\E (X) = \int^\infty_0 x\lm e^{-\lm x} dx = \int^\infty_0 xd\bb{-e^{-\lm x}} = \bsb{-xe^{-\lm x} }^\infty_0 + \int^\infty_0 e^{-\lm x}dx = \frac 1{\lm}.
\ee
Furthermore, using integration by parts again, we may also obtain that
\be
\E \bb{X^2} = \int^\infty_0 x^2\lm e^{-\lm x} dx = \int^\infty_0 x^2 d\bb{-e^{-\lm x}} = \bsb{-x^2e^{-\lm x}}^\infty_0 + 2\int^\infty_0 xe^{-\lm x} dx = \frac 2{\lm^2}.
\ee
using the previous calculation, so that $\var (X) = \E\bb{X^2} - (\E X)^2 = 1/\lm^2$.

The exponential distribution is sometimes used to model the lifetime of a component. If $X$ is the lifetime and $X \sim \sE(\lm)$, then the probability that the component survives a length of time $x > 0$ is $\pro (X > x) = e^{-\lm x}$. Then for $x > 0$ and $y > 0$,
\be
\pro (X > x + y | X > y) = \frac{\pro (X > x + y,X > y)}{\pro (X > y)} = \frac{\pro (X > x + y)}{\pro (X > y)} = \frac{e^{-\lm(x+y)}}{e^{-\lm y}} = e^{-\lm x},
\ee
so that, given the component has survived a length of time $y$ the probability that it will survive a further time $x$ is the same as if it has just been installed. This property, which is crucial to the study of stochastic processes, is known as the lack of memory property of the exponential distribution. 
\end{example}

\begin{theorem}\label{thm:inverse_density}
Suppose that $X$ is a continuous random variable with p.d.f. $f(x)$ and $g : \R \to \R$ is a continuous function which is either strictly increasing or strictly decreasing and with $g^{-1}$ is differentiable, then $g(X)$ is a continuous random variable with p.d.f.
\be
f\bb{g^{-1}(x)} \abs{\frac{d}{dx}g^{-1}(x)}.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Suppose that $g$ is strictly increasing (then $g^{-1}$ is also, so its derivative is positive), we see that the distribution of $g(X)$ is
\be
\pro (g(X) \leq x) = \pro\bb{X \leq g^{-1}(x)} = F\bb{g^{-1}(x)},
\ee
differentiating with respect to $x$ to obtain the p.d.f. gives the result. When $g$ is decreasing so also is its inverse (so $\frac{d}{dx}g^{-1}(x)$ is negative) and we have
\be
\pro (g(X) \leq x) = \pro\bb{X \geq g^{-1}(x)} = \pro \bb{X > g^{-1}(x)} = 1 - F\bb{g^{-1}(x)} ,
\ee
because $\pro \bb{X = g^{-1}(x)} = 0$, since $X$ is continuous, and the result follows by differentiating.
\end{proof}

\begin{example}[The normal distribution\index{normal distribution}]
The normal distribution (also known as the Gaussian distribution\index{Gaussian distribution}) is the most important continuous distribution, its significance stems from the Central Limit Theorem which we will consider later. The probability density is specified by two parameters $\mu$, $-\infty < \mu < \infty$, and $\sigma > 0$, and is given by
\be
f(x) = \frac 1{\sqrt{2\pi} \sigma} e^{-(x-\mu)^2/(2\sigma^2)},\quad -\infty < x < \infty.
\ee
First, we must check that this is indeed a p.d.f. in that it integrates to 1. By making the substitution $u = (x -\mu)/\sigma$ we see that
\be
I = \int^\infty_{-\infty} \frac 1{\sqrt{2\pi} \sigma} e^{-(x-\mu)^2/(2\sigma^2)}dx = \int^\infty_{-\infty} \frac 1{\sqrt{2\pi}} e^{-\frac 12 u^2} du = 2\int^\infty_0 \frac 1{\sqrt{2\pi}} e^{-\frac 12 u^2} du,
\ee
by the symmetry of the integrand around $u = 0$. Then we may calculate as follows,
\be
I^2 = \frac 2{\pi} \int^\infty_0 \int^\infty_0 e^{-\frac 12} (u^2+v^2)dudv,
\ee
then going to polar coordinates $u = r \cos \theta$ and $v = r \sin \theta$, this
\be
= \frac 2{\pi} \int^\infty_0 \int^{\pi/2}_0 e^{-\frac {r^2}2} rdrd\theta = \frac 2{\pi}\int^{\pi/2}_0 \bb{\int^\infty_0 e^{-\frac 12 r^2} d\bb{r^2/2}} d\theta = 1,
\ee
showing that $I = 1$. To calculate the mean, by making the substitution $u = (x-\mu)/\sigma$, we see that
\be
\E X = \int^\infty_{-\infty} \frac x{\sqrt{2\pi}\sigma } e^{-(x-\mu)^2/(2\sigma^2)}dx = \sigma \int^\infty_{-\infty} \frac{u}{\sqrt{2\pi}} e^{-\frac 12 u^2} du + \mu \int^\infty_{-\infty}\frac 1{\sqrt{2\pi}} e^{-\frac 12 u^2} du = \mu,
\ee
because the first integral is 0, since the integrand is an odd function, and the second integral is 1, as we have just established. The same substitution shows that
\be
\var (X) = \E (X -\mu)^2 = \int^\infty_{-\infty} \frac{(x - \mu)^2}{\sqrt{2\pi}\sigma} e^{-(x-\mu)^2/(2\sigma^2)} dx = \sigma^2 \int^\infty_{-\infty} \frac{
u^2}{\sqrt{2\pi}} e^{-\frac 12 u^2} du
\ee
then, integrating by parts, this
\be
= \sigma^2 \int^\infty_{-\infty} \frac{u}{\sqrt{2\pi}} d\bb{-e^{-\frac 12 u^2}} = \sigma^2 \bb{\bsb{-\frac u{\sqrt{2\pi}}e^{-\frac 12 u^2}}^\infty_{-\infty}
+ \int^\infty_{-\infty} \frac 1{\sqrt{2\pi}} e^{-\frac 12 u^2} du} = \sigma^2.
\ee
We see that the two parameters $\mu$ and $\sigma^2$ of the normal distribution represent the mean and variance of $X$, ($\sigma$ is the standard deviation of $X$), we usually write $X \sim \sN\bb{\mu, \sigma^2}$. The special case $\mu = 0$ and $\sigma^2 = 1$ gives what is known as the standard normal  distribution, $\sN(0, 1)$, the distribution function in this case is usually denoted by $\Phi(x)$ and is given by
\be
\Phi(x) = \int^x_{-\infty} \frac 1{\sqrt{2\pi}} e^{-\frac 12 u^2} du.
\ee

Denote the p.d.f. of the standard normal distribution by $\phi(x) = \Phi'(x) = e^{-x^2/2}/\sqrt{2\pi}$, then, since $\phi(x) = \phi(-x)$, we have that
\be
\Phi(x) = 1 - \Phi(-x),\quad\quad -\infty < x < \infty.
\ee
Note that if $X \sim \sN\bb{\mu, \sigma^2}$ and $Y = aX +b$ where $a$ and $b$ are constants with $a \neq 0$, then $Y \sim \sN\bb{a\mu + b, a^2\sigma^2}$. To see this, apply Theorem \ref{thm:inverse_density} with $y = g(x) = ax+b$, so that the inverse is $g^{-1} (y) = (y - b)/a$, to show that the p.d.f. of $Y = g(X)$ evaluated at $y$ is
\be
\frac 1{\sqrt{2\pi}\sigma} e^{-(g^{-1}(y)-\mu)^2/(2\sigma^2)} \abs{\frac{d}{dy} g^{-1}(y)} = \frac 1{\sqrt{2\pi} \abs{a}\sigma} e^{-(y-a\mu-b)^2/(2a^2\sigma^2)},
\ee
as required. Note that, when $X \sim \sN\bb{\mu,\sigma^2}$, it follows that $((X -\mu)/\sigma) \sim \sN(0, 1)$. This fact is important since it enables the calculation of a probability for any $X \sim \sN(\mu,\sigma^2)$ to be expressed in terms of the standard normal distribution, by subtracting off the mean $\mu$ and dividing by the standard deviation $\sigma$, as for example
\be
\pro (X \leq a) = \pro\bb{\frac{X -\mu}{\sigma} \leq \frac{a - \mu}{\sigma}} = \Phi\bb{\frac{a-\mu}{\sigma}}.
\ee
Important points of the standard normal distribution function are
\begin{center}
\begin{tabular}{ccccc}
$x$ & 1.28 & 1.64 & 1.96 & 2.33\\
\hline
$\Phi(x)$ & 0.90 & 0.95 & 0.975 & 0.99\\
\end{tabular}
\end{center}

The third of these points leads to an important observation: for $X \sim \sN\bb{\mu,\sigma^2}$,

\be
\pro\bb{\mu-2\sigma \leq X \leq \mu + 2\sigma} = \pro\bb{\abs{\frac{X-\mu}{\sigma} }\leq 2} \geq \pro\bb{\abs{\frac{X-\mu}{\sigma} } \leq 1.96} =0.95,
\ee
\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.02 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0 

\move (-2.5 0) \avec(2.5 0) 
\move (0 0) \avec(0 1.5) 

\move (-2 0) \clvec (-1.8 0)(-1.5 0)(-1.2 0.2) 
\move (-1.2 0.2) \clvec (-0.9 0.4)(-0.5 1.2)(0 1.2) 
\move (2 0) \clvec (1.8 0)(1.5 0)(1.2 0.2) 
\move (1.2 0.2) \clvec (0.9 0.4)(0.5 1.2)(0 1.2)

\htext (-1.5 -0.15){-1.96}
\htext (1.5 -0.15){1.96}
\htext (0.05 1.35){$\phi(x)$}
\htext (1.55 0.2){Area = 0.025}
\htext (2.5 -0.15){$x$}
\htext (-2 1){standard normal p.d.f.}

\move (-2 0) \clvec (-1.8 0)(-1.5 0)(-1.2 0.2) \lvec (-1.2 0) \lvec (-2 0)  \lfill f:0.8
\move (2 0) \clvec (1.8 0)(1.5 0)(1.2 0.2) \lvec (1.2 0) \lvec (2 0)  \lfill f:0.8

\move(0 1.6)
}

which is usually summed up in the statement "more than 95\% of the normal distribution is within two standard deviations of the mean". 
\end{example}

\begin{example}[Uniform distribution\index{Uniform distribution}]
For constants $a < b$, let $f(x) = 1/(b - a)$ for $a \leq x \leq b$, and $f(x) = 0$, otherwise. Then the random variable has the uniform distribution on the interval $[a, b]$, and we write $X \sim U[a, b]$. Note that
\be
\E X = \int^b_a x/(b - a)dx = (a + b)/2,
\ee
and similarly $\E \bb{X^2} = \bb{a^2 + ab + b^2} /3$, which implies that $\var (X) = \frac 1{12} (b - a)^2$. 

In the case where $X \sim U(0, 1]$, let $Y = -log (X)$, then for $y > 0$,
\be
\pro(Y \leq y) = \pro (-\log(X) \leq y) = \pro \bb{X \geq e^{-y}} = \int^1_{e^{-y}} dx = 1 - e^{-y},
\ee
so that $Y \sim \sE(1)$, that is, $Y$ has the exponential distribution with parameter 1.
\end{example}

A result that is important for computer simulation of random variables is the following. 

\begin{theorem}
Suppose that $U \sim U[0, 1]$, then for any continuous distribution function $F$, the random variable $X = F^{-1}(U)$ has distribution function $F$.
\end{theorem}

\begin{proof}[\bf Proof]
Note that for $u \in [0, 1]$, $\pro (U \leq u) = u$, so we have
\be
\pro (X \leq x) = \pro\bb{F^{-1} (U) \leq x} = \pro(U \leq F(x)) = F(x),
\ee
which gives the result.
\end{proof}

Note. There is a corresponding result for discrete random variables. Suppose that $F$ is the distribution function of a discrete random variable and that $p_j = F(x+j)-F(x_j-) > 0$, $j = 1, 2,\dots$, for values $x_1,x_2,\dots$, where $\sum_j p_j = 1$. Now suppose that $U \sim U[0, 1]$ and define a random variable $X$, by setting $X = x_1$ when $0 < U \leq p_1$, and for $j > 1$, set
\be
X = x_j,\quad \text{ when } \sum^{j-1}_{i=1} p_i < U \leq \sum^j_{i=1} p_i,
\ee
then $\pro (X = x_j) = p_j$, for each $j$, and $X$ has the distribution function $F$. As a consequence, in order to simulate any random variable it is only necessary to use a random number generator to provide a random number uniform in $[0, 1]$ and then use the above procedures in the continuous and discrete cases.

\begin{definition}
The median\index{median} $m$ of a continuous random variable $X$ with density function $f$ is the point which satisfies
\be
\pro (X \geq m) = \int^\infty_m f(x)dx = \int^m_{-\infty} f(x)dx = \pro (X \leq m) = \frac 12.
\ee
\end{definition}

Thus half the distribution lies on one side of $m$ and half on the other. For a discrete random variable, $X$, a median $m$ is a point satisfying
\be
\pro (X \geq m) \geq \frac 12 \quad \text{and}\quad \pro (X \leq m) \geq \frac 12.
\ee

Note that for the normal distribution $\sN(\mu,\sigma^2)$ the mean is equal to the median (and this is true for any symmetric distribution).

\begin{definition}
A mode\index{mode} of a continuous random variable, with density function $f$, is a point $m$ for which $f(m) \geq f(x)$ for all $x$, that is the density function is maximized at a mode. For a discrete random variable a mode is a point, $m$, for which $\pro(X = m) \geq \pro (X = x)$ for all possible values $x$.
\end{definition}

In the case of the normal distribution function, the mean and median are also the mode. For example, for the $\sE(\lm)$ distribution with density function $\lm e^{-\lm x}$ for $x > 0$, we have seen that the mean is $1/\lm$, it is easy to check that the median is $\log 2/\lm$ and the mode is 0.


\subsection{Joint distribution functions}

To start with, to keep the notation simpler, consider just the case of two random variables. 

\begin{definition}
The joint distribution function\index{joint distribution function} of $X$ and $Y$ is
\be
F(x, y) = P (X \leq x, Y \leq y),\quad -\infty < x < \infty, -\infty < y < \infty,
\ee
so that $F : \R^2 \to  [0, 1]$. If there exists a function $f(\cdot, \cdot)$ with
\be
F(x, y) = \int^x_{-\infty}\int^y_{-\infty} f(u, v)dudv, \quad\text{so that }f(x, y) = \frac{\partial^2F}{\partial x\partial y} ,
\ee
then $f$ is the joint probability density function\index{joint probability density function} of $X$ and $Y$. Note that, for any region $C \subseteq \R^2$,
\be
\pro ((X, Y ) \in C) = {\int\int}_{(x,y)\in C} f(x, y)dxdy.
\ee
Furthermore,
\be
f_X(x) = \int^\infty_{-\infty} f(x, y)dy \quad \text{and}\quad f_Y (y) = \int^\infty_{-\infty} f(x, y)dx
\ee
are the marginal probability density functions\index{marginal probability density function} of $X$ and $Y$, respectively.
\end{definition}


Properties of the joint distribution function $F(x, y)$
\ben
\item [1.] $F(x, y)$ is non-decreasing in $y$ for each fixed $x$, and in $x$ for each fixed $y$.
\item [2.] $F(x, y)$ is right continuous in $y$ for each fixed $x$, and in $x$ for each fixed $y$.
\item [3.] $F(1,1) = \lim_{x\ua \infty}\lim_{y\ua \infty} F(x, y) = 1$, for each fixed $x$, $F(x,-\infty) = \lim_{y \da -\infty} F(x, y) = 0$ and for each fixed $y$, $F(-\infty, y) = \lim_{x\da -\infty} F(x, y) = 0$. Furthermore, $F(x,\infty) = \pro (X \leq x)$ and $F (\infty, y) = \pro (Y\leq y)$ are the marginal probability distributions of $X$ and $Y$, respectively.
\item [4.] For all $x_1, x_2, y_1$ and $y_2$ with $x_1 < x_2$, $y_1 < y_2$,
\be
F(x_2, y_2) - F(x_1, y_2) - F(x_2, y_1) + F(x_1, y_1) \geq 0.
\ee
\begin{proof}[\bf Proof]
The result follows from the observation that the expression that the left-hand side 
\be
\pro (X \leq x_2, Y \leq y_2) - \pro (X \leq x_1, Y \leq y_2) - \pro (X \leq x_2, Y \leq y_1) + \pro (X \leq x_1, Y \leq y_1)
\ee
equals $\pro (x_1 < X \leq x_2, y_1 < Y \leq y_2) \geq 0$. This is most easily seen by plotting in $\R^2$ the different regions in which $(X, Y)$ lies corresponding to the different probabilities.
\end{proof}
\een


Properties of the joint probability density function $f(x, y)$
\ben
\item [1.] $f(x, y) \geq 0$, for all $x, y$.
\item [2.] $\int^\infty_{-\infty}\int^\infty{-\infty}f(x, y)dxdy = 1$.
\een
For any random variable of the form $g(X, Y )$, for some function $g$, we compute the expectation as
\be
\E g(X, Y ) = \int^\infty_{-\infty}\int^\infty{-\infty}g(x,y)f(x, y)dxdy,
\ee
in particular, we may obtain the covariance in the continuous case with the same definition as in the discrete case
\be
\cov (X, Y ) = \E ((X - \E X) (Y - \E Y )) = \E (XY ) - (\E X) (\E Y ) ,
\ee
and it has the same properties as set out previously. Likewise for the correlation coefficient in the context of continuous random variables, it is defined in the same way as for discrete random variables,
\be
\corr (X, Y ) = \cov (X, Y )/\sqrt{\var (X)\var (Y )},
\ee
and it has the same properties as mentioned in the discrete case.

\begin{definition}
We define the conditional density\index{conditional density} of $X$ given $Y = y$ to be
\be
f_{X|Y} (x | y) = \frac{f(x, y)}{f_Y (y)} ,
\ee
note that the Law of Total Probability here is that the marginal density of $X$ may be expressed as
\be
f_X(x) = \int^\infty_{-\infty} f_{X|Y} (x | y)f_Y (y)dy.
\ee
\end{definition}

Then the conditional expectation of $X$ given $Y = y$ is
\be
\E (X | Y = y) = \int^\infty_{-\infty} x f_{X|Y} (x | y)dx.
\ee
If we set $g(y) = \E (X | Y = y)$, then the random variable $g(Y ) = \E (X | Y )$ is the conditional expectation of $X$ given $Y$ and has the same properties as given for the conditional expectation in the discrete case.

\begin{example}
Consider the joint density for $X$ and $Y$ given by

\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.02 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0 

\move (-0.2 0) \avec(1.5 0) 
\move (0 -0.2) \avec(0 1.5) 

\move (0 0) \lvec (1.2 1.2)\lvec(0 1.2)\lvec(0 0) \lfill f:0.8
\move (0 0) \lvec (1.2 0)\lvec (1.2 1.2)\lvec(0 1.2)\lvec(0 0) 

\htext (1.2 -0.15){1}
\htext (-0.1 1.1){1}
\htext (0.05 1.3){$y$}
\htext (1.5 -0.15){$x$}

\htext (-3 0.6){$f(x, y) = \left\{\ba{ll} 8xy \quad\quad & 0 \leq x \leq y \leq 1,\\ 0 & \text{otherwise.}\ea\right.$}

\move(0 1.6)
}

Here, $(X, Y)$ are distributed over the upper half of the unit square as illustrated in the diagram. You should check that this is indeed a joint p.d.f. in that it integrates to 1 over the region. Compute the marginal densities of $X$ and $Y$,
\be
f_X(x) = \int^1_x 8xy dy = 4x(1 - x^2) \quad\text{and}\quad f_Y (y) = \int^y_0 8xy dx = 4y^3,
\ee
for $0 \leq x \leq 1$ and $0 \leq y \leq 1$. Calculate that
\be
\E X = \int^1_0 xf_X(x)dx = \int^1_0 4x^2(1 - x^2)dx = 4\bb{\frac 13 - \frac 15} = \frac 8{15}
\ee
and similarly $\E Y = \frac 45$. The conditional densities are
\be
f_{X|Y} (x | y) = \frac{8xy}{4y^3} = \frac{2x}{y^2},\quad\quad f_{Y|X}(y | x) = \frac{8xy}{4x(1 - x^2)} = \frac{2y}{1 - x^2},
\ee
for $0 \leq x \leq y \leq 1$. We then have
\be
\E (X | Y = y) = \int^y_0 x\frac{2x}{y^2} dx = \frac {2y}3,\quad\quad \E (Y | X = x) = \int^1_x y\frac{2y}{1 - x^2} dx = \frac{2(1 - x^3)}{3(1 - x^2)}.
\ee
We see that $\E (X | Y ) = 2Y/3$ and $\E (Y | X) = 2\bb{1- X^3}/\bb{3\bb{1- X^2}}$. Check that we have $\E (\E (X | Y )) = \E X$, and $\E (\E (Y | X)) = \E Y$. 
\end{example}


The joint distribution function and density function extends to any number of random variables, in the obvious way. For random variables $X_1, \dots ,X_n$, the joint distribution
function is
\beast
F (x_1,\dots , x_n) & = & \pro (X_1 \leq x_1,\dots ,X_n \leq x_n),\quad -\infty < x_i < \infty,\ 1 \leq i \leq n,\\
& = & \int^{x_1}_{-\infty} \dots \int^{x_n}_{-\infty} f (u_1,\dots , u_n) du_1\dots du_n,
\eeast
where $f (u_1,\dots , u_n)$ is the joint probability density function. Note that
\be
f (x_1,\dots , x_n) = \frac{\partial^nF}{\partial x_1 \dots \partial x_n} .
\ee
The expectation of a function of $X_1,\dots ,X_n$ is computed as
\be
\E g (X_1,\dots ,X_n) = \int^\infty_{-\infty} \dots\int^\infty_{-\infty} g (x_1,\dots , x_n) f (x_1,\dots , x_n) dx_1\dots dx_n.
\ee
Independence for continuous random variables may be defined similarly to the discrete case. Random variables $X_1,\dots ,X_n$ are independent\index{independent} if 
\be
\pro(X_1 \in S_1,X_2 \in S_2,\dots ,X_n \in S_n) = \pro (X_1 \in S_1) \pro (X_2 \in S_2) \dots \pro (X_n \in S_n) ,
\ee
for all $S_i \subseteq \Omega_{X_i}$, $1 \leq i \leq n$, this is equivalent to each of the statements that the joint distribution function
\be
F (x_1, x_2,\dots , x_n) = F_{X_1} (x_1)F_{X_2} (x_2) \dots F_{X_n}(x_n), \quad \text{for all }x_i, 1 \leq i \leq n,
\ee
factors into the product of the marginal distribution functions, $F_{X_i}$, and the joint probability density function
\be
f (x_1, x_2,\dots , x_n) = f_{X_1} (x_1)f_{X_2} (x_2) \dots f_{X_n}(x_n),\quad\text{for all }x_i,\ 1 \leq i \leq n,
\ee
factors into the product of the marginal densities, $f_{X_i}$. It follows that if $X_1,\dots ,X_n$ are independent then, for functions $g_1,\dots , g_n$,
\beast
\E \bb{\prod^n_{i=1} g_i (X_i)} & = & \int^\infty_{-\infty} \dots \int^\infty_{-\infty} \bb{\prod^n_{i=1} g_i(x_i)} f (x_1, x_2,\dots , x_n) dx_1\dots dx_n\\
& = & \int^\infty_{-\infty} \dots \int^\infty_{-\infty} \bb{\prod^n_{i=1} g_i(x_i)} f_{X_1}(x_1)f_{X_2}(x_2)\dots f_{X_n}(x_n) dx_1\dots dx_n\\
& = & \prod^n_{i=1} \bb{\int^\infty_{-\infty} g_i(x_i)f_{X_i} (x_i)dx_i} = \prod^n_{i=1}\bb{\E (g_i (X_i))},
\eeast
that is, as in the discrete case, the expectation of the product is the product of the expectations. This shows, as in the discrete case, that if $X$ and $Y$ are independent then $\cov (X, Y ) = \E(XY ) - (\E X)(\E Y ) = 0$.

Note that for independent random variables $X$, $Y$ the conditional density of $X$ given $Y = y$ is
\be
f_{X|Y} (x | y) = \frac{f(x, y)}{f_Y (y)} = \frac{f_X(x)f_Y (y)}{f_Y (y)} = f_X(x),
\ee
which is of course just the unconditioned density function of $X$.

\begin{example}
Suppose that $X$ and $Y$ are independent random variables each with the $U[0, 1]$ distribution and that we wish to calculate $\pro(X < Y)$. There are several ways that we might proceed. Firstly, the joint p.d.f. of $X$ and $Y$ is $f(x, y) = f_X(x)f_Y (y) = 1$ for $0 \leq x \leq 1$ and $0 \leq y \leq 1$. Then,
\be
\pro(X < Y ) = {\int\int}_{0\leq x<y\leq 1} f(x, y)dxdy = \int^1_0\int^1_x dy dx = \int^1_0 (1- x)dx = \bsb{x - x^2/2}^1_0 = \frac 12.
\ee
Alternatively we could write, using the Law of Total Probability,
\be
\pro(X < Y ) = \int^1_0 \pro(X < Y | Y = y) f_Y (y)dy = \int^1_0 \pro (X < y) dy = \int^1_0 ydy = \bsb{y^2/2}^1_0 = \frac 12 .
\ee

Or, finally in this case we can argue graphically, since the joint distribution of $X$ and $Y$ is uniform over the unit square,

\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.02 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0 

\move (-0.2 0) \avec(1.5 0) 
\move (0 -0.2) \avec(0 1.5) 

\move (0 0) \lvec (1.2 1.2)\lvec(0 1.2)\lvec(0 0) \lfill f:0.8
\move (0 0) \lvec (1.2 0)\lvec (1.2 1.2)\lvec(0 1.2)\lvec(0 0) 

\htext (1.2 -0.15){1}
\htext (-0.1 1.1){1}
\htext (0.05 1.3){$y$}
\htext (1.5 -0.15){$x$}

\move(0 1.5)
}

then $\pro(X < Y )$ is just the area of the shaded region, which is $\frac 12$. 
\end{example}


For independent random variables $X$ and $Y$, the density function of $X + Y$ may be expressed in terms of the densities of $X$ and $Y$ as
\be\label{equ:sum_of_continuous_random}
f_{X+Y} (z) = \int^\infty_{-\infty} f_X(z - y)f_Y (y)dy = \int^\infty_{-\infty} f_X(x)f_Y (z - x)dx, 
\ee
this is known as the convolution\index{convolution} of the two densities. It is derived from the corresponding statements involving distribution functions, when $F_{X+Y} (z) = \pro (X + Y \leq z)$, which are
\bea
F_{X+Y} (z) & = & \int^\infty_{-\infty} \pro(X + Y \leq z | Y = y) f_Y (y)dy = \int^\infty_{-\infty} F_X(z - y)f_Y (y)dy\nonumber\\
& = & \int^\infty_{-\infty} \pro (X + Y \leq z | X = x) f_X(x)dx = \int^\infty_{-\infty} f_X(x)F_Y (z - x)dx.\label{equ:distribution_convolution}
\eea

Then (\ref{equ:sum_of_continuous_random}) is obtained by differentiating with respect to $z$ either of the two expressions in (\ref{equ:distribution_convolution}).

\begin{example}[Minimum of exponentials is exponential]
Suppose that $X \sim \sE(\lm)$ and $Y \sim \sE(\lm)$ are independent then consider the distribution of $\min(X, Y)$. Using the independence, we see that for $x \geq 0$,
\beast
\pro (\min(X, Y ) \leq x) & = & 1 - \pro (\min(X, Y ) > x) = 1 - \pro (X > x, Y > x) \\
& = & 1 - \pro (X > x) \pro (Y > x) = 1 - e^{-\lm x}e^{-\mu x} = 1 - e^{-(\lm +\mu)x},
\eeast
so that $\min(X, Y ) \sim \sE(\lm + \mu)$.

We may extend this, using induction on $n$, to see that if $X_1,\dots ,X_n$ are independent, with $X_i \sim \sE(\lm_i)$, then 
\be
\min_{1\leq i\leq n} X_i \sim \sE(\lm_1 + \dots + \lm_n).
\ee
In particular, when $X_1,\dots ,X_n$ are i.i.d. with each $X_i \sim \sE(\lm)$, then $\min_{1\leq i \leq n} X_i \sim \sE(n\lm)$. 
\end{example}

\begin{example}[Order statistics of a random sample]
Independent, identically random variables $X_1,\dots ,X_n$ each having the continuous distribution $F(x)$ are said to be a random sample from the distribution $F$. The values of these random variables arranged in increasing order are usually written as
\be
X_{(1)} \leq X_{(2)} \leq \dots \leq X_{(n-1)} \leq X_{(n)}.
\ee
The values $Y_i = X_{(i)}$ are said to be the order statistics\index{order statistics} of the sample. Thus, $Y_1 = \min_{1\leq i\leq n} X_i$ is the smallest of the random variables, $Y_2$ is the second smallest and so on with $Y_n = \max_{1\leq i\leq n} X_i$. As in the previous example, we may calculate the distribution of $Y_1$,
\beast
\pro (Y_1 \leq x) & = & \pro\bb{\min_{1\leq i\leq n} X_i \leq x} = 1-\pro\bb{\min_{1\leq i\leq n} X_i > x}\\
& = & 1 - \pro (X_1 > x,\dots ,X_n > x) = 1 - \prod^n_{i=1} \pro(X_i > x) = 1 - (1 - F(x))^n.
\eeast
Then the p.d.f. of $Y_1$ is $n (1 - F(x))^{n-1} f(x)$, where $f(x) = F'(x)$ is the p.d.f. of the $\{X_i\}$. A similar calculation shows that for $Y_n$,
\be
\pro(Y_n \leq x) = \pro\bb{\max_{1\leq i\leq n} X_i \leq x} = (F(x))^n \quad \text{and its p.d.f. is }n (F(x))^{n-1} f(x).
\ee
We may also see that the joint p.d.f. of $Y_1,\dots , Y_n$ is given by
\be
g(y_1,\dots , y_n) = \left\{\ba{ll}
n! f(y_1) \dots f(y_n)\quad\quad & y_1 < \dots < y_n,\\
0 & \text{otherwise.}
\ea\right.
\ee
To see this consider the joint probabilities that $Y_i \in (y_i, y_i + dy_i)$, $1 \leq i \leq n$, and see that there are $n$ choices from the $\{X_i\}$ for the smallest order statistic, $n- 1$ choices for the second smallest and so on to understand how the factor $n!$ in the expression for the joint density is obtained. 
\end{example}

\subsection{Moment generating functions}

The moment generating function\index{moment generating function} (m.g.f.) of a random variable $X$, with p.d.f. $f(x)$, is
\be
m(\theta) = \E \bb{e^{\theta X}} = \int^\infty_{-\infty} e^{\theta x} f(x)dx,
\ee

defined for those values of $\theta$ for which the expectation is finite. Note that it is always defined for $\theta = 0$, and that $m(0) = 1$. When discussing moment generating functions we will assume that we are considering random variables for which the m.g.f. is defined for some non-trivial interval of values $\theta$, (including 0). The m.g.f. plays the same role for more general random variables as the p.g.f. does for non-negative integer-valued random variables. Its importance stems from the following result, which we will not prove. 

\begin{theorem}
The moment generating function $m(\theta) = \E\bb{e^{\theta X}}$ determines the distribution of $X$ uniquely provided it is defined for some open interval of values of $\theta$. 
\end{theorem}

The name moment generating function stems from the following result.

\begin{theorem}\label{thm:moments_mgf}
If the moment generating function $m(\theta) = \E \bb{e^{\theta X}}$ is defined for some open interval of values of $\theta$, then for each $r \geq 1$, $m^{(r)}(0) = \E (X^r)$, where $m^{(r)}$ is the $r$th derivative of $m$.
\end{theorem}

Here, it is possible that $m(\theta)$ is not differentiable at $\theta = 0$ since it is possible that $m(\theta)$ is not defined for, say, $\theta > 0$, (or alternately for $\theta < 0$), but we may interpret $m^{(r)}(0)$ as $\lim_{\theta \ua 0} m^{(r)}(\theta)$ or $\lim_{\theta \da 0} m^{(r)}(\theta)$, as appropriate, and the result is still true. We will not give a formal proof of Theorem \ref{thm:moments_mgf}, but to see intuitively why it holds, observe that
\be
e^{\theta X} = 1 + \theta X + \frac {\bb{\theta X}^2}{2!} + \frac{\bb{\theta X}^3}{3!} + \dots,
\ee
so that, after taking expectations we see that
\be
m(\theta) = 1 + \theta \E (X) + \frac {\theta^2\E\bb{X^2}}{2!} + \frac{\theta^3\E \bb{X^3}}{3!} + \dots ,
\ee
now differentiate $r$ times with respect to $\theta$ and set $\theta = 0$.

The other important application for moment generating functions is for studying sums of independent random variables since, if $X_1,\dots ,X_n$ are independent random variables with m.g.f.s $m_{X_1} (\theta),\dots ,m_{X_n}(\theta)$, respectively, then the m.g.f. of $X_1 + \dots + X_n$ is
\be
m_{X_1 +\dots +X_n}(\theta ) = \E\bb{e^{\theta (X_1+\dots +X_n)}} =  \prod^n_{i=1} \E\bb{e^{\theta X_i}} = \prod^n_{i=1} m_{X_i} (\theta),
\ee
just the product of the individual generating functions.

\begin{example}[The Gamma distribution\index{Gamma distribution}]
A random variable $X$ with p.d.f. 
\be
f(x) = e^{-\lm x} \lm^n x^{n-1} /((n - 1)!), \quad x \geq 0,\quad (f(x) = 0, x < 0),
\ee
is said to have a Gamma distribution with parameters $\lm > 0$ and integer $n \geq 1$, usually written $X \sim \Gamma(n,\lm)$. Notice that the case $n = 1$ is the exponential distribution introduced previously. We need to check that the function $f$ is indeed a p.d.f., that is, it integrates to 1, but this follows by integration by parts since, for $n > 1$,
\be
I_n = \int^\infty_0 e^{-\lm x} \frac{\lm^nx^{n-1}}{(n - 1)!}dx = \int^\infty_0 \frac{(\lm x)^{n-1}}{(n - 1)!} d\bb{-e^{-\lm x} } = \bsb{-e^{-\lm x} \frac{(\lm x)^{n-1}}{(n - 1)!}}^\infty_0 + I_{n-1} = I_{n-1},
\ee
and $I_1 = 1$. The moment generating function of $X$, for $\theta < \lm$, is
\be
m(\theta) = \E \bb{e^{\theta X}} = \int^\infty_0 e^{\theta x} e^{-\lm x} \frac{\lm^nx^{n-1}}{(n - 1)!} dx = \bb{\frac{\lm}{\lm-\theta}}^n \int^\infty_0
e^{-(\lm-\theta)x} \frac{(\lm - \theta)^nx^{n-1}}{(n - 1)!} dx = \bb{\frac{\lm}{\lm - \theta}}^n,
\ee
since the last integral is 1 by the above argument (replacing $\lm$ by $\lm-\theta$). In particular, if $X\sim \sE(\lm)$ then $X$ has m.g.f. $\lm/(\lm -\theta)$. Then
\be
m'(\theta) = \frac{n\lm^n}{(\lm -\theta)^{n+1}}, \quad \text{so that }\E (X) = m'(0) = \frac n{\lm},
\ee
and similarly $\E\bb{X^2} = m''(0) = n(n + 1)/\lm^2$, so that $\var (X) = n/\lm^2$. Now if $Y$ is independent of $X$ and $Y \sim \Gamma(m, \lm)$ the the m.g.f. of $X + Y$ is
\be
\E\bb{e^{\theta (X+Y )}} = \E \bb{e^{\theta X}}\E \bb{e^{\theta Y}} = \bb{\frac{\lm}{\lm- \theta}}^n \bb{\frac{\lm}{\lm- \theta}}^m = \bb{\frac{\lm}{\lm- \theta}}^{n+m},
\ee
so that $X + Y \sim \Gamma(n + m, \lm)$. Using induction, we may deduce that if $X_1,\dots ,X_n$ are i.i.d. with $X_1 \sim \sE(\lm)$, then $X_1 + \dots + X_n \sim \Gamma(n,\lm)$. Note that this gives an alternate explanation of why for the Gamma distribution the mean and variance are $n/\lm$ and $n/\lm^2$, respectively. Note further that the Gamma distribution generalizes to non-integer parameter $\alpha > 0$ (replacing $n$) if $(n - 1)!$ is replaced in the definition of the probability density by the Gamma function $\Gamma(\alpha) = \int^\infty_0 e^{-x} x^{\alpha -1} dx$.
\end{example}

\begin{example}[The Normal distribution\index{Normal distribution}] 
Suppose that $X \sim \sN(\mu, \sigma^2)$, then the m.g.f. is
\be
m(\theta) = \E\bb{e^{\theta X}} = \int^\infty_{-\infty} e^{\theta x} \frac 1{\sqrt{2\pi}\sigma} e^{-(x-\mu)^2/(2\sigma^2)} dx,
\ee
but the argument of the exponential in the integral is
\be
\theta x - \frac{(x - \mu)^2}{2\sigma^2} = \mu \theta + \frac{\theta^2\sigma^2}2 - \frac{(x -\mu-\theta \sigma^2)^2}{2\sigma^2} ,
\ee
so that
\be
m(\theta) = e^{\mu \theta + \theta^2\sigma^2/2} \int^\infty_{-\infty} \frac 1{\sqrt{2\pi}\sigma} e^{-(x-\mu - \theta \sigma^2)^2/(2\sigma^2)} dx = e^{\mu \theta + \theta^2 \sigma^2/2},
\ee
since the integrand is just the p.d.f. of the $\sN\bb{\mu+ \theta\sigma^2, \sigma^2}$-distribution. We may check the fact that we established previously that a linear transformation of $X$ has a normal distribution, that is $aX + b \sim \sN\bb{a\mu + b, a^2\sigma^2}$, for constants $a$ and $b$ since the m.g.f. of $aX + b$ is
\be
\E\bb{e^{\theta(aX+b)} }= e^{b\theta }\E\bb{e^{(a\theta)X}} = e^{b\theta} e^{a\theta \mu +a^2\theta^2\sigma^2/2} = e^{\theta(a\mu+b)+a^2\theta^2\sigma^2/2},
\ee
which has the required form. If $Y\sim \sN(\nu, \tau^2)$ is independent of $X$ we see that the m.g.f. of $X + Y$ is
\be
e^{\mu \theta+\theta^2\sigma^2/2} e^{\nu\theta+\theta^2\tau^2/2} = e^{(\mu +\nu)\theta+\theta^2(\sigma^2+\tau^2)/2},
\ee
which is the m.g.f. of the $\sN\bb{\mu + \nu, \sigma^2 + \tau^2}$-distribution, we conclude that if we sum independent normally-distributed random variables we get a normally-distributed random variable-sum the means and sum the variances.
\end{example}

\subsection{Transformations of random variables}

We first consider the case of two random variables $X$, $Y$, with joint p.d.f. $f(x, y)$, and suppose that $U$ and $V$ are random variables which are functions of $X$ and $Y$ derived from a one-to-one transformation $(x, y) \mapsto (u, v)$, so that $U = a(X, Y)$, $V = b(X, Y)$, say, and moreover $X$ and $Y$ may be written as functions of $U$ and $V$ as $X = A(U, V)$ and $Y = B(U, V)$. In order to obtain the joint p.d.f. $g(u, v)$ of the pair $U$ and $V$, recall the definition of the Jacobian
\be
\fp{(x, y)}{(u, v)} = \bevm 
\fp{x}{u} & \fp{x}{v}\\
\fp{y}{u} & \fp{y}{v}
\eevm
= \fp{x}{u} \fp{y}{v} - \fp{x}{v}\fp{y}{u}.
\ee
of the transformation $(u, v) \mapsto (x, y)$. Then the joint p.d.f. $g(u, v)$ is given by
\be\label{equ:Jacobian_gf}
g(u, v) = f (x, y) \abs{\fp{(x, y)}{(u, v)}} . 
\ee

This follows from the fact that if a region $S$ in the $(x, y)$-plane maps into the region $T$ in the $(u, v)$-plane then we must have
\be
\pro((X, Y ) \in S) = {\int\int}_S f(x, y)dxdy = {\int\int}_T g(u, v)dudv = \pro ((U, V ) \in T).
\ee
The change-of-variable formula in multiple integration comes from the following idea. the element of area, which may be thought of as a rectangle in the the $(u, v)$-plane with sides of length $\triangle u$ and $\triangle v$, maps into a parallelogram in the $(x, y)$-plane bounded by vectors $r$ and $s$ (which we think of as being in $\R^3$) as illustrated,

\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.02 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0 

\move (0 0) \lvec(1 0) \lvec(1 0.5) \lvec(0 0.5) \lvec(0 0)\lfill f:0.8

\move (3 0) \lvec(4 0.2) \lvec(4.6 0.6) \lvec(3.6 0.4)\lvec (3 0) \lfill f:0.8

\move (3 0) \avec (3.5 0.1)
\move (3 0) \avec (3.3 0.2)

\htext (-0.2 -0.15){$(u,v)$}
\htext (0.9 0.55){$(u+\triangle u, v+\triangle v)$}
\htext (3.4 0.4){$s$}
\htext (3.7 0){$r$}
\htext (2.8 -0.15){$(x,y)$}

\move(0 0.8)
}

where 
\beast
r & = & (x(u + \triangle u, v) - x(u, v), y(u + \triangle u, v) - y(u, v), 0)\\
& \approx & \triangle u\bb{\fp{x}{u},\fp{y}{u}, 0} = \triangle u \bb{\fp{x}{u}i + \fp{y}{u} j},
\eeast
and similarly, $s \approx \triangle v\bb{\fp{x}{v} i + \fp{y}{v}j}$, here $i$, $j$ and $k$ are the standard basic unit vectors in $\R^3$. Then by the determinant rule, the cross product between $r$ and $s$ is
\be
r \times s = \triangle u \triangle v \bevm
i & j & k\\
\fp{x}{u} & \fp{y}{u} & 0\\
\fp{x}{v} & \fp{y}{v} & 0
\eevm
= \triangle u \triangle v \fp{(x, y)}{(u, v)}k.
\ee

It follows that the area of the parallelogram is $\abs{r \times s} = \abs{\fp{(x, y)}{(u, v)}} triangle u \triangle v$, from which we see the relation (\ref{equ:Jacobian_gf}).

\begin{example}
Suppose that $X$ and $Y$ are independent, identically distributed random variables each with the $\sE(\lm)$ distribution. Let $U = X + Y$ and $V = X/(X +Y)$. The joint probability density function of $X$ and $Y$ is
\be
f_{X,Y} (x, y) = \lm^2e^{-\lm(x+y)},\quad\quad 0 < x < \infty, 0 < y < \infty.
\ee
Then we have $u = x+y$ and $v = x/(x+y)$, so solving for x and y in terms of $u$ and $v$ gives
\be
x = uv, y = u(1 - v),\quad\quad 0 < u < \infty, 0 < v < 1.
\ee

We calculate the Jacobian,
\be
J = \bevm
\fp{x}{u} & \fp{x}{v} \\
\fp{y}{u} & \fp{y}{v}
\eevm
= \bevm
v & u\\
1 - v & -u
\eevm = -vu - u(1 - v) = -u.
\ee

The joint density of $U$ and $V$ is then
\be
g_{U,V} (u, v) = f_{X,Y} (uv, u(1 - v)) \abs{J} = \lm^2ue^{-\lm u},\quad\quad 0 < u < \infty, 0 < v < 1.
\ee
We see that this can be viewed as the product of the two probability densities, $g_U (u) = \lm^2ue^{-\lm u}$, which is the density of the $\Gamma(2,\lm)$ distribution, and $g_V (v) = 1$, which is the density of the $U(0, 1)$ distribution, we can conclude that $U$ and $V$ are independent with $g_U$ and $g_V$ as their marginal density functions.

Whenever we calculate a joint probability density function in this way and we see that it splits into a product of functions of the variables separately in such a way that we may normalize the functions so that they become the marginal probability densities of the two random variables, then we may conclude that the random variables are independent.
\end{example}


\begin{example}
Suppose that X and Y have joint p.d.f. given by

\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.02 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0 

\move (-0.2 0) \avec(1.5 0) 
\move (0 -0.2) \avec(0 1.5) 

\move (0 0) \lvec (1.2 0) \lvec (1.2 1.2)\lvec(0 1.2)\lvec(0 0) \lfill f:0.8
\move (0 0) \lvec (1.2 0)\lvec (1.2 1.2)\lvec(0 1.2)\lvec(0 0) 

\htext (1.2 -0.15){1}
\htext (-0.1 1.1){1}
\htext (0.05 1.3){$y$}
\htext (1.5 -0.15){$x$}

\htext (-3 0.5){$f(x,y)=\left\{\ba{ll} 4xy \quad\quad & 0< x<1,0<y<1,\\ 0 & \text{otherwise}\ea\right.$}

\move(0 1.5)
}

and that $U = X/Y$ and $V = XY$. Then $x = \sqrt{uv}$ and $y =\sqrt{v/u}$, and the Jacobian is
\be
\bevm
\fp{x}{u} & \fp{x}{v}\\
\fp{y}{u} & \fp{y}{v}
\eevm
= \bevm
\frac 12 \sqrt{\frac vu} & \frac 12 \sqrt{\frac uv}\\
-\frac 12 \frac {\sqrt{v}}{u^{3/2}} & \frac 12 \frac 1{\sqrt{uv}}
\eevm
= \frac 1{4u} + \frac 1{4u} = \frac 1{2u}.
\ee
We see that the joint density of $U$ and $V$ (when it is non-zero) is then of the form $2v/u$, however, $U$ and $V$ are not independent since the region over which the density is positive does not allow the joint density to split into the product of the marginal densities. We have

\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.02 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 1 

\move (0 0) \lvec(1.2 1.2) \clvec (1.5 1)(2 0.6)(3 0.5) \lvec (3 0)\lfill f:0.8

\linewd 0.01 \setgray 0

\move (-0.2 0) \avec(3.2 0) 
\move (0 -0.2) \avec(0 1.5) 


\move (0 0) \lvec(1.2 1.2) \clvec (1.5 1)(2 0.6)(3 0.5)

\htext (1.2 -0.15){1}
\htext (-0.1 1.1){1}
\htext (0.05 1.3){$v$}
\htext (3.2 -0.15){$u$}

\htext (0.2 0.6){$u=v$}
\htext (2.8 0.6){$uv=1$}

\htext (-3 0.5){$g(x,y)=\left\{\ba{ll} \frac {2v}u \quad\quad & 0< uv<1,0<v/u<1,\\ 0 & \text{otherwise}\ea\right.$}

\move(0 1.5)
}

which is concentrated on the region shown. We may calculate the marginal density of $U$,
\be
g_U (u) = \int^1_0 g(u, v)dv = \int^u_0 \frac{2v}u dv = \bsb{\frac{v^2}u}^u_0 = u,\quad\quad u\leq 1 
g_U (u) = \int^1_0 g(u, v)dv = \int^{1/u}_0 \frac{2v}u dv = \bsb{\frac{v^2}u}^{1/u}_0 = \frac 1{u^3},\quad\quad u > 1.
\ee
Calculating the marginal density of $V$, for $0 < v < 1$, we obtain
\be
g_V (v) = \int^\infty_0 g(u, v)du = \int^{1/v}_v \frac{2v}u du = [2v \log u]^{1/v}_v = -4v \log v,
\ee
and we see that $g(u, v) \neq g_U (u)g_V (v)$. 
\end{example}

\begin{example}[Sums and Convolution]
Suppose that $X$ and $Y$ have joint probability density function $f(x, y)$ and let $U = X + Y$ and $V = Y$, so that $X = U - V$ and $Y = V$. The Jacobian
\be
J = \bevm
\fp{x}{u} & \fp{x}{v}\\
\fp{y}{u} & \fp{y}{v}
\eevm
= 
\bevm
1 & -1\\
0 & 1
\eevm
= 1,
\ee
so that the joint density of $U$ and $V$ is $g(u, v) = f(u - v, v)$. We may then derive the marginal density of $X + Y$ as
\be
f_{X+Y} (u) = \int^\infty_{-\infty}f(u - v, v)dv.
\ee
In the particular case that $X$ and $Y$ are independent we have $f(x, y) = f_X(x)f_Y (y)$ and we derive the formula for the convolution of two independent random variables
\be
f_{X+Y} (u) = \int^\infty_{-\infty} f_X(u - v)f_Y (v)dv,
\ee
that we had derived previously.
\end{example}

\begin{example}
Suppose that $X$ and $Y$ are i.i.d. each with the $\sN(0, 1)$ distribution and let $D = X^2 + Y^2$ and $\Theta = \arctan(Y/X)$. The joint density function of $X$ and $Y$ is
\be
f(x, y) = \frac 1{\sqrt{2\pi}} e^{-x^2/2}  \frac 1{\sqrt{2\pi}} e^{-y^2/2} =  \frac 1{\sqrt{2\pi}} e^{-(x^2+y^2)/2}.
\ee
Then for $d = x^2 + y^2$ and $\theta = \arctan(y/x)$, consider the Jacobian
\be
J = \bevm
\fp{d}{x} & \fp{d}{y}\\
\fp{\theta}{x} & \fp{\theta}{y}
\eevm
= 
\bevm
2x & 2y\\
-\frac y{x^2 + y^2} & \frac x{x^2 + y^2}
\eevm 
= 2,
\ee
so the Jacobian of the inverse transformation is $\frac 12$. It follows that the joint density of $D$ and $Theta$ is
\be
g(d, \theta) = \frac 1{4\pi} e^{-d/2},\quad\quad 0 \leq d < \infty, 0 \leq \theta \leq 2\pi,
\ee
which we may see can be expressed as the product of the marginal densities of $D$ and $\Theta$ as $g(d, \theta) = g_D(d)g_\Theta(\theta)$, where
\be
g_D(d) = \frac 12 e^{-d/2},\quad 0 \leq d < \infty, \quad\quad g_\Theta(\theta) = \frac 1{2\pi},\quad 0 \leq \theta 2\pi.
\ee
This means that $D \sim \sE\bb{\frac 12}$ and $\Theta \sim U[0, 2\pi]$ and they are independent random variables. This suggests a way of simulating $\sN(0, 1)$ random variables. Take $U_1$ and $U_2$ as independent $U[0, 1]$ random variables. Then $D = -2 \log(U_1)$ has the $\sE\bb{\frac 12}$ distribution, while $\Theta = 2\pi U_2$ has the $U[0, 2\pi]$ distribution and we see that
\be
X = \sqrt{D}\cos\Theta = \sqrt{-2 \log U_1} \cos (2\pi U_2),\quad\quad Y = \sqrt{D}\sin\Theta = \sqrt{-2 \log U_1} \sin (2\pi U_2) ,
\ee
are independent standard normals.
\end{example}


We may generalize these ideas to one-to-one transformations of $n$ random variables. Suppose that $X_1,\dots ,X_n$ are random variables with joint probability density function $f(x_1,\dots , x_n)$ and that the random variables $U_1,\dots ,U_n$ are given as functions $U_i = a_i(X_1,\dots ,X_n)$ which we can invert so that $X_i = A_i(U_1,\dots ,U_n)$. The Jacobian of the transformation is
\be
\fp{(x_1, x_2,\dots , x_n)}{(u_1, u_2,\dots , u_n)} =
\bevm
\fp{x_1}{u_1} & \fp{x_1}{u_2} & \dots & \fp{x_1}{u_n}\\
\vdots & \vdots & \vdots & \vdots \\
\fp{x_n}{u_1} & \fp{x_n}{u_2} & \dots & \fp{x_n}{u_n}
\eevm
\ee
and the joint probability density function of $U_1,\dots ,U_n$ is obtained by setting
\be
g(u_1,\dots , u_n) = f(x_1,\dots , x_n)\abs{\fp{(x_1, x_2,\dots , x_n)}{(u_1, u_2,\dots , u_n)}}.
\ee
In particular, if the $\{X_i\}$ are just a linear transformation of the $\{U_j\}$, so that in vector notation
\be
X = \bepm
X_1\\
\vdots \\
X_n
\eepm = AU = A \bepm
U_1\\
\vdots \\
U_n
\eepm,
\ee
where $A$ is an $n \times n$ matrix, then the Jacobian of the transformation is $\det A$. We then have $g(u) = f(Au)\abs{\det A}$.

\begin{example}
Suppose that $X_1,\dots ,X_n$ are independent identically distributed random variables with $X_i \sim \sE(\lm)$, for each $i$, $1 \leq i \leq n$. Let $Y_1,\dots , Y_n$ be the order statistics of the $\{X_i\}$ so that $Y_1 = \min_i X_i$ is the smallest of the $\{X_i\}$, $Y_2$ is the second smallest, and so on, with $Y_n = \max_i X_i$. Think of $X_1,\dots ,X_n$ representing the lifetimes of $n$ components which are plugged in simultaneously at time 0, then $Y_1$ is the time of the first failure, $Y_2$ is the time of the second failure and so on. Set 
\be
Z_1 = Y_1,\quad Z_2 = Y_2 - Y_1,\quad\dots ,\quad Z_n = Y_n - Y_{n-1},
\ee
so that
\be
\bepm
Z_1\\
\vdots\\
Z_n
\eepm = A \bepm
Y_1\\
\vdots\\
Y_n
\eepm, \quad\quad\text{with }A = \bepm
1 & 0 & 0 & \dots & 0 & 0\\
-1 & 1 & 0 & \dots & 0 & 0\\
0 & -1 & 1 & \dots & 0 & 0\\
\vdots & \vdots & \vdots & \dots & \vdots & \vdots\\
0 & 0 & 0 & \dots & -1 & 1
\eepm,
\ee
note that $\det A = 1$ and that $y_j =\sum^j_{i=1}z_i$ for each $j$. Recall that the joint p.d.f. of the order statistics $Y_1,\dots , Y_n$ is 
\be
g(y_1,\dots , y_n) = n! f(y_1) \dots f(y_n) \quad \text{where }f(x) = \lm e^{-\lm x},
\ee
we then obtain the joint p.d.f. of $Z_1,\dots ,Z_n$ as 
\be
h(z_1,\dots , z_n) = n! \lm^ne^{-\lm(y_1+\dots+y_n)} = n! \lm^ne^{-\lm(nz_1+(n-1)z_2+\dots+2z_{n-1}+z_n)} = \prod^n_{i=1} \bb{\lm(n - i + 1)e^{-\lm (n-i+1)z_i}}.
\ee
As the joint p.d.f. factors into n individual probability densities we conclude that the random variables $Z_1,\dots ,Z_n$ are independent with $Z_i \sim \sE(\lm(n- i + 1))$.

Note that this puts together formally two ideas that we have seen from our previous consideration of the exponential distribution. the time until the first failure is the minimum of $n$ i.i.d. exponential random variables, with parameter $\lm$, and so has the exponential distribution with parameter $n\lm$, by the lack of memory property of the exponential distribution, when the first failure of a component occurs, the time from then until the failure of the other components is exponential with the same parameter $\lm$, so the time until the second failure is the minimum of $n - 1$ i.i.d. exponentials and thus is exponential with parameter $(n - 1)\lm$, and so on. 
\end{example}

\subsection{Bivariate normal distribution}

Recall that the random variable $X$ has the $\sN(\mu, \sigma^2)$-distribution if its probability density function is
\be
f_X(x) = \frac 1{\sqrt{2\pi}\sigma} e^{-(x-\mu)^2/(2\sigma^2)},\quad -\infty < x < \infty,
\ee
and that $\mu = \E(X)$ and $\sigma^2 = \var (X)$. We say that random variables $X$ and $Y$ have a bivariate normal distribution (or bivariate Gaussian distribution or joint normal distribution) if their joint probability density function has the form
\be
f_{X,Y} (x, y) = \frac 1{2\pi \sigma \tau \sqrt{1-\rho^2}} \exp\bb{- \frac 1{2(1- \rho^2)} \bb{\frac{(x -\mu)^2}{\sigma^2} - 2\rho \frac{(x -\mu)(y - \nu)}{\sigma \tau} + \frac{(y- \nu)^2}{\tau^2}}}
\ee
for $-\infty < x < \infty$ and $-\infty < y < \infty$ where the parameters satisfy $-\infty < \mu < \infty$, $-\infty < \nu < \infty$, $\sigma > 0$, $\tau> 0$ and $-1 < \rho < 1$. The first task is to check that this expression is indeed a joint density function in that it integrates to 1. By making the substitutions $u = (x - \mu)/\bb{\sigma\sqrt{1 - \rho^2}}$ and $v = (y - \nu)/(\tau\sqrt{1 -\rho^2})$, we have 
\beast
I & = & {\int\int}_{-\infty<x,y<\infty} f_{X,Y} (x, y)dxdy = {\int\int}_{-\infty<x,y<\infty} \frac{\sqrt{1 -\rho^2}}{2\pi} e^{-\frac 12 (u^2-2\rho uv+v^2)} dudv\\
& = & {\int\int}_{-\infty<x,y<\infty} \frac{\sqrt{1 -\rho^2}}{2\pi} e^{-\frac 12 ((u-\rho v)^2+(1-\rho^2)v^2)}dudv.
\eeast

Now put $w = u - \rho v$ and $z = v \sqrt{1 - \rho^2}$, or $u = w + \rho z/\sqrt{1 - \rho^2}$ and $v = z/\sqrt{1 - \rho^2}$, and calculate the Jacobian of this transformation
\be
\fp{(u, v)}{(w, z)} =
\bevm
1 & \frac{ p}{\sqrt{1 - \rho^2}}\\
0 & \frac 1{\sqrt{1 - \rho^2}}
\eevm=
\frac 1{\sqrt{1 - \rho^2}},
\ee
then we see that
\be
I = {\int\int}_{-\infty<w,z<\infty} \frac 1{2\pi} e^{-(w^2+z^2)/2}dwdz = \bb{\int^\infty_{-\infty}\frac 1{\sqrt{2\pi}} e^{-w^2/2}dw}^2 = 1.
\ee

Marginal distributions. To see the relationship with the ordinary (univariate) normal distribution and to determine the marginal distributions, consider the random variables
\be
U = X,\quad V = Y - \nu - \rho\tau (X -\mu)/\sigma.
\ee
Putting $X$ and $Y$ in terms of $U$ and $V$ gives
\be
X = U,\quad Y = V + \nu + \rho \tau(U - \mu)/\sigma.
\ee

The Jacobian of this transformation is
\be
J = \bevm
\fp{x}{u} & \fp{x}{v}\\ 
\fp{y}{u} & \fp{y}{v}
\eevm
=
\bevm
1 & 0\\ 
\rho\tau/\sigma & 1
\eevm
= 1.
\ee
We may now calculate the joint density function of $U$ and $V$ , evaluated at $(u, v)$, as
\be
\bb{\frac 1{\sqrt{2\pi}\sigma} e^{-(u-\mu)^2/(2\sigma^2)}} \bb{\frac 1{\sqrt{2\pi} \tau \sqrt{1 -\rho^2}} e^{-v^2/(2\tau^2(1-\rho^2))}},
\ee
and we recognize these two expressions, the first in u is the density of the $\sN(\mu, \sigma^2)$ distribution, and the second in $v$ is the density of the $\sN(0, \tau^2(1 - \rho^2))$ distribution, and moreover, because the joint density factors into the product of these two densities, $U$ and $V$ are independent random variables. We conclude that the marginal distribution of $X$ is $\sN(\mu, \sigma^2)$ and, by the symmetry of the joint density of $X$ and $Y$, we can see that the marginal density of $Y$ is $\sN(\nu, \tau^2)$. To interpret the remaining parameter $\rho$, calculate
\beast
\cov (X, Y ) & = & \cov (U, V + \nu + \rho \tau(U -\mu)/\sigma)\\
& = & \cov (U, V ) + \cov (U, \rho\tau(U -\mu)/\sigma),\quad\quad \text{ since $\nu$ is constant},\\
& = & \cov (U, \rho\tau(U - \mu)/\sigma),\quad\quad \text{ since $U$ and $V$ are independent},\\
& = & \rho \tau \var (U)/\sigma = \rho\sigma\tau = \rho \sqrt{\var (X)\var (Y )}.
\eeast

Thus the parameter $\rho = \corr (X, Y )$ is the correlation coefficient of the random variables $X$ and $Y$. We may see immediately that 
\be
f_{X,Y} (x, y) = \bb{\frac 1{\sqrt{2\pi}\sigma} e^{-(x-\mu)^2/(2\sigma^2)} }\bb{\frac 1{\sqrt{2\pi} \tau}e^{-(y-\nu)^2/(2\tau^2)}} = f_X(x)f_Y (y),
\ee
for all $x$ and $y$, if and only if $\rho = 0$, or equivalently if and only if $\cov (X, Y ) = 0$. Thus random variables which have a joint normal distribution are independent if and only if their covariance is zero. Recall that in general the covariance between random variables being zero does not imply independence of the random variables, we see here the important and useful property that the covariance being zero is sufficient to show independence for normally distributed variables.

Conditional distributions. We may calculate the conditional density of one of the random variables $Y$, say, given the value of the other variable $X = x$, that is, the density $f_{Y|X}(y |x) = f_{X,Y} (x, y)/f_X(x)$, which equals
\beast
& & \left.\frac{\exp\bb{- \frac 1{2(1-\rho^2)}\bb{\frac{(x-\mu)^2}{\sigma^2} - 2\rho \frac{(x-\mu)(y-\nu)}{\sigma \tau} + \frac{(y-\nu )^2}{\tau^2}} }} {2\pi \sigma \tau \sqrt{1 - \rho^2}} \right/\frac{\exp \bb{-\frac{(x-\mu)^2}{2\sigma^2}}}{\sigma \sqrt{2\pi}} \\
& = & \left.\exp \bb{-\frac 1{2(1 - \rho^2)}\bb{ \frac{\rho^2(x - \mu)^2}{\sigma^2} - 2\rho \frac{(x - \mu)(y - \nu )}{\sigma \tau} + \frac{(y - \nu )^2}{\tau^2}}}\right/ \tau\sqrt{2\pi (1 - \rho^2)}\\
& = & \left.\exp\bb{-\frac 1{2\tau^2(1 - \rho^2)} (y - \nu  - \rho \tau(x - \mu)/\sigma )^2}\right/ \tau\sqrt{2\pi(1 - \rho^2)}.
\eeast
We recognize this last expression as being the density (in $y$) of the normal distribution with mean $\nu  + \rho \tau(x - \mu)/\sigma$ and variance $\tau^2(1 - \rho^2)$, so that, in shorthand notation,
\be
Y|X \sim \sN\bb{\nu  + \rho \tau(X - \mu)/\sigma , \tau^2(1 - \rho^2)}.
\ee

Notice that the conditional expectation of $Y$ given $X$, which is
\be
\E\bb{Y|X} = \nu  + \rho \tau(X - \mu)/\sigma ,
\ee
depends on $X$, but the variance of $Y$ conditional on $X$ is the constant $\tau^2(1-\rho^2)$, which is less than the unconditioned variance of $Y$, that is $\tau^2$.

Linear transformations. A further property that you might wish to check is that if $X$ and $Y$ have a joint normal distribution and we define random variables $R$ and $S$ by
\be
\bepm
R\\
S
\eepm = \bepm
a & b\\
c & d
\eepm
\bepm
X\\
Y
\eepm
+\bepm
\theta\\
\phi
\eepm,
\ee
where $a$, $b$, $c$, $d$, $\theta$ and $\phi$ are constants with $ad \neq bc$, then $R$ and $S$ have a joint normal distribution, so that normal distributions are preserved under linear transformations. You should check that the condition $ad \neq bc$ is needed to ensure that $\abs{corr (R, S)}\neq 1$, even if this condition does not hold, the random variables $R$ and $S$ will individually have normal distributions but their correlation coefficient will be 1 or -1.

Multivariate normal distribution. We may generalize the above to define the joint normal distribution for $n$ random variables. Suppose that $Z_1,\dots ,Z_n$ are i.i.d. random variables each with the standard $\sN(0, 1)$ distribution. Suppose that $A$ is a $n\times n$ invertible matrix and (using vector notation) suppose that
\be
{\bf X} = \bepm
X_1\\
\vdots\\
X_n
\eepm = 
\bepm
\mu_1\\
\vdots\\
\mu_n
\eepm
+ A\bepm
Z_1\\
\vdots\\
Z_n
\eepm = {\bf \mu} + A{\bf Z},
\ee
where $\mu_1,\dots , \mu_n$ are constants. Since each of the random variables $\{Z_j\}$ has mean zero, we see first that $\E X_i = \mu_i$, for each $i$. The joint probability density function of the components of ${\bf Z}$ at $\bz = (z_1,\dots , z_n)^T$ is
\be
f(\bz) = \prod^n_{i=1} \frac 1{\sqrt{2\pi}} e^{-z^2_i/2} = \bb{\frac 1{\sqrt{2\pi}}}^{n/2} e^{-\sum^n_{i=1} z^2_i/2} = \bb{\frac 1{\sqrt{2\pi}}}^{n/2} e^{-\bz^T \bz/2}.
\ee

Writing $\bz = A^{-1}(\bx-{\bf\mu})$, the Jacobian of the transformation is $\abs{\det A}^{-1}$, so that the joint density for ${\bf X}$ is
\beast
g(\bx) & = & \frac 1{\abs{\det A}} f\bb{A^{-1}(\bx - {\bf \mu})} = \frac 1{\abs{\det A}} \bb{\frac 1{\sqrt{2\pi}}}^{n/2} e^{-\frac 12 (A^{-1}(\bx-{\bf\mu}))^T(A^{-1}(\bx-{\bf\mu}))}\\
& = & \frac 1{\abs{\det A}} \bb{\frac 1{\sqrt{2\pi}}}^{n/2}  e^{-\frac 12 (\bx-{\bf\mu})^T(A^{-1})^TA^{-1} (\bx-{\bf\mu})}\\
& = & \frac 1{\sqrt{\abs{\det V}}} \bb{\frac 1{\sqrt{2\pi}}}^{n/2} e^{-\frac 12(\bx-{\bf\mu})^TV^{-1}(\bx-{\bf\mu})},\quad\quad \text{ where }V = AA^T.
\eeast

To interpret the matrix $V$ we see that for any pair $(i, j)$, $1 \leq i, j \leq n$,
\be\label{equ:multivariate_normal_distribution}
\cov (X_i,X_j) = \E ((X_i - \mu_i)(X_j - \mu_j)) = \E \bb{\bb{\sum_r A_{ir}Z_r}\bb{\sum_s A_{js}Z_s}} = \sum_r A_{ir}A_{jr} = \bb{AA^T}_{ij} = V_{ij},
\ee
so that the entries of the matrix V are the covariances between the components of the random vector ${\bf X}$. Any joint density of the form (\ref{equ:multivariate_normal_distribution}) is a multivariate normal distribution with mean ${\bf\mu}$ and covariance matrix $V$, usually written $\sN({\bf\mu}, V)$.

Notice that $V$ is a symmetric matrix and it is positive definite in that $x^TVx > 0$ for all vectors $x \neq 0$, this follows because $x^TV x = \dabs{A^Tx}^2 > 0$, since $A$ is invertible.

Furthermore, in the case when $n = 2$ and $X$ and $Y$ have the bivariate normal distribution described above we see that if, for any angle $\theta$, we take $A$ to be the matrix 
\be
A = \bepm
\sigma  \cos\bb{\theta + \arccos \rho } & \sigma  \sin\bb{\theta+ \arccos \rho }\\
\tau \cos \theta & \tau \sin \theta
\eepm
\ee
we see that
\be
AA^T = \bepm
\sigma^2  & \rho \sigma \tau\\
\rho \sigma \tau & \tau^2
\eepm
= V,
\ee
and
\be
A^{-1} \bepm
X - \mu\\
Y - \nu 
\eepm
= \bepm
Z_1\\
Z_2
\eepm,
\ee
where $Z_1$ and $Z_2$ are independent random variables each with the standard normal distribution, $\sN(0, 1)$.

\subsection{Multivariate moment generating functions}

For random variables $X_1,\dots ,X_n$ and real numbers $\theta_1,\dots , \theta_n$ set ${\bf \theta} = (\theta_1,\dots , \theta_n)^T$ and ${\bf X} = (X_1,\dots ,X_n)^T$, then we define
\be
m({\bf\theta}) = m(\theta_1,\dots , \theta_n) = \E\bb{e^{\theta_1X_1+\dots+\theta_nX_n}} ,
\ee
to be the joint moment generating function of the random variables. The moment generating function is only defined for those ${\bf\theta}$ for which $m({\bf \theta}) < \infty$. The properties of the multivariate generating function are similar to those we have seen previously for the moment generating function of a single random variable.

Properties of $m({\bf \theta})$
\ben
\item [1.] Provided $m({\bf \theta})$ is finite for a non-trivial range of $\theta_i$ for each $i$, then $m({\bf \theta})$ determines the joint distribution of $X_1,\dots ,X_n$.
\item [2.] We may determine moments of the $X_i$ from partial derivatives of $m$,
\be
\left.\frac{\partial^r m}{\partial \theta^r_i}\right|_{{\bf \theta}=0} = \E (X^r_i),\quad\quad  \left.\frac{\partial^{r+s} m}{\partial \theta^r_i \partial\theta^s_j} \right|_{{\bf \theta}=0} = \E\bb{X^r_i X^s_j}, \quad \quad r \geq 1,\ s \geq 1.
\ee
In particular, we may calculate covariances as
\be
\cov (X_i,X_j) = \E (X_iX_j) - (\E X_i) (\E X_j) = \bsb{\frac{\partial^2 m}{\partial\theta_i \partial \theta_j} - \bb{\fp{m}{\theta_i}}\bb{\fp{m}{\theta_j}}}_{\bf \theta = 0}.
\ee
\item [3.] The moment generating function factors
\be
m({\bf\theta}) = \prod^n_{i=1} \bb{\E\bb{e^{\theta_iX_i}}},
\ee
into the product of the moment generating functions of the individual random variables if and only if $X_1,\dots ,X_n$ are independent.
\een
For the particular case of random variables $X$ and $Y$ having the bivariate normal distribution considered in the previous section, then we may use the form for the moment generating function of the normal distribution
\be
\E\bb{e^{\theta X} } = e^{\theta\mu+\frac 12 \theta^2\sigma^2}, \quad X \sim \sN \bb{\mu, \sigma^2},
\ee
and the form of the conditional distribution of $Y$ given $X$ to calculate (here, to avoid subscripts take $\theta_1 = \theta$ and $\theta_2 = \phi$),
\beast
\E\bb{e^{\theta X+\phi Y}} & = & \E\bb{\E \bb{e^{\theta X+\phi Y} |X}} = \E\bb{e^{\theta X} \E\bb{e^{\phi Y}|X}}\\
& = & \E\bb{e^{\theta X} e^{\phi\E (Y |X)+\frac 12 \phi^2\var (Y |X)}} = \E\bb{e^{\theta X+\phi(\nu +\rho \tau(X-\mu)/\sigma)+ \frac 12 \phi^2\tau^2(1-\rho^2)}}\\
& = & e^{\phi(\nu -\mu\rho \tau/\sigma )+ \frac 12 \phi^2\tau^2(1-\rho^2)} \E \bb{e^{(\theta +\phi \rho \tau/\sigma )X}}\\
& = & e^{\phi(\nu -\mu\rho \tau/\sigma )+ \frac 12 \phi^2\tau^2(1-\rho 2)} e^{(\theta+\phi\rho \tau/\sigma )\mu+ \frac 12 \sigma^2(\theta+\phi\rho\tau/\sigma)^2}\\
& = & e^{\theta\mu+\phi\nu +\frac 12 (\theta^2\sigma^2+\phi^2\tau^2+2\theta \phi\rho \sigma \tau)}.
\eeast
We see that this factors into the product
\be
\bb{e^{\theta \mu+\frac 12 \theta^2\sigma^2}}\bb{e^{\phi\nu +\frac 12 \phi^2\tau^2}}
\ee
of the individual generating functions of $X$ and $Y$ for all $\theta$ and $\phi$ if and only if $\rho  = 0$, as we have seen previously, the random variables are independent in this case if and only if their covariance is zero.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Inequalities, Limit Theorems and Geometric Probability}

\subsection{Inequalities}

Suppose that $X \geq 0$ is a random variable taking non-negative values and that $c > 0$ is a constant. Then
\be
\pro (X \geq c) \leq \frac{\E(X)}c,
\ee
is Markov's inequality\index{Markov's inequality}. It follows because
\be
\pro (X \geq c) = \E\bb{I_{(X\geq c)}} \leq \E \bb{ \frac{X}c I_{(X\geq c)}} \leq \E\bb{\frac Xc} = \frac{\E(X)}c.
\ee

From this result we may deduce Chebyshev's inequality\index{Chebyshev's inequality}: for any random variable $X$ and constant $c > 0$,
\be
\pro (\abs{X}\geq c) \leq \frac{\E(X^2)}{c^2}.
\ee
This follows by observing that
\be
\pro(\abs{X}\geq c) = \pro\bb{X^2 \geq c^2},
\ee
and applying Markov's inequality for the random variable $Y = X^2$ and constant $c^2$. We should note the following points about Chebyshev's inequality:

\ben
\item [1.] The inequality is 'distribution free', it holds for all random variables irrespective of the distribution of the random variable.
\item [2.] If $\E(X^2) \geq c^2$, then the inequality provides no bound on the probability.
\item [3.] If $\E(X^2) < c^2$, the inequality is the best possible in the sense that given $c$ there is a random variable $X$ for which the inequality holds with equality. To see this suppose that $d < c^2$ and let $X$ be the random variable
\be
X = \left\{\ba{ll}
c \quad\quad \text{with probability }\frac d{2c^2} ,\\
-c & \text{with probability }\frac d{2c^2} ,\\
0 & \text{with probability }1 - \frac d{c^2}.
\ea\right.
\ee
Then $\E X^2 = d$ and $\pro(\abs{X} \geq c) = d/c^2 = \E X^2/c^2$.
\een

Suppose that $\phi : [0,\infty) \to [0,\infty)$ is a non-decreasing function, with $\phi(x) > 0$ for $x > 0$, then we obtain the generalized Chebyshev's inequality\index{generalized Chebyshev's inequality}: for any random variable $X$ and $c > 0$,
\be
\pro (\abs{X} \geq c) \leq \frac{\E (\phi (\abs{X}))}{\phi(c)}.
\ee
This follows in the same way by observing that $\pro (\abs{X} \geq c) \leq \pro (\phi (\abs{X})\geq \phi(c))$, and using Markov's inequality with $Y = \phi(\abs{X})$. As an example, take $\phi(x) = x^4$ and we obtain $\pro(\abs{X} \geq c) \leq \E(X^4) /c^4$.

The next inequality involving random variables that we will consider is based on the concept of convexity. A function $f : \R \to \R$ is convex if for all $x_1, x_2 \in \R$ and $\lm^1 \geq 0$, $\lm^2 \geq 0$ with $\lm_1 + \lm_2 = 1$, we have 

\centertexdraw{
    
\drawdim in

\def\bdot {\fcir f:0 r:0.02 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04

\linewd 0.01 \setgray 0

\move (-0.2 0) \avec(2.2 0) 
\move (1 0) \avec(1 1.5) 

\move (0.5 0.51) \lvec (1.5 0.8)

\move (0.5 0.51) \bdot
\move (1.5 0.8) \bdot

\move (0 0.8) \clvec(0.8 0.2)(1.2 0.4)(2 1.3)

\htext (0.5 -0.15){$x_1$}
\htext (1.5 -0.15){$x_2$}

\htext (1.05 1.4){$f(x)$}
\htext (2.8 0.6){$uv=1$}

\htext (-3 0.5){$f(\lm_1x_1 + \lm_2 x_2)\leq \lm_1 f(x_1) + \lm_2 f(x_2)$}

\move(0 1.8)
}



Thus a function is convex if the chord joining any two points $(x_1, f(x_1))$ and $(x_2, f(x_2))$ on the graph of the function lies above the function between the points. It is easy to see that if f(x) is a convex\index{convex} function then for $x_1 < x_2 < x_3$, the slope of the chord joining the points $(x_1, f(x_1))$ and $(x_2, f(x_2))$ is less than or equal to the slope of the chord joining the points $(x_2, f(x_2))$ and $(x_3, f(x_3))$, that is
\be\label{equ:convex_1}
\frac{f(x_2) - f(x_1)}{x_2 - x_1} \leq \frac{f(x_3) - f(x_2)}{x_3 - x_2},
\ee
this follows from the definition of convexity, since
\be
x_2 = \bb{\frac{x_3 - x_2}{x_3 - x_1}} x_1 + \bb{\frac{x_2 - x_1}{x_3 - x_1}} x_3,
\ee
so that
\be\label{equ:convex_2}
f(x_2) \leq \bb{\frac{x_3 - x_2}{x_3 - x_1}} f(x_1) + \bb{\frac{x_2 - x_1}{x_3 - x_1}}f(x_3),
\ee
and rearranging (\ref{equ:convex_2}) gives (\ref{equ:convex_1}). Furthermore, from (\ref{equ:convex_1}), it is immediate that for points $x_1 < x_2 < x_3 < x_4$, we have
\be\label{equ:convex_3}
\frac{f(x_2) - f(x_1)}{x_2 - x_1} \leq \frac{f(x_4) - f(x_3)}{x_4 - x_3}, 
\ee
since we may apply (\ref{equ:convec_1}) twice to obtain
\be
\frac{f(x_2) - f(x_1)}{x_2 - x_1} \leq \frac{f(x_3) - f(x_2)}{x_3 - x_2} \leq \frac{f(x_4) - f(x_3)}{x_4 - x_3}
\ee
It is the case that a function $f$ is convex if and only if (\ref{equ:convex_1}) holds for all choices of $x_1 < x_2 < x_3$.

Moreover, when $f$ is differentiable then $f$ being convex is equivalent to the derivative $f'(x)$ being non-decreasing (or $f''(x) > 0$), this may be seen by letting $x_2 \to x_1$ and $x_4 \to x_3$ in (\ref{equ:convex_3}). With a similar argument, we may see that, when $f$ is convex, then
\be\label{equ:convex_5}
f(y) - f(x) \geq (y - x)f'(x),\quad \text{for all }x, y
\ee
Now if $f$ is a convex function, for each $n \geq 2$, and for any points $x_1, \dots, x_n \in \R$ and any $lm_i \geq 0$, $1 \leq i \leq n$, with $\lm_1 + \dots + \lm_n = 1$, we have
\be\label{equ:convex_6}
f (\lm_1x_1 + \dots + \lm_nx_n) \leq \lm_1f(x_1) + \dots+ \lm_nf(x_n).
\ee
The proof of the inequality (\ref{equ:convex_2}) is by induction on $n$. The case $n = 2$ is just the definition of convexity. So assume that (\ref{equ:convex_6}) holds for any $n$ points $x_1, \dots, x_n \in \R$ and any $\lm_i \geq 0$, $1 \leq i \leq n$, with $\lm_1 + \dots + \lm_n = 1$, and suppose that we are given $x_1,\dots, x_{n+1}\in \R$ and $\lm_i \geq 0$, $1 \leq i \leq n + 1$, with $\lm_1 + \dots + \lm_{n+1} = 1$. We may assume that $\lm_i > 0$ each $i$, otherwise the result follows by the inductive step immediately. Then, by first using the case $n = 2$ and then the inductive hypothesis, we have
\beast
f\bb{\sum^{n+1}_{i=1} \lm_ix_i } & = & f\bb{(1 - \lm_{n+1})\sum^n_{i=1} \bb{\frac{\lm_i}{1-\lm_{n+1}}}x_i + \lm_{n+1} x_{n+1}}\\
& \leq & (1 - \lm_{n+1})f \bb{\sum^n_{i=1} \bb{\frac{\lm_i}{1 - \lm_{n+1}} x_i}} + \lm_{n+1}f(x_{n+1}),\\
& \leq & (1 - \lm_{n+1})\bb{\sum^n_{i=1} \bb{\frac{\lm_i}{1 - \lm_{n+1}}}f(x_i)} + \lm_{n+1} f(x_{n+1}) = \sum^{n+1}_{i=1} \lm_if(x_i),
\eeast
completing the induction.

\begin{theorem}[Jensen's Inequality\index{Jensen's Inequality}]
For a random variable $X$ and a convex function $f$, 
\be
f(\E X) \leq \E f(X).
\ee
The proof of Jensen's Inequality in the case when $X$ takes on just finitely many values $x_1, \dots, x_n$, with probabilities $p_i = \pro(X = x_i)$, $1 \leq i \leq n$, with $p_i \geq 0$ and $\sum^n_{i=1} p_i = 1$, is just a restatement of (\ref{equ:convex_6}), since 
\be
f(\E X) = f\bb{\sum^n_{i=1} p_ix_i} \leq \sum^n_{i=1} p_if(x_i) = \E f(X).
\ee
In general, for any random variable, we may use (\ref{equ:convex_5}), to see that
\be
f(X) - f(\E X) > (X - \E X)f'(\E X),
\ee
and taking the expectation of both sides we see that
\be
\E f(X) - f(\E X) \geq \E (X - \E X) f'(\E X) = 0,
\ee
which gives the result.
\end{theorem}

\begin{example}[Arithmetic-Geometric Mean Inequality\index{Arithmetic-Geometric Mean Inequality}]
For positive real numbers $x_1, \dots, x_n$,
\be
\bb{\prod^n_{i=1} x_i}^{1/n} \leq \frac 1n \sum^n_{i=1} x_i.
\ee
This follows by Jensen's inequality applied to the convex function $f(x) = -\log x$, and the random variable $X$ which takes the value $x_i$ with probability $\frac 1n$, so that
\be
-\frac 1n \sum^n_{i=1} \log x_i = \E (-\log(X)) \geq -\log(\E X) = -\log\bb{\frac 1n \sum^n_{i=1}x_i}
\ee
from which we see that
\be
\log\bb{\bb{\prod^n_{i=1} x_i}^{1/n}} \leq \log\bb{\frac 1n \sum^n_{i=1} x_i},
\ee
which gives the result, since $\log$ is an increasing function.
\end{example}

\subsection{Weak Law of Large Numbers}

\begin{theorem}[Weak Law of Large Numbers\index{Weak Law of Large Numbers}]\label{thm:wlln}
Let $X_1,X_2, \dots$ be independent, identically distributed random variables with $\E X_1 = \mu$ and $\var (X_1) < \infty$. For any constant $\ve > 0$,
\be
\pro\bb{\abs{\frac{X_1 + \dots + X_n}n - \mu} > \ve} \to 0,\quad \text{ as }n \to \infty.
\ee
\end{theorem}
\begin{proof}[\bf Proof]
By Chebyshev's inequality we have
\beast
\pro\bb{\abs{\frac{X_1 + \dots + X_n}n - \mu} > \ve} & \leq & \frac 1{\ve^2} \E\bb{\frac{X_1 + \dots + X_n}n - \mu}^2\\
& = & \frac 1{n^2\ve^2} \E (X_1 + \dots + X_n - n\mu)^2\\
& = & \frac 1{n^2\ve^2} \var (X_1 + \dots + X_n) = \frac{n}{n^2\ve^2}\var (X_1)\\
& = & \frac 1{n\ve^2} \var (X_1) \to 0,
\eeast
as required.
\end{proof}

Notes 
\ben
\item [1.] The statement in Theorem \ref{thm:wlln} is normally referred to by saying that the random variable $(X_1 + \dots+ X_n)/n$ 'converges in probability' to $\mu$, written 
\be
\frac{X_1 + \dots + X_n}n \stackrel{P}{\to} \mu, \quad \text{as }n \to \infty.
\ee
\item [2.] This result should be distinguished from the Strong Law of Large Numbers which states that
\be
\pro \bb{\frac{X_1 + \dots + X_n}n \to \mu,\ \text{ as }n \to \infty} = 1.
\ee
As the name implies, the Strong Law of Large Numbers implies the Weak Law. The mode of convergence in the Strong Law is referred to as 'convergence with probability one' or 'almost sure convergence'.

\item [3.] Notice that the requirement that the random variables in the Weak Law be independent is not one that we may dispense with. For example, suppose that ­$\Omega = \{\omega_1, \omega_2\}$ has just two points and let $X_n(\omega_1) = 1$ and $X_n(\omega_2) = 0$ for each $n$, so that the random variables are identically distributed, but not of course independent. Let $p = \pro(\{\omega_1\}) = 1 -\pro(\{\omega_2\})$, where $0 < p < 1$, then $\E X_1 = p$, and we have
\be
\frac{X_1(\omega_1) + \dots + X_n(\omega_1)}n = 1,\quad\quad \frac{X_1(\omega_2) + \dots + X_n(\omega_2)}n = 0,\quad  \text{for all }n,
\ee
so that the conclusion of Theorem \ref{thm:wlln} cannot hold.

\item [4.] By giving a more refined argument it is possible to dispense with the requirement in the statement of the Theorem that $\var (X_1) < \infty$. The conclusion still holds provided $\E \abs{X_1} < \infty$.

\item [5.] It should be noticed that the Weak Law of Large Numbers is 'distribution free' in that the particular distribution of the summands $\{X_i\}$ only influences the result through the mean, $\mu$, (and, in the form we have stated it, through the fact that the variance is finite) but otherwise the underlying distribution does not enter the conclusion of the Theorem.

\item [6.] The Weak Law of Large Numbers underlies the 'frequentist' interpretation of probability. Suppose that we have independent repetitions of an experiment, and we set $X_i = 1$ if a particular outcome occurs on the ith repetition (e.g., 'Heads'), and $X_i = 0$, otherwise (e.g., 'Tails'). Then $\E X_i = p$, say, where $p = \pro(X_i = 1)$ is the probability of the outcome. Then $(X_1 +\dots+X_n)/n$ is the average number of occurrences of the outcome in
$n$ repetitions and this converges in the above sense to $\E X_i = p$, thus the probability $p$ is  the long-run proportion of times that the outcome occurs.
\een

\subsection{Central Limit Theorem}

\begin{theorem}[Central Limit Theorem\index{Central Limit Theorem}]\label{thm:clt}
Let $X_1,X_2, \dots$ be independent, identically distributed random variables with $\E X_1 = \mu$ and $\sigma^2 = \var (X_1)$, where $0 < \sigma^2 < \infty$. For any $x$, $-\infty < x < \infty$,
\be
\lim_{n\to \infty} \pro\bb{\frac{X_1 + \dots + X_n - n\mu}{\sigma \sqrt{n}} \leq x}  \frac 1{\sqrt{2\pi}} \int^x_{-\infty} e^{-y^2/2} dy = \Phi(x),
\ee
which is the distribution function of the standard $\sN(0,1)$ distribution.
\end{theorem}

\begin{proof}[\bf Proof]
We will illustrate why the result of the Theorem holds by using moment generating functions in the case when the moment generating function of the $\{X_i\}$, $m(\theta) = \E\bb{e^{\theta X_1}}$, satisfies $m(\theta) < \infty$ for a non-trivial range of values of $\theta$ (that is, an open interval which necessarily contains the point $\theta = 0$). It should be noted that this condition on the moment generating function is not necessary for the result to hold. Let 
\be
Y_n = \frac{X_1 + \dots + X_n - n\mu}{\sigma \sqrt{n}}, \quad\quad m_n(\theta) = \E\bb{e^{\theta Y_n}},
\ee
then we will show that as $n \to \infty$, $m_n(\theta) \to e^{\theta^2/2}$, which is the moment generating function of the $\sN(0, 1)$ distribution. This is su±cient to establish the conclusion of the Theorem, although we will not prove that in this course. We now have

\be
m_n(\theta) = \E \bb{e^{\theta(X_1+\dots+X_n-n\mu)/(\sigma \sqrt{n})}} = e^{-\theta\mu \sqrt{n}/\sigma} \E\bb{e^{\theta(X_1+\dots+X_n)/(\sigma \sqrt{n})}}
\ee
and since the $\{X_i\}$ are independent and identically distributed, this
\be
= e^{-\theta\mu \sqrt{n}/\sigma} \bsb{\E\bb{e^{\theta X_1/(\sigma \sqrt{n})}}}^n = \bsb{e^{-\theta\mu/(\sigma \sqrt{n})} m\bb{\frac{\theta}{\sigma \sqrt{n}}}}^n.
\ee

Expand the two terms using Taylor's Theorem to see that $m_n(\theta)$ equals
\be
\bsb{\bb{1 - \frac{\theta\mu}{\sigma \sqrt{n}} + \frac{\theta^2\mu^2}{2\sigma^2 n} + O\bb{\frac 1{n^{3/2}}}} \bb{1 + \frac{\theta}{\sigma \sqrt{n}} m'(0) + \frac{\theta^2}{2\sigma^2 n} m''(0) + O\bb{\frac 1{n^{3/2}}} }}^n,
\ee
now, using the fact that $m'(0) = \E X_1 = \mu$, and $m''(0) = \E\bb{X^2_1} = \sigma^2 + \mu^2$, this shows that
\beast
m_n(\theta) & = & \bsb{1 - \frac{\theta^2\mu^2}{\sigma^2n} + \frac{\theta^2(\sigma^2 + \mu^2)}{2\sigma^2n} + \frac{\theta^2\mu^2}{2\sigma^2n} + O\bb{\frac 1{n^{3/2}}}}^n\\
& = & \bsb{1 + \frac{\theta^2}{2n} + O\bb{\frac 1{n^{3/2}}}}^n \to e^{\theta^2/2},
\eeast
as required.
\end{proof}

Notes.
\ben
\item [1.] The mode of convergence described in Theorem \ref{thm:clt} for the random variables
\be
Y_n = (X_1 + \dots + X_n - n\mu)/(\sigma \sqrt{n})
\ee
is known as 'convergence in distribution' and the conclusion is written as $Y_n \stackrel{d}{\to} Z$, where $Z \sim \sN(0, 1)$.

\item [2.] Note that, like the Weak Law of Large Numbers, the Central Limit Theorem is distribution free, in that the underlying distribution of the $\{X_i\}$ influences the form of the result only through the mean $\mu = \E X_1$ and variance $\sigma^2 = \var (X_1)$.

\item [3.] Note that in the Central Limit Theorem, by subtracting off at each stage the mean of the sum of the random variables $X_1 + \dots + X_n$, that is $n\mu$, and dividing by its standard deviation, $\sigma \sqrt{n}$, we are ensuring that the random variable $Y_n$ has $\E Y_n = 0$  and $\var (Y_n) = 1$, for each $n$.
\een

\begin{example}[Normal approximation to the binomial distribution]
If the random variable $Y \sim \bd(n, p)$, we may think of the distribution of $Y$ as being the same as that of the sum of $n$ i.i.d. random variables each of which has the Bernoulli distribution, thus the random variable $(Y -np)/\sqrt{np(1 - p)}$ has approximately the $\sN(0, 1)$ distribution for large $n$.
Note that here $p$ is being held fixed and $n \to \infty$, unlike the situation we described in the Poisson approximation to the binomial where $n \to \infty$ and $p \to 0$ in such a way that the product $np \to \lm > 0$.
\end{example}

\begin{example}[Normal approximation to the Poisson distribution]
When the random variable $Y \sim \pd(n)$, where $n \geq 1$ is an integer, we may think of $Y$ as having the same distribution as that of the sum of $n$ i.i.d. random variables each with the $\pd(1)$ distribution. Thus $(Y - n)/\sqrt{n}$ has approximately the $\sN(0, 1)$ distribution for large $n$. The same conclusion is true for $Y \sim \pd(\lm)$ for non-integer $\lm$, that is, $(Y - \lm)/\sqrt{\lm}$ is approximately $\sN(0, 1)$ for $\lm$ large.
\end{example}

\begin{example}[Opinion polls]
Suppose that the proportion of voters in the population who vote Labour is $p$, where $p$ is unknown. A random sample of $n$ voters is taken and it is found that $S$ voters in the sample vote Labour and we estimate $p$ by $S/n$. We want to ensure that $\abs{S/n - p} < \ve$, for some small given $\ve$, with high probability, $\geq 0.95$, say. How large must $n$ be? Note that $S \sim \bd(n, p)$, so that $\E S = np$ and $\var (S) = np(1 - p)$. Then by the Central Limit Theorem, we require
\beast
\pro\bb{\abs{\frac Sn - p} < \ve} & = & \pro\bb{-\ve\sqrt{\frac{n}{p(1 - p)}} < \frac{S - np}{\sqrt{np(1 - p)}} < \ve\sqrt{\frac{n}{p(1 - p)}}}\\
& \approx & \Phi \bb{\ve\sqrt{\frac n{p(1 - p)}}} - \Phi \bb{-\ve \sqrt{\frac n{p(1 - p)}}} \\
& = & 2\Phi \bb{\ve \sqrt{\frac n{p(1 - p)}}} - 1 \geq 0.95, \quad\quad \text{since }\Phi(x) = 1 - \Phi(-x),
\eeast
so that we need $\ve\sqrt{n/p(1 - p)} > 1.96$, that is
\be
n \geq (1.96)^2p(1 - p)/\ve^2.
\ee
We do not know the value of $p$, but it is always the case that $p(1-p) \leq \frac 14$, with equality occurring when $p = \frac 12$, so to ensure that we have the required bound we need $n \geq (1.96/2\ve)^2$. For example, if we take $\ve = 0.02$, so that the estimate of the percentage of Labour voters is
accurate to within 2 percentage points with 95\% probability we would need to take a sample with $n \geq 2401$. The typical sample size in opinion polls is $n \approx 1000$ which corresponds to an error $\ve\approx 0.03$.
\end{example}

\subsection{Geometric probability}[Buffon's Needle]
Consider a needle of length $r$ which is thrown at random on to a plane surface on which there are parallel straight lines at distance $d > r$ apart. What is the probability that the needle intersects one of the lines?

Think of the parallel lines running West-East and let $X$ be the distance from the point representing the Southern end of the needle to the nearest line North of that point, if the needle is parallel to the lines, take the right-hand end point. Let $\Theta$ be the angle that the needle makes with the West-East lines. Then we will assume that $X$ is uniformly distributed on $[0, d)$ and $\Theta$ is uniformly distributed on $[0, \pi]$, and that $X$ and $\Theta$ are independent.

\centertexdraw{
    \drawdim in

    \arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
    
    \linewd 0.01 \setgray 0
    
    \move (0 0)
    \clvec (0.8 2.2)(1.2 2.2)(2 0)
    \lvec (0 0) 
    \lfill f:0.8

    %\linewd 0.00 \setgray 1
    %\move (0 1.414) \lvec(2 1.414) \lvec(2 2) \lvec(0 2) \lvec(0 1.414)
    %\lfill f:1
   
    %\move (0 1.414) \lvec(2 1.414) \lvec(2 0)
    %\move (0 2) \lvec(2 2)

    \linewd 0.01 \setgray 0 

    \move (-0.2 0) \avec(2.3 0)
    \move (0 -0.2) \avec(0 2.3)
    \move (0 0)
    \clvec (0.8 2.2)(1.2 2.2)(2 0)

    \move (0 2) \lvec(2 2) \lvec(2 0) 

    \htext (2.3 0.1){$\theta$}
    \htext (0.1 2.2){$x$}
    \htext (-0.1 -0.15){$0$}
    
    \htext (-0.1 2){$d$}
    \move (0 2) \lvec(2 2)
    \htext (-0.1 1.6){$r$}
    \htext (2 -0.15){$\pi$}

%    \move (0.5 1.414) \lvec(0.5 0)
%    \htext (0.4 -0.2){$\arcsin\left(\frac dr\right)$}
    
    \move (2.1 1) \avec(1.67 1)
    \htext (2.2 0.95){$r\sin\theta$}

    \move (-3 1.8) \lvec (-1 1.8)
    \move (-3 1) \lvec (-1 1)
    \move (-3 0.2) \lvec (-1 0.2)

    \move (-2.5 1.8) \lvec (-2.5 1)
    \htext (-2.45 1.4){$d$}
 
    \lpatt(0.05 0.05)
    \move (0 1.65)  \lvec (1 1.65)
    \move (-2 0.5)  \lvec (-2 1)
    \htext (-2.15 0.7){$X$}
    
    \move (-2 0.5) \lvec (-1 0.5)

    \lpatt( )
    \move (-2 0.5)
    \larc r:0.2 sd:0 ed:45
    \htext (-1.7 0.6){$\Theta$}
    
    
    \linewd 0.04 \setgray 0 
    \move (-2 0.5) \lvec (-1.2 1.3)
    \htext (-1.5 1.2){$r$}
}

The joint probability density of $(X, \Theta)$ is
\be
f(x, µ) = \left\{ \ba{ll}
\frac 1{\pi d}, \quad\quad & 0 \leq x < d, \ 0 \leq \theta \leq \pi,\\
0 & \text{otherwise}.
\ea\right.
\ee


Then if $A$ is the shaded area in the $x-\theta$ plane illustrated, the probability that the needle intersects a line is
\be
\pro (X \leq r \sin \Theta) = {\int\int}_{A} f(x, \theta)dxd\theta = \int^\pi_0 \int^{r \sin \theta}_0 \frac 1{\pi d} dxd\theta = \frac r{\pi d} \int^\pi_0
\sin \theta d\theta = \frac{2r}{\pi d}.
\ee
This probability was derived in 1777 by the French mathematician and naturalist Georges Louis Leclerc, Comte de Buffon, who suggested a method of approximating the value of $\pi$ by repeatedly dropping a needle and estimating the probability that a line is intersect (and hence the value of $\pi$) by recording the proportion of times that the needle crosses a line. First note that if $X \sim \sN(\mu, \sigma^2)$, where $\sigma^2$ is small, and $g : \R \to \R$ then
\be
g(X) = g(\mu) + (X - \mu)g'(\mu) + \dots \simeq \sN\bb{g(\mu), (g'(\mu))^2 \sigma^2},
\ee
where the symbol $\simeq$ may be read as "approximately distributed as". Now, if $S_n = X_1 + \dots + X_n$ denotes the total number of times that the needle intersects a line in $n$ drops of the needle, where $X_i$ is the indicator of the event that a line is intersected on the $i$th drop, then $S_n \sim \bd(n, p)$ where $p = 2r/(\pi d)$. By the Central Limit Theorem we have that $S_n/n \simeq \sN(p, p(1 - p)/n)$. Let $g(x) = 2r/(xd)$, so that $g(p) = \pi$ and $g'(p) = -\pi^2d/(2r)$. We see that an estimate of $\pi$ is given by $\wh{\pi} = g(S_n/n)$, where
\be
\wh{\pi} \simeq \sN\bb{\pi, \frac{\pi^2}{2rn} (\pi d - 2r)}.
\ee
One small difficulty that arises when one tries to replicate Buffon's procedure for estimating $\pi$ on a computer is how to do the simulation without using the value of $\pi$ to take random samples of $\Theta$ from the uniform distribution on $(0, \pi]$. One way around this is to generate a sample $(X, Y)$ which has the uniform distribution over a quadrant of the circle centre the origin and of unit radius as follows:

Step 1. generate independent $X$ and $Y$ each with the uniform distribution on $[0, 1]$,

Step 2. if $X^2 + Y^2 > 1$ repeat Step 1, otherwise take $(X, Y )$.

Now set $\Theta = 2\arctan(Y/X)$, which will be uniform on $(0, \pi)$.


Bertrand's Paradox This results from the following question posed by Bertrand in 1889: What is the probability that a chord chosen at random joining two points of a circle of radius $r$ has length $\leq r$?

The difficulty with the question is that there is not a unique interpretation of what it means for a chord to be chosen 'at random', there are different ways to do this and they lead to different probabilities for the length, $C$, of the chord being less than $r$. We will consider two approaches.

Approach 1. Let $X$ be a random variable having the uniform distribution on $(0, r)$ and let $\Theta$ be a random variable, independent of $X$, with the uniform distribution on $(0, 2\pi)$.

\centertexdraw{
    \drawdim in

    \arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
    
    \linewd 0.01 \setgray 0
    
    \move (-2 0.8) \lcir r:0.8

\move (-2 0.8)\larc r:0.2 sd:0 ed:135

\htext (-2.1 1.1){$\Theta$}
\htext (-2.3 0.8){$X$}
\htext (-5 1.2){The length of the chord is}
\htext (-5 0.8){$C = 2\sqrt{r^2 - X^2}$}

\move(-2 1.6) \lvec(-2.8 0.8)
\move(-2 0.8) \lvec(-2.5656 1.3656)


\lpatt (0.05 0.05)
\move (-2 0.8) \lvec(-1 0.8)

\move (-2 1.8)
}

Construct the chord by taking a reference line (the $x$-axis, say) and drawing the radius at angle $\Theta$ with the line. Then take the chord at right angles to this radius at distance $X$ from the centre of the circle. We then have 
\be
\pro(C \leq r) = \pro\bb{4(r^2 - X^2) \leq r^2} = \pro\bb{\sqrt{3} r/2 \leq X} = 1 - \sqrt{3}/2 \approx 0.134.
\ee

Approach 2. Let $\Theta_1$ and $\Theta_2$ be independent random variables each with the uniform distribution on $(0, 2\pi)$. Take the end points of the chord as the points $(r \cos\Theta_i, r \sin\Theta_i)$, $i = 1, 2$, on the circumference of the circle, where the angles are measured from some reference line. The length of the chord is $C = 2r \sin\bb{\frac{\abs{\Theta_1-\Theta_2}}2}$.

\centertexdraw{
    \drawdim in

    \arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
    
    \linewd 0.01 \setgray 0
    
    \move (-0.2 0) \avec(1.8 0)
    \move (0 -0.2) \avec(0 1.8)

    \move (0 1.5) \lvec(1.5 1.5) \lvec(1.5 0)
   
    \move (1.25 1.5) 
    \lvec(1.5 1.5) \lvec(1.5 1.25) \lvec(0.25 0) \lvec(0 0) \lvec(0 0.25) \lvec(1.25 1.5) 
    \lfill f:0.8

    \htext (1.8 0.1){$\Theta_1$}
    \htext (0.1 1.7){$\Theta_2$}
    \htext (-0.15 0.2){$\frac {\pi}3$}
    \htext (0.2 -0.2){$\frac {\pi}3$}
    \htext (-0.2 1.2){$\frac {5\pi}3$}
    \htext (1.2 -0.2){$\frac {5\pi}3$}

    \htext (-0.2 1.5){$2\pi$}
    \htext (1.5 -0.15){$2\pi$}

\move(0 1.25) \lvec(0.25 1.5) \lvec(0 1.5)\lvec(0 1.25) \lfill f:0.8
\move(1.25 0) \lvec(1.5 0.25) \lvec(1.5 0)\lvec(1.25 0) \lfill f:0.8

    \move (-2 0.8) \lcir r:0.8

\move (-2 0.8)\larc r:0.2 sd:0 ed:135
\move (-2 0.8)\larc r:0.3 sd:0 ed:45

    \htext (-1.7 0.9){$\Theta_1$}
    \htext (-2.1 1.1){$\Theta_2$}

\move(-2 0.8) \lvec(-1.4344 1.3656) \lvec(-2.5656 1.3656)\lvec(-2 0.8)

\lpatt (0.05 0.05)
\move (-2 0.8) \lvec(-1 0.8)
}

The probability is then
\be
\pro (C \leq r) = \pro\bb{\sin\bb{\frac{\abs{\Theta_1 - \Theta_2}}2} \leq \frac 12} = \pro\bb{\abs{\Theta_1 - \Theta_2} \leq \frac {\pi}3 \text{ or } \abs{\Theta_1 - \Theta_2}\geq \frac{5\pi}3},
\ee
this probability is the area of the shaded region in the square divided by $(2\pi)^2$ which gives the probability to be $\frac 13 \approx 0.3333$. This probability is the same as when you take one end of the chord as fixed (say on the reference line) and take the other end at a point at angle $\Theta$ uniformly distributed on $(0, 2\pi)$.

It should be noted that both probabilities may arise as the outcomes of physical experiments which are choosing the chord 'at random'. For example, the probability in Approach 1 would be found if a circular disc of radius $r$ is thrown randomly onto a table on which parallel lines at distance $2r$ are drawn, the chord would be determined by the unique line intersecting the circle and the distribution of the distance of center of the circle to the nearest line would be uniform on $(0, r)$. By contrast, the probability in Approach 2 is obtained if the disc is pivoted on a point on its circumference, which is on a given straight line and the disc is spun around that point, then the chord would be determined by the intersection of the given line and the circumference of the disc.

