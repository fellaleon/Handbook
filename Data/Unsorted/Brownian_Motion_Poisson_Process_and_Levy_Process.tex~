\chapter{Brownian Motion, Poisson Process and \levy\ Process}

\section{Brownian Motion}

\subsection{History and definition}

%This chapter is devoted to the construction and some properties of one of probability theory's most fundamental objects. 

Brownian motion earned its name after R. Brown, who observed around 1827 that tiny particles of pollen in water have an extremely erratic motion. It was observed by Physicists that this was due to an important number of random shocks undertaken by the particles from the (much smaller) water molecules in motion in the liquid. A. Einstein established in 1905 the first mathematical basis for Brownian motion, by showing that it must be an isotropic Gaussian process. The first rigorous mathematical construction of Brownian motion is due to N. Wiener in 1923, using Fourier theory. \levy\ studied the sample path properties of Brownian motion, and Kakutani and Doob made the link with potential theory. It\^o's calculus was developed in 1950.

%Our treatment follows later ideas of \levy\ and Kolmogorov.

In order to motivate the introduction of this object, we first begin by a ``microscopical" depiction of Brownian motion. Suppose $(X_n)_{n \geq 0}$ is a sequence of $\R^d$ valued random variables with mean 0 and covariance matrix $\sigma^2I_d$, which is the identity matrix in $d$ dimensions, for some $\sigma^2 > 0$. Namely, if $X_1 = (X_1^1, \dots,X^d_1)$,
\be
\E\bb{X^i_1} = 0,\quad \E\bb{X^i_1X^j_1} = \sigma^2\delta_{ij},\quad 1 \leq i, j \leq d.
\ee

We interpret $X_n$ as the spatial displacement resulting from the shocks due to water molecules during the $n$-th time interval, and the fact that the covariance matrix is scalar stands for an isotropy assumption (no direction of space is privileged).

From this, we let $S_n = X_1 + \dots + X_n$ and we embed this discrete-time process into continuous time by letting 
\be
B^{(n)}_t = \frac 1{\sqrt{n}}S_{\floor{nt}},\quad t \geq 0.
\ee

Recall Definition \ref{def:heat_kernel}, we have

\begin{definition}[transition density of standard Brownian motion\index{transition density!Brownian motion}]\label{def:transition density_standard_brownian_motion}
Let $\abs{\cdot}$ be the Euclidean norm\footnote{need definition} on $\R^d$ and for $t > 0$ and $x \in \R^d$, define
\be
p_t(x) = \frac 1{(2\pi t)^{d/2}} \exp\bb{-\frac{\abs{x}^2}{2t}},
\ee
which is the density of the Gaussian distribution $\sN(0, tI_d)$ with mean 0 and covariance matrix $tI_d$ (see Definition \ref{def:standard_gaussian_density}). By convention, the Gaussian law $\sN(m, 0)$ is the Dirac mass at $m$.
\end{definition}

%\qcutline

\begin{proposition}\label{pro:marginal_distribution_brownian_motion}
Let $0 = t_1 < t_2 < \dots < t_k$. Then the finite marginal distributions of $B^{(n)}$ with respect to times $t_1,\dots, t_k$ converge weakly as $n \to\infty$. More precisely, if $f$ is a bounded continuous function, and letting $x_0 = 0$, $t_0 = 0$, 
\be
\E\bsb{f\bb{B^{(n)}_{t_1} ,\dots,B^{(n)}_{t_k} }} \to \int_{(\R^d)^k} f(x_1,\dots, x_k) \prod_{1\leq i\leq k} p_{\sigma^2(t_i-t_{i-1})}(x_i - x_{i-1})dx_i\quad \text{as }n \to \infty.
\ee

Otherwise said, $(B^{(n)}_{t_1} ,\dots,B^{(n)}_{t_k})$ converges in distribution to $(G_1,G_2, \dots,G_k)$, which is a random vector whose law is characterized by the fact that $(G_1,G_2-G_1, \dots,G_k-G_{k-1}) = (N_1,\dots, N_k)$ are independent centered Gaussian random variables with respective covariance matrices $\sigma^2(t_i - t_{i-1})I_d$. ($(B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}}, 1 \leq i \leq k)$ converges in distribution to $(N_i)_{1 \leq i \leq k}$, where the $N_i$'s are independent and $N_i \sim \sN (0, (t_i - t_{i-1})I_d)$. In particular, the law of $B^{(n)}_t$, $\sL(B^{(n)}_t )\to \sN (0, tI_d)$.)
\end{proposition}

\begin{remark}
This suggests that $B^{(n)}$ should converge to a process $B$ whose increments are independent and Gaussian with covariances dictated by the above formula. This will be set in a rigorous way later with Donsker's invariance theorem\footnote{need theorem}.%in the course, 
\end{remark}

\begin{proof}[\bf Proof]
With the notations of the theorem, we first check that $\bb{B^{(n)}_{t_1} ,B^{(n)}_{t_2}-B^{(n)}_{t_1} ,\dots,B^{(n)}_{t_k} - B^{(n)}_{t_{k-1}}}$ is a sequence of independent random variables. Indeed, one has for $1 \leq i \leq k$
\be
B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}} = \frac 1{\sqrt{n}} \sum^{\floor{nt_i}}_{j=\floor{nt_{i-1}}+1} X_j ,
\ee
and the independence follows by the fact that $(X_j , j \geq 0)$ is an i.i.d. family. Even better, we have the identity in distribution for the $i$-th increment
\be
B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}} \stackrel{d}{=} \frac{\sqrt{\floor{nt_i} - \floor{nt_{i-1}}}}{\sqrt{n}} \frac{ 1}{\sqrt{\floor{nt_i} - \floor{nt_{i-1}}}} \sum^{\floor{nt_i}-\floor{nt_{i-1}}}_{j=1} X_j
\ee
and the central limit theorem (Theorem \ref{thm:central_limit}) shows that this converges in distribution to $G_j - G_{j-1}$ with a Gaussian law $\sN(0, \sigma^2(t_i - t_{i-1})I_d)$.

Summing up our study, and introducing characteristic functions, we have shown that for every $\xi = (\xi_j)_{1 \leq j \leq k}$, since $B^{(n)}_{t_j} - B^{(n)}_{t_{j-1}}$ are independent,
\beast
\E\bb{\exp\bb{i\sum^{k}_{j=1} \xi_j(B^{(n)}_{t_j} - B^{(n)}_{t_{j-1}})}}  & = & \prod^k_{j=1} \E\bb{\exp\bb{i\xi_j(B^{(n)}_{t_j} - B^{(n)}_{t_{j-1}})}} \quad \text{(by Theorem \ref{thm:characteristic_function})}\\
& \to & \prod^k_{j=1} \E\bb{\exp (i\xi_j(G_j - G_{j-1}))} \quad \text{(by Theorem \ref{thm:equivalent_modes_of_convergence})}\\
& = & \E\bb{\exp\bb{i\sum^k_{j=1} \xi_j(G_j - G_{j-1})}}\quad \text{(by Theorem \ref{thm:characteristic_function})}.
\eeast
as we assume $G_j - G_{j-1}$ are independent. %where $G_1,\dots,G_k$ is distributed as in the statement of the proposition. 
By \levy's convergence theorem (Theorem \ref{thm:levy_convergence}) we deduce that increments of $B^{(n)}$ between times $t_i$ converge to increments of the sequence $G_i$ weakly. %, which is easily equivalent to the statement. 
That is, by Corollary \ref{cor:convergence_in_distribution_iff_expectation}, for any bounded continuous function $F$,
\be
\E\bb{f(B_{t_1} ,\dots, B_{t_k} )} \to \int_{(\R^d)^k} f(x_1, \dots, x_k) \prod^k_{i=1} p_{\sigma^2(t_i-t_{i-1})} (x_i - x_{i-1})d x_1 \dots d x_k
\ee
as $n\to \infty$.
%With a appropriate change of variables, we may write
%\be
%\E[G(B_{t_i} - B_{t_{i-1}} , 1 \leq i \leq k)] = \int_{(\R^d)^k} G(x_1, \dots, x_k) \prod^k_{i=1} p_{t_i-t_{i-1}} (x_i)d x_1 \dots d x_k
%\ee
\end{proof} 


We now start to define and study Brownian motion.

\begin{definition}[standard Brownian motion\index{Brownian motion!standard}]\label{def:standard_brownian_motion_d}
An $\R^d$-valued stochastic process $(B_t, t \geq 0)$ is called a standard Brownian motion if it is a (sample) continuous process (has continuous sample path a.s.), that satisfies the following conditions:
\ben
\item [(i)] $B_0 = 0$ a.s.,
\item [(ii)] for every $0 = t_0 \leq t_1 \leq t_2 \leq \dots \leq t_k$, the increments $(B_{t_1} - B_{t_0} ,B_{t_2} - B_{t_1} ,\dots, B_{t_k} - B_{t_{k-1}})$ are independent, 
\item [(iii)] for every $t, s \geq 0$, the law of $B_{t+s} - B_t$ is Gaussian with mean 0 and covariance $sI_d$.
\een
\end{definition}

\begin{remark}
The term ``standard" refers to the choice $B_0 = 0$ a.s..%the fact that $B_1$ is normalized to have variance $I_d$, and
\end{remark}

\begin{definition}[Brownian motion\index{Brownian motion}]\label{def:brownian_motion_d}
An $\R^d$-valued stochastic process $(B_t, t \geq 0)$ is called a Brownian motion if it is a (sample) continuous process (has continuous sample path a.s.), that satisfies the following conditions:
\ben
\item [(i)] for every $0 = t_0 \leq t_1 \leq t_2 \leq \dots \leq t_k$, the increments $(B_{t_1} - B_{t_0} ,B_{t_2} - B_{t_1} ,\dots, B_{t_k} - B_{t_{k-1}})$ are independent,
\item [(ii)] for every $t, s \geq 0$, the law of $B_{t+s} - B_t$ is Gaussian with mean 0 and covariance $sI_d$.%the law of $B_{t+s} - B_t$ is Gaussian with mean 0 and covariance $cI_d$ for some $c$ proportional to $s$.
\een
\end{definition}

%As a first approximation, consider $S_n = X_1 + \dots+ X_n$, where the $X_j$ are $\R^d$-valued i.i.d. r.v.'s and $X_1 = \pm e_i$ with probability $\frac 1{2d}$, where $e_i = (0, \dots, 0, 1, 0,\dots, 0)$. By the CLT,
%\be
%\frac{S_n}{\sqrt{n}} \stackrel{(d)}{\to} \sN (0, I_d ) \stackrel{(d)}{=} N,
%\ee
%where $N_i$ are i.i.d. $\sN (0, 1)$. Consider $B^{(n)}_t = \frac{S_{\floor{tn}}}{\sqrt{n}}$ for $t \geq 0$, so in particular $\sL(B^{(n)}_1) \stackrel{(w)}{\to}  N (0, I_d)$. Brownian motion is a continuous process $B$ such that '$B^{(n)} \to B$ in distribution.'

%The finite dimensional marginal distributions $\sL(B_{t_1} , \dots, B_{t_k})$ for every $k$ should be given by
%\be
%\lim_{n\to \infty} \sL\bb{B^{(n)}_{t_1} ,\dots, B^{(n)}_{t_k} }.
%\ee



%\begin{proof}[\bf Proof]
%(For $d = 1$.) Using characteristic functions. Let $\xi \in \R^k$.
%\beast
%\E\bsb{\exp\bb{i\sum^k_{j=1} \xi_j(B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}} )}} & = & \prod^k_{j=1} \E\bsb{\exp\bb{i\xi_j(B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}})}} = \prod^k_{j=1} \E\bsb{\exp\bb{i\xi_j\sum^{\floor{nt_j}-\floor{nt_{j-1}}}_{r=1} X_r}} \\
%& \stackrel{CLT}{\to } & \prod^k_{j=1} \E\bsb{\exp\bb{i\xi_jN_j)}} = \E \bsb{\exp\bb{i\sum^k_{j=1}\xi_jN_j}}
%\eeast
%since $B^{(n)}_{t_i} -B^{(n)}_{t_{i-1}} = \frac 1{\sqrt{n}} \sum^{\floor{nt_j}}_{r=\floor{nt_{j-1}}+1} X_r$, so the $\{B^{(n)}_{t_i} -B^{(n)}_{t_{i-1}}\}$ are $k$ independent r.v.'s.

%And further
%\be
%B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}} = \frac 1{\sqrt{n}} \sum^{\floor{nt_j} -\floor{nt_{j-1}}}_{r=1}X_r.
%\ee

%Conclude with L\'evy's Theorem.
%\end{proof}

%
%\begin{definition}
%Let $(B_t )_{t\geq 0}$ be a stochastic process in $\R^d$. We say that $(B_t)$ is a standard Brownian motion if
%\ben
%\item [(i)] $B_0 = 0$;
%\item [(ii)] For $0 = t_0 < t_1 < \dots < t_k$, $(B_{t_i} - B_{t_{i-1}}, 1 \leq i \leq k)$ are independent;
%\item [(iii)] $B_t - B_s$ has distribution $\sN (0, (t -s)I_d )$ for all $s < t$.
%\item [(iv)] For all $\omega \in \Omega$, $t \to B_t (\omega)$ is continuous.
%\een
%\end{definition}

%Here 'standard' refers to the fact that $B_0 = 0$ and $\cov B_1 = I_d$. The finite dimensional marginal distribution for $t_1 < \dots < t_k$ is the law of $(B_{t_1},\dots, B_{t_k})$. Let
%\be
%p_t (x) = \frac 1{(2\pi t)^{\frac d2}} e^{-\frac{\abs{x}^2}{2t}}
%\ee
%be the probability density function of $\sN (0, t I_d )$. Then

\begin{example}
Suppose that $(B_t)_{t\geq 0}$ is a standard Brownian motion and $U$ is an independent random variable uniformly distributed on $[0,1]$. Then the process $(\wt{B}_t)_{t\geq 0}$ defined by 
\be
\wt{B}_t = \left\{ \ba{ll}
B_t \quad\quad & t \neq U\\
0 & t = U
\ea\right.
\ee
has the same finite dimensional marginal distribution as Brownian motion, but is discontinuous if $B(U)\neq 0$, which happens with probability one, and hence it is not a Brownian motion.
\end{example}

The characteristic properties (i), (ii), (iii) exactly amount to say that the finite dimensional marginals of a Brownian motion are given by the formula of Proposition \ref{pro:marginal_distribution_brownian_motion}. Therefore the law of the Brownian motion is uniquely determined by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}. We now show Wiener's theorem that Brownian motion exists!

\begin{theorem}[Wiener theorem]\label{thm:wiener_brownian_motion_existence}
There exists a standard Brownian motion on some probability space.
\end{theorem}

\begin{proof}[\bf Proof]
We will first prove the theorem in dimension $d = 1$ and construct a process $(B_t)_{t\in [0,1]}$ satisfying the properties of a Brownian motion. This proof is essentially due to P. \levy\ in 1948. The proof given by Ciesielski\cite{Ciesielski_1961} is the ultimate refinement of Wiener's original idea of representing Brownian motion as a random Fourier series.

%Before we start, we will need the following lemma, which is left as an exercise.
%\begin{lemma}\label{lem:gaussian_boundary}
%Let $\sN$ be a standard Gaussian random variable. Then
%\be
%\frac{x^{-1} - x^{-3}}{\sqrt{2\pi }} e^{-x^2/2} \leq \pro(\sN > x) \leq \frac{x^{-1}}{\sqrt{2\pi }} e^{-x^2/2}.
%\ee
%\end{lemma}

Let $\sD_0 = \{0, 1\}$, $\sD_n = \{k2^{-n}, 0 \leq k \leq 2^n\}$ for $n \geq 1$, and $\sD = \bigcup_{n\geq 0} \sD_n$ be the set of dyadic rational numbers in $[0, 1]$. On some probability space $(\Omega,\sF,\pro)$, let $(Z_d)_{d \in D}$ be a collection of i.i.d. random variables all having a Gaussian distribution $\sN(0, 1)$ with mean 0 and variance 1. 

It is a well-known and important fact that if the random variables $X_1,X_2,\dots$ are linear combinations of independent centered Gaussian random variables, then $X_1,X_2,\dots$ are independent if and only if they are pairwise uncorrelated, namely $\cov(X_i,X_j) = \E\bb{X_iX_j} = 0$ for every $i \neq j$\footnote{need theorem}.

\centertexdraw{
    
    \arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
    \def\bdot {\fcir f:0 r:0.02 }
    
    \drawdim in
    \linewd 0.01 \setgray 0
 
    \lpatt (1 0)

    %\move (2 1.4)
    \move (2 -0.3)
 
    \move (0 0) \avec(0 1.3)
    \move (-0.2 0) \avec(2.3 0)
    %\move (0 0) \lvec(1 1.1) \lvec(2 0.6)
    \move (0 0) \bdot
    \move (1 1.1) \bdot
    \move (2 0.6) \bdot

    \htext (0 -0.15){$d^-$}
    \htext (2 -0.15){$d^+$}
    \htext (1 -0.15){$d$}

    \htext (-0.25 0.2){$X^n_{d^-}(=X^{n-1}_{d^-})$}
    \htext (1.8 0.7){$X^n_{d^+}(=X^{n-1}_{d^+})$}
    \htext (0.9 1.15){$X^n_{d}$}
    \htext (0.82 0.3){$\left. \ba{l} \\ \\ \\ \\ \ea\right\}\frac{Z_d}{2^{(n+1)/2}}$}
   
    \lpatt (0.05 0.05)

    \move (0 0) \lvec(2 0.6)
    \move (1 1.1) \lvec(1 0)
    \move (2 0.6) \lvec(2 0)
    %\move (4 0) \lvec(4.75 1.1) \lvec(5.5 0.6)
    	
    %\linewd 0.02 \setgray 0
    %\move(-1.5 0) \lvec(-1 0) \lvec(-0.5 0.5) \lvec(0.5 0.5)\lvec(1 0)\lvec(1.5 0)

}

We set $X^0_0 = 0$ and $X^0_1 = Z_1$. Inductively, given $(X^{n-1}_d)_{d \in \sD_{n-1}}$, we build $(X^n_d)_{d \in \sD_n}$ in such a way that $(X^n_d)_{d \in \sD_n}$ satisfies (i), (ii), (iii) in the definition of the Brownian motion (where the instants under consideration are taken in $\sD_n$). To this end, take $d \in \sD_n \bs \sD_{n-1}$, and let $d^- = d - 2^{-n}$ and $d^+ = d + 2^{-n}$ so that $d^-$, $d^+$ are consecutive dyadic numbers in $\sD_{n-1}$. Then define:
\be
X^n_d = \frac{X^{n-1}_{d^-} + X^{n-1}_{d^+}}2 + \frac{Z_d}{2^{(n+1)/2}} .
\ee
and put $X^{n}_{d^-} = X^{n-1}_{d^-}$ and $X^{n}_{d^+} = X^{n-1}_{d^+}$. Note that with these definitions,
\be
X^{n}_{d} - X^{n}_{d^-} = N_d + N'_d,\quad \quad X^{n}_{d^+} - X^{n}_{d} = N_d - N'_d,%\quad \quad (*)
\ee
where $N_d := (X^{n-1}_{d^+} - X^{n-1}_{d^-} )/2$, $N'_d  = Z_d/2^{(n+1)/2}$ are by the induction hypothesis two independent centered Gaussian random variables with variance $2^{-n-1}$. From this, one deduces
\be
\cov(N_d + N'_d ,N_d - N'_d) = \var(N_d) - \var(N'_d) = 0,
\ee
so that the increments $X^n_d - X^n_{d^-}$ and $X^n_{d^+} - X^n_{d}$ are independent with variance $2^{-n}$, as should be.

Moreover, these increments are independent of the increments $X^n_{d'+2^{-n}} - X^n_{d'}$ for $d' \in \sD_{n-1}$ with $d' \neq d^-$ and of $Z_{d'}$ with $d'\in\sD_n\bs \sD_{n-1}$, $d' \neq d$ so they are independent of the increments $X_{d''+2^{-n}} - X_{d''}$ for $d'' \in \sD_n$ with $d'' \notin \{d^-, d\}$. This allows the induction argument to proceed one step further. 

We have thus defined a process $(X^n_d)_{d \in \sD_n}$ which satisfies properties (i), (ii) and (iii) for all dydadic times $t_1, t_2,\dots, t_k \in \sD_n$. Observe that if $\sD \in \sD_n$, $X^m_d = X^n_d$ for all $m \geq n$. Hence for all $d \in \sD$,
\be
X_d = \lim_{n\to \infty} X^n_d
\ee
is well-defined and the process $(X_d)_{d \in \sD}$ obviously satisfies (i), (ii) and (iii). 


To extend this to a process defined on the entire interval $[0, 1]$, we proceed as follows. Define, for each $n \geq 0$, a process $Y^n_t$, $0 \leq t \leq 1$ to be the linear interpolation of the values $(X_d)_{d \in\sD_n}$ the dyadic times at level $n$ (with straight line connecting $X_d$ for $d\in \sD_n$). 

\centertexdraw{
    
    \arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
    \def\bdot {\fcir f:0 r:0.02 }
    
    \drawdim in
    \linewd 0.01 \setgray 0
 
    \lpatt (1 0)

    \move (0 1.4)
    \move (0 -0.3)
    \move (0 -0.2) \avec(0 1.3)
    \move (-0.2 0) \avec(1.5 0)

    \move (0 0) \lvec(1.5 0.6)

    \move (2 -0.2) \avec(2 1.3)
    \move (1.8 0) \avec(3.5 0)
    \move (2 0) \lvec(2.75 1.1) \lvec(3.5 0.6)

    \move (4 -0.2) \avec(4 1.3)
    \move (3.8 0) \avec(5.5 0)
    \move (4 0) \lvec(4.375 0.8) \lvec(4.75 1.1) \lvec(5.125 0.7) \lvec(5.5 0.6)


    \htext (0.5 -0.2){$Y^0$}
    \htext (2.5 -0.2){$Y^1$}
    \htext (4.5 -0.2){$Y^2$}
    
   
    \lpatt (0.05 0.05)

    \move (2 0) \lvec(3.5 0.6)
    \move (4 0) \lvec(4.75 1.1) \lvec(5.5 0.6)
    	
    %\linewd 0.02 \setgray 0
    %\move(-1.5 0) \lvec(-1 0) \lvec(-0.5 0.5) \lvec(0.5 0.5)\lvec(1 0)\lvec(1.5 0)

}


Note that if $d \in \sD$, say $d \in \sD_m$ with $m \geq 0$, then for any $n \geq m$, $Y^n_d = Y^m_d = B_d$. Furthermore, define an event $A_n$ by 
\be
A_n = \bra{\sup_{0\leq t\leq 1} \abs{Y^n_t - Y^{n-1}_t} > 2^{-n/4}}.
\ee

We then have, let $Z$ be a standard gaussian random variable.
\beast
\pro(A_n) & = & \pro\bb{\bigcup^{2^{n-1}-1}_{j=0} \bra{\sup_{t\in[(2j)2^{-n},(2j+2)2^{-n}]} \abs{Y^n_t - Y^{n-1}_t} > 2^{-n/4}}}  \\
& \leq & \sum^{2^{n-1}-1}_{j=0} \pro\bb{\sup_{t\in[(2j)2^{-n},(2j+2)2^{-n}]} \abs{Y^n_t - Y^{n-1}_t} > 2^{-n/4}} \leq \sum^{2^{n-1}-1}_{j=0} \pro\bb{\frac{\abs{Z_{(2j+1)2^{-n}}}}{2^{(n+1)/2}} > 2^{-n/4}}\\
& \leq & \sum^{2^{n-1}-1}_{j=0} \pro\bb{\abs{Z} > 2^{(n+2)/4}}  \leq \sum^{2^{n-1}-1}_{j=0} \pi^{-1/2} 2^{-n/4-1} \exp\bb{-2^{n/2}} \leq \pi^{-1/2} 2^{3n/4} \exp\bb{-2^{n/2}}
\eeast
by Proposition \ref{pro:bound_of_gaussian_law}. We conclude that $\sum\limits^\infty_{n=0} \pro(A_n) < \infty$ and by Borel-Cantelli theorem (Theorem \ref{lem:borel_cantelli_1_probability}), the events $A_n$ occur only finitely often. We deduce immediately that the sequence of functions $Y^n$ is almost surely Cauchy in $C[0, 1]$ equipped with the topology of uniform convergence, and hence $Y^n$ converges toward a (sample) continuous limit function $(B_t)_{0 \leq t \leq 1}$ uniformly, almost surely. If $\omega$ is in the null set, we simply define $B\equiv 0$.

Since $Y^n_t$ is constantly equal to $X_t$ for $t \in \sD$ and for $n$ large enough, it must be that $X_t = B_t$ for all $t \in \sD$. Thus $B$ is a continuous extension of $X$. (The extension of $(X_d, d \in D)$ could have been obtained by appealing to the existence of a continuous modification (version), whose existence is provided by Kolmogorov's criterion below\footnote{need theorem, see unsorted part}.%This can also obtained by the following argument. 

For $s, t \in \sD$, $s < t$, $\E\bb{\abs{X_s-X_t }^p} = \abs{t-s}^{\frac p2} \E\bb{N^p}$ for $p > 2$, where $N \sim \sN (0, 1)$, since $X_s - X_t \sim \sN (0, t -s) \sim \sqrt{t -s}\sN (0, 1)$. But this latter is $C_p\abs{t -s}^{1+(\frac p2 -1)}$ for some $C_p > 0$. By the Kolmogorov Criterion (Theorem \ref{thm:kolmogorov_continuity_criterion}), there exists a.s. a continuous function $(B_t)_{t\in [0,1]}$ that extends $(X_d)_{d\in \sD}$.) %Let $\Omega_0 = \{\omega| (B_d (\omega))_{d\in D}\}$ does not have a continuous extension to $[0, 1]$. On $\Omega_0$, let $B_t (\omega) = 0$ for all $t \in [0, 1]$. (Notice that $\pro(\Omega_0) = 0$.)

%and we still denote this extension by $X$. 

We now deduce properties (i), (ii) and (iii) for $B$ by continuity and the fact that $Y^n$ satisfies these properties. Indeed, let $k \geq 1$ and let $0 < t_1 < t_2 \dots < t_k < \infty$. Fix $\alpha_1, \alpha_2,\dots, \alpha_k \geq 0$. For every $1 \geq i \geq k$, fix a sequence $(d^{(n)}_i)_t$ such that $\lim_{n\to \infty} d^{(n)}_i = t_i$, and assume (since $\sD$ is dense in $[0, 1]$) that $d^{(n)}_i\in \sD$ and $t_{i-1} < d^{(n)}_i \leq t_i$. Thus, $X_{d^{(n)}_i} \to B_{t_i}$ a.s.. Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}), 
\beast
& & \E\bb{\exp \bb{i\alpha_1 B_{t_1} + i\alpha_2(B_{t_2} - B_{t_1}) +\dots+ i\alpha_k(B_{t_k} - B_{t_{k-1}})}}\\
& = & \lim_{n\to \infty} \E\bb{\exp\bb{i\alpha_1X_{d^{(n)}_1} + i\alpha_2\bb{X_{d^{(n)}_2} - X_{d^{(n)}_1}}  +\dots+ i\alpha_k\bb{X_{d^{(n)}_k}- X_{d^{(n)}_{k-1}}}}}\\
& = & \lim_{n\to \infty} \exp\bb{-\frac{\alpha^2_1}2 d^{(n)}_1 -\dots- \frac{\alpha^2_k}2 \bb{d^{(n)}_k - d^{(n)}_{k-1}}} = \exp\bb{-\frac{\alpha^2_1}2 t_1 -\dots - -\frac{\alpha^2_k}2(t_k - t_{k-1})}.
\eeast

Thus, we can say $B$ is a standard Brownian motion on $[0,1]$ by \levy\ convergence theorem (Theorem \ref{thm:levy_convergence}).

It is now easy to construct a Brownian motion indexed by $\R^+$. Simply take independent standard Brownian motions $(B^i_t)_{t\in [0 ,1],i \geq 0}$ as we just constructed, and let
\be
B_t = \sum^{\floor{t}-1}_{i=0} B^i_1 + B^{\floor{t}}_{t-\floor{t}},\quad t \geq 0.
\ee

It is easy to check that this has the wanted properties\footnote{need details}.

Finally, it is straightforward to build a Brownian motion in $\R^d$, by taking $d$ independent copies $B^1,\dots,B^d$ of $B$ and checking that $((B^1_t ,\dots, ,B^d_t ), t \geq 0)$ is a Brownian motion in $\R^d$\footnote{need details}.
\end{proof}

%\begin{proof}[\bf Proof]
%Let $D_n = \{k2^{-n}| 0 \leq k < 2^n\}$ for $n \geq 1$, and $D_0 = \{0, 1\}$. Let us construct $(B_d)_{d\in D}$, where $D = \bigcup_n D_n$ are the dyadic rationals, that satisfy (i), (ii), and (iii) of the definition. (We are doing the case $d = 1$ on the time interval $[0, 1]$.) Let $(Z_d)_{d\in D}$ be a family of independent r.v.'s with distribution $\sN (0, 1)$. We'll construct $B_d$ such that for all $d$, $B_d \in \text{Vect}(Z_{d'} , d' \in D) =: G$. For $X_1,\dots, X_k \in G$, the $(X_i)$'s are independent if and only if they are pairwise orthogonal, i.e. $\E[X_iX_j] = 0$ for all $i \neq j$.

%The construction is by induction. For $n = 0$ take $B_0 = 0$ and $B_1 = Z_0$. Suppose now that $(B_d )_{d\in D_{n-1}}$ satisfies properties (i), (ii), and (iii) of the definition, and that $(B_d )_{d\in D_{n-1}}$ is independent of $(Z_d)_{d\in D\bs D_{n-1}}$. Let $d \in D_n \bs D_{n-1}$, and defined $d^- = d -2^{-n}$ and $d^+ = d +2^{-n}$, so $d^-, d^+ \in D_{n-1}$. Let $B_d = \frac 12 (B_{d^+} +B_{d^-})+ \frac 1{2^{\frac{n+1}2}} Z_d$. Now $B_d -B_{d^-}$, $B_d -B_{d^+}$ are independent $\sN (0, \frac 1{2^n})$ (since (check that) if $N_1$, $N_2$ are $\sN (0,\sigma^2)$ and independent then $N_1+N_2$ and $N_1-N_2$ are independent $\sN (0, 2\sigma^2)$. Indeed, $\cov(N_1 + N_2),N_1 - N_2) = \cov(N_1,N_1)-\cov(N_2,N_2) = 0$.)

%Further, the same method (check this) proves that these increments are independent of all other $B_{d'} - B_{d'^-}$ for $d'\in D\bs D_{n-1}$, so the induction step is proved. By induction we get $(B_d )_{d\in D}$ satisfying (i), (ii), (iii) of the definition.

%Let us check (i), (ii), (iii) for $(B_t)_{t\in [0,1]}$. (i) is trivial. Let $0 = t_0 < t_1 < \dots < t_k$, and consider $0 \leq t^n_1 < \dots < t^n_k$ such that $t^n_i \in D_n$ for all $n$ and $t^n_i \to t_i$ as $n\to \infty$. We know that $(B_{t^n_i} - B_{t^n_{i-1}}, 1 \leq i \leq k)$ are independent $\sN (0, t^n_i - t^n_{i-1})$ for all $n$.

%Claim. If $(N^n_1 , \dots,N^n_k)$ are independent Gaussian $N_i \sim \sN (0, (\sigma^n_i)^2)$, then if $\sigma^n_i \to \sigma_i$ then $(N^n_1 , \dots,N^n_k ) \to (N_1, \dots,N_k)$ in distribution, where $N_i$ are independent $N_i \sim \sN (0,\sigma^2_i)$.

%Indeed, use L\'evy's convergence theorem.

%By continuity of $B$, $B_{t^n_i} - B_{t^n_{i-1}} \to  B_{t_i} - B_{t_{i-1}}$ for every $\omega$, so by the lemma we do have $(B_{t_i} - B_{t_{i-1}}, 1 \leq i \leq k)$ are independent $\sN (0, t_i - t_{i-1})$, so $(B_t )_{t\in[0,1]}$ is a Brownian motion.

%To get a Brownian motion $(B_t)_{t\geq 0}$, take Brownian motions $(B^n_t)_{t\in [0,1]}$ independent on $[0, 1]$ for $n \geq 1$ and define $B_t = \sum^{\floor{t}-1}_{n=0} B^n_1 + B^{\floor{t}}_{t-\floor{t}}$ for $t \geq 0$. In higher dimensions $d > 1$ take $B^{(1)}, \dots, B^{(d)}$ independent Brownian motion in $\R$ and set $B_t = (B^{(1)}_t , \dots, B^{(d)}_t ), t \geq 0$, which is a Brownian motion in $\R^d$ (check).
%\end{proof}

%\begin{proof}[\bf Alternative proof]
%\end{proof}

The uniqueness of Brownian motion is in the distributional sense by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}.%\footnote{add Wiener measure}

\begin{definition}[Wiener's measure\index{Wiener's measure}]\label{def:wiener_measure}
Let $\Omega_W = C(\R^+,\R^d)$ (space of (sample) continuous functions) be the Wiener space, endowed with the product $\sigma$-algebra $\sW$ (or the Borel $\sigma$-algebra associated with the compact-open topology (wrt time)).

Let $(B_t)_{t \geq 0}$ be a Brownian motion, and let $\W$ be the law of $B$. that is, for any $A \in \sW$,
\be
\W(A) = \pro\bb{(B_t)_{t \geq 0}\in A}.
\ee

Then $\W$ is called Wiener's measure.
\end{definition}

\begin{remark}
We must check that this definition makes sense (there is a bijection between Brownian motion and Wiener measure), i.e., that $\W$ does not depend on the construction of $B$. To see this, note that the finite-dimensional distributions (i.e., the joint law of $(B_{t_1} ,\dots,B_{t_n})$ are entirely specified by the definition of a Brownian motion. 

Since the $\sigma$-field $\sW$ is generated by cylinder events of the form $\{B_{t_1} \in A_1,\dots,B_{t_n} \in A_n\}$ for $A_i \in \sB(\R^d)$, the right-hand side in the above display is indeed uniquely specified by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}.
\end{remark}


%Recalling Definition \ref{def:canonical_process}, we have

\begin{proposition}
Let $B$ be a Brownian motion on $(\Omega,\sF,\pro)$. Then the canonical process $Y$ is a Brownian motion under Wiener's measure.
\end{proposition}

\begin{proof}[\bf Proof]
Recalling ($*$) in Definition \ref{def:canonical_process}, we let $(\phi(\omega))(t) = B_t(\omega)$ for $\phi:\Omega \to (\R^d)^{\R^+},\ \omega \mapsto \phi(\omega)$. Thus, for $A_i\in \sB(\R^d)$,
\beast
\W \bb{\phi(\omega) \in \Omega_W:Y_{t_1}(\phi(\omega))\in A_1,\dots, Y_{t_n}(\phi(\omega))\in A_n} & = & \W \bb{\phi(\omega) \in \Omega_W:(\phi(\omega))(t_1) \in A_1,\dots, (\phi(\omega))(t_n)\in A_n}\\
& = & \W\bb{\phi(\omega)\in \Omega_W: B_{t_1}(\omega)\in A_1,\dots B_{t_n}(\omega)\in A_n} \\
& = & \pro\bb{\omega \in \Omega: B_{t_1}(\omega)\in A_1,\dots B_{t_n}(\omega)\in A_n} \\
& = & \pro\bb{B_{t_1}\in A_1,\dots B_{t_n}\in A_n} . %\pro\circ \phi^{-1} \bb{Y_{t_1}\in A_1,\dots, Y_{t_n}\in A_n} 
\eeast

Thus, two sides have the same distribution since they have the same finite dimensional marginal distribution (by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}).

Hence, we can see that the canonical process $Y$ is actually a Brownian motion under Wiener's measure.
\end{proof}

%since $B$ and $Y$ have the same finite dimensional marginal distributions, 
%the law of process $B$ and $Y$ are the same by 

\begin{remark}
This is the canonical construction of Brownian motion. 
%\end{remark}



%\begin{definition}
%We now think of $Omega$ as our probability space. For $\omega\in \Omega$ define.
%\be
%X_t(\omega) = \omega(t), \quad t \geq 0
%\ee

%We call $(X_t(\omega), t \geq 0)$ the canonical process. Then $(X_t, t \geq 0)$, under the probability measure $\W$, is a Brownian motion. 
%\end{definition}

%\begin{remark}

It is rarely the case in probability theory that we put some emphasis on the probability space on which a certain random process is constructed. (In all practical cases, we usually assume that such a random process is given to us). However the full advantage of specifying the probability space and measure will come to light when we deal with Girsanov's change of measure theorem.
\end{remark}

\begin{definition}
For $x \in \R^d$ we also let $\W_x(dw)$ be the image measure of $\W$ by $(w_t)_{t \geq 0} \mapsto (x+w_t)_{t \geq 0}$. 

A (continuous) process with law $\W_x(dw)$ is called a Brownian motion started at $x$. 

We let $(\sF^B_t)_{t \geq 0}$ be the natural filtration of $(B_t)_{t \geq 0}$, completed by zero-probability events.
\end{definition}


\begin{definition}
We say that $B$ is a Brownian motion (started at $X$) if $(B_t - X)_{t \geq 0}$ is a standard Brownian motion which is independent of $X$. 
\end{definition}

\begin{remark}
Otherwise said, it is the same as the definition as a standard Brownian motion, except that we do not require that $B_0 = 0$. If we want to express this on the Wiener space with the Wiener measure, we have for every measurable functional $F:\Omega\to \R^+$,
\be
\E\bb{F(B_t)_{t \geq 0}} = \int_{\R^d} \bb{\int_\Omega F(\bb{x + w(t)}_{t \geq 0}) \W(dw) }\pro(X \in dx) = \int_{\R^d} \W_x(F) \pro(X \in dx).
\ee

It will be handy to use the notation $W_X(F)$ for the random variable $\omega \mapsto W_{X(\omega)}(F)$, so that the right-hand side can be shortened as $\E(W_X(F))$.
\end{remark}

\subsection{Basic properties}

\begin{proposition}\label{pro:brownian_motion_basic_properties}
Let $B$ be a standard Brownian motion in $\R^d$.
\ben
\item [(i)] rotational invariance. If $U \in O(d)$ is an orthogonal matrix, then $UB = (UB_t)_{t \geq 0}$ is again a Brownian motion. In particular, $-B$ is a Brownian motion.
\item [(ii)] scaling invariance. If $\lm > 0$ then $(\lm^{-1/2} B_{\lm t})_{t \geq 0}$ is a standard Brownian motion.
\item [(iii)] time-inversion. $\wt{B}_t := (tB_{1/t})_{t \geq 0}$ is also a Brownian motion (at $t = 0$ the process is defined by its value 0 a.s., i.e., $\wt{B}_0 = 0$ a.s.).
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Since $U$ is orthogonal matrix, we have $U^TU = UU^T = I$. Then\footnote{need theorem} $UB$ has covariance matrix $U^TVU$ where $V$ is the covariance matrix of $B$, say $\sigma^2 I_d$. Thus the covariance matrix of $UB$ is
\be
U^T \sigma^2 I_d U = \sigma^2 U^T I_d U = \sigma^2 U^TU = \sigma^2 I_d \ \ra \ \text{$UB$ is a Brownian motion.}
\ee
%Use the fact that $N \sim \sN (0,\sigma^2 I_d)$ then $UN \sim \sN (0,\sigma^2 I_d )$.
\item [(ii)] Obviously, $B_0 = 0$ a.s.. Since
\be
\frac 1{\sqrt{\lm}} B_{\lm t} \sim \frac 1{\sqrt{\lm}} \sN (0,\lm t) \sim \sN (0, t)
\ee
we have the increments of $\bb{\lm^{-1/2} B_{\lm t}}_{ t \geq 0}$ are independent Gaussian. Thus, $(\lm^{-1/2} B_{\lm t})_{t \geq 0}$ is a standard Brownian motion.

\item [(iii)] At any time $t>0$, we have
\be
tB_{1/t} \sim t\sN\bb{0,\frac 1t} \sim \sN\bb{0\cdot t, t^2 \frac 1t} = \sN(0,t) \sim B_t.
\ee

Thus, the only thing left to prove is that $B_0 = 0$ a.s.. Consider the event $\wt{A} = \bra{\omega :\wt{B}_t(\omega) \to 0\text{ as } t\to 0}$ is %($\forall \ve >0$, $\exists \delta > 0$ such that $t< \delta$, then $\wt{B}_t(\omega) < \ve$)
\be
\wt{A} = \bigcap_n \bigcup_m \bigcap_{q\in \Q\cap (0,1/m]} \bra{\abs{\wt{B}_q} \leq \frac 1n}.
\ee
since $\wt{B}$ is certainly continuous on $(0,\infty)$. But the processes $(B_t)_{t\geq 0}$ and $(\wt{B}_t)_{t\geq 0}$ have the same distribution (they are Gaussian processes with the same covariance), so
\be
\pro\bb{\wt{A}} = \pro\bb{{A}} = 1
\ee
where $A$ is the event $\bigcap_n \bigcup_m \bigcap_{q\in \Q\cap (0,1/m]} \bra{\abs{B_q} \leq \frac 1n}$ that $B\to 0$ at 0, a.s., which, by definition of $B$ ($B_0 = 0$ a.s.).%are continuous
\een
\end{proof}

\begin{corollary}
Let $B$ be a standard Brownian motion, then $\lim_{t\to \infty} \frac{B_t}{t} = 0$ a.s.
\end{corollary}

\begin{remark}
Of course one can show the above result directly using the strong law of large numbers, i.e., $\lim_{n\to \infty} B_n/n = 0$. The one needs to show that $B$ does not oscillate too much between $n$ and $n+1$.\footnote{see example sheet, need details}
\end{remark}

\begin{proof}[\bf Proof]
Recalling Proposition \ref{pro:brownian_motion_basic_properties}.(iii), suppose that $B_t = t\wt{B}_{1/t}$ where $\wt{B}$ is another standard Brownian motion. Thus,
\be
\lim_{t\to\infty} \frac {B_t}t = \lim_{t\to \infty} \wt{B}_{1/t} = \wt{B}_0 = 0 \text{ a.s.}.
\ee
\end{proof}

\begin{lemma}\label{lem:standard_brownian_motion_covariance}
Let $B$ be a standard Brownian motion and $s, t \geq 0$. Then $\cov(B_s,B_t) = s \land t$.
\end{lemma}

\begin{proof}[\bf Proof]
Wlog, we assume $t\geq s$, by independent Gaussian increment of Brownian motion, we have
\be
\cov \bb{B_s,B_t} = \E\bb{B_sB_t} - \E B_s \E B_t = \E\bb{B_s(B_t - B_s + B_s)} - 0 = \E\bb{B_s(B_t-B_s)} + \E\bb B_s^2 = 0 + s = s.
\ee

Thus, we have $\cov(B_s,B_t) = \min\bra{s,t} = s \land t$.
\end{proof}

Recall Theorem \ref{thm:local_martingale_indistinguishable_0}, we have
\begin{proposition}\label{pro:brownian_motion_no_finite_variation}
Brownian motion is not of finite variation.
\end{proposition}

\subsection{Markov property}

We now start to discuss ideas revolving around the Markov property of Brownian motion and its applications to path properties. We begin with the simple Markov property, which takes a particularly nice form in this context. 


\begin{theorem}[simple Markov property]\label{thm:simple_markov_property_brownian_motion}
Let $(B_t, t \geq 0)$ be a Brownian motion, and let $s > 0$. Then
\be
\bb{\wt{B}_t := B_{t+s} - B_s}_{t \geq 0}
\ee
is a Brownian motion, independent of the $\sigma$-field $\sF^B_{s^+} = \bigcap_{t>s} \sF^B_t$.
\end{theorem}

\begin{remark}
In $\sF_s^+$, we allow an additional infinitesimal glance into the future.
\end{remark}

\begin{proof}[\bf Proof]
Since $\wt{B}$ is continuous and $\wt{B}_0 = 0$, to show that $\wt{B}$ is a Brownian motion it suffices to check that the increments have the correct distribution. However if $t \geq u$, $\wt{B}_t - \wt{B}_u = B_{s+t} - B_{s+u}$ so this follows directly from the fact that $B$ itself is a Brownian motion. It remains to show that $\wt{B}$ is independent from $\sF_{s^+}$. 

We start by checking independence with respect to $\sF_s$, for which we can assume $d = 1$. 
%Now, to prove independence of $\wt{B}$ with repsect to $\sF_s$,
It suffices to check that the finite dimensional marginals are independent. i.e., if $s_1 \leq\dots s_m \leq s$ and $t_1 \leq \dots t_n$, we want to show that
\be
(B_{s_1} ,\dots,B_{s_m}) \text{ and }(\wt{B}_{t_1} ,\dots, \wt{B}_{t_n})
\ee
are independent. However, the $m+n$-coordinate vector $(B_{s_1} ,\dots,B_{s_m}, \wt{B}_{t_1} ,\dots, \wt{B}_{t_n})$ is a Gaussian vector (since it is the image by a linear combination of Gaussian random variables), and it suffices to check that the covariance of two distinct terms is 0. Since each term has zero expectation, we have (by Lemma \ref{lem:standard_brownian_motion_covariance})
\be
\cov(\wt{B}_{t_i} ,B_{s_j}) = \E(\wt{B}_{t_i}B_{s_j}) = \E(B_{s+t_i}B_{s_j}) - \E(B_sB_{s_j}) = s_j \land (s + t_i) - (s_j \land s) = s_j - s_j = 0
\ee
which proves the independence with respect to $\sF_s$. 

To show that $\wt{B}$ is independent from $\sF_{s^+}$, we wish to show that for every function $f:(\R^d)^k \to \R$ continuous and bounded and $A \in \sF_{s^+}$, 
\be
\E\bb{\ind_A f(\wt{B}_{t_1} ,\dots, \wt{B}_{t_k})} = \pro(A)\E\bb{f(\wt{B}_{t_1} ,\dots, \wt{B}_{t_k})}.%\footnote{need explain}.
\ee

Now, for any $\ve > 0$, $A \in \sF_{s^+} \subseteq \sF_{s+\ve}$, thus, using the property just proved ($B_{t_i+s+\ve}-B_{s+\ve}$ are independent of $\sF_{s+\ve}$)
\be
\E(\ind_Af(B_{t_1+s+\ve}-B_{s+\ve},\dots,B_{t_k+s+\ve}-B_{s+\ve})) =\pro(A)\E(f(B_{t_1+s+\ve}-B_{s+\ve},\dots,B_{t_k+s+\ve}-B_{s+\ve}))
\ee

Letting $\ve \to 0$ in the above identity, since $B$ is continuous and $f$ is bounded and continuous, then by Proposition \ref{pro:sigma_algebra_random_variable_independence},
\be
f(B_{t_1+s+\ve}-B_{s+\ve},\dots,B_{t_k+s+\ve}-B_{s+\ve}) \to f(B_{t_1+s}-B_{s},\dots,B_{t_k+s}-B_{s})
\ee

we have (by bounded convergence theorem, Theorem \ref{thm:bounded_convergence_probability}),
\be
\E\bb{\ind_Af(\wt{B}_{t_1} ,\dots, \wt{B}_{t_k})} = \pro(A)\E\bb{f(\wt{B}_{t_1} ,\dots, \wt{B}_{t_k})}.
\ee

Hence, we can say that $\bra{\wt{B}_{t_1},\dots,\wt{B}_{t_k}}$ is independent of the $\sigma$-field $\sF^B_{s^+}$ by Proposition \ref{pro:sigma_algebra_random_variable_independence}. Thus, $\wt{B}$ is independent of $\sF^B_{s^+}$ by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}.%Theorem \ref{thm:uniqueness_of_extension_measure} and 
\end{proof}

\begin{theorem}[Blumenthal's 0-1 law\index{Blumenthal's 0-1 law}]\label{thm:blumenthal_zero_one_law}
Let $B$ be a standard Brownian motion. The $\sigma$-algebra $\sF^B_{0^+} = \bigcap_{\ve>0} \sF^B_\ve$ is trivial, for all
$A \in \sF^B_{0^+}$, $\pro(A) \in \bra{0, 1}$. % i.e. constituted of events of probability 0 or 1.
\end{theorem}

%\begin{proof}[\bf Proof]
%By the previous result, $(B_t, t \geq 0)$ is independent from $\sF_{0^+}$. However $\sF^B_\infty$ contains $\sF^B_{0^+}$, so this implies that the $\sigma$-field $\sF_{0^+}$ is independent of itself, and $\pro(A) = \pro(A \cap A) = \pro(A)^2$ by independence. Thus $\pro(A)$ is solution to the equation $x = x^2$ whose roots are precisely 0 and 1.
%\end{proof}

%\begin{proposition}[Blumenthal's 0-1 law]
%Let $(B_t , t \geq 0)$ be a standard Brownian motion and let $(\sF^B_t)$ be its natural filtration, and $\sF^B_{0^+} = \bigcap_{t>0} \sF^B_t$. Then for all $A \in \sF^B_{0^+}$, $\pro(A) \in \{0, 1\}$.
%\end{proposition}

\begin{proof}[\bf Proof]
Let $A \in \sF^B_{0^+}$, then $A$ is independent of $\sigma(B_{t_i} , 1 \leq i \leq k)$, for all $t_1, \dots , t_k > 0$ %.so for all $\ve > 0$, $A \in \sF^B_\ve$. Let $f$ be a continuous bounded function $(\R^d)^k \to \R$ and $t_1, \dots, t_k > 0$. Then 
by simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion}). 
%\beast
%\E\bb{\ind_A f(B_{t_1} , \dots, B_{t_k})} & = & \lim_{\ve\to 0}\E\bb{\ind_Af(B^{(\ve)}_{t_1-\ve}, \dots,B^{(\ve)}_{t_k-\ve})} \quad\text{by DCT}\\
%& = & \lim_{\ve\to 0} \pro(A)\E\bb{f(B^{(\ve)}_{t_1-\ve}, \dots,B^{(\ve)}_{t_k-\ve})} \quad\text{SM}\\
%& = & \pro(A)\E\bb{f(B_{t_1},\dots, B_{t_k})}
%\eeast
%\be
%\E\bb{\ind_A f(B_{t_1} , \dots, B_{t_k})} = \pro(A)\E\bb{f(B_{t_1},\dots, B_{t_k})}
%\ee
Therefore $A$ is independent of $\sigma(B_s, s \geq 0) = \sF^B_\infty \supseteq \sF^B_{0^+}$, so $\sF^B_{0^+}$ is independent of itself. Thus for any $A \in \sF^B_{0^+}$, $\pro(A) = \pro(A\cap A) = \pro(A)^2$, so $\pro(A) \in \{0, 1\}$.
\end{proof}

\begin{proposition}
Suppose that $(B_t)_{t\geq 0}$ is a standard Brownian motion in 1 dimension. Definie 
\be
\tau = \inf\bra{t>0:B_t >0},\quad \quad \sigma = \inf\bra{t>0: B_t = 0}.
\ee

Then $\pro\bb{\tau = 0} = \pro\bb{\sigma = 0} = 1$.
\end{proposition}

\begin{proof}[\bf Proof]
For all $n$ we have
\be
\bra{\tau = 0} = \bigcap_{k\geq n} \bra{\exists 0< \ve < 1/k: B_\ve >0}
\ee
and thus $\bra{\tau = 0} \in \sF^B_{1/n}$ for all $n$ and hence $\bra{\sF^+_0}$. Therefore, $\pro\bb{\tau = 0} \in \bra{0,1}$ by Blumental 0-1 law (Theorem \ref{thm:blumenthal_zero_one_law}). It remains to show that it has positive probability. Clearly, for all $t>0$ we have
\be
\pro\bb{\tau > t} \geq \pro\bb{\sup_{0\leq s\leq t}B_s \leq 0} \leq \pro\bb{B_t \leq 0} \ \ra \ \pro\bb{\tau \leq t} \geq \pro\bb{B_t >0} = \frac 12.
\ee

Hence by letting $t\da 0$ we get that $\pro\bb{\tau = 0} \geq 1/2$ and this finishes the proof.

In exactly the same way we get that (by symmetry)
\be
\inf\bra{t>0: B_t < 0} = 0 \text{ a.s.}.
\ee

Since $B$ is a (sample) continuous function, by the intermediate value theorem (Theorem \ref{thm:intermediate_value}), we have that 
\be
\pro\bb{\sigma = 0} = 1.
\ee
\end{proof}


\begin{proposition}\label{pro:brownian_motion_limit_value}
Let $(B_t)_{t \geq 0}$ be standard Brownian motion in dimension $d = 1$. Let $S_t = \sup_{0\leq s \leq t} B_s$ and $I_t = \inf_{0\leq s\leq t} B_s$. Then
\ben
\item [(i)] a.s. for all $t > 0$, $S_t > 0$ and $I_t < 0$;
\item [(ii)] a.s. $S_\infty = \sup_{t\geq 0} B_t = +\infty$ and $I_\infty = \inf_{t\geq 0} B_t = -\infty$;
\item [(iii)] In dimensions $d \geq 2$, let $C$ be an open cone with origin at 0 and non-empty interior. Let $H_C = \inf\{t > 0: B_t \in C\}$. Then a.s. $H_C = 0$.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Let $t_k$ be a positive decreasing sequence with $t_k \to 0$ as $k \to\infty$. Let $A_k = \{B_{t_k} > 0\}$, so $\limsup_{k} A_k = \{B_{t_k} > 0 \text{ i.o.}\}\in \sF^B_t$ for any $t > 0$. Therefore it is in $\sF^B_{0^+}$. By Fatou's lemma (Lemma \ref{lem:fatou_set}),
\be
\pro\bb{\limsup_k A_k} \geq \limsup_k \pro(A_k) = \limsup_k \pro(B_{t_k} > 0) = \frac 12.
\ee
%On the other hand, 

By Blumenthal's 0-1 law (Theorem \ref{thm:blumenthal_zero_one_law}), $\pro(\limsup_k A_k) \in \{0, 1\}$, so it must be one. But 
\be
\limsup_k A_k = \limsup_k \bra{B_{t_k} > 0} = \bigcap_n \bigcup_{m\geq n}\bra{B_{t_m} >0} \subseteq \bigcap_n \bra{S_{t_n} >0} % \bigcup_n \bra{B_{t_n} >0} \subseteq \bigcup_n \bra{\sup_{0\leq s\leq t_n}B_s >0}
\ee

Thus, $\pro\bb{B_t > 0} = 1$ for any $t >0$. %$\limsup_k A_k \subseteq \{S_t > 0 | \forall t > 0\}$ we have the result. 
$(-B_t)_{t \geq 0}$ is a Brownian motion, so the result for It follows.

\item [(ii)] By scaling invariance of Brownian motion (Proposition \ref{pro:brownian_motion_basic_properties}.(ii)) we get that for any $\lm > 0$,
\be
S_\infty = \sup_{t\geq 0} B_t = \sup_{t\geq 0} B_{\lm t} \stackrel{d}{=} \sup_{t\geq 0} \sqrt{\lm }B_t = \sqrt{\lm} S_\infty.
\ee

%Therefore $S_\infty \stackrel{(d)}{=} \sqrt{\lm} \sup_{t\geq 0} B_t = \sqrt{\lm} S_\infty$ for all $\lm > 0$, so $\pro(S_\infty \in (\ve, \ve^{-1})) = \pro(\lm S_\infty \in (\ve, \ve^{-1})) \to  0$ as $\lm \to\infty$. Therefore $S_\infty \in \{0,\infty\}$. But $S_\infty \geq S_1 > 0$ a.s. by (i). Same for $I_t$.

Thus for all $x>0$ the probability $\pro(S_\infty \geq x)$ is a constant $c$, and hence $\pro\bb{S_\infty \geq 0} =c$. But we have already showed that $\pro\bb{S_\infty \geq 0} = 1$ as $B_0 = 0$ a.s.. Therefore, for all $x >0$ we have
\be
\pro\bb{S_\infty \geq x} = 1 \ \ra \ \pro\bb{S_\infty = \infty} =1.
\ee

The same argument for $I_\infty$ gives the required result.


\item [(iii)] Since the cone $C$ is invariant under multiplication by a positive scalar, by the scaling invariance property of Brownian motion we get that for any $t$, $\pro\bb{B_t\in C} = \pro(B_1\in C)$. Since $C$ has non-empty interior, we can have that $\pro\bb{B_1\in C} >0$ and thus $\pro(B_t \in C) >0$ for any $t$.

%Copy the proof of (i), 

Let $t_k$ be a positive decreasing sequence with $t_k \to 0$ as $k \to\infty$. Let $A_k = \{B_{t_k} \in C\}$, so $\limsup_{k} A_k = \{B_{t_k} \in C \text{ i.o.}\}\in \sF^B_t$ for any $t > 0$. Therefore it is in $\sF^B_{0^+}$. By Fatou's lemma (Lemma \ref{lem:fatou_set}),
\be
\pro\bb{\limsup_k A_k} \geq \limsup_k \pro(A_k) = \limsup_k \pro(B_{t_k} \in C) > 0.%= \frac 12.
\ee
%On the other hand, 

By Blumenthal's 0-1 law (Theorem \ref{thm:blumenthal_zero_one_law}), $\pro(\limsup_k A_k) \in \{0, 1\}$, so it must be one. Thus, 
\be
\pro\bb{B_{t_k} \in C \text{ i.o.}} = 1 \ \ra \ \pro\bb{H_C = 0} = 1.
\ee
%But 
%\be
%\limsup_k A_k = \limsup_k \bra{B_{t_k} \in C} = \bigcap_n \bigcup_{m\geq n}\bra{B_{t_m} \in C} \subseteq \bigcap_n \bra{S_{t_n} >0} % \bigcup_n \bra{B_{t_n} >0} \subseteq \bigcup_n \bra{\sup_{0\leq s\leq t_n}B_s >0}
%\ee
%Thus, $\pro\bb{B_t > 0} = 1$ for any $t >0$. %$\limsup_k A_k \subseteq \{S_t > 0 | \forall t > 0\}$ we have the result. $(-B_t)_{t \geq 0}$ is a Brownian motion, so the result for It follows.
%and note that since $\lm C = C$ for all $\lm > 0$, $\pro(A_k) = \pro(\sN (0, I_d )\in C) > 0$ since $C$ has non-empty interior (and the density is everywhere positive?).
\een
\end{proof}

We now want to prove an important analog of the simple Markov property, where deterministic times are replaced by stopping times. To begin with, we extend a little the definition of Brownian motion, by allowing it to start from a random location, and by working with filtrations that are (slightly) larger than the natural filtration of a standard Brownian motion.

\begin{definition}
Let $(\sF_t)_{t \geq 0}$ be a filtration. We say that a Brownian motion $B$ is an $(\sF_t)$-Brownian motion if $B$ is adapted to $(\sF_t)_{t \geq 0}$, and if $B^{(t)} = (B_{t+s}-B_t)_{s \geq 0}$ is independent of $\sF_t$ for every $t \geq 0$.
\end{definition}

\begin{remark}
For instance, if $(\sF_t)$ is the natural filtration of a 2-dimensional Brownian motion $(B^1_t ,B^2_t)_{t \geq 0}$, then $(B^1_t)_{t \geq 0}$ is an $(\sF_t)$-Brownian motion. If $B'$ is a standard Brownian motion and $X$ is a random variable independent of $B'$, then $B = (X + B'_t)_{t \geq 0}$ is a Brownian motion (started at $B'_0 = X$), and is an $(\sF^B_t) = (\sigma(X) \lor \sF^{B'}_t)$-Brownian motion. A Brownian motion is always an $\sF^B_t)$-Brownian motion. If $B$ is a standard Brownian motion, then the completed filtration $\sF_t = \sF^B_t \lor \sN$ ($\sN$ being the set of events of probability 0) can be shown to be right-continuous, i.e. $\sF_{t^+} = \sF_t$ for every $t \geq 0$, and $B$ is an $(\sF_t)$-Brownian motion. 
\end{remark}


\begin{theorem}[Strong Markov property]\label{thm:strong_markov_property_brownian_motion}
Let $(B_t)_{t \geq 0}$ be an $(\sF_t)$-Brownian motion in $\R^d$ and $T$ be an $(\sF_t)$-stopping time. We let 
\be
B^{(T)} = \left\{\ba{ll}
(B_{T+t} - B_T)_{t \geq 0}\quad\quad & T < \infty \\
0& \text{otherwise}
\ea\right.
\ee
%$B^{(T)}_t = B_{T+t} - B_T$ for every $t \geq 0$ on the event $\{T < \infty\}$, and 0 otherwise. 

Conditionally on $\{T < \infty\}$, the process $B^{(T)}$ is a standard Brownian motion, which is independent of $\sF_{T^+}$. Otherwise said, conditionally given $\sF_T$ and $\{T < \infty\}$, the process $(B_{T+t})_{t \geq 0}$ is an $(\sF_{T+t})$-Brownian motion started at $B_T$.
\end{theorem}

%\begin{theorem}[Strong Markov Property]
%Let ($B_t$) be an ($\sF_t$)-Brownian motion and let $T$ be an ($\sF_t$)-stopping time. Then, conditionally on $\{T < \infty\}$, the process 
%is a standard Brownian motion which is independent of $\sF_T$.
%Equivalently, given $\sF_T$ , the process $(B_{t+T})_{t \geq 0}$ is an $(\sF_{T+t})_{t \geq 0}$-Brownian motion.
%\end{theorem}


\begin{proof}[\bf Proof]
Suppose first that $T < \infty$ a.s. We will first prove for the stopping times $T_n = 2^{-n}\ceil{2^nT}$ that discretely approximate $T$ from above. We write $B^{(k)}_t = B_{t+k2^{-n}} - B_{k2^{-n}}$ which is a Brownian motion and $B^*$ for the process defined by
\be
B^*_t = B_{t+T_n} - B_{T_n}.
\ee

We will first show that $B^*$ is a Brownian motion independent of $\sF_{T_n^+}$. Let $A \in \sF_{T_n^+}$ and $0\leq t_1 \leq \dots \leq t_m$. Then for any bounded continuous function $f$, %For every event $\bra{B^* \in D}$, we have
\beast
\E\bb{\ind_A f\bb{B^*_{t_1},\dots,B^*_{t_m}}} & = & \E\bb{\sum^\infty_{k=0}\ind_A f\bb{B^*_{t_1},\dots,B^*_{t_m}}\ind_{\bra{T_n = k2^{-n}}}} = \E\bb{\sum^\infty_{k=0}\ind_A f\bb{B^{(k)}_{t_1},\dots,B^{(k)}_{t_m}}\ind_{\bra{T_n = k2^{-n}}}}\\
& = & \sum^\infty_{k=0} \E\bb{\ind_{A \cap \bra{T_n = k2^{-n}}} f\bb{B^{(k)}_{t_1},\dots,B^{(k)}_{t_m}}}\quad\quad (\text{by Fubini theorem (Theorem \ref{thm:fubini})})\\
& = & \sum^\infty_{k=0} \pro\bb{A \cap \bra{T_n = k2^{-n}}} \E\bb{f\bb{B^{(k)}_{t_1},\dots,B^{(k)}_{t_m}}}\quad\quad (\text{by Proposition \ref{pro:sigma_algebra_random_variable_independence}})\\
& = & \pro\bb{A} \E\bb{f\bb{B^{(k)}_{t_1},\dots,B^{(k)}_{t_m}}}.
\eeast

By simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion}), $B^{(k)}$ is Brownian motion. Thus, for standard Brownian motion $B$, $B^{(k)}\stackrel{d}{=}B$. Let $A$ be the whole space $\Omega$. Then we have
\be
\E\bb{f\bb{B^*_{t_1},\dots,B^*_{t_m}}} = \E\bb{f\bb{B^{(k)}_{t_1},\dots,B^{(k)}_{t_m}}}.
\ee

Thus, letting $f(x)$ be $\ind_{x\in D}$, we have that $B^*$ and $B$ has the same finite dimensional marginal distributions. Thus, we have
\be
\E\bb{\ind_A f\bb{B^*_{t_1},\dots,B^*_{t_m}}} = \pro\bb{A} \E\bb{f\bb{B^{*}_{t_1},\dots,B^{*}_{t_m}}}.
\ee

Then by Proposition \ref{pro:sigma_algebra_random_variable_independence}, $\bra{B^*_{t_1},\dots,B^*_{t_m}}$ is independent of $\sF_{T_n^+}$. (This can also be proved by checking $\pro\bb{\bra{B^* \in D}\cap A} = \pro(A)\pro\bb{B^* \in D}$ where $D\in \sW$.)

By the continuity of Brownian motion we get that 
\be
B_{t+s+T} - B_{s+T} = \lim_{n\to \infty} \bb{B_{s+t+T_n} - B_{s+T_n}}.
\ee

The increments $(B_{t+s+T_n} - B_{s+T_n})$ are normally distributed with 0 mean variance equal to $t$. Thus for any $s \geq 0$ the increments $B_{t+s+T} - B_{s+T}$ are also normally distributed with 0 mean and variance $t$. As the process $(B_{t+T} - B_T)_{t \geq 0}$ is a.s. continuous, it is a Brownian motion. It only remains to show that it is independent of $\sF_{T^+}$.

Let $A \in \sF_{T^+}$ and $0\leq t_1\leq \dots \leq t_m$. Also, since $T_n > T$, it follows that $A \in \sF_{T_n^+}$. Thus, for any bounded continuous function $f : (\R^d)^m \to \R$,
%\beast
%\E\bb{\ind_A f\bb{B^*_{t_1},\dots, B^*_{t_m}}} & = & \E\bb{\ind_A f\bb{B_{t_1+T_n}-B_{T_n},\dots, B_{t_m+T_n}-B_{T_n}}}= \E\bb{\ind_A f\bb{B^{(T)}_{t_1},\dots, B^{(T)}_{t_m}}} \\
%& = & \pro(A)\E\bb{f\bb{B^{(T)}_{t_1},\dots, B^{(T)}_{t_m}}}  \\
%& = & \pro\bb{A}\E\bb{f\bb{B_{t_1+T}-B_T,\dots, B_{t_m+T}-B_T}}
%\eeast
\be
\E\bb{\ind_A f\bb{B^*_{t_1},\dots, B^*_{t_m}}} = \pro\bb{A} \E\bb{f\bb{B^{*}_{t_1},\dots,B^{*}_{t_m}}}.
\ee

But we know that since $f$ and $B$ are continuous, as $n\to \infty$, $T_n \to T$,
\beast
f\bb{B^{(T)}_{t_1},\dots, B^{(T)}_{t_m}} = f\bb{B_{t_1+T}-B_T,\dots, B_{t_m+T}-B_T} = \lim_{n\to \infty} f\bb{B_{t_1+T_n}-B_{T_n},\dots, B_{t_m+T_n}-B_{T_n}} = \lim_{n\to \infty} f\bb{B^*_{t_1},\dots, B^*_{t_m}}
\eeast
and $\ind_A f\bb{B^{(T)}_{t_1},\dots, B^{(T)}_{t_m}} = \lim_{n\to \infty} \ind_A f\bb{B^*_{t_1},\dots, B^*_{t_m}}$. Then by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability}),
\beast
\E\bb{\ind_A f\bb{B^{(T)}_{t_1},\dots, B^{(T)}_{t_m}}} = \lim_{n\to \infty}\E\bb{\ind_A f\bb{B^*_{t_1},\dots, B^*_{t_m}}} = \pro\bb{A} \lim_{n\to \infty}  \E\bb{f\bb{B^{*}_{t_1},\dots,B^{*}_{t_m}}} = \pro\bb{A} \E\bb{f\bb{B^{(T)}_{t_1},\dots,B^{(T)}_{t_m}}}.
\eeast

Thus, we have that $B$ is independent of $\sF_{T^+}$ by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}.%Theorem \ref{thm:uniqueness_of_extension_measure} and 

Finally, if $\pro(T = \infty) > 0$, check that the above is true when replacing $A$ by $A \cap \bra{T < \infty}$, and divide by $\pro\bb{T < \infty}$.
\end{proof}

%We will show that for any function $f : (\R^d)^m \to \R$ continuous and bounded we have
%\be
%E[1(A)F((Bt1+T - BT ; : : : ;Btk+T - BT ))] = P(A)E[F((Bt1+T - BT ; : : : ;Btk+T - BT ))]:
%\ee
%Using the continuity again and the dominated convergence theorem, we get that
%\be
%E[1(A)F((Bt1+T-BT ; : : : ;Btk+T-BT ))] = lim_{n\to \infty}E[1(A)F((Bt1+Tn-BTn; : : : ;Btk+Tn-BTn))]:
%\ee
%But we already showed that the process $(B_{t+T_n} - B_{T_n})_{t\geq 0}$ is independent of $\sF_{T_n^+}$, hence using the continuity and dominated convergence one more time gives the claimed independence.

%\be
%\pro\bb{B^*_{t_1},\dots,B^*_{t_m}}
%\ee

%\be
%\pro\bb{\bra{B^* \in D} \cap A} =  \pro\bb{\sum^\infty_{k=0} \bra{B^* \in D} \cap A \cap \bra{T_n = k2^{-n}}} = \sum^\infty_{k=0}  \pro\bb{\bra{B^* \in D} \cap A \cap \bra{T_n = k2^{-n}}}
%\ee
%= 1 k=0 P(B(k) 2 A)P(E \ fTn = k2-ng); 57

%\qcutline

%Let $A \in \sF_T$, and consider times $t_1 < t_2 < \dots < t_p$. We want to show that for every bounded continuous function $f$ on $(\R^d)^p$,
%\be
%\E\bb{\ind_A f(B^{(T)}_{t_1} ,\dots,B^{(T)}_{t_p})} = \pro(A)\E\bb{f(B_{t_1} ,\dots,B_{t_p})}.\quad\quad (*)
%\ee

%Indeed, taking $A = \Omega$ entails that $B^{(T)}$ is a Brownian motion, while letting A vary in $\sF_T$ entails the %independence of $(B^{(T)}_{t_1} ,\dots,B^{(T)}_{t_k})$ and $\sF_T$ for every $t_1,\dots,t_k$, hence of $B^{(T)}$ and $\sF_T$.
%\beast
%\E\bb{\ind_Af(B^{(T)}_{t_1} ,\dots,B^{(T)}_{t_p})} & = & \lim_{n\to \infty} \sum^\infty_{k=1} \E\bb{\ind_{A\cap \{(k-1)2^{-n}<T\leq k2^{-n}\}}]f(B^{(k2^{-n})}_{t_1} ,\dots,B^{(k2^{-n})}_{t_p})} \\
%& = & \lim_{n\to \infty} \sum^\infty_{k=1} \pro\bb{A \cap \{(k - 1)2^{-n} < T \leq k2^{-n}\}}\E\bb{f(B_{t_1} ,\dots,B_{t_p})}\\
%& = & \pro(A)\E\bb{f(B_{t_1} ,\dots,B_{t_p})}.
%\eeast
%where we used the simple Markov property and the fact that $A\cap \{(k-1)2^{-n} < T \leq k2^{-n}s \} \in \sF_{k2^{-n}}$ by definition. 




%\begin{proof}[\bf Proof]
%Let $T$ be such that $\pro(T < \infty) = 1$. Let $A \in \sF_T$ and $0 \leq t_1 \leq \dots \leq t_k$ be fixed times. Let $F : (\R^d )^k \to \R$ be a bounded continuous function. Assume first that $T$ takes values in $D$, the dyadic rationals. Then
%\beast
%\E[\ind_AF(B^{(T)}_{t_1},\dots, B^{(T)}_{t_k})] & = & \sum_{d\in D} \E[\ind_{A\cap\{T=d\}}F(B^{(T)}_{t_1}, \dots, B^{(T)}_{t_k})]\\
%& = & \sum_{d\in D} \pro(A, T = d)\E[F(B_{t_1},\dots, B_{t_k})] \quad\quad \text{SM}\\
%& = & \pro(A)\E[F(B_{t_1} ,\dots, B_{t_k})]
%\eeast

%In general, let $T_n = 2^{-n}\ceil{2^nT}$, so that $T_n \da T$. $\sF_T \subseteq \sF_{T_n}$ since $T \leq T_n$, so $A \in \sF_{T_n}$, and by the previous argument,
%\be
%\E[\ind_AF(B^{(Tn)}_{t_1}, \dots, B^{(T_n)}_{t_k})] = \pro(A)\E[F(B_{t_1},\dots, B_{t_k})].
%\ee

%Since $F$ is continuous and bounded and Brownian motion is continuous, by the DCT the left hand side goes to $\E[\ind_AF(B^{(T)}_{t_1}, \dots, B^{(T)}_{t_k})]$ as $n\to \infty$. For $A = \Omega$, we obtain that $B^{(T)}$ is a standard Brownian motion. By letting $A$ vary in $\sF_T$ , we obtain that $\sF_T$ is independent of $\sigma(B^{(T)}_{t_1}, \dots, B^{(T)}_{t_k} )$, and is therefore independent of $B^{(T)}$.

%Finally, if $\pro(T <\infty) < 1$ then use the same argument with $A\cap \{T < \infty\}$ instead of $A$, and divide both sides of the above equation by $\pro(T <\infty)$.
%\end{proof}

\begin{example}
Let $\tau= \inf\bra{t \geq 0 : B_t = \max_{0\leq s\leq 1} B_s}$. It is intuitively clear that $\tau$ is not a stopping time. To prove that, first show that $\tau < 1$ a.s. The increment $B_{t+\tau} -B_\tau$ is negative in a small neighbourhood of 0, which contradicts the strong Markov property.
\end{example}


An important example of application of the strong Markov property is the so-called reflection principle. %Recall that $S_t = \sup_{0\leq s\leq t} B_s$.

\subsection{Reflection principle}

\begin{theorem}[reflection principle\index{reflection principle}]\label{thm:reflection_principle_brownian_motion}
Let $T$ be an a.s. finite stopping time and $(B_t)_{t \geq 0}$ a standard Brownian motion. Then the process $(\wt{B}_t)_{t \geq 0}$ defined by
\be
\wt{B}_t = B_t\ind_{\bra{t \leq T}} + (2B_T - B_t)\ind_{\bra{t > T}}
\ee
is also a standard Brownian motion and we call it Brownian motion reflected at $T$.
\end{theorem}

\begin{proof}[\bf Proof]
By the strong Markov property (Theorem \ref{thm:strong_markov_property_brownian_motion}), the process $B^{(T)} = (B_{T+t} - B_T)_{t \geq 0}$ is a standard Brownian motion independent of $(B_t)_{0 \leq t \leq T}$. Also the process $-B^{(T)} = (B_T - B_{t+T})_{t \geq 0}$ is a standard Brownian motion independent of $(B_t)_{0 \leq t \leq T}$. Therefore, the pair $((B_t)_{0 \leq t \leq T},B^{(T)})$ has the same law as $((B_t)_{0 \leq t \leq T},-B^{(T)})$.

We now define the concatenation operation\index{concatenation operation} at time $T$ between two continuous paths $X$ and $Y$ by
\be
\Psi(X, Y)(t) = X_t\ind_{\bra{t \leq T}} + (X_T + Y_{t-T})\ind_{\bra{t > T}}.
\ee

Applying $\Psi_T$ to $B$ and $B^{(T)}$ gives us the Brownian motion $B$, while applying it to $B$ and $-B^{(T)}$ gives us the process $\wt{B}$.

Let $\sA$ be the product $\sigma$-algebra on $C[0,\infty)$, the space of continuous functions on $[0,\infty)$. It is easy
to see that $\Psi_T$ is a measurable mapping from $(C[0,\infty) \times C[0,\infty), \sA \otimes \sA)$ to $(C[0,\infty),\sA)$ (by approximating $T$ by discrete stopping times).

Hence, $B$ and $\wt{B}$ have the same law.
\end{proof}


\begin{theorem}[Reflection principle]\label{thm:reflection_principle_brownian_motion_upper_bound}
Let $(B_t)_{t \geq 0}$ be a standard Brownian motion and $a > 0$ and $b \leq a$, then for every $t \geq 0$, 
\be
\pro(S_t \geq a,B_t \leq b) = \pro(B_t \geq 2a - b)\quad\quad \text{where $S_t = \sup_{0\leq s\leq t} B_s$.}
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Let $T_a = \inf\{t \geq 0: B_t \geq a\}$ be the entry time\footnote{need definition} of $B_t$ in $[a,\infty)$ for $a > 0$. Then $T_a$ is an $(\sF^B_t)$-stopping time for every $a$ and $T_a < \infty$ a.s. since $S_\infty = \sup_{t\geq 0}B_t = \infty$ a.s. where $S_\infty = \lim_{t\to \infty} S_t$ (Note that $\bra{S_t \geq a} = \bra{T_a \leq t}$). 

Now by continuity of $B$, $B_{T_a} = a$ for every $a$. We thus have $B_t - B_{T_a} = B^{(T_a)}_{t-T_a}$ and thus
\beast
\pro(S_t \geq a,B_t \leq b) & = & \pro(T_a \leq t,B_t \leq b) = \pro(T_a \leq t,B_t - B_{T_a} \leq b - a)\\
& = & \pro\bb{T_a \leq t,B^{(T_a)}_{t-T_a} \leq b - a} = \pro\bb{T_a \leq t,-B^{(T_a)}_{t-T_a} \geq a - b}.
\eeast


%= \pro\bb{T_a \leq t, 2a - B_t \geq 2a -b} = \pro\bb{T_a \leq t, 2B_{T_a} - B_t \geq 2a -b}.

By the strong Markov property (Theorem \ref{thm:strong_markov_property_brownian_motion}) at time $T_a$, $B^{T_a}$ is a Brownian motion independent of $\sF_{T_a}$ and thus of $T_a$. In particular, we deduce that the joint law of $(T_a,B^{(T_a)})$ is identical to the joint law of $(T_a,-B^{(T_a)})$, by symmetry of Brownian motion. It follows that 
\beast
\pro(S_t \geq a,B_t \leq b) & = & \pro\bb{T_a \leq t,-B^{(T_a)}_{t-T_a} \geq a - b} = \pro\bb{T_a \leq t,B^{(T_a)}_{t-T_a} \geq a - b} \\
& = & \pro\bb{T_a \leq t,B_{T_a} + B^{(T_a)}_{t-T_a} \geq 2a - b} = \pro(T_a \leq t,B_t \geq 2a - b) = \pro(B_t \geq 2a - b)
\eeast
since $\bra{B_t \geq 2a-b}$ implies that $\bra{S_t \geq a} =\bra{T_a\leq t}$.
\end{proof}

\begin{remark}
We can prove the above by Theorem \ref{thm:reflection_principle_brownian_motion}.
\end{remark}

\begin{corollary}\label{cor:abs_sup_equal_brownian_motion}
Let $B$ be a standard Brownian motion and $S_t = \sup_{0\leq s\leq t}B_s$. We have the following identities in distribution. for all $t \geq 0$, 
\be
S_t \sim \abs{B_t} \sim \abs{\sN (0, t)}.%\stackrel{d}{=}
\ee
\end{corollary}

\begin{proof}[\bf Proof]
We write, for all $t \geq 0$ and all $a \geq 0$,
\beast
\pro(S_t \geq a) & = & \pro(S_t \geq a,B_t \leq a) + \pro(S_t \geq a,B_t \geq a) \\
& = & \pro(B_t \geq 2a - a) + \pro(B_t \geq a) = 2\pro(B_t \geq a) = \pro(\abs{B_t} \geq a)
\eeast
since when $B_t \geq a$, $S_t \geq a$ automatically as well. 
\end{proof}

\begin{proposition}\label{pro:stopping_time_brownian_motion_touch_special_point}
For every $x > 0$, the random time $T_x := \inf\bra{t\geq 0:B_t = x}$ has same law as $(x/B_1)^2$. 

Also, $T_x$ has independent and stationary increments.%We leave the computation of the distribution of $T_x$ as an exercise (cf. Problem 1.1).
\end{proposition}

\begin{proof}[\bf Proof]
Let $x>0$. We know that $T_x< \infty$ a.s. because $B$ is continuous and $\sup_{t\geq 0}B_t = \infty$ a.s. Then for $x>0$, the reflection principle and Brownian scaling property (Proposition \ref{pro:brownian_motion_basic_properties}.(ii)) give that
\be
\pro\bb{T_x\leq t} = \pro\bb{S_t \geq x} = \pro\bb{\abs{B_t} \geq x} = \pro\bb{\sqrt{t}\abs{B_1} \geq x} = \pro\bb{\bb{\frac x{B_1}}^2\leq t}.
\ee

So $T_x$ has same law as $(x/B_1)^2$.

By the strong Markov property at $T_x$, we have that $B^{(T_x)} = (B_{T_x+t} -B_{T_x})_{t\geq 0} = (B_{T_x+t} -x)_{t\geq 0}$ is a Brownian motion independent of $\sF_{T_x}$. Therefore,
\be
T_y - T_x := \inf\bra{t\geq 0: B_{T_x + t} = y}  = \inf\bra{t\geq 0: B_{T_x + t} - x = y-x} = \inf\bra{t\geq 0: B^{(T_x)} = y-x}
\ee
has the same law as $T_{y-x}$ and is independent of $\sF_{T_x}$. Hence we can see that $(T_x)_{x \geq 0}$ has independent and stationary increments (see Definition \ref{def:independent_stationary_increments_stochastic_process}).
\end{proof}


\begin{proposition}\label{pro:brownian_motion_stopping_time_distribution}
Let $T_x = \inf\bra{t\geq 0:B_t = x}$ for $x>0$. Then
\be
f_{T_x}(t) = \frac{x e^{-\frac{x^2}{2t}}}{\sqrt{2\pi t^3}}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
By Corollary \ref{cor:abs_sup_equal_brownian_motion}, we have%\footnote{see Rogers' book}
\be
\pro\bb{T_x\leq t} = \pro\bb{S_t \geq x} = \pro\bb{\abs{B_t} \geq x} = 2\int^\infty_x \frac{1}{\sqrt{2\pi t}}e^{-\frac{x^2}{2t}}dx = 2\int^\infty_{x/\sqrt{t}} \frac{1}{\sqrt{2\pi }}e^{-\frac{x^2}{2}}dx.
\ee

Then we take the differentiation,
\be
f_{T_x}(t) = -\frac{2}{\sqrt{2\pi}}e^{-\frac{x^2}{2t}}\bb{ -\frac 12 \frac {x}{\sqrt{t^3}}} = \frac{x e^{-\frac{x^2}{2t}}}{\sqrt{2\pi t^3}}.
\ee
\end{proof}

\subsection{Martingales for Brownian Motion}

\begin{theorem}\label{thm:brownian_motion_martingale}
Let $(B_t)_{t \geq 0}$ be an $(\sF_t)$-Brownian motion in $d \geq 1$ dimensions.
\ben
\item [(i)] If $d = 1$ and $B_0 \in \sL^1(\Omega,\sF,\pro)$, the process $(B_t)_{t \geq 0}$ is a $(\sF_{t^+})$-martingale.
\item [(ii)] If $d \geq 1$ and $B_0 \in \sL^2(\Omega,\sF,\pro)$, the process $(\abs{B_t}^2 - dt)_{ t \geq 0}$ is a ($\sF_{t^+}$)-martingale.
\item [(iii)] Let $d \geq 1$ and $u = (u_1,\dots, u_d) \in \C^d$\footnote{If $x, y \in \C^d$, we note $\inner{x}{y} = \sum^d_{i=1} x_i\overline{y}_i$ their complex scalar product.}. Assume that $\E\abs{\exp(\inner{u}{B_0})} < \infty$, the process defined by
\be
M_t = \exp\bb{\bsa{u,B_t} - t\abs{u}^2/2}
\ee
is also a ($\sF_{t^+}$)-martingale for every $u \in \C^d$, where $\abs{u}^2$ is a notation for $\sum^d_{i=1} u_i\ol{u_i}$.
\een
\end{theorem}

\begin{remark}
All the above martingales are also $(\sF_t)$-martingales (as $B_t = B_{t^+}$ a.s. and $\sF_{t}\subseteq \sF_{t^+}$).

Notice that in (iii), we are dealing with $\C$-valued processes. Also, the hypothesis on $B_0$ in (iii) is automatically satisfied whenever $u = iv$ is purely imaginary, i.e., $v \in \R$.

Note that the alternative proof by using It\^o formula is given later (see Theorem \ref{thm:brownian_motion_martingale_ito}).%\footnote{need link}.
\end{remark}

\begin{proof}[\bf Proof]
The adaptedness of these three process is obvious, Thus, we only check the integrability and conditional expectation.
\ben
\item [(i)] If $s \leq t$, then since $B$ is continuous a.s.,
\be
\E\bb{B_t -B_{s^+}|\sF_{s^+}} = \E\bb{B_t -B_{s}|\sF_{s^+}} = \E\bb{B^{(s)}_{t-s}|\sF_{s^+}}
\ee
where $B^{(s)}_u = B_{u+s} -B_s$ has mean 0 and is independent of $\sF_{s^+}$, by the simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion}). Thus, $\E\bb{B_t -B_s|\sF_{s^+}} = \E\bb{B^{(s)}_{t-s}|\sF_{s^+}} = \E\bb{B^{(s)}_{t-s}} = 0$ a.s. by Proposition \ref{pro:conditional_expectation_tower_independence}.(v).

The integrability of the process is obvious by assumption on $B_0 \in \sL^1(\Omega,\sF,\pro)$ as 
\be
\E\abs{B_t} \leq \E\abs{B_t- B_0} + \E\abs{B_0} \leq \dabs{B_t- B_0}_2 + \E\abs{B_0} < \infty.
\ee

\item [(ii)] Since $B_0 \in \sL^2(\Omega,\sF,\pro)$ and $B_t - B_0$ is a normal random variable, we have for any $t\geq 0$
\be
\E\abs{\abs{B_t}^2 -dt} \leq \E\abs{B_t - B_0 + B_0}^2 + dt = \dabs{B_t - B_0 + B_0}^2_2 + dt \leq \bb{\dabs{B_t - B_0}_2 + \dabs{B_0}_2}^2 + dt < \infty.
\ee
by Minkowski inequality (Theorem \ref{thm:minkowski_inequality_expectation}). %by the triangle inequality that $B_t \in \sL^2(\Omega,\sF,\pro)$. 
Thus, for $s \leq t$, as $B_{u+s} -B_s$ has mean 0 and is independent of $\sF_{s^+}$ (by the simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion})), ($B_s = B_{s^+}$ a.s., $B_{s^+},B_t-B_s \in \sL^2 (\Omega,\sF,\pro)$)
\beast
\E\bb{\left.\abs{B_t}^2\right|\sF_{s^+}} & = & \E\bb{\left.\abs{B_t-B_{s}}^2 + 2B_{s^+}\cdot (B_t-B_s) + \abs{B_{s^+}}^2\right|\sF_{s^+}} \\
& = & \E\bb{\left.\abs{B_t-B_{s}}^2\right|\sF_{s^+}} + 2\E\bb{ B_{s^+}\cdot (B_t-B_s)|\sF_{s^+}} + \E\bb{\left.\abs{B_{s^+}}^2\right|\sF_{s^+}}\\
& \stackrel{\text{a.s.}}{=} & \E\bb{\left.\abs{B_t-B_{s}}^2\right|\sF_{s^+}} + 2B_{s^+}  \cdot \E\bb{B_t-B_s|\sF_{s^+}} + \E\bb{\left.\abs{B_{s^+}}^2\right|\sF_{s^+}}\quad (\text{Proposition \ref{pro:conditional_expectation_tower_independence}.(iii)})\\
& \stackrel{\text{a.s.}}{=} & \E\abs{B_t-B_{s}}^2 + 2B_{s^+} \cdot\E\bb{B_t-B_s} + \abs{B_{s^+}}^2 \quad (\text{Proposition \ref{pro:conditional_expectation_tower_independence}.(v) and Proposition \ref{pro:conditional_expectation_basic_property}.(ii)})\\
& \stackrel{\text{a.s.}}{=}  & d(t-s) + 2B_{s^+} \cdot 0 + \abs{B_{s^+}}^2 = d(t-s) + \abs{B_{s^+}}^2 = d(t-s) + \abs{B_{s}}^2
\eeast
since $B$ is continuous a.s. %Taking conditional expectation given $\sF_s$ and using the simple Markov property gives that $\E[B^2_t] = (t - s) + B^2_s$, hence the result. 
Thus, the process $(\abs{B_t}^2 - dt)_{ t \geq 0}$ is a ($\sF_{t^+}$)-martingale. 

%Note that the alternative proof by using It\^o formula is given later\footnote{need link}.

%\qcutline

\item [(iii)] To check integrability, note that by Proposition \ref{pro:cf_gaussian}.(iv),
\be
\E\bb{\exp(\lm B_t)} = \exp\bb{t\lm^2/2}
\ee
whenever $B$ is a standard Brownian motion, and since $\abs{e^z} = e^{\Re z}$, then we have 
\beast
\E\abs{\exp\inner{u}{B_t}} & = & \E\abs{\exp\inner{u}{(B_t - B_0 + B_0)}} = \E\abs{\exp \inner{u}{B_t - B_0} \exp \inner{u}{B_0}}\\
& = & \E\abs{\exp\inner{u}{B_t - B_0}}\E\abs{\exp\inner{u}{B_0}}\quad (\text{Proposition \ref{pro:expectation_of_independent_product}})\\
& = & \exp\bb{t\sum^d_{i=1} (\Re u_i)^2/2}\E\abs{\exp\inner{u}{B_0}} < \infty.
\eeast

Then, for $s\leq t$,
\beast
\E\bb{M_t|\sF_{s^+}} & = & e^{-\abs{u}^2t/2}\E\bb{\left.\exp\inner{u}{B_t}\right|\sF_{s^+}} = e^{-\abs{u}^2t/2}\E\bb{\left.\exp\inner{u}{B_t-B_{s^+}}\exp\inner{u}{B_{s^+}}\right|\sF_{s^+}} \\
& \stackrel{\text{a.s.}}{=} &  e^{-\abs{u}^2t/2}\exp\inner{u}{B_{s^+}} \E\bb{\exp\inner{u}{B_t-B_{s^+}}|\sF_{s^+}} \quad (\text{Proposition \ref{pro:conditional_expectation_tower_independence}.(iv)})\\
& \stackrel{\text{a.s.}}{=} &  e^{-\abs{u}^2t/2}\exp\inner{u}{B_{s^+}} \E\bb{\exp\inner{u}{B_t-B_{s^+}}} \quad (\text{Proposition \ref{pro:conditional_expectation_tower_independence}.(v)})\\
& \stackrel{\text{a.s.}}{=} &  e^{-\abs{u}^2t/2}\exp\inner{u}{B_{s^+}} \exp\bb{(t-s^+)\sum^d_{i=1} u_i^2/2} \\
& = &  e^{-\abs{u}^2t/2}  e^{\abs{u}^2(t-s^+)/2} \exp\inner{u}{B_{s^+}} \stackrel{\text{a.s.}}{=} e^{-\abs{u}^2s^+/2} \exp\inner{u}{B_{s}} = M_{s}.
\eeast

Therefore, $M$ is a ($\sF_{t^+}$)-martingale.
\een
\end{proof}


%begin{proposition}
%et $(B_t)$ be a standard Brownian motion and $d = 1$, and for $x \in \R$, $T_x = \inf\{t \geq 0 | B_t = x\}$. Then for $x, y > 0$, $\pro(T_x < T_{-y}) = \frac y{x+y}$ and $\E[T_x \land T_{-y}] = x y$.
%\end{proposition}


A classical application of these martingales is to show the following result, often referred to as the gambler's ruin\footnote{need definition} estimates.

\begin{theorem}\label{thm:brownian_motion_double_bounded}
Let $(B_t)_{t \geq 0}$ be a standard Brownian motion and $T_x = \inf\bra{t \geq 0:B_t = x}$. Then for $x, y > 0$, one has
\be
\pro(T_{-y} < T_x) = \frac x{x + y},\quad\quad \E\bb{T_x \land T_{-y}} = xy.
\ee
\end{theorem}

%\begin{proof}[\bf Proof]
%We use martingales stopped at $T_x \land T_{-y}$.
%\be
%0 = \E[B_{T_x\land T_{-y}}] = x \pro(T_x < T_{-y})- y(1-\pro(T_x < T_{-y})),
%\ee
%so $\pro(T_x < T_{-y}) = \frac y{x+y}$. Similarly,
%\be
%\E[T_x \land T_{-y}] = \E[B^2_{T_x\land T_{-y}}] = x^2 \frac y{x + y} + y^2 \frac x{x + y} = x y.
%\ee
%\end{proof}

\begin{proof}[\bf Proof]
Let $T = T_{-y} \land T_x$ (by Proposition \ref{pro:debut_time_closed_set_stopping_time}), which is a stopping time. 

Since $B$ is a martingale, the stopped martingale $B^T$ is still a martingale (by Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}.(ii)). Moreover, $B^T$ is bounded (by $\max(x, y)$) so it is UI martingale. Then we may apply the optional stopping theorem (Theorem \ref{thm:optional_stopping_ui_continuous}) to find that $\E(B_T) = \E(B_0) = 0$ a.s.. On the other hand, 
\be
\E(B_T) = - yp + x(1 - p),
\ee
where $p = \pro(T_{-y} < T_x)$ is the probability of interest to us. Thus $py = (1 - p)x \ \ra \ p = \frac{x}{x+y}$. %and the first statement follows easily. 

For the second statement, we can observe that $B^2_t-t$ is a martingale and thus $B^2_{t\land T} - (t\land T)$ is a martingale (by Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}.(ii)). Therefore,
\be
\E(B^2_{t\land T} ) = \E(t \land T).
\ee

We may let $t \to\infty$ since the left-hand side is bounded (we use bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability})) and the right-hand side is monotone (we use monotone convergence theorem (Theorem \ref{thm:monotone_convergence_probability})), and deduce that
\be
\E(B^2_T) = \E(T).
\ee

Using the first statement, 
\be
\E(B^2_T) = \frac x{x + y} y^2 + \frac y{y + x}x^2 = xy \ \ra \  \E T = \E\bb{T_x \land T_{-y}} = xy.
\ee%and the claim follows.
\end{proof}

%Similarly,

%\qcutline

\begin{theorem}
Let $B$ be a standard Brownian motion. The Laplace transform of $T_x =  \inf\bra{t \geq 0:B_t = x}$ for $x \in \R$ is given by $\E(e^{-qT_x}) = e^{-\abs{x}\sqrt{2q}}$. Moreover, for $x,y >0$, the random variable $T = T_x \land T_{-y}$ has a Laplace transform given by 
\be
\E\bb{e^{-qT}} = \frac{\sinh(\sqrt{2q}x) + \sinh(\sqrt{2q}y)}{\sinh(\sqrt{2q}(x + y))} 
\ee
and when $y = x$, $T$ is independent from the event $\bra{T_{-x} < T_x}$.
\end{theorem}

\begin{proof}[\bf Proof]
Let $x\geq 0$ and $\lm \geq 0$. We know the fact that $\bb{\exp\bb{\lm B_t-\lm^2t /2}}_{t\geq 0}$ is a martingale by Theorem \ref{thm:brownian_motion_martingale}. Therefore,
\be
\bb{\exp\bb{\lm B_t^{T_x}-\frac{\lm^2}2(t\land T_x)}}_{t\geq 0}
\ee
is also a martingale by Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}.(ii). Since it is bounded, it is UI and thus by optional stopping theorem (Theorem \ref{thm:optional_stopping_ui_continuous}), we have that
\be
\E\bb{\exp\bb{\lm x-\frac{\lm^2}2 T_x}} = \E\bb{\exp\bb{\lm B_{T_x}-\frac{\lm^2}2 T_x}} = 1  \ \ra \ \exp\bb{-\lm x} = \E\bb{\exp\bb{-\lm^2 T_x/2}}.
\ee

Letting $\lm = \sqrt{2q}$, we can have the required result. Similarly, for $x\geq 0$, we will let $\lm \leq 0$ to guarantee the boundedness of the martingale (and then let $\lm = -\sqrt{2q}$).

%By bounded convergence theorem ()
%\be
%eewhich is bounded  The first statement follows directly from the optional stopping theorem and the fact thatwhen stopped at $T_x$ if $x \geq 0$ (which we may assume by symmetry). 

The second statement is a bit more involved. Let 
\be
M_t = e^{-\lm^2t/2} \sinh(\lm(B_t + y))
\ee
is also a martingale since it can be written as
\be
\frac 12 e^{-\lm^2t/2}e^{\lm(B_t+y)} -\frac 12 e^{-\lm^2t/2}e^{-\lm(B_t+y)}
\ee
which is the sum of two martingales. Now, stopping at $T = T_{-y} \land T_{x}$, $M^T$ is bounded and thus UI, so we can use the optional stopping theorem (Theorem \ref{thm:optional_stopping_ui_continuous}) to obtain:
\be
\sinh(\lm y) = M_0 = M_T = \E(\sinh(\lm (B_T + y))e^{-\lm^2T/2} = \E\bb{\sinh(\lm(x + y)e^{-\lm^2T/2}\ind_{\bra{T_x>T_{-y}}}}.
\ee

Thus,
\be
\E\bb{e^{-\lm^2T/2}\ind_{\bra{T_x>T_{-y}}}} = \frac{\sinh(\lm y)}{\sinh(\lm(x + y))} \ \ra \ \E\bb{e^{-\lm^2T/2}\ind_{\bra{T_{-y}>T_x}}} = \frac{\sinh(\lm x)}{\sinh(\lm (x + y))}
\ee
by symmetry. Adding up the two terms,
\be
\E(e^{-\lm^2T/2}) = \frac{ \sinh(\lm x)+\sinh(\lm y) }{\sinh(\lm(x + y))}
\ee

Again, letting $\lm = \sqrt{2q}$, we can have the required result. When $x = y$, 
\be
\E\bb{e^{-\lm^2T/2}\ind_{\bra{T_{-x}>T_x}}} = \frac{\sinh(\lm x)}{\sinh(2\lm x)} = \frac 12 \frac{2\sinh(\lm x)}{\sinh(2\lm x)}  = \frac 12 \E(e^{-\lm^2T/2}) = \E\bb{e^{-\lm^2T/2}}\pro(T_{-x} < T_x)
\ee
%it suffices to check that which is easy to check.


Hence, $T$ is independent from the event $\bra{T_{-x} < T_x}$ by Corollary \ref{cor:mgf_indicator_expectation_independent}. %\footnote{need details}.
\end{proof}

\begin{theorem}\label{thm:brownian_motion_integral_martingale}
Let $f : \R^+ \times \R^d \to \R,\ (t,x) \mapsto f(t,x)$ be $C^{1,2}$ (i.e., continuously differentiable in the first coordinate (time) and twice continuously differentiable in the second coordinate (space)) and $(B_t)_{t\geq 0}$ is a standard Brownian motion with respect to $(\sF_t)_{t\geq 0}$. %, and be such that all partial derivatives are bounded. 
If there exists a constant $K$ such that, for all $t\geq 0$, $x\in \R^d$,
\be
\abs{f(t,x)} + \abs{\fp{f(t,x)}{t}} + \sum^d_{i=1} \abs{\fp{f(t,x)}{x_i}} + \sum^d_{i=1}\sum^d_{i=1} \abs{\fp{f(t,x)}{x_i}} \leq K e^{K(t+\abs{x})}\qquad (*),
\ee

then 
\be
M_t := f (t, B_t )- f (0, B_0)- \int^t_0 \sL f(s,B_s)ds
\ee
is an $(\sF_{t^+})$-martingale, where (for the Laplaceian $\Delta$)
\be
\sL f(t,B_t) := \fp{f(t,x)}{t} + \frac 12 \Delta f(t,x) = \fp{f(t,x)}{t} + \frac12 \sum^d_{i=1} \frac{\partial^2 f (t, x)}{\partial x^2_i}.
\ee
\end{theorem}

\begin{remark}
We can extend this to the Brownian motions with start point $x$ a.s., i.e. $\wt{B}_t = x+ B_t$ where $B_t$ is standard Brownian motions.

If we assume that $f$ and its derivatives up to second order are bounded, they will satisfy $(*)$ automatically and the conclusion will hold for any Brownian motion (see Definition \ref{def:brownian_motion_d}).
\end{remark}

\begin{example}
\ben
\item [(i)] Let $f(t,x) = x$, then $M_t = B_t - B_0 - \int^t_0 0\ ds = B_t$ which is Theorem \ref{thm:brownian_motion_martingale}.(i). 

\item [(ii)] Let $f(t,x) = x^2$, then $M_t = B_t^2 - B_0^2 - \frac 12 \int^t_0 2d\ ds = B_t^2 - dt$ which is Theorem \ref{thm:brownian_motion_martingale}.(ii). 
\een
\end{example}

\begin{proof}[\bf Proof]
First we prove that $K e^{K(t+\abs{B_t})}$ is integrable for any $t\geq 0$.
\be
\E\abs{K e^{K(t+\abs{B_t})}} = Ke^{Kt} \E\bb{e^{K\abs{B_t}}} \leq 2Ke^{Kt} \E\bb{e^{K (B_t-B_0} + KB_0} = 2Ke^{2Kt}\E\bb{e^{KB_0}} < \infty
\ee
as $B_0 = 0$ a.s.. Then for any $t\geq 0$, by ($*$),
\be
\E\abs{M_t} = \E\abs{f (t, B_t )- f (0, B_0)- \int^t_0 \sL f(s,B_s)ds} \leq \E\abs{K e^{K(t+\abs{B_t})}} + 1 + \E\abs{\int^t_0 K e^{K(s+\abs{B_s})}ds}
\ee

Since $\abs{B_t}$ has the same distribution as $S_t = \sup_{0\leq s\leq t}B_s$ (Corollary \ref{cor:abs_sup_equal_brownian_motion}),
\beast
\E\abs{M_t} & \leq & \E\abs{K e^{K(t+\abs{B_t})}} + 1 + \E\abs{\int^t_0 K e^{K(s+ S_s)}ds} \leq \E\abs{K e^{K(t+\abs{B_t})}} + 1 + \E\abs{\int^t_0 K e^{K(s+ S_t)}ds} \\
& = & \E\abs{K e^{K(t+\abs{B_t})}} + 1 + \E\abs{e^{K\abs{B_t}} \int^t_0 K e^{Ks}ds} = \E\abs{K e^{K(t+\abs{B_t})}} + 1 + \E\abs{e^{K\abs{B_t}} \bb{e^{Kt}-1}}\qquad (\dag)\\
& \leq & \E\abs{(K+1) e^{K(t+\abs{B_t})}} + 1 < \infty
\eeast
as $K e^{K(t+\abs{B_t})}$ is integrable for any $t\geq 0$. Therefore, $M_t$ is integrable.

From the assumption, $f(t,x)$ is continuous with respect to $x$. We know that 
\be
\sL f(t,B_t) = \fp{f(t,x)}{t} + \frac12 \sum^d_{i=1} \frac{\partial^2 f (t, x)}{\partial x^2_i}
\ee
is dominated by $K e^{K(t+\abs{B_t})}$ which is integrable. Furthermore, we have $\sL f(t,B_t+\ve) \to \sL f(t,B_t)$ as $\ve\to 0$ since $f(t,x)$ is twice continuously differentiable with respect to $x$. Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}),
\be
\int^t_0 \sL f(s,B_s+\ve)ds \to \int^t_0 \sL f(s,B_s)ds,\qquad \text{as }\ve \to 0.
\ee

Thus, $\int^t_0 \sL f(s,B_s)ds$ is continuous and therefore $M_t$ is $\sF_t$-measurable (and $\sF_{t^+}$-measurable) by Proposition \ref{pro:continuous_measurable}.

We will now show the martingale property. Let $t\geq 0$. Then
\beast
M_{t+s} - M_s & = & f(t+s,B_{t+s}) -f(s,B_s) - \int^{s+t}_s \bb{\fp{}{r} + \frac 12 \Delta}f(r,B_r)dr\\
& = & f(t+s,B_{t+s}) -f(s,B_s) - \int^{t}_0 \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr.
\eeast

Thus, by Proposition \ref{pro:conditional_expectation_basic_property}.(iii),
\be
\E\bb{\left.M_{t+s} - M_s\right|\sF_{s^+}} = \E\bb{\left.f(t+s,B_{t+s})\right|\sF_{s^+}} -f(s,B_s) - \E\bb{\left.\int^{t}_0 \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \right|\sF_{s^+}}
\ee

%Then since $B_{s} = B_{s^+}$ a.s. (sample continuity of standard Brownian motion),%

Since $B_{t+s} - B_s$ is independent of $\sF_{s^+}$ by Theorem \ref{thm:simple_markov_property_brownian_motion} and $B_s$ is $\sF_{s^+}$-measurable, by Proposition \ref{pro:density_function_probability} and Proposition \ref{pro:conditional_expectation_tower_independence}.(v),
\be
\E\bb{\left.f(t+s,B_{t+s})\right|\sF_{s^+}} = \E\bb{\left.f(t+s,B_{t+s} - B_s + B_s)\right|\sF_{s^+}} \stackrel{\text{a.s.}}{=}  \int_{\R^d} \bb{f(t+s,x + B_{s})} p_t(x)dx \qquad (\dag\dag)
\ee
where 
\be
p_t(x) = \frac 1{(2\pi t)^{d/2}} \exp\bb{-\frac{\abs{x}^2}{2t}}
\ee
is defined in Definition \ref{def:transition density_standard_brownian_motion} and satisfies (by Proposition\footnote{need proposition})
\be
\bb{\fp{}{t} - \frac 12\Delta} p_t(x) = 0\qquad (\dag\dag\dag).
\ee

Let 
\be
Y = \E\bb{\left.\int^{t}_0 \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \right|\sF_{s^+}},\qquad Z = \E\bb{\left.\bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})\right|\sF_{s^+}}.
\ee

Then for any $A\in \sF_{s^+}$, by definition of conditional expectation and Fubini theorem (Theorem \ref{thm:fubini} as $\int^t_0\bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr$ is integrable\footnote{it dominated by $e^{K\abs{B_t}} \bb{e^{Kt}-1}$, see ($\dag$) which is integrable.})
\beast
\E\bb{Y\ind_A} & = & \E\bb{\int^{t}_0 \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \ind_A} = \int^{t}_0 \E\bb{ \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})\ind_A} dr\\
& = & \int^t_0 \E(Z\ind_A)dr = \E\bb{\int^t_0 Z dr \ind_A} \ \ra \ Y = \int^t_0 Z dr \text{ a.s.}
\eeast

That is,
\be
\E\bb{\left.\int^{t}_0 \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \right|\sF_{s^+}} = \int^t_0 \E\bb{\left.\bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})\right|\sF_{s^+}} dr \text{ a.s.}
\ee

Also, by the same argument in $(\dag\dag)$, we have
\be
\E\bb{\left.\bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})\right|\sF_{s^+}} \stackrel{\text{a.s.}}{=}  \int_{\R^d} \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,x+ B_s) p_r(x) dx.
\ee

Then
\beast
\E\bb{\left.\int^{t}_0 \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \right|\sF_{s^+}} & \stackrel{\text{a.s.}}{=} & \int^t_0 \int_{\R^d} \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,x+ B_s) p_r(x) dx dr\\
& = & \lim_{\ve \to 0} \int^t_\ve \int_{\R^d} \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,x+ B_s) p_r(x) dx dr
\eeast
by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}). Furthermore, using integral by parts, we have 
\beast
& & \int^t_\ve \int_{\R^d} \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,x+ B_s) p_r(x) dx dr \\
& = & \int_{\R^d} \int^t_\ve \fp{}{r} f(r+s,x+ B_s) p_r(x) dr dx + \int^t_\ve \int_{\R^d} \frac 12 \Delta f(r+s,x+ B_s) p_r(x) dx dr\\
& = & \int_{\R^d} \bb{f(t+s,x+B_s)p_t(x) - f(\ve+s,x+B_s)p_\ve(x) - \int^t_\ve f(r+s,x+ B_s) \fp{}{r} p_r(x) dr} dx \\
& & \qquad + \frac 12\int^t_\ve \bb{ \left.\sum^d_{i=1} \fp{f(r+s,x+B_s)}{x_i} p_r(x)\right|_{\R^d} - \left. f(r+s,x+B_s) \sum^d_{j=1}  \fp{p_r(x)}{x_j}\right|_{\R^d} + \int_{\R^d}  f(r+s,x+B_s) \Delta p_r(x)dx}  dr\\
& = & \int_{\R^d} \bb{f(t+s,x+B_s)p_t(x) - f(\ve+s,x+B_s)p_\ve(x)}dx - \int_{\R^d}  f(r+s,x+B_s) \bb{\fp{}{r} - \frac 12\Delta} p_r(x)dx dr
\eeast
by Fubini theorem (Theorem \ref{thm:fubini} by assumption ($*$)). Then by $(\dag\dag\dag)$, we have
\beast
\E\bb{\left.\int^{t}_0 \bb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \right|\sF_{s^+}} \stackrel{\text{a.s.}}{=} \lim_{\ve \to 0} \int_{\R^d} \bb{f(t+s,x+B_s)p_t(x) - f(\ve+s,x+B_s)p_\ve(x)}dx\qquad (\dag\dag\dag\dag)
\eeast

Then combining $(\dag\dag)$ and $(\dag\dag\dag\dag)$, we have that
\beast
\E\bb{\left.M_{t+s} - M_s\right|\sF_{s^+}} & \stackrel{\text{a.s.}}{=} & \lim_{\ve \to 0}\int_{\R^d} f(\ve+s,x+B_s)p_\ve(x)dx - f(s,B_s)\\
& \stackrel{\text{a.s.}}{=} & \lim_{\ve \to 0} \E\bb{f(\ve+s,B_{s+\ve})|\sF_{s^+}} - f(s,B_s) = 0
\eeast
by the continuity of Brownian motion and of $f$ and by the conditional dominated convergence theorem (Theorem \ref{thm:dominated_convergence_conditional_expectation}). 

Furthermore, we know that $M_t$ is continuous with respect to $t$ as $f$ is continuous and the integral is also continuous by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}) as the integrand is dominated by $Ke^{K(t+\abs{B_t})}$ (which is integrable). 

Therefore, $M_s = M_{s^+}$ and hence $(M_t)_{t\geq 0}$ is an $(\sF_{t^+})$-martingale.
\end{proof}

%We prove the case where $f (t, x) = f (x)$ does not depend on time. Let $p_t (x) = \frac 1{(2\pi t)^{\frac d2}} \exp\bb{- \frac 1{2t}\abs{x}^2}$ and let $g$ be bounded and continuous. We claim that if $(B_t)$ is a standard Brownian motion then
%\be
%\int^t_0 \int_{\R^d} g(x)\fp{}{s} p_s(x)d xds = \E[g(B_t )]- g(0).
%\ee

%Indeed,
%\beast
%\int^t_0 \int_{\R^d} g(x)\fp{}{s} p_s(x)d xds & = & \lim_{\ve\to 0} \int^t_\ve \int_{\R^d}g(x) \fp{}{s}p_s(x)d xds = \lim_{\ve\to 0} \int_{\R^d} g(x)(p_t (x)- p_\ve(x))d x\\
%& = & \lim_{\ve\to 0}\E[g(B_t )]-\E[g(B_\ve)] = \E[g(B_t )]- g(0)
%\eeast
%by DCT since $B_\ve \to 0$ and $g$ is bounded.

%For $s, t \geq 0$,
%\be
%\E\bsb{f (B_{s+t})- f (B_t )- \int^{t+s}_t \frac {\Delta}2 f (B_u)du | \sF_t} = \underbrace{\int p_s(x) f(x + B_t )dx - f (B_t )}_{(1)} - \underbrace{\E\bsb{\int^{t+s}_t \frac {\Delta}2 f (B_u - B_t + B_t )du |\sF_t}}_{(2)}.
%\ee
%But
%\beast
%(2) & = & \E\bsb{\int^s_0 \frac {\Delta}2 f (B_{t+u} - B_t + B_t )du |\sF_t} = \int_{\Omega_W} W_0(dw) \int^s_0 \frac {\Delta}2 f (w_u + B_t )du\\
%& = & \int^s_0 du \int_{\Omega_W} W_0(dw)\frac {\Delta}2 f (w_u + B_t ) = \int^s_0 du\int p_u(x)d x \frac {\Delta}2 f (x + B_t )
%\eeast

%By integrating by parts, this is equal to
%\be
%(2) = \int^s_0 du \int \frac {\Delta}2 p_u(x) f (x + B_t )d x.
%\ee

%It can be checked that $\bb{\fp{}{t} - \frac {\Delta}2}p_t (x) = 0$, i.e. the density of the Gaussian law is a solution to the heat equation. Therefore
%\be
%(2) = \int^s_0 du \int \partial_up_u(x) f (x + B_t )d x = \int_{\R^d} p_s(x) f (x + B_t )d x - f (B_t ) = (1)
%\ee
%by the claim.

\subsection{Recurrence and Transience}

%Assume given $(\Omega,\sF, (\pro_x )_{x\in \R^d})$ probability spaces such that $(B_t )$ is under $\pro_x$ a Brownian motion started at $x \in \R^d$. (For example, $(\Omega_W, prod, (W_x )_{x\in \R^d} ))$.

We will write $\pro_x$ to indicate that the Brownian motion starts from $x$, i.e., under $\pro_x$ the process $(B_t-x)_{t\geq 0}$ is a standard Brownian motion.

\begin{theorem}
Let $(B_t)_{t\geq 0}$ be a Brownian motion starting from particular point in $d\geq 1$ dimensions.
\ben
\item [(i)] In $d = 1$, $B$ is point-recurrent, i.e., under $\pro_0$ (or any $\pro_y$, $y\in \R$),
\be
\bra{t : B_t = x}\text{ are unbounded for every }x\in \R\text{ a.s..}
\ee
%a.s. under $P_0$, the sets $\{t | B_t = x\}$ are unbounded, i.e. Brownian motion in dimension one is 'point recurrent.'

\item [(ii)] In $d = 2$, $B$ is neighbourhood-recurrent, i.e., under $\pro_x$,
\be
\bra{t:\abs{B_t-z} \leq r} \text{ is unbounded for every $x,z\in \R^d$, $r >0$ a.s.}
\ee

However, points are polar, i.e., for every $x\in \R^d$,
\be
\pro_0(H_x = \infty)=1,\qquad \text{where }H_x = \inf\bra{t>0:B_t = x}\text{ is the hitting time of $x$.}
\ee

%a.s. under $P_x$ , $\bra{t | \abs{B_t} \leq \ve}$ is unbounded for all $\ve > 0$, i.e. Brownian motion in dimension two is 'neighbourhood recurrent.' However, if $H_x = \inf\{t > 0| B_t = x\}$ then $\pro_x (H_y <\infty) = 0$, i.e. 'points are polar.'

\item [(iii)] In $d \geq 3$, $B$ is transient, i.e., $\abs{B_t}\to \infty$ as $t\to \infty$ $\pro_0$-a.s.. Also, for $0< r<\abs{x}$,
\be
\pro_x\bb{H_r < \infty} = \bb{\frac{r}{\abs{x}}}^{d-2}.
\ee
%$(B_t)$ is transient, i.e. $\abs{B_t}\to \infty$ as $t \to \infty$.
\een
\end{theorem}

\begin{remark}
For 2-dimensional Brownian motion, since $\{t | B_t \in B(x, r)\}$ is unbounded for every $x \in \R^2$ and $r > 0$ and $\R^2$ may be covered by a countable union of balls of a fixed radius, the trajectory of Brownian motion is dense in $\R^2$. On the other hand, a.s. for all $t > 0$, $B_t \notin \Q^2$.
\end{remark}

\begin{proof}[\bf Proof]
\ben
\item [(i)] We already know that $\sup_{t\geq 0} B_t = -\inf_{t\geq 0} B_t = \infty$ by Proposition \ref{pro:brownian_motion_limit_value}, so we have point recurrence by continuity of $B$.

\item [(ii)] Note that it suffices to show the claim for $z= 0$. That is, $\bra{t:\abs{B_t} \leq r}$ is unbounded for every $x\in \R^d$, $r >0$ $\pro_x$-a.s..

Let $r > 0$, $R > r$, and $D_{r,R} = \bra{x \in \R^2 :r \leq \abs{x} \leq R}$. Let $f : \R^2 \to \R$ be such that $f : x \to\log\abs{x}$ on $D_{r,R}$, so $f$ is bounded, is $C^\infty$, and has bounded derivatives. Also, we have that $\Delta \log \abs{x} = 0$ on $r\leq \abs{x} \leq R$. %the interior of $D_{\ve,R}$. 
That is, 
\beast%\frac{\partial^2 \sqrt{x^2_1 + x^2_2}}{\partial x_1^2} + \frac{\partial^2 \sqrt{x^2_1 + x^2_2}}{\partial x_2^2} 
\Delta \log \abs{x} = \frac{\partial }{\partial x_1} \bb{\frac{x_1}{x^2_1 + x^2_2}} + \frac{\partial }{\partial x_2} \bb{\frac{x_2}{x^2_1 + x^2_2}} = \frac{x^2_1 + x^2_2 - 2x_1^2}{\bb{x^2_1 + x^2_2}^2} + \frac{x^2_1 + x^2_2 - 2x_2^2}{\bb{x^2_1 + x^2_2}^2} = 0.
\eeast

Then by Theorem \ref{thm:brownian_motion_integral_martingale},
\be
M_t = f(B_t)- f (x)- \int^t_0 \frac 12 \Delta f (B_s)ds = \log\abs{B_t} -\log\abs{x}
\ee
is a martingale. Let $T_r = \inf\bra{t \geq 0 :\abs{B_t} \leq r}$ and $T_R = \inf\bra{t \geq 0: \abs{B_t} \geq R}$. Therefore let $T = T_r \land T_R$, then $T < T_R < \infty$ a.s. since Brownian motion is unbounded a.s. (by Proposition \ref{pro:brownian_motion_limit_value}).
%\be
%M_{t\land T} = \log\abs{B_{t\land T}}- \log\abs{x}
%\ee%(since ${\Delta}f = 0$ on $D_{\ve,R}$)
%is a martingale as a stopped martingale is still a martingale (Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}), so $(M_{t\land T})_{t \geq 0}$ is a bounded martingale. 

By optional stopping theorem (Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}.(iii)), 
\be
\E\bb{M_T} = M_0 \ \ra \ \log \abs{x} = \E_x\bb{\log\abs{B_T}} = \log r\pro_x(T_r < T_R)+ \log R \pro_x (T_R < T_r).
\ee

Thus,
\be
\pro_x (T_r < T_R) = \frac{\log R - \log \abs{x}}{\log R -\log r },\quad \pro_x (T_R < T_r) = \frac{\log \abs{x} - \log r}{\log R -\log r }.\quad \quad (*)
\ee

Applying Proposition \ref{pro:brownian_motion_stopping_time_distribution}, we have that $\pro\bb{T_n < \infty}=1$ for all $n > \floor{x}$, $n\in \N$ (this is for one dimensional case, but this implies higher dimensional case as $\abs{B_t} \geq R$ if any dimension exceeds $R$). We know that 
\be
\bra{T_r < \infty} = \bigcup_{n > \floor{x}}\bra{T_r\leq T_n} \cap \bra{T_n <\infty} \ \ra \ \ind_{\bra{T_r \leq T_n}} \ua \ind_{\bra{T_r < \infty}} \text{ a.s.}
\ee

Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}), we have
\be
\pro\bb{T_r < T_R} \to \pro\bb{T_r < \infty} \ \ra \ \pro\bb{T_r < \infty} = \lim_{R\ua \infty} \frac{\log R - \log \abs{x}}{\log R -\log r } = 1.
\ee
%We see that 
%\beast
%\pro_x (T_r < \infty) & = & \pro_x(T_r < \infty, T_R = \infty) + \pro_x \bb{T_r < \infty, T_R < \infty} \\
%& = & 0 + \pro_x \bb{T_r < T_R < \infty} + \pro_x \bb{T_R < T_r < \infty}\\
%& = & \pro\bb{T_r < T_R} - \pro_x\bb{T_r < T_R =\infty} + \pro_x \bb{T_R < T_r < \infty}\\
%& = & \pro_x(T_r < T_R) - 0 + \pro_x \bb{T_R < T_r <\infty} \\
%& = & \pro_x(T_r < T_R)+ \pro_x \bb{T_R < T_r <\infty}.\qquad (**)
%\eeast
%Letting $R\to \infty$ in ($*$), we have $\pro_x (T_r < T_R) = 1 \ \ra \ \pro_x (T_r < \infty)=1$\footnote{need dominated convergence theorem? $T_R \to \infty$ a.s.?} 
which shows that 
\be
\pro_x\bb{\abs{B_t}\leq r, \text{ for some }t>0} = 1.
\ee
for any $x\in \R^2$. But
\beast
\pro_x (\abs{B_t} \leq r, \text{ for some }t>n) & = & \pro_x (\abs{B_t - B_n + B_n} \leq r,\text{ for some }t>n) = \pro_x \bb{\abs{B^{(n)}_t + B_n} \leq r, \text{ for some }t>n} \\
& = & \int_{\R^2} \pro_0 \bb{\abs{B_t +y} \leq r, \text{ for some }t>0} \pro_x(B_n \in d y) \quad\quad \text{(by Simple Markov Property)}\\
& = & \int_{\R^2} \pro_y \bb{\abs{B_t} \leq r, \text{ for some }t>0} \pro_x(B_n \in d y) \\
& = &  \int_{\R^2} 1\cdot \pro_x(B_n \in d y) = \pro\bb{B_n\in \R^2} = 1,%\pro_y (T_r <\infty) = 1,
\eeast
so $\bra{t :\abs{B_t} \leq r}$ is unbounded $\pro_x$-a.s.. 

On the other hand, we know that
\be
\bra{T_0 < \infty} = \bigcup_{n > \floor{x}}\bra{T_0\leq T_n} \cap \bra{T_n <\infty} = \bigcup_{n > \floor{x}}\bra{\bra{\bigcap_{m>1/\abs{x}} \bra{T_{1/m}\leq T_n} }\cap \bra{T_n <\infty}} 
\ee
which gives that
\be
\ind_{\bigcup_{n > \floor{x}}\bigcap_{m>1/\abs{x}} \bra{T_{1/m}\leq T_n} } \to \ind_{\bra{T_0 < \infty}} \text{ a.s.\ as }m,n\to\infty.
\ee

Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}), we have
\be
\pro\bb{T_0 < \infty} = \lim_{n\to\infty} \lim_{m\to\infty}\pro\bb{T_{1/m}\leq T_n} = \lim_{n\to\infty}\lim_{m\to\infty} \frac{\log n - \log \abs{x}}{\log n + \log m } = \lim_{n\to\infty} 0 = 0.
\ee
%letting $r \to 0$ in ($*$) shows that $\pro_x (T_0 < T_R) = 0$, we have
%\beast
%\pro_x\bb{T_0 < \infty} & = & \pro_x\bb{T_0 < T_R < \infty} + \pro_x\bb{T_0 < T_R = \infty} + \pro_x\bb{T_R<T_0 < \infty} \\
%& \leq & \pro_x\bb{T_0 < T_R} + 0 + \pro_x\bb{T_R <T_0 } = 0 + \pro_x\bb{T_R <T_0 } = \lim_{r\da 0}\frac{\log \abs{x} - \log r}{\log R -\log r }
%\eeast
%and letting $R \to \infty$ gives $\pro_x (T_0 <\infty) = 0$ 
for all $x \neq 0$. That is, for all $x\neq 0$, 
\be
\pro_x\bb{B_t = 0,\text{ for some }t>0} = 0.
\ee

Now we only need to show that
\be
\pro_0\bb{B_t = 0,\text{ for some }t>0} = 0.
\ee

Again, by simple Markov property at $a>0$ we get
\beast
\pro_0\bb{B_t = 0, \text{ for some }t\geq a} & = & \int_{\R^2}\pro_0 \bb{B_{t+a} -B_a + y} \pro_0(B_a\in dy)\\
& = & \int_{\R^2} \pro_y \bb{B_t=0,\text{ for some }t >0} \frac 1{(2\pi a)^{d/2}} e^{-\abs{y}^2/{2a}} dy = 0%\\% \stackrel{a\to 0}{\to} \pro_0(\exists t > 0 : B_t = 0) & = & \pro_0(\exists t > 0 : B^{(a)}_t + B_a = 0)\\ & = & \int_{\R^2} P_0(B_a \in d x)\pro_x (\exists t \geq 0 : B_t = 0) \quad\quad \text{SM}\\& = & 0
\eeast
since for all $y \neq 0$ we have already proved that $\pro_y \bb{B_t=0,\text{ for some }t >0} = 0$. Thus, this holds for all $a>0$, letting $a\to 0$, we have that $\pro_0\bb{B_t = 0,\text{ for some }t>0} = 0$. Hence, for all $x$, we have
\be
\pro_x\bb{B_t = 0,\text{ for some }t>0} = 0
\ee
which means that the Brownian motion can not touch 0 from $x$ a.s..
%because the law of $B_a$ under $P_0$ is a Gaussian law that does not charge $\{0\}$. Therefore for each $x, y \in \R^2$, $\pro_x (H_y <\infty) = 0$.

\item [(iii)] Let $f$ be a bounded $C^\infty$ functions with all derivatives bounded such that $f (x) = \frac 1{\abs{x}^{d-2}}$ on $D_{r,R}$. Again, $\Delta f = 0$ on $D_{r,R}$, that is,
\beast
\Delta f & = & \sum^d_{i=1}\frac{\partial^2}{\partial x_i^2}\bb{\frac{1}{\bb{x_1^2 + \dots + x_d^2}^{d/2-1}}} = \sum^d_{i=1} \fp{}{x_i}\bb{\frac{(2-d)x_i}{\bb{x_1^2 + \dots + x_d^2}^{d/2}}}\\
& = & (2-d)\sum^d_{i=1} \fp{}{x_i}\bb{\frac{\bb{x_1^2 +\dots + x_d^2}^{d/2} - d x_i^2 \bb{x_1^2 + \dots + x_d^2}^{d/2-1}}{\bb{x_1^2 + \dots + x_d^2}^d}} = 0.
\eeast

Then by Theorem \ref{thm:brownian_motion_integral_martingale},
\be
M_t = f(B_t)- f (x)- \int^t_0 \frac 12 \Delta f (B_s)ds = \frac 1{\abs{B_t}^{d-2}} - \frac 1{\abs{x}^{d-2}}
\ee
is a martingale. Furthermore, let $T_r = \inf\bra{t \geq 0 :\abs{B_t} \leq r}$ and $T_R = \inf\bra{t \geq 0: \abs{B_t} \geq R}$. Therefore let $T = T_r \land T_R$. Then the stopped martingale $M_{t\land T} = \frac 1{\abs{B_{t\land T}}^{d-2}} - \frac 1{\abs{x}^{d-2}}$ is also a martingale (Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}), so $(M_{t\land T})_{t \geq 0}$ is a bounded martingale and therefore a UI martingale. %then $T < T_R < \infty$ a.s. since Brownian motion is unbounded a.s. (by Proposition \ref{pro:brownian_motion_limit_value}).%so $(f (B_{T\land t}), t \geq 0)$ is a bounded martingale.
Applying optional stopping theorem (Theorem \ref{thm:optional_stopping_ui_continuous}), we have
\be
\frac 1{\abs{x}^{d-2}} = \E\bb{\frac 1{\abs{B_T}^{d-2}}} = \bb{\frac 1r}^{d-2} \pro_x (T_r < T_R)+ \bb{\frac 1R}^{d-2}\pro_x (T_r > T_R) 
\ee
which implies 
\be
\pro_x (T_r < T_R) = \frac{\bb{\frac 1R}^{d-2} - \bb{\frac 1{\abs{x}}}^{d-2}}{\bb{\frac 1R}^{d-2} -\bb{\frac 1r}^{d-2}},\quad \pro_x (T_R < T_r) = \frac{\bb{\frac 1{\abs{x}}}^{d-2} - \bb{\frac 1r}^{d-2}}{\bb{\frac 1R}^{d-2} - \bb{\frac 1r}^{d-2}}  \quad\quad (\dag)
\ee
for $x \in D_{r,R}$. %Using the same argument with ($**$), we have
%\be
%\pro\bb{T_r< \infty} = \pro_x(T_r < T_R)+ \pro_x \bb{T_R < T_r <\infty} = \pro_x(T_r < T_R)+ \pro_x \bb{T_r <\infty|T_R<T_r} \pro_x \bb{T_R < T_r}
%\ee
We know that 
\be
\bra{T_r < \infty} = \bigcup_{n > \floor{x}}\bra{T_r\leq T_n} \cap \bra{T_n <\infty} \ \ra \ \ind_{\bra{T_r \leq T_n}} \ua \ind_{\bra{T_r < \infty}} \text{ a.s.}
\ee

Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}), we have
\be
\pro\bb{T_r < T_R} \to \pro\bb{T_r < \infty} \ \ra \ \pro\bb{T_r < \infty} = \lim_{R\ua \infty} \frac{\bb{\frac 1R}^{d-2} - \bb{\frac 1{\abs{x}}}^{d-2}}{\bb{\frac 1R}^{d-2} -\bb{\frac 1r}^{d-2}} = \bb{\frac{r}{\abs{x}}}^{d-2}. \qquad (\dag\dag)
\ee
%Letting $R \to \infty$ in ($\dag$) we have that %$\pro_x(T_r < T_R)+ \pro_x \bb{T_r <\infty|T_R<T_r} \pro_x \bb{T_R < T_r} = \bb{\frac{r}{\abs{x}}}^{d-2} + 0 =$
%\be
%\pro_x (T_r < \infty) =  \bb{\frac{r}{\abs{x}}}^{d-2} \qquad (\dag\dag)
%\ee
which is the probability of ever visiting the ball centred at 0 and of radius $r$ when starting from $\abs{x}\geq r$.

Now we will show that
\be
\pro_0 \bb{\abs{B_t} \to \infty \text{ as }t\to \infty} = 1.
\ee

%{\bf Approach 1}.
%Fix $r > 0$ and define $S_1 = \inf\{t \geq 0 |\abs{B_t} \leq r\}$, and for all $k \geq 1$ define $T_k = \inf\{t \geq S_k |\abs{B_t}\geq 2r\}$ and $S_{k+1} = \inf\{t \geq T_k |\abs{B_t}\leq r\}$. Now $\{S_k < \infty\} = \{T_k < \infty\}$ because $S_k < T_k$ (giving one inclusion) and Brownian motion is unbounded (giving the other inclusion).
%\beast
%\pro_x (S_{k+1} <\infty| S_k <\infty) & = & \pro_x (S_{k+1} <\infty| T_k <\infty)\\
%& = & \pro_x (\exists t \geq T_k : \abs{B_t}\leq r| T_k <\infty)\\
%& = & \pro_x (\exists t \geq T_k : \abs{B^{(T_k)}_t + B_{T_k}} \leq r | T_k <\infty)\\
%& = & \int_{\R^3} \pro_x (B_{T_k} \in d y)\pro_y (\exists t \geq 0 : \abs{B_t} \leq r)
%\eeast
%by the Strong Markov property. But $\abs{B_{T_k}} = 2r$, so a.s. under $\pro_x (B_{T_k} \in d y)$, $\abs{y} = 2r$, whence $\pro_y (\exists t : \abs{B_t}\leq r) = \pro_y (T_r <\infty) = \frac 12$. Therefore $\pro_x (S_{k+1} < \infty| S_k < \infty) = \frac 12$. It follows that $\pro_x (S_k < \infty) = \frac 1{2^{k-1}} \pro_x (S_1 < \infty)$ given that $S_1 < \infty$. By the Borel-Cantelli Lemma, $\sup\{k |S_k < \infty\} < \infty$ a.s. for all $r$. Letting $r \to \infty$ along $\Z^+$, this shows that eventually $\abs{B_t}\geq r$ for any $r$, so $\abs{B_t}\to \infty$ a.s.
%{\bf Approach 2}. 

Since the first three components of Brownian motion in $\R^d$ are a Brownian motion in $\R^3$, it suffices to prove the result for $d = 3$. For fix $r>0$, we define $S_r = \inf\bra{t >0:\abs{B_t}=r}$ and
\be
A_n = \bra{\abs{B_t} >n \text{ for all }t\geq S_{n^3}}. 
\ee

By the unboundedness of Brownian motion (Proposition \ref{pro:brownian_motion_limit_value}), it is clear that
\be
\pro_0 (S_{n^3} < \infty) = 1.
\ee

Applying the strong Markov property (Theorem \ref{thm:strong_markov_property_brownian_motion}) at time $T_{n^3}$ we obtain
\beast
\pro_0(A_n^c) & = & \pro_0 \bb{\abs{B_{t}} \leq n \text{ for some }t\geq S_{n^3}} = \pro_0 \bb{\abs{B_{t+ S_{n^3}} - B_{S_{n^3}} + B_{S_{n^3}}} \leq n \text{ for some }t\geq 0} \\
& = & \int_{\R^3} \pro_0 \bb{\abs{B^{(S_{n^3})}_t + y} \leq n \text{ for some }t\geq 0} \pro_0 \bb{B_{S_{n^3}} \in dy}\\
& = & \int_{\R^3} \pro_y \bb{\abs{B_t} \leq n \text{ for some }t\geq 0} \pro_0 \bb{B_{S_{n^3}} \in dy} = \int_{\R^3} \pro_{n^3} \bb{\abs{B_t} \leq n \text{ for some }t\geq 0} \pro_0 \bb{B_{S_{n^3}} \in dy}\\
& = & \pro_{n^3} \bb{\abs{B_t} \leq n \text{ for some }t\geq 0} \int_{\R^3}  \pro_0 \bb{B_{S_{n^3}} \in dy} = \pro_{n^3} \bb{\abs{B_t} \leq n \text{ for some }t\geq 0}\\
& = & \pro_{n^3} \bb{T_n < \infty} = \frac n{n^3} = \frac 1{n^2}
\eeast
by ($\dag\dag$). Since the $\pro_0 (A_n^c)$ is summable, by first Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_1_probability}) we get that $A_n$ happens eventually $\pro_0$-a.s., which implies that $\abs{B_t}\to \infty$ as $t\to\infty$ $\pro_0$-a.s..
\een
\end{proof}

%\qcutline

\subsection{Conditioned Brownian motion and Brownian bridge}

Now let $B$ be a standard Brownian motion (with $B_0 = 0$ a.s.) and let $0<s<t$. What's the conditional distribution of $B_s$ given $B_t$. The next result gives a nice way of working with the relevant normal distributions.

\begin{lemma}\label{lem:brownian_bridge_independent_of_brownian_motion}
let $B$ be a standard Brownian motion and $0<s<t$. Then $B_s - s B_t/t$ is independent of $B_t$.
\end{lemma}

\begin{proof}[\bf Proof]
First we know that $B_s - s B_t/t$ and $B_t$ are Gaussian random variables. Also for any $\theta_1,\theta_2\in \R$, we have
\be
\theta_1 \bb{B_s - s B_t/t} + \theta_2 B_t = \bb{\theta_1 + \theta_2 - s \theta_1/t} B_s + (\theta_2 - s\theta_1/t) (B_t- B_s)
\ee
is still Gaussian as $B_s$ and $B_t - B_{s}$ are independent. Thus, $B_s - s B_t/t$ and $B_t$ are multivariate Gaussian random variables. So it suffices to show that $\cov\bb{B_s - sB_t/t, B_t} = 0$ by Theorem \ref{thm:multivariate_gaussian_rv_property}.(v).

To verify this, just note that
\be
\cov \bb{B_s - s B_t/t, B_t} = \cov \bb{B_s,B_t} - \frac st \cov\bb{B_t,B_t} = s\land t - \frac st t = 0.
\ee
\end{proof}

\begin{proposition}[conditioned Brownian motion]\label{pro:conditioned_brownian_motion}
let $B$ be a standard Brownian motion and $0\leq s<t$. Then for given $B_t$\footnote{Note that $B_t$ is not random variable but a value here.}
\be
B_s|_{B_t} \sim \sN\bb{\frac{sB_t}t, \frac{s(t-s)}{t}}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
To check the moment generating function of the conditioned Brownian motion $B_s|_{B_t}$. For any $\theta\in \R$,
\beast
\E\bb{\left.e^{\theta B_s}\right| B_t} & = & \E\bb{\left.e^{\theta \bb{B_s - sB_t/t}}e^{\theta sB_t/t} \right|B_t} \stackrel{\text{a.s.}}{=} e^{\theta sB_t/t}  \E\bb{\left.e^{\theta \bb{B_s - sB_t/t}}\right|B_t} \quad \text{by Proposition \ref{pro:conditional_expectation_tower_independence}.(ii)}\\
& \stackrel{\text{a.s.}}{=} & e^{\theta sB_t/t} \E\bb{e^{\theta \bb{B_s - sB_t/t}}} \quad \text{by Lemma \ref{lem:brownian_bridge_independent_of_brownian_motion} and Proposition \ref{pro:conditional_expectation_tower_independence}.(v)}\\
& = & e^{\theta sB_t/t} \E\bb{e^{\theta \bb{\frac{t-s}{t}B_s - \frac st B_{t-s}}}} = e^{\theta sB_t/t} \E\bb{e^{\frac{t-s}{t}\theta B_s}}\E\bb{e^{-\frac st \theta B_{t-s}}}\quad \text{by Proposition \ref{pro:independent_mgf}}\\
& = & e^{\theta sB_t/t} e^{\frac{\theta^2 (t-s)^2 s}{2t^2}} e^{\frac {\theta^2 s^2(t-s) }{2t^2}} = e^{\theta sB_t/t} e^{\frac{\theta^2 s(t-s)}{2t}}.
\eeast

Thus, we have the required result.
\end{proof}

\begin{proof}[\bf Alternative proof]
By Example \ref{exa:bivariate_gaussian}, we have $\rho = \sqrt{s/t}$ for $s\neq 0$,
\beast
f_{B_s|B_t}(x,B_t) & = & \frac{1}{\sqrt{2 \pi} \sqrt{s} \sqrt{1-s/t}} \exp\left( -\frac{1}{2(1-s/t)} \bb{\frac{x}{\sqrt{s}} - \frac{\sqrt{s/t} B_t}{\sqrt{t}}}^2 \right)\\
& = & \frac{1}{\sqrt{2 \pi} \sqrt{s(t-s)/t}} \exp\left( -\frac{1}{2s(t-s)/t} \bb{x - \frac{s B_t}{t}}^2 \right).
\eeast
\end{proof}

A more general proposition is given by

\begin{proposition}[conditioned Brownian motion]\label{pro:conditioned_brownian_motion_general}
let $B$ be a standard Brownian motion and $0\leq s<t<u$. Then for given $B_s$ and $B_u$,
\be
B_t|_{B_s,B_u} \sim \sN\bb{B_s + \frac{t-s}{u-s} (B_u - B_s), \frac{(t-s)(u-t)}{u-s}}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Let $\wt{B}_v := B_{s+v} - B_s$ for $v\geq 0$. Then by simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion}), we have $\wt{B}_v$ is still a standard Brownian motion. Thus,
\be
B_t|_{B_s,B_u} = \wt{B}_v + B_s|_{\wt{B}_w+ B_s, B_s} = \underbrace{\wt{B}_v|_{\wt{B}_w, B_s} + B_s \sim \wt{B}_v|_{\wt{B}_w} + B_s}_{\text{by Proposition \ref{pro:conditional_expectation_tower_independence}.(ii)}}.
\ee

Then, by Proposition \ref{pro:conditioned_brownian_motion} ($t = s+v$, $u = s+w$),
\be
\wt{B}_v|_{\wt{B}_w} \sim \sN\bb{\frac{v\wt{B}_w}w, \frac{v(w-v)}{w}} = \sN\bb{\frac{(t-s)(B_u - B_s)}{u-s}, \frac{(t-s)(u-t)}{u-s}}.
\ee

Therefore, the conditional distribution $B_t|B_s,B_u$ is as required.
\end{proof}

\begin{example}
Given $B_s = a$, $B_u = b$, the conditional distribution of $B_t|_{B_s =a,B_u=b}$ is
\be
\sN\bb{a + \frac{t-s}{u-s} (b - a), \frac{(t-s)(u-t)}{u-s}}.
\ee

Note that the variance is maximized at time $t = (s+u)/2$ with value $(u-s)/4$.
\end{example}

Also, we check the conditional joint density function,% for $0\leq s<t<u$,

\begin{lemma}\label{lem:joint_conditional_brownian_motion}
let $B$ be a standard Brownian motion and $0\leq s<t<u$. Then for given $B_u$, $B_s|B_u$ and $B_t|B_u$ are multivariate Gaussian with covariance $\frac{s(u-t)}{u}$.
%\be
%f_{B_s,B_t|B_u}(x,y|B_u) = .%\sN\bb{B_s + \frac{t-s}{u-s} (B_u - B_s), \frac{(t-s)(u-t)}{u-s}}.
%\ee
\end{lemma}

\begin{proof}[\bf Proof]
We have $w = (x,y,z)^T$ and by multivariate Gaussian distribution,
\be
f_{B_s,B_t,B_u}(x,y,z) = \frac 1{\sqrt{(2\pi)^d\abs{\Sigma}}}\exp\bb{-\frac 12 w^T \Sigma^{-1}w}
\ee
where $d = 3$ and 
\be
\Sigma = \bepm s & s & s \\ s & t & t \\ s & t & u \eepm,\qquad \abs{\Sigma} = s(t-s)(u-t),\qquad \Sigma^{-1} = \bepm
\frac{t}{s(t-s)} & - \frac 1{t-s} & 0 \\ -\frac{1}{t-s} &  \frac {u-s}{(t-s)(u-t)} & -\frac 1{u-t} \\ 0 & -\frac{1}{u-t} & \frac 1{u-t} 
\eepm
\ee

Therefore, we can have $B_s|B_u$ and $B_t|B_u$ are joint Gaussian distribution (for $f_{B_u}(B_u)\neq 0$)
\beast
f_{B_s,B_t|B_u}(x,y,B_u) & = & f_{B_s,B_t,B_u}(x,y,B_u)/f_{B_u}(B_u) \\
& = & \frac{1}{2 \pi \sigma_x \sigma_y \sqrt{1-\rho^2}} \exp\left( -\frac{1}{2(1-\rho^2)}\left[ \frac{(x-\mu_x)^2}{\sigma_x^2} + \frac{(y-\mu_y)^2}{\sigma_y^2} - \frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x \sigma_y} \right] \right)
\eeast
where 
\be
\mu_x = sB_u/u,\quad \mu_y = tB_u/u,\quad\sigma_x = \sqrt{\frac{s(u-s)}{u}},\quad \sigma_y = \sqrt{\frac{t(u-t)}{u}},\quad \rho = \sqrt{\frac{s(u-t)}{t(u-s)}}.
\ee

Then by Proposition \ref{pro:conditional_probability_density_function}, we have
\be
\cov(B_s,B_t|B_u) \stackrel{\text{a.s.}}{=} \rho \sigma_x \sigma_y = \sqrt{\frac{s(u-t)}{t(u-s)}}\sqrt{\frac{s(u-s)}{u}} \sqrt{\frac{t(u-t)}{u}} = \frac{s(u-t)}{u}.
\ee
\end{proof}

Now we define Brownian bridge:

\begin{definition}[Brownian bridge\index{Brownian bridge}]\label{def:brownian_bridge}
A Brownian bridge $X$ is a Gaussian process % (see Definition \ref{def:gaussian_process} with $M = 0$ a.s.) 
defined on $[0,T]$ and with covariance $\cov(X_s,X_t) = s(T-t)/T$ for $s\leq t\leq T$.
\end{definition}

\begin{remark}
The easiest way to find a Brownian bridge is use $X_t = B_t - tB_T/T$ where $B$ is a standard Brownian motion. We can easily calculate that for $s\leq t\leq T$,
\beast
\cov\bb{X_s,X_t} & = & \cov\bb{B_s-sB_T/T, B_t-tB_T/T} \\
& = & \E\bb{B_s-sB_T/T, B_t-tB_T/T} - \E\bb{B_s-sB_T/T}\E\bb{B_t-tB_T/T}\\
& = & s\land t - \frac tT s\land T - \frac sT t\land T + \frac{st}{T^2} T = s - ts/T = s(T - t)/T.
\eeast

Thus, $X_t$ is a continuous version of the Brownian bridge. 

Note that $X_T = 0$ a.s. when Brownian bridge is between 0 and 0 and we can denote it as $X^0_t$. If the Brownian bridge is between 0 and $x$, we can use
\be
X^x_t = B_t - t(B_T - x)/T = X^0_t + tx/T.
\ee

However, these differences do not change the covariance, so $X^x_t$ is also a Brownian bridge.
\end{remark}

\begin{proposition}
let $B$ be a standard Brownian motion ($B_0 = 0$ a.s.) and $0\leq s<t$. Then $B_s|_{B_t}$ is a Brownian bridge on $[0,t]$.%\footnote{Note that $B_t$ is a random variable here}.
\end{proposition}

\begin{proof}[\bf Proof]
Direct result from Definition \ref{def:brownian_bridge} and Lemma \ref{lem:joint_conditional_brownian_motion}.%Proposition \ref{pro:conditioned_brownian_motion}.
\end{proof}

\subsection{Quadratic variation}

\begin{lemma}\label{lem:brownian_motion_subdivision_convergence}
Let $B$ be a standard Brownian motion and let $t \geq 0$ be fixed. For $n \geq 1$, let 
\be
\Delta_n := \bra{0 = t_0(n) < t_1(n) <\dots t_{m_n}(n) = t}
\ee
be a subdivision of $[0, t]$, such that $\eta_n := \max_{1\leq i\leq m_n} \bra{t_i(n) - t_{i-1}(n)}$. Then
\be
\lim_{n\to\infty} \sum^{m_n}_{i=1} \bb{B_{t_i} - B_{t_{i-1}}}^2 = t \qquad \text{in }\sL^2(\Omega,\sF,\pro). 
\ee
\end{lemma}

\begin{proof}[\bf Proof]%To show that 
%\be
%\lim_{n\to \infty} \sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 = t\quad\text{in }\sL^2(\pro),
%\ee
To show the result, we must show that ($\sum^{m_n}_{i=1} \bb{B_{t_i} - B_{t_{i-1}}}^2\in \sL^2(\Omega,\sF,\pro)$ is obvious)
\be
\E\bb{\bb{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - t}^2} \to 0 
\ee
as $n\to \infty$. But we have (by Proposition \ref{pro:moments_gaussian})
\beast
\E\bb{\bb{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - t}^2} & = & \E\bb{\bb{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2}^2 - 2t\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 + t^2}\\
& = & \E\bb{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^4 + \sum^{m_n}_{i\neq j} (B_{t_i} - B_{t_{i-1}})^2(B_{t_j} - B_{t_{j-1}})^2 - 2t\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 + t^2}\\
& = & 3\sum^{m_n}_{i=1} \bb{t_i - t_{i-1}}^2 + \sum^{m_n}_{i\neq j} (t_i - t_{i-1})(t_j - t_{j-1}) - 2t\sum^{m_n}_{i=1} (t_i- t_{i-1}) + t^2\\
& = & 2\sum^{m_n}_{i=1} \bb{t_i - t_{i-1}}^2 + \bb{\sum^{m_n}_{i=1} \bb{t_i - t_{i-1}}}^2 - 2t^2 + t^2\\
& = & 2\sum^{m_n}_{i=1} (t_i - t_{i-1})^2 \leq 2\eta_n \sum^{m_n}_{i=1} (t_i - t_{i-1}) = 2t\eta_n \to 0
\eeast
as $n\to \infty$.
\end{proof}


\begin{definition}[quadratic variation]
Let $B$ be a standard Brownian motion and let $t \geq 0$ be fixed. For $n \geq 1$, let
\be
\bsb{B}^n_t := \sum^{\floor{2^nt} -1}_{k=0} \bb{B_{(k+1)2^{-n}} -B_{k2^{-n}}}^2
\ee
for dyadic subdivision. $\bsb{B}^n_t$ is called quadratic variation of $B$ with respect to $n$-dyadic subdivision.
\end{definition}

\begin{lemma}\label{lem:brownian_motion_dyadic_convergence}
Let $B$ be a standard Brownian motion and let $t \geq 0$ be fixed. For $n \geq 1$, %if the subdivision is dyadic,
\be
\bsb{B}^n_t \to t\quad \text{a.s..}
\ee
\end{lemma}

\begin{proof}[\bf Proof]
If the subdivision is dyadic, we can take $t_{m_n}(n) = \floor{2^nt}2^{-n}$ and have that (by Markov inequality (Theorem \ref{thm:markov_inequality_probability}))
\be
\pro\bb{\abs{\bsb{B}^n_t - \floor{2^nt}2^{-n}} \geq \ve} = \pro\bb{\abs{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - \floor{2^nt}2^{-n}} \geq \ve} \leq \frac 1{\ve^2} \E\bb{\bb{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - \floor{2^nt}2^{-n}}^2}\nonumber
\ee
by calculations in Lemma \ref{lem:brownian_motion_subdivision_convergence}, it is equal to 
\be
\frac 2{\ve^2} \sum^{m_n}_{i=1} (t_i - t_{i-1})^2 = \frac 2{\ve^2} \frac 1{2^{2n}} \bb{\floor{2^nt}2^{-n}} 2^n = \frac{2\floor{2^nt}}{\ve^2}\frac 1{2^{2n}} \leq \frac{2t}{\ve^2 2^{n}}.
\ee

Thus,
\be
\sum^\infty_{n=1} \pro\bb{\abs{\sum^{m_n}_{i=1} \bb{B_{t_i} - B_{t_{i-1}}}^2 - \floor{2^nt}2^{-n}} \geq \ve} < \infty,
\ee
hence we deduce by first Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_1_probability}) that 
\be
\pro\bb{\abs{\sum^{m_n}_{i=1} \bb{B_{t_i} - B_{t_{i-1}}}^2 - \floor{2^nt}2^{-n}} < \ve \text{ ev.}} = 1.
\ee

Hence, $\forall \ve >0$, for all $n = n(\omega)$ sufficiently large we have that 
\be
\abs{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - \floor{2^nt}2^{-n}} < \ve \text{ a.s.}
\ee

But we also have
\be
\abs{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - t} \leq \abs{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - \floor{2^nt}2^{-n}} + \abs{\floor{2^nt}2^{-n} - t} < \ve/2 + \ve/2 = \ve\quad \text{a.s..}
\ee

Thus, we have $\bsb{B}^n_t \to t$ a.s..
\end{proof}

Since Brownian motion $B$ is a special stochastic process, we can give the stronger version of its quadratic variation.

\begin{theorem}\label{thm:brownian_motion_quadratic_variation_ucas}
Let $B$ be a standard Brownian motion ($B_0 = 0$ a.s.) and let $t \geq 0$ be fixed. For $n \geq 1$, $\bb{[B]^n_t}_{t\geq 0}$ converges to $(t)_{t\geq 0}$ uniformly on compacts almost surely (u.c.a.s.), i.e.,
\be
\sup_{0\leq s\leq t}\abs{\bsb{B}^n_s - s} \to 0 \quad \text{a.s..},\qquad \text{or}\qquad \pro\bb{\lim_{n\to\infty}\sup_{0\leq s\leq t}\abs{\bsb{B}^n_s - s} = 0} = 1.
\ee
\end{theorem}

%Note that for general subdivision, 

\begin{proof}[\bf Proof]%To prove this, we only need to show that $\bsb{B}^n_t - \floor{2^nt}2^{-n}$ is a martingale and then apply Doob's maximal inequality (Theorem \ref{thm:doob_maximal_inequality_continuous}) with the same argument in Lemma \ref{lem:brownian_motion_dyadic_convergence}.
Clearly, $\bsb{B}^n_t - \floor{2^nt}2^{-n}$ is adapted and integrable. From Lemma \ref{lem:product_martingale_minus_cross_difference_is_martingale_discrete}, we know that
\be
B_{\floor{2^nt}2^{-n}}^2 - [B]^n_t \quad \text{is a martingale.} 
\ee

But we know that $\bb{B_{\floor{2^nt}2^{-n}}^2 - \floor{2^nt}2^{-n}}_{t\geq 0}$ is martingale as $B_t^2 -t$ is a martingale (by Theorem \ref{thm:brownian_motion_martingale}.(ii)). Thus, we have that $\bsb{B}^n_t - \floor{2^nt}2^{-n}$ is actually a martingale. Then by Doob's maximal inequality (Theorem \ref{thm:doob_maximal_inequality_continuous}), we have
\be
\pro\bb{\sup_{0\leq s\leq t}\abs{\bsb{B}^n_s - \floor{2^ns}2^{-n}} \geq \ve} \leq \frac 1{\ve^2}\E\bb{\bb{\bsb{B}^n_t - \floor{2^nt}2^{-n}}^2}.
\ee

Then with the same argument in Lemma \ref{lem:brownian_motion_dyadic_convergence}, we have $\forall \ve >0$, for all $n = n(\omega)$ sufficiently large we have that 
\be
\sup_{0\leq s\leq t}\abs{[B]^n_s - \floor{2^ns}2^{-n}} < \ve \text{ a.s.}
\ee

But we also have
\be
\sup_{0\leq s\leq t}\abs{[B]^n_s - s} \leq \sup_{0\leq s\leq t}\abs{[B]^n_s - \floor{2^ns}2^{-n}} + \sup_{0\leq s\leq t}\abs{\floor{2^ns}2^{-n} - s} < \ve/2 + \ve/2 = \ve\quad \text{a.s..}
\ee

Thus, we have $\bb{\bsb{B}^n_t}_{t\geq 0} \to (t)_{t\geq 0}$ u.c.a.s.. %the required result.
\end{proof}


\section{Problems}

\subsection{Brownian motion}

\begin{problem}[Zhou\cite{Zhou_2008}.$P_{130}$]
Let $(B_t)_{t\geq 0}$ be a Brownian motion with $B_t \sim \sN(0,\sigma^2 t)$. For $t_1< t_2$, what's the probability that $B_{t_1} >0$ and $B_{t_2} < 0$?

In particular, what's the probability that $B_1 > 0$ and $B_2 < 0$?
\end{problem}

\begin{solution}[\bf Solution.]
Since $B_{t_2} - B_{t_1}$ is independent of $B_{t_1}$,
\beast
\pro\bb{B_{t_1}>0,B_{t_2} < 0} & = & \pro\bb{B_{t_1}>0,B_{t_2} - B_{t_1} < -B_{t_1}} = \pro\bb{B_{t_1}>0} \pro\bb{B_{t_2-t_1} < -B_{t_1}|B_{t_1}}\\
& = & \int^\infty_0 \frac 1{\sqrt{2\pi t_1}\sigma} e^{-\frac{x^2}{2t_1\sigma^2}} \int^{-x}_{-\infty}\frac 1{\sqrt{2\pi (t_2-t_1)}\sigma} e^{-\frac{y^2}{2(t_2-t_1)\sigma^2}} dy dx\\%& = & \int^\infty_{a/(\sigma\sqrt{t_1})} \frac 1{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \int^{(b-x\sigma\sqrt{t_1})/\sigma\sqrt{t_2-t_1}}_{-\infty}\frac 1{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy dx\\
& = & \int^\infty_{0} \frac 1{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \int^{(-x\sigma\sqrt{t_1})/\sigma\sqrt{t_2-t_1}}_{-\infty}\frac 1{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy dx
\eeast

Thus, we converse it to polar coordinate
\be
\pro\bb{B_{t_1}>0,B_{t_2} < 0} = \frac 1{2\pi}\int^{-\arctan \sqrt{\frac{t_1}{t_2-t_1}}}_{-\frac{\pi}{2}} d\theta = \frac 1{2\pi}\arctan\sqrt{\frac{t_2-t_1}{t_1}}.\quad (*)
\ee

If $t_1=1$ and $t_2 = 2$, we have $B_1 \sim (B_2 - B_1)$. Therefore,
\beast
\pro\bb{B_1 >0,B_2 < 0} & = & \pro\bb{B_1 >0, B_2 - B_1 < -B_1} = \pro\bb{B_1>0,\abs{B_2-B_1} > \abs{B_1}, B_2 - B_1 < 0} \\
& = & \pro\bb{B_1>0} \pro\bb{\abs{B_2-B_1} > \abs{B_1}, B_2 - B_1 < 0|B_1>0} \\
& = & \pro\bb{B_1>0} \pro\bb{B_2 - B_1 <0} \pro\bb{\abs{B_2-B_1} > \abs{B_1} |B_1>0,B_2 - B_1 <0} = \frac 12 \frac 12 \frac 12 = \frac 18.
\eeast
which is consistent with ($*$).
\end{solution}

\begin{problem}
Define $R = \inf\bra{t \geq 1 : B_t = 0}$ and let $L = \sup\bra{t \leq 1 : B_t = 0}$.
\ben
\item [(a)] Show that $L$ is not a stopping time. Is $R$ a stopping time?
\item [(b)] Show that 
\be
\pro(R > 1 + u) = 1 - \frac 2{\pi}\arctan(\sqrt{u}).
\ee
%(it is recalled that the quotient of two independent standard Gaussian random variables has the Cauchy distribution).
\item [(c)] Deduce that $L$ has the arcsince law\footnote{need citation of P.Levy, see Ito \& McKean $P_28$}:
\be
\pro(L < t) = \int^t_0 \frac{dx}{\pi \sqrt{x(1 - x)}} = \frac{2}{\pi}\arcsin\sqrt{t}.
\ee
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(a)] $L$ is not a stopping time, because the event $\bra{L>t}$ depends only on $(B_{s+t})_{s\geq 0}$ so it not in $\sF_{t}^B$.

$R$ is a stopping time, because for $t< 1$, $\bra{R\leq t} = \emptyset \in \sF_t^B$. For $t\geq 1$, we have
\be
\bra{R\leq t} = \bra{B_1 >0, \inf_{1\leq s\leq t} B_s < 0} \cup \bra{B_1 < 0 , \sup_{1\leq s\leq t} B_s > 0} \cup \bra{B_1 = 0} \in \sF_t^B.
\ee

\item [(b)] We have $R = \inf\bra{t\geq 1: B_t = 0} = 1 + \inf\bra{t\geq 0: B_{t+1} = 0} = 1+ \inf\bra{t\geq 0:B_{t+1} - B_1 = - B_1}$.

Thus, $R-1 = \inf\bra{t\geq 0:B_{t+1} - B_1 = - B_1} = \inf\bra{t\geq 0:B'_t = - B_1}$. Then by the simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion}), we have that $(B_t' = B_{t+1}-B_1)_{t\geq 0}$ is a standard Brownian motion independent of $B_1$. %We also have (by tower property)
%\be
%\pro\bb{R-1 > u} = \E \bb{\pro\bb{R-1>u |B_1}}.
%\ee
If we know $B_1$ then $R-1$ is the first hitting time of $-B_1$, which we know that $R-1$ has the same distribution as $\bb{\frac{B_1}{B_1'}}^2$ by Proposition \ref{pro:stopping_time_brownian_motion_touch_special_point}. Thus, by tower property (Proposition \ref{pro:conditional_expectation_tower_independence}),
\be
\pro\bb{R-1 > u} = \E\bb{\left.\pro\bb{R-1 > u}\right|B_1} = \E\bb{\left.\pro\bb{\bb{\frac{B_1}{B_1'}}^2 > u}\right|B_1}  = \pro\bb{\bb{\frac{B_1}{B_1'}}^2>u } %\E \bb{\pro\bb{\left.\bb{\frac{B_1}{B_1'}}^2>u \right|B_1}} = \E \bb{\pro\bb{\left.\bb{\frac{X}{X'}}^2>u \right|X}}
\ee
where $B_1$ and $B'_1$ are two independent Gaussian with zero mean and variance 1. Thus, $B_1/B'_1$ is Cauchy distributed, $\pro(B_1/B'_1 \leq k) = \frac 12 + \frac 1{\pi}\arctan k$ (Proposition \ref{pro:two_independent_standard_gaussian_quotient_implies_cauchy}),
\beast
\pro\bb{R-1 > u} & = & \pro\bb{\abs{\frac{B_1}{B'_1}}>\sqrt{u}}  = \pro\bb{\frac{B_1}{B'_1}>\sqrt{u}} + \pro\bb{\frac{B_1}{B'_1} < -\sqrt{u}} \\
& = & 1 - \bb{\frac 12 + \frac 1{\pi}\arctan k} + \frac 12 + \frac 1{\pi}\arctan\bb{-\sqrt{u}} = 1- \frac 2{\pi} \arctan\sqrt{u}.
\eeast
\item [(c)] We have that %$\pro(R-1 > u) = 1 - \$
\be
R^{-1} = \sup\bra{\frac 1t:t\geq 1, B_t = 0} = \sup\bra{u\leq 1: B_{1/u} = 0} = \sup\bra{u\leq 1: uB_{1/u} = 0}.
\ee

But we know that $\bb{uB_{1/u}}_{u\geq 0}$ has the same law as $(B_u)_{u\geq 0}$, so $R^{-1} \sim L$. Thus,
\be
\pro\bb{L < t} = \pro\bb{R > \frac 1t} = \pro\bb{R > 1 + \frac{1-t}{t}} = 1- \frac 2{\pi} \arctan\sqrt{\frac{1-t}{t}} 
%& = & 2{\pi} \arctan\sqrt{\frac{1-t}{t}}
\ee

Taking differentiation wrt $t$ we have
\be
\bb{1- \frac 2{\pi} \arctan\sqrt{\frac{1-t}{t}} }' = - \frac{1}{1+ \frac{1-t}{t}} \bb{\bb{\frac{1-t}{t}}^{1/2}}' = -t \bb{\frac{1-t}{t}}^{-1/2} \frac{-t - (1-t)}{t^2} = \frac 1{\pi\sqrt{t(1-t)}}
\ee
as required.
\een
\end{solution}
