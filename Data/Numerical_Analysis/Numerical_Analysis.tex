\chapter{Numerical Analysis}

\section{Least square fitting of polynomials}

Suppose we want to use $m$ distinct points $(X,Y)$ (where $X = (x_1,\dots,x_m)^T$, $Y = (y_1,\dots,y_m)^T$) to fit a $n$th degree polynomials
\be
y = a_0 + a_1 x + \dots + a_n x^n
\ee
with $m\geq n+1$ and $A = (a_0,\dots,a_n)^T$. The residual is given by
\be
R^2 = \sum^m_{i=1} \bb{y_i - (a_0 + a_1x_i + \dots + a_n x_i^n)}^2.
\ee

Then partial derivatives are
\beast
0 & = & \fp{R^2}{a_0} = -2\sum^m_{i=1} \bb{y_i - (a_0 + a_1x_i + \dots + a_n x_i^n)}, \\
0 & = & \fp{R^2}{a_1} = -2\sum^m_{i=1} \bb{y_i - (a_0 + a_1x_i + \dots + a_n x_i^n)}x_i, \\
& \vdots & \\
0 & = & \fp{R^2}{a_n} = -2\sum^m_{i=1} \bb{y_i - (a_0 + a_1x_i + \dots + a_n x_i^n)}x_i^n.
\eeast

These lead to the equations
\beast
\sum^m_{i=1} y_i & = &  a_0 m + a_1 \sum^m_{i=1} x_i + \dots + a_n \sum^m_{i=1} x_i^n \\
\sum^m_{i=1} x_i y_i & = &  a_0 \sum^m_{i=1} x_i + a_1 \sum^m_{i=1} x_i^2 + \dots + a_n \sum^m_{i=1} x_i^{n+1} \\
& \vdots & \\
\sum^m_{i=1} x_i^n y_i & = &  a_0 \sum^m_{i=1} x_i^n + a_1 \sum^m_{i=1} x_i^{n+1} + \dots + a_n \sum^m_{i=1} x_i^{2n}.
\eeast

Or we can write it in matrix form
\beast
\bepm
m &  \sum^m_{i=1} x_i & \dots & \sum^m_{i=1} x_i^n \\
\sum^m_{i=1} x_i &  \sum^m_{i=1} x_i^2 & \dots & \sum^m_{i=1} x_i^{n+1} \\
& & \ddots & \\
\sum^m_{i=1} x_i^n &  \sum^m_{i=1} x_i^{n+1} & \dots & \sum^m_{i=1} x_i^{2n}
\eepm \bepm a_0 \\ a_1 \\ \vdots \\ a_n\eepm = \bepm \sum^m_{i=1} y_i \\ \sum^m_{i=1} x_i y_i \\ \vdots \\ \sum^m_{i=1} x_i^n y_i \eepm
\eeast

Let $V\in M_{m,n+1}(\F)$ be a Vandermonde matrix
\be
V = \bepm
1 & x_1 & x_1^2 & \dots & x_1^n \\
1 & x_2 & x_2^2 & \dots & x_2^n \\
& & & \ddots & \\
1 & x_m & x_m^2 & \dots & x_m^n
\eepm
\ee
such that
\be
\bepm \sum^m_{i=1} y_i \\ \sum^m_{i=1} x_i y_i \\ \vdots \\ \sum^m_{i=1} x_i^n y_i \eepm  =
\bepm
1 & 1 & \dots & 1 \\
x_1 & x_2 & \dots & x_m \\
x_1^2 & x_2^2 & \dots & x_m^2 \\
& & \ddots &  \\
x_1^n & x_2^n & \dots & x_m^n
\eepm  \bepm  y_1 \\ y_2 \\ \vdots \\ y_m \eepm = V^T \bepm  y_1 \\ y_2 \\ \vdots \\ y_m \eepm = V^T Y
\ee
and
\be
\bepm
m &  \sum^m_{i=1} x_i & \dots & \sum^m_{i=1} x_i^n \\
\sum^m_{i=1} x_i &  \sum^m_{i=1} x_i^2 & \dots & \sum^m_{i=1} x_i^{n+1} \\
& & \ddots & \\
\sum^m_{i=1} x_i^n &  \sum^m_{i=1} x_i^{n+1} & \dots & \sum^m_{i=1} x_i^{2n}
\eepm \bepm a_0 \\ a_1 \\ \vdots \\ a_n\eepm =
\bepm
1 & 1 & \dots & 1 \\
x_1 & x_2 & \dots & x_m \\
x_1^2 & x_2^2 & \dots & x_m^2 \\
& & \ddots &  \\
x_1^n & x_2^n & \dots & x_m^n
\eepm
\bepm
1 & x_1 & x_1^2 & \dots & x_1^n \\
1 & x_2 & x_2^2 & \dots & x_2^n \\
& & & \ddots & \\
1 & x_m & x_m^2 & \dots & x_m^n
\eepm
\bepm a_0 \\ a_1 \\ \vdots \\ a_n\eepm = V^TV A.
\ee

Since $m\geq n+1$ and $x_i$ are distinct, $V^TV$ is full rank and therefore its inverse exists. Hence
\be
V^T V A = V^T Y  \ \ra\ A = \bb{V^TV}^{-1} V^T Y.
\ee
