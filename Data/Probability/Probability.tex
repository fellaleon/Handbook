\chapter{Probability}

\section{Basic Concepts}

\subsection{Sample spaces, events and probability}

\begin{definition}[sample space, observation]
We will deal with a experiment which has a ``random'' outcome, the possible outcomes being, say, $\omega_1,\omega_2,\dots$. The set of all possible outcomes $\Omega = \{\omega_1,\omega_2,\dots\}$ is
the sample space\index{sample space} of the experiment. A particular point $\omega \in \Omega$ is an observation\index{observation}.
\end{definition}

\begin{example}\label{exa:sample_space}
We have the following experiments
\ben
\item [(i)] Tossing a normal six-faced die, $\Omega = \{1, 2,\dots, 6\}$.
\item [(ii)] Picking a card from a standard pack, $\Omega$ is the pack of 52 cards.
\item [(iii)] Administer a drug to 20 patients. For simplicity assume there are only two possible outcomes for each patient, $R$ if the patient recovers, $D$ otherwise. Then
\be
\Omega = \{(i_1,\dots,i_{20}): i_j = R \text{ or }D\}
\ee
where $i_j$ is $R$ or $D$ according as patient $j$ recovers or not, so  has $2^{20}$ points.
\item [(iv)] Pick a point from the unit interval, $[0,1]$, then $\Omega = [0,1]$.
\een
\end{example}

All but (iv) are examples of discrete sample spaces.




\begin{definition}[event]
A subset A of $\Omega$, $A\subset \Omega$ is called an event\index{event}. If the experiment is performed with outcome $\omega$ and $\omega \in A$, the event A is said to occur\index{occur}.
\end{definition}

\begin{remark}
In Example \ref{exa:sample_space} (i), $A$ might be the event that "the outcome is even", so that $A = \{2,4,6\}$, in which case $A$ occurs if one of 2,4,6 is shown on the die.
\end{remark}

\begin{example}
Let $A,B,C$ be three events. we express the following in symbols:
\ben
\item [(i)] only $A$ occurs, $A\cap(B\cup C)^c=A\cap B^c\cap C^c$.
\item [(ii)] all three events occur, $A\cap B\cap C$.
\item [(iii)] at least one event occurs, $A\cup B\cup C$.
\item [(iv)] one and only one event occurs, $(A\cap(B\cup C)^c) \cup (B\cap(A\cup C)^c) \cup (C\cap(A\cup B)^c)$.
\item [(v)] no event occurs, $(A \cup B \cup C)^c = A^c \cap B^c \cap C^c$.
\item [(vi)] not more than two events occurs, $(A \cap B \cap C)^c = A^c \cup B^c \cup C^c$.
\een
\end{example}


\begin{definition}\label{def:probability_space}
Let $(E,\sE,\mu)$ be a measure space (Definition \ref{def:measure}). If $\mu(E)=1$, the measure space is a probability space\index{probability space}. Usually the probability space is denoted as
$(\Omega, \sF, \pro)$, wihch has the following interpretation: \bit
\item $\Omega$ is the sample space, the set of possible outcomes,
\item $\sF$ is the set of observable sets of outcomes, or events $A_i$,
\item $\pro(A)$ is the probability of the event $A\in \sF\ (A\subseteq \Omega)$.
\eit
\end{definition}

\begin{definition}
Furthermore, certain set-theoretic notions have special interpretations in probability.

\bit
\item The complement\index{complement} in $\Omega$ of the event $A$, $A^c$ is the event "not $A$", and occurs if and only if $A$ does not occur.
\item The union\index{union} $A \cup B$ of two events $A$ and $B$ is the event "at least one of $A$ or $B$ occurs".
\item The intersection\index{intersection} $A \cap B$ is the event "both $A$ and $B$ occur".
\item The inclusion relation\index{inclusion} $A \subseteq B$ means "the occurrence of $A$ implies the occurrence of $B$".
\item Events $A$ and $B$ are said to be mutually exclusive\index{mutually exclusive} if they are disjoint, $A \cap B = \emptyset$, and so both cannot occur together.
\eit
\end{definition}

\begin{definition}[almost surely event]\label{def:almost_surely_event}
Let $(\Omega, \sF, \pro)$ be a probability space.

Then $A \in \sF$ occurs almost surely\index{almost surely!set} (often abbreviated a.s.) if $\pro(A) = 1$. Equivalently, $A$ happens almost surely if the probability of $A$ not occurring is zero: $\pro\brb{A^c} = 0$.

More generally, any event $A$ (not necessarily in $\sF$) happens almost surely if $A^c$ is a null set (a subset of some $N\in \sF$ such that $\pro(N)=0$. see Defintion \ref{def:null_set_measure})
\end{definition}


\begin{example}[Classical probability]
Classical probability was concerned with modelling situations where there are just a finite number of possible outcomes of the experiment and each of these outcomes is "equally likely". For example,
tossing a fair coin or an unloaded die, or picking a card from a standard well-shuffled pack. Here $\Omega$ is a finite set with $N$ points, say, $\Omega = \{\omega_1,\dots,\omega_N\}$, and attached
to each event $A \subseteq \Omega$ is a measure of how "likely" the event $A$ is to occur.

Here we take $\pro(A)$, the probability of the event $A$ to be
\be
\pro(A) = \frac{N_A}N = \frac{\text{number of points in }A}{\text{number of points in }\Omega}
\ee
where $N_A$ is the number of points in $A$. Thus the probability $\pro(\cdot)$ is just a real-valued function defined on subsets $A$ of $\Omega$.
\end{example}

\begin{example}
Consider $\Omega = [0, 2\pi)$, $\sF$ the Borel $\sigma$-algebra, and $\pro(A) = \mu(A)/2\pi$ where $\mu$ is the Lebesgue measure. Then $(\Omega, \sF, \pro)$ may serve as a probability space for
choosing a point uniformly at random on the unit circle. e.g., if $A = [0, 2\pi) \bs\Q$ then $\pro(A) = 1$ so almost surely, the angle of the point is irrational.
\end{example}

\begin{example}
Infinite coin-tossing. Let
\be
\Omega = \{0, 1\}^\N = \{\omega = (\omega_1, \dots)\text{ with }\omega_n \in \{0, 1\} \text{ for all }n \geq 1\}.
\ee

We interpret the event $\{\omega_n = 1\}$ as the event that the $n$th toss results in a heads. We take for the $\sigma$-algebra $\sF$ the $\sigma$-algebra generated by events of the form
\be
A = \{\omega_1 = \ve_1, \dots, \omega_n = \ve_n\}
\ee
for any fixed sequence $(\ve_1, \dots) \in \{0, 1\}^\N$. We will soon define a probability measure $\pro$ on $(\Omega, \sF)$ under which the outcomes of the tosses are independent.
\end{example}


\subsection{Basic Properties of probability}

Recall Definitions \ref{def:set_function}, \ref{def:increasing_set_function}, \ref{def:additive_set_function}, \ref{def:countably_additive_set_function}, \ref{def:measure},
\ref{def:probability_space} and Proposition \ref{pro:measure_property} we have the following properties:

\begin{proposition}\label{pro:probability_property}
Let $(\Omega,\sF,\pro)$ be a measure space, $\forall A,B\in \sF$,
\ben
\item [(i)] $\pro(\emptyset) = 0$, $\pro(\Omega) = 1$.
\item [(ii)] $0 \leq \pro(A) \leq 1$, $\pro(A^c) = 1 - \pro(A)$.
\item [(iii)] If $A \cap B = \emptyset$, then $\pro(A \cup B) = \pro(A) + \pro(B)$. If $A_n\in \sF$, $n\in\N$ are disjoint, $\pro\brb{\bigcup\limits_n A_n} = \sum\limits_n\pro\brb{A_n}$.
\item [(iv)] If $A \subseteq B$ then $\pro(A) \leq \pro(B)$.
\item [(v)] $\pro(A \cup B) = \pro(A) + \pro(B) - \pro(A \cap B) \leq \pro(A) + \pro(B)$.
\item [(vi)] (inclusion-exclusion formula\index{inclusion-exclusion formula!probability}.) $\forall A_n\in \sF$, $n\in\N$, \be\label{equ:inclusion_exclusion_probability} \pro\brb{\bigcup^n_{i=1}A_i}
    = \sum^n_{i=1} \pro(A_i) - \sum_{1\leq i_1<i_2 \leq n} \pro(A_{i_1} \cap A_{i_2}) + \sum_{1\leq i_1<i_2<i_3\leq n} \pro(A_{i_1} \cap A_{i_2} \cap A_{i_3}) - \dots + (-1)^{n-1}
    \pro\brb{\bigcap^n_{i=1} A_i}. \ee
\item [(vii)] $\pro\brb{\bigcup_n A_n} \leq \sum_n \pro(A_n)$.
\een
\end{proposition}



\begin{example}
We now consider that how many of the number 1,...,500 are not divisible by 7 but are divisible by 3 or 5.

Let $A$ be the event that number $1\sim 500$ that divisible by 7, $B$ be the event that the number divisible by 3, $C$ be the event that the number divisible by 5. so
\beast
A & = &\{7,14,\dots,497\},|A|=71, \\
B & = &\{3,6,\dots,498\},|B|=166, \\
C & = &\{5,10,\dots,500\},|C|=100, \\
A\cap B & = &\{21,42,\dots,483\},|A\cap B|=23\\
A\cap C & = &\{35,70,\dots,490\},|A\cap C|=14 \\
B\cap C & = &\{15,30,\dots,495\},|B\cap C|=33 \\
A\cap B\cap C & = &\{105,210,\dots,420\},|A\cap B\cap C|=4
\eeast
so by inclusion-exclusion formula (Proposition \ref{pro:probability_property}.(vi)), we have $166 + 100 - 33 -23 - 14 + 4 = 200$.
\end{example}




\begin{example}
What is the probability that in a round of Bridge at least one player holds exactly two aces and two kings? Let $\Omega$ be the sample space of all possible (unordered) hands for 4 players, then the
number of points in $\Omega$ is $N = \binom{52}{13\ 13\ 13\ 13}$. For each player $i$, $i = 1, 2,3,4$, let $A_i$ be the event that player $i$ holds exactly two aces and two kings. Then \be \pro
(A_i) = \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9} \binom{39}{13\ 13\ 13}}{\binom{52}{13\ 13\ 13\ 13}} = \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9}}{\binom{52}{13}}. \ee think of choosing the
aces for player $i$, then the kings and then 9 other cards and finally distributing the remaining 39 cards among the other three players. If $i \neq j$, then \be \pro (A_i \cap A_j) =
\frac{\binom{4}{2}\binom{4}{2} \binom{44}{9\ 9\ 26} \binom{26}{13}}{\binom{52}{13\ 13\ 13\ 13}} = \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9\ 9\ 26}}{\binom{52}{13\ 13\ 26}}, \ee as before, think
of picking the aces and kings for player $i$ with the remaining aces and kings going to player $j$, and then picking 9 other cards for each of $i$ and $j$. If $i, j, k$ are unequal then $A_i \cap
A_j \cap A_k = \emptyset$, so that by inclusion exclusion we have the required probability is \be \pro \brb{\bigcup^4_{i=1} A_i} = 4\times \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9}
}{\binom{52}{13}} - \binom{4}{2}\times \frac{\binom{4}{2}\binom{4}{2} \binom{44}{9\ 9\ 26}}{\binom{52}{13\ 13\ 26}}. \ee
\end{example}

\begin{example}
Suppose that $n$ students leave their $n$ coats outside a lecture room and when they leave they pick up their coats at random. What is the probability that at least one student has his own coat? Let
consist of all permutations of $1,\dots,n$, so that if $(i_1,\dots,i_n) \in \Omega$ then $i_j$ is the index of the coat got by student $j$. Denote by $A_i$ the event that student $i$ gets his own
coat. Then for $i_1 < i_2 < \dots < i_r$, the probability that all of the students $i_1,\dots, i_r$ get their own coats is
\be
\pro\brb{\bigcap^r_{k=1} A_{ik}} = \frac{(n - r)!}{n!},
\ee
since the numerator is the number of ways that the remaining $n-r$ coats may be permuted among the remaining $n- r$ students. Then by inclusion-exclusion we have
\be
\pro\brb{\bigcup^n_{i=1} A_i} =m\sum^n_{r=1} \left[(-1)^{r-1} \sum_{i_1<\dots<i_r} \pro\brb{\bigcap^r_{k=1} A_{ik}}\right] = \sum^n_{r=1} (-1)^{r-1} \binom{n}{r} \frac{(n - r)!}{n !} = \sum^n_{r=1} (-1)^{r-1} \frac 1{r!},
\ee
so that for large $n$ the probability is approximately equal to
\be
1 - \frac 1{2 !} + \frac 1{3 !} - \frac 1{4 !} + \dots = 1 - e^{-1} \approx 0.632.
\ee
\end{example}










\begin{corollary}[Bonferroni's inequalities\index{Bonferroni's inequalities}]
For any events $A_1,A_2,\dots,A_n$ and for any $r$, $1 \leq r \leq n$,
\beast
\pro\brb{\bigcup^n_{i=1} A_i} \ba{c}
\leq \\
\text{ or }\\
\geq \ea \sum_i \pro(A_i) - \sum_{i_1<i_2} \pro(A_{i_1} \cap A_{i_2}) + \sum_{i_1<i_2<i_3} \pro(A_{i_1} \cap A_{i_2} \cap A_{i_3}) - \dots + (-1)^{r-1} \sum_{i_1<\dots<i_r} \pro(A_{i_1} \cap \dots
\cap A_{i_r}), \eeast according as $r$ is odd or even. That is, if the sum in the inclusion-exclusion formula is truncated after $r$ terms it overestimates or underestimates the probability of the
union of the $n$ events according as $r$ is odd or even.
\end{corollary}

\begin{proof}[\bf Proof]
The proof proceeds by induction on $n$. Assume it is true for $n$, then for $n + 1$ events it is true when $r = n + 1$, by the inclusion-exclusion formula, and for $r \leq n$, apply the inductive
hypothesis to probability of the two unions of $n$ events in relation (\ref{equ:inclusion_exclusion_probability}) to get the result.
\end{proof}





\subsection{Absolutely measurable sets}

\begin{definition}
Let $(\Omega,\sF)$ be a measurable space. Then we define
\be
\ol{\sF} := \bigcap_{\pro} \sF^{\pro}
\ee
which is taken over all probability measures $\pro$ on $(\Omega,\sF)$. Note that $\sF^{\pro}$ is the completion of $\sF$ with respect to $\pro$.

The system $\ol{\sF}$ is a $\sigma$-algebra whose sets are said to be absolutely measurable sets in the measurable space $\brb{\Omega,\sF}$.
\end{definition}





\subsection{Combinatorial analysis}

Fundamental rule: There are $r$ multiple choices to be made in sequence: there are $m_1$ possibilities for the first choice, after making the first choice there are $m_2$ possibilities for the
second choice, after making the first two choices there are $m_3$ possibilities for the third choice, and so on until after making the first $r-1$ choices there are $m_r$ possibilities for the $r$th
choice. Then the total number of different possibilities for the set of choices is $m_1m_2 \dots m_r$.

\begin{example}
A restaurant menu has 6 starters, 7 main courses and 6 puddings. The total number of different three-course meals that may be served is $6 \times 7 \times 6 = 252$.
\end{example}

\begin{example}[Sampling models]
Many of the standard calculations that arise in classical probability where outcomes need to be counted may be put in a standard framework of sampling\index{sampling!probability}. Think of drawing
$m$ balls from an urn which initially contains n distinguishable balls (they are numbered 1 to $n$, say). This may be done in a number of ways: \ben
\item [(i)] {\bf Sampling with replacement and with ordering.} The balls are replaced between successive draws and the order in which balls are drawn is noted. Then the Fundamental Rule shows that
    there are $n^m$ possible ways.
\item [(ii)] {\bf Sampling without replacement and with ordering ($m \leq n$).} The balls are not replaced after drawing and the order is noted. Then the number of ways is \be n (n- 1) \dots (n - m
    + 1) = \frac{n!}{(n - m)!} = P^n_m. \ee The symbol $P^n_m$ represents the number of permutations of $n$ objects $m$ at a time. An important special case occurs when $n = m$. We get the number of
    permutations of $n$ distinguishable objects (that is the number of distinguishable ways they may be laid out in a line, say) is $n!$.

\item [(iii)] {\bf Sampling without replacement and without ordering.} If we take $m$ balls from $n$ and they were ordered there would be $n(n - 1) \dots (n - m + 1) = n!/(n - m)!$ ways but each
    unordered selection may be permuted in $m!$ different ways (or give $m!$ ordered arrangements) so that the total number of unordered ways is \be \frac{n!}{(n - m)!m!} = \binom{n}{m} = C^n_m. \ee

The symbol $\binom{n}{m}$ is the binomial coe��cient, usually read "$n$ choose $m$", and represents the number of ways of picking $m$ objects from $n$, note that $\binom{n}{m}$ is the coefficient of
$x^m$ in the expansion of $(1 + x)^n$ using the binomial theorem.

Suppose that, of the $n$ balls, $m_1$ are of colour 1, $m_2$ are of colour 2 and so on up to $m_k$ balls of colour $k$, where $n = m_1 + \dots + m_k$. Consider the number of permutations of the $n$
balls when the balls are distinguishable only by colour. For example, if $n = 4$ and there are two black balls and two white balls then there are 6 possible arrangements: \be BBWW\quad BWBW\quad
BWWB\quad WBBW\quad WBWB\quad WWBB \ee

If the balls are distinguishable then there are $n!$ permutations, but within colour $i$ there are $m_i!$ ways of permuting the balls so the Fundamental Rule gives that for each distinguishable
arrangement there are $\prod^k_{i=1} (m_i!)$ ways of permuting the balls leaving the arrangement the same if the balls are distinguishable only by colour. Thus the number of arrangements
distinguishable only by colour is \be \frac{n!}{m_1!m_2! \dots m_k!} = \binom{n}{m_1 \dots m_k}, \ee which is the multinomial coefficient (the coefficient of $x^{m_1}_1 x^{m_2}_2 \dots x^{m_k}_k$ in
the expansion of $(x_1 + \dots + x_k)^n$). An alternative way of seeing this is to think of first choosing the positions for the balls of colour 1, which can be done in $\binom{n}{m_1}$ ways, then
choose the positions for the balls of colour 2 in $\binom{n- m_1}{m_2}$ ways and so on to see that the total number of ways, using the Fundamental Rule, is \be \binom{n}{m_1}\binom{n -m_1}{m_2}
\binom{n -m_1 -m_2}{m_3} \cdots \binom{n - m_1 - \dots - m_{k-1}}{m_k} = \binom{n}{m_1 \dots m_k}. \ee

\item [(iv)] {\bf Sampling with replacement and without ordering.} Draw $m$ balls one after another, each time noting the number of the ball and replacing it before the next draw,
\begin{center}
\begin{tabular}{lcc c|c|c|c|c|c}
Ball No. & & & 1 & 2 & 3 & 4 & \dots & $n$\\
Times drawn & & & $\surd$ & $\surd\surd$ & $\surd$ & $\surd\surd$ & \dots & $\surd$\\
\end{tabular}
\end{center}

so that the number of $\surd$ is the number of times the ball is drawn. The number of ways is then the number of ways that $n - 1$ vertical lines may be put between $m$ $\surd$ which is
\be
\binom{n + m -1}{n - 1}.
\ee
think of choosing $n-1$ slots for the vertical lines out of $n+m-1$ slots and then the checks go in the remainder.
\een
\end{example}

\begin{example}[Allocation models]
An alternative way of thinking of these counting schemes is to think about allocating $m$ tokens (labelled $1,\dots,m)$ to $n$ boxes (labelled $1,\dots, n$). The four cases considered previously
correspond to: \ben
\item [(i)] Each box may contain any number of tokens and the labels on the tokens are observed.
\item [(ii)] No box may contain more than one token and the labels on the tokens are observed.
\item [(iii)] No box may contain more than one token and the labels on the the tokens are not observed.
\item [(iv)] Each box may contain any number of tokens and the labels on the tokens are not observed. \een In the next examples in which each outcome in the probability space is equally likely we
    will calculate the probability of the event $A$ by computing the number of points $N$ in the sample space $\Omega$ and then computing the number $N_A$ of points in $A$. In each case the
    probability will then be \be \pro(A) = \frac{N_A}N \ee
\end{example}

\begin{example}
What is the probability that a Poker hand shows five different face values? (A poker hand contains 5 cards.) One way to do this is to think of  consisting of all possible poker hands, that is,
unordered sets of five cards chosen from a standard pack of 52. We have $N = \binom{52}{5}$ and $A$ consists of those hands showing five different face values so that \be N_A = \binom{13}{5} \times
4^5 \ee

since we may think of first choosing the unordered sets of 5 face values and then the different suits for each value, and use the Fundamental Rule. An alternative way in this problem, and in other
situations involving sampling without replacement, is to think of comprising all ordered sets of 5 cards (imagine the cards being dealt in sequence). Then $N = 52\times 51\times 50\times 49 \times
48$ and $N_A = 52\times 48\times 44\times 40\times 36$, thinking of the possible choices for the first card, second card in the hand and so on to give 5 different face values. You should check that
these two different approaches give the same probabilities. Either approach is fine, but remember to be consistent - if your sample space has unordered (respectively, ordered) points then the points
in A must be unordered (respectively, ordered).
\end{example}


\begin{example}
What is the probability that a Bridge hand (13 cards) contains 5 spades ($\spadesuit$), 3 hearts ($\heartsuit$), 4 diamonds ($\diamondsuit$) and 1 club ($\clubsuit$)? Take $\Omega$ to be the set of
unordered Bridge hands, so we have $N = \binom{52}{13}$ while \be N_A = \binom{13}{5} \times \binom{13}{3} \times \binom{13}{4} \times \binom{13}{1}. \ee since we may think of choosing in sequence
the spades, hearts, diamonds and club and then use the Fundamental Rule to get the total number of ways.
\end{example}

\begin{example}
If there are $r$ people in a room, what is the probability that at least two have the same birthday? In this case the number of points in the sample space $\Omega$ is $N = (365)^r$, since the sample
space consists of all possible $r$-tuples of birthdays. If $A$ is the required event, this is a case where it is easier to calculate the number of points in $A^c$, the complement of $A$, since $A^c$
is just the event that no two of the r people share a birthday. We then have
\be
N_{A^c} = 365 \times 364 \times 363 \times \dots \times (365 -r + 1)
\ee
which implies
\be
\pro(A) = 1 - \pro(A^c) = 1 - \frac{365 \times 364 \times 363 \times \dots \times (365 - r + 1)}{(365)^r} = p_r.
\ee

It is interesting to note the following values
\begin{center}
\begin{tabular}{c|ccccccccccc}
$r$ & 10 & 15 & 20 & 21 & 22 & 23 & 24 & 25 & 30 & 40 & 55\\
\hline
$p_r$ & 0.12 & 0.25 & 0.41 & 0.44 & 0.48 & 0.51 & 0.54 & 0.57 & 0.71 & 0.89 & 0.99\\
\end{tabular}
\end{center}
which shows the well known fact that if there are 23 or more people in a room there is higher than evens chances that at least 2 share a birthday.
\end{example}





\subsection{Independent events and Borel-Cantelli lemmas}

Relative to measure theory, probability theory is enriched by the significance attached to the notion of independence.

\begin{definition}[independent sets (events)]\label{def:independent_set}
Let $I$ be a countable set. Say that events $A_i$, $i \in I$, are independent\index{independent!set} if, for all finite subsets $J \subseteq I$,
\be
\pro\brb{\bigcap_{i\in J} A_i} = \prod_{i\in J} \pro(A_i).
\ee
\end{definition}

\begin{example}
Roll a die twice and let $A$ be the event that a 1 is obtained on the first roll and $B$ be the event that a 5 is obtained on the second roll. The sample space $\Omega=\{(i, j) : 1 \leq i \leq  6, 1
\leq j \leq 6\}$ has 36 points, all of which have the same probability $\frac 1{36}$. Then $A \cap B$ is just the outcome $(1, 5)$ and $\pro(A) = \frac 16 = \pro(B)$ so we can conclude that $A$ and
$B$ are independent.
\end{example}

\begin{proposition}
If $A$ and $B$ are independent then so are
\be
\text{(i) $A$ and $B^c$},\quad\quad \text{(ii) $A^c$ and $B^c$},\quad\quad \text{(iii) $A^c$ and $B$}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
For (i), $\pro(A \cap B^c) = \pro(A) - \pro(A \cap B) = \pro(A) - \pro(A)\pro(B) = \pro(A)(1 - \pro(B)) = \pro(A)\pro(B^c)$.

(ii) and (iii) follow from (i).
\end{proof}



The Definition \ref{def:independent_set} implies that if we take any two of the events $A_i$ and $A_j$ ($i$ distinct from $j$) then they are independent, so the events are said to be pairwise
independent\index{pairwise independent}. However, it should be noted that pairwise independence does not imply independence as the next example shows.

\begin{example}
Suppose that a fair coin is tossed twice (by a fair coin we mean the 4 possible outcomes $HH$, $HT$, $TH$ and $TT$ are equally likely and equal to $\frac 14$, so that a Head or a Tail on either toss
has probability $\frac 12$). Let $A_1$ be the event that a head is obtained on the first toss, $A_2$ the event that there is a head on the second toss and $A_3$ the event that exactly 1 head is
obtained. Then $\pro(A_i) = \frac 12$ for $i = 1, 2, 3$. It may be seen that the events are pairwise independent since, for example, \be \pro(A_1 \cap A_3) = \pro(A_1 \cap A^c_2) = \frac 14 =
\pro(A_1)\pro(A_3), \ee and similarly for $A_2$ and $A_3$ (and $A_1$ and $A_2$). But \be \pro(A_1 \cap A_2 \cap A_3) = 0 \neq \pro(A_1)\pro(A_2)\pro(A_3) = \frac 18, \ee so the three events are not
independent.
\end{example}



More than the independence of events, we give the independence of $\sigma$-algebra.

\begin{definition}[independent $\sigma$-algebra]
$\sigma$-algebras $\sA_i \subseteq \sF$, $i \in I$, are independent\index{independent!$\sigma$-algebra} if $A_i$, $i \in I$, are independent whenever $A_i \in \sA_i$ for all $i$.
\end{definition}

Here is a useful way to establish the independence of two $\sigma$-algebras.

\begin{theorem}\label{thm:pi_sigma_independent}
Let $\sA_1$ and $\sA_2$ be $\pi$-systems contained in $\sF$ and suppose that
\be
\pro(A_1 \cap A_2) = \pro(A_1)\pro(A_2)
\ee
whenever $A_1 \in \sA_1$ and $A_2 \in \sA_2$. Then $\sigma(\sA_1)$ and $\sigma(\sA_2)$ are independent.
\end{theorem}

\begin{proof}[\bf Proof]
Fix $A_1 \in \sA_1$ and define for $A \in \sF$, $\mu(A) = \pro(A_1 \cap A)$, $\nu(A) = \pro(A_1)\pro(A)$. Then $\mu$ and $\nu$ are measures which agree on the $\pi$-system $\sA_2$, with $\mu(\Omega)
= \nu(\Omega) = \pro(A_1) < \infty$. So, by uniqueness of extension, for all $A_2 \in \sigma(\sA_2)$, \be \pro(A_1 \cap A_2) = \mu(A_2) = \nu(A_2) = \pro(A_1)\pro(A_2). \ee

Now fix $A_2 \in \sigma(\sA_2)$ and repeat the argument with
\be
\mu'(A) = \pro(A \cap A_2),\quad\quad \nu'(A) = \pro(A)\pro(A_2)
\ee
to show that, for all $A_1 \in \sigma(\sA_1)$, $\pro(A_1 \cap A_2) = \pro(A_1)\pro(A_2)$.
\end{proof}


Then we extend the above theorem.

\begin{theorem}\label{thm:pi_sigma_independent_finite_many}
Let $\sA_i,i\in I$ be $\pi$-systems contained in $\sF$. $I$ is a countable set and $J\subseteq I$ is a finite set. Suppose that
\be
\pro\lob\bigcap_{i\in J}A_i\rob = \prod_{i\in J}\pro(A_i)
\ee
whenever $A_i\in \sA_i$. Then $\sigma(\sA_i),i\in I$ are independent.
\end{theorem}

\begin{proof}[\bf Proof]
We prove this by induction. First assume that for $\pi$-systems $\sA_i,i\leq k$ and $A_i \in \sA_i,i\leq k$,
\be
\pro\lob\bigcap\limits_{i\leq k}A_i\rob = \prod_{i\leq k}\pro(A_i) \ \ra \ \sigma(\sA_i),i\leq k \text{ are independent.}
\ee

Thus, Fix $A_i\in \sA_{i},i\leq k$ and define for $A\in \sF$,
\be
\mu(A) = \pro\lob\lob\bigcap\limits_{i\leq k}A_i \rob\bigcap A\rob,\quad \quad \nu(A) = \lob\prod_{i\leq k}\pro(A_i) \rob\pro(A)
\ee

Then $\mu$ and $\nu$ are measures which agree on the $\pi$-system $\sA_{k+1}$, with $\mu(\Omega)=\nu(\Omega)=\pro\lob\bigcap\limits_{i\leq k}A_i\rob<\infty$. So by uniqueness of extension, for all $A_{k+1}\in \sigma(\sA_{k+1})$, $\pro\lob\bigcap\limits_{i\leq k+1}A_i\rob = \mu(A_{k+1}) = \nu(A_{k+1}) = \prod\limits_{i\leq k+1}\pro(A_i) \ \ra \ \sigma(\sA_i),i\leq k+1$ are independent.
\end{proof}

\begin{proposition}\label{pro:independent_event_generated}
Let $(\Omega,\sF,\pro)$ be a probability space and $A_n, n\in \N$, a sequence of events. Then $A_n, n\in \N$ are independent if and only if the $\sigma$-algebras they generate
\be
\sigma(A_n) = \{\emptyset, A_n, A_n^c, \Omega\} \quad \text{ are independent.}
\ee
\end{proposition}

\begin{proof}[\bf Proof]
($\la$) Recall the definition that $\sigma$-algebras $\sA_i\subseteq \sF$, $i\in I$ are independent if $A_i$, $i\in I$ are independent whenever $A_i\in \sA_i$ for all $i$.

($\ra$) We take $\sA_n=\{\emptyset,A_n\}$. It is easy to check that $\sA_n$ is a $\pi$-system. Thus, by Theorem \ref{thm:pi_sigma_independent_finite_many}, the independence of $A_n$ implies the independence of $\sigma(A_n)$, the $\sigma$-algebras they generate.
\end{proof}


%\begin{proposition}
%For $\sigma$-algebras $(\sF_i)_{1\leq i\leq m}$ and $(\sG_j)_{1\leq j\leq n}$, if $\sF_1,\dots,\sF_m$ are independent of $\sG_1,\dots,\sG_n$ ($\sF_i$, $\sG_j$ are not necessarily independent), then $\sigma\brb{\bigcup_i \sF_i}$ and $\sigma\brb{\bigcup_j \sG_j}$ are independent.
%\end{proposition}

\begin{theorem}\label{thm:pi_sigma_independent_finite_many_ij}
Let $(\Omega,\sF,\pro)$ be a probability space. Suppose $(\sF_{i,j})_{1\leq i \leq n, 1 \leq j \leq m(i)}$ are independent % between different $i$ indices\footnote{e.g., $\sF_{1,1}$ and $\sF_{2,1}$ are independent but $\sF_{1,1}$ and $\sF_{1,2}$ are not}
and let $\sG_i = \sigma\brb{\bigcup_j \sF_{i,j}}$. Then $\sG_1, \dots, \sG_n$ are independent.
\end{theorem}

\begin{proof}[\bf Proof]
Let $\sA_i$ be the collection of sets of the form $\bigcap_jA_{i,j}$ where $A_{i,j} \in \sF_{i,j}$. Thus, $\sA_i$ is $\pi$-system of $\sigma$-algebra $\sigma\brb{\bigcup_j \sF_{i,j}} = \sG_i$. Also, $\sA_i$ are independent as $\sF_{i,j}$ are independent, i.e.,
\be
\pro\brb{\bigcap_i \bigcap_j A_{ij}} = \prod^n_{i=1}\pro\brb{\bigcap_j A_{ij}}.
\ee
%$\sA_i$ is a $\pi$-system that contains $\Omega$ and contains $\bigcup_j\sF_{i,j}$, so $\sG_i \subseteq \sigma(\sA_i)$. Also, $\bigcap_j A_{i,j} \in \bigcup_j \sF_{i,j}$ and thus $\sigma(\sA_i) \subseteq \sigma\brb{\bigcup_j \sF_{i,j}}$. Therefore, $\sigma(\sA_i) = \sG_i$.

Hence, $\sigma(\sA_i) =\sG_i$ are independent by Theorem \ref{thm:pi_sigma_independent_finite_many}.
\end{proof}


\subsection{Borel-Cantelli lemmas}


\begin{lemma}[first Borel-Cantelli lemma\index{Borel-Cantelli Lemma!probability}]\label{lem:borel_cantelli_1_probability}
Let $(\Omega,\sF,\pro)$ be a measure space. For sequence $A_n \subseteq \Omega$, if $\sum\limits_n \pro(A_n) < \infty$, then $\pro(A_n\text{ i.o.}) = 0$. (That is almost surely, $A_n^c$ occurs eventually.)
\end{lemma}

We note that first Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_1_measure}) is valid whether or not $\mu$ is a probability measure. The converse is not true, but if we add the independence, then it is true, which is the second lemma.

\begin{lemma}[second Borel-Cantelli lemma\index{Borel-Cantelli Lemma!probability}]\label{lem:borel_cantelli_2}
Assume that the events $(A_n:n\in\N)$ are independent. If $\sum_n \pro(A_n)=\infty$, then $\pro(A_n \text{ i.o.})=1$. Hence, almost surely, $A_n$ holds infinitely often.
\end{lemma}

\begin{proof}[\bf Proof]
We use the inequality $1-a\leq e^{-a}$. Set $a_n = \pro(A_n)$. Then, for all $n$ we have
\vspace{2mm}

$\qquad\qquad\pro\brb{\bigcap\limits_{m\geq n}A_m^c} = \prod\limits_{m\geq n}(1-a_m) \leq \exp\brb{-\sum\limits_{m\geq n}a_m} = 0 \quad\ra\quad \pro(A_n \text{ i.o.})=1-\pro\brb{\bigcup\limits_n \bigcap\limits_{m\geq n}A_m^c}=1$.
\end{proof}

\begin{example}
Infinite coin-toss. Let $0 < p < 1$. We will soon construct a probability measure $\pro$ on the infinite coin-toss measurable space $(\Omega, \sF)$ such that the events $\{\omega_n = 1\}$, $n \in \sN$ are independent and for all $n \geq 1$, $\pro(\omega_n = 1) = p$. Then
\be
\sum_n \pro(\omega_n = 1) = \infty,
\ee
hence, since these events are independent, we may apply the second Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_2}): almost surely, there are infinitely many heads in the coin-tossing experiment (no matter how small $p$ is).
\end{example}




\section{Random variables}

Recall the definition in measure theory, we have the following definitions:

\subsection{Random variables}\label{subsec:random_variable}

\begin{definition}
Let $(\Omega, \sF, \pro)$ be a probability space and let $(E, \sE)$ be a measurable space. A measurable function $X : \Omega \to E$ is called a random variable\index{random variable} in $E$.
\end{definition}

It has the interpretation of a quantity, or state, determined by chance. Where no space $E$ is mentioned, it is assumed that $X$ takes values in $\R$.

\begin{example}
Recall Definition \ref{def:indicator_function}. A real-valued random variable which takes on just the two values 0 and 1 is known as an indicator random variable\index{indicator random variable}, suppose that the event on which it takes the value 1 is $A \subseteq \Omega$ and otherwise 0, then the random variable is denoted by $\ind_A$.
\end{example}

\begin{definition}[distribution function of random variable]\label{def:r_random_variable_law}
The image measure (see Definition \ref{def:image_measure}) $\mu_X = \pro \circ X^{-1}$ is called the law\index{law} or distribution\index{distribution} of $X$. For real-valued random variables ($E=\R$), $\mu_X$ is uniquely determined by its values on the $\pi$-system of intervals $(-\infty, x]$, $x \in \R$, given by
\be
F_X(x) = \mu_X((-\infty, x]) = \pro(X \leq x) = \pro\brb{\{\omega:X(\omega) \leq x\}}.
\ee
Then $\mu_X$ is unique for any measurable $A\in \sB(\R)$ by uniqueness of extension (by Theorem \ref{thm:uniqueness_of_extension_measure}). The function $F_X$ is called the distribution function\index{distribution function of random variable} of $X$.
\end{definition}


\begin{definition}[equivalence of distribution]
Let $X$ and $Y$ be two random variables with distribution functions $F_X$ and $F_Y$. Then we say $X$ and $Y$ have the same distribution if their law functions are consistent for any $t\in \R$. That is,
\be
F_X(t) = \pro\brb{X\leq t} = \mu_X(-\infty,t] = \mu_Y(-\infty,t] = \pro\brb{Y\leq t} = F_Y(t),\qquad \forall t\in \R.
\ee
\end{definition}


\begin{proposition}[properties of distrubution function of random variables]
For any random variable $X$, the distribution function $F_X$ of $X$ has these properties:
\ben
\item [(i)] $F_X$ is increasing (non-decreasing).
\item [(ii)] $F_X$ is right-continuous but not necessarily left-continuous.
\item [(iii)] $\lim_{x\to -\infty} F_X(x) = 0$ and $\lim_{x\to \infty}F_X(x) = 1$.
\item [(iv)] $F_X(x^-) := \lim_{y\ua x} F_X(y) = \pro\brb{X< x}$.
\item [(v)] jump of $F_X$ at $x$ is $F_X(x) - F_X(x^-) = \pro\brb{X=x}$.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] $F_X$ is also increasing by definition of measure.
\item [(ii)] Let $\ve = \frac 1n$, $\bigcap_n \lob x,x+\frac 1n\right] = \emptyset$,
\be
F_X(x+\ve) -F_X(x) = \mu_X((-\infty,x+\ve]) - \mu_X((-\infty,x]) = \mu_X\brb{(x,x+\ve]}  \to \mu_X\brb{\bigcap_n \lob x,x+\frac 1n\right]} = \mu_X(\emptyset) = 0
\ee
by Corollary \ref{cor:measure_decreasing_sequence}. Thus, $F_X$ is right-continuous. However, $F_X$ is not left-continuous since
\be
F_X(x-\ve) -F_X(x) = \mu_X((-\infty,x-\ve]) - \mu_X((-\infty,x]) = -\mu_X\brb{(x-\ve,x]} \to -\mu_X\brb{\bigcap_n \lob x-\frac 1n,x\right]} = -\mu_X(\{x\}),
\ee
which could be an atom (Definition \ref{def:atom_distribution}).

\item [(iii)] Direct result from definition of $F_X$.
\item [(iv)] Since $F_X$ is also increasing and bounded, the left limit exists for $F_X$ at $x$, denoted by $F_X(x^-)$. Since $(-\infty,x-\frac 1n] \ua (-\infty, x)$, we have $x_n = x-\frac 1n$
\be
F_X(x^-) := \lim_{x_n\ua x}F_X(x_n) = \lim_{n\to \infty}\mu_X\brb{\left(-\infty,x-\frac 1n\right]} = \mu_X\brb{(-\infty,x)} = \pro\brb{X<x}
\ee
by montone convergence theorem.
\item [(v)] By (iv), $F_X(x) - F_X(x^-) = \pro\brb{X\leq x} - \pro\brb{X<x} = \pro\brb{X=x}$.
\een
\end{proof}

\begin{definition}[atom and discontinuity of distribution]\label{def:atom_distribution}
If $\pro(X=c)>0$, then the law $\mu_X$ of $X$ is said to have an atom\index{atom!distribution function} at $c$, and the distribution function $F_X$ of $X$ has a discontinuity at $c$:
\be
\mu_X\brb{\bra{c}} = F_X(c) - F_X(c^-) = \pro(X=c). \quad\quad(\text{as $F_X$ is right-continuous})
\ee
\end{definition}

\begin{remark}
$\mu$ can have at most $n$ atoms of mass at least $1/n$, so that the number of atoms of $\mu$ is at most countable. So we have,
\end{remark}

\begin{proposition}\label{pro:non_atoms_dense}
given $x\in \R$, the set of non-atoms is dense, i.e. there exists a sequence $(y_n)$ of reals with $y_n \da x$ such that every $y_n$ is a non-atom of $\mu$ (equivalently a point of continuity of $F_X$), and then by right-continuous of $F_X$, $F_X(y_n) \da F_X(x)$.
\end{proposition}

\begin{proof}[\bf Proof]
\footnote{need proof}.
\end{proof}


\begin{definition}\label{def:rn_random_variable_law}
The law $\mu_X$ on $(\R^n,\sB(\R^n))$ is
\be
\mu_X(A) = \pro \brb{X_1\leq x_1,\dots, X_n \leq x_n},
\ee
where $A = \bigotimes\limits^n_{i=1}(-\infty,x_i] := (-\infty,x_1]\times \dots (-\infty,x_n],\ x_i \in \R$. Then $\mu_X$ is unique for any measurable $A\in \sB(\R^n)$.
\end{definition}


\subsection{Distribution function}

\begin{definition}[distribution function\index{distribution function}]
If an increasing and right-continuous function $F : \R \to [0, 1]$ satisfies the following conditions
\be
\lim_{x\to-\infty} F(x) = 0,\quad\quad \lim_{x\to\infty} F(x) = 1,
\ee
we call it distribution function.
\end{definition}


\begin{center}
\psset{yunit=5cm,xunit=6cm}
\begin{pspicture}(-0.25,-0.1)(1.1,1.1)
%\psaxes{->}(0,0)(-0.5,0)(1.1,1.1)%Dy=0.25,dy=0.25

\pstGeonode[PointSymbol=o,PointName=none,dotscale=1.5](0.25,0.2){A1}(0.4,0.4){A2}(0.6,0.7){A3}(0.75,0.85){A4}
\pstGeonode[PointSymbol=*,PointName=none,dotscale=1.5](0.25,0.25){B1}(0.4,0.5){B2}(0.6,0.75){B3}(0.75,0.9){B4}

\psline[linestyle=dashed](-0.25,1)(1.2,1)
\psline(-0.25,0)(1.2,0)
\psline(-0.25,0)(-0.25,1)

\rput[lb](0.85,0.6){$F(x)$}
\rput[lb](0.22,-0.08){$x_1$}
\rput[lb](0.37,-0.08){$x_2$}
\rput[lb](0.57,-0.08){$x_3$}
\rput[lb](0.72,-0.08){$x_4$}
\rput[lb](-0.42,0.2){$F(x_1)$}
\rput[lb](-0.42,0.45){$F(x_2)$}
\rput[lb](-0.42,0.7){$F(x_3)$}
\rput[lb](-0.42,0.85){$F(x_4)$}
\rput[lb](-0.3,0.95){1}
\rput[lb](-0.3,0){0}
\pstGeonode[PointSymbol=none,PointName=none,CurveType=curve] (-0.25,0){A1}(0,0.02){A2}(0.15,0.1){A3}(0.25,0.2){A4}
\pstGeonode[PointSymbol=none,PointName=none,CurveType=curve] (0.25,0.25){A1}(0.3,0.3){A2}(0.4,0.4){A3}
\pstGeonode[PointSymbol=none,PointName=none,CurveType=curve] (0.4,0.5){A1}(0.5,0.65){A2}(0.6,0.7){A3}
\pstGeonode[PointSymbol=none,PointName=none,CurveType=curve] (0.6,0.75){A1}(0.68,0.83){A2}(0.75,0.85){A3}
\pstGeonode[PointSymbol=none,PointName=none,CurveType=curve] (0.75,0.9){A1}(0.85,0.97){A2}(1,1){A3}

\psline[linestyle=dashed](0.75,0.9)(0.75,0)
\psline[linestyle=dashed](0.6,0.75)(0.6,0)
\psline[linestyle=dashed](0.4,0.5)(0.4,0)
\psline[linestyle=dashed](0.25,0.25)(0.25,0)

\psline[linestyle=dashed](0.75,0.9)(-0.25,0.9)
\psline[linestyle=dashed](0.6,0.75)(-0.25,0.75)
\psline[linestyle=dashed](0.4,0.5)(-0.25,0.5)
\psline[linestyle=dashed](0.25,0.25)(-0.25,0.25)
\end{pspicture}
\end{center}

%
%\centertexdraw{
%
%\drawdim in
%
%\def\bdot {\fcir f:0 r:0.03 }
%\def\ebdot {\lcir r:0.02 }
%\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
%\linewd 0.01 \setgray 0
%
%
%\move (-0.5 0)\clvec (0.2 0)(0.4 0.3)(0.5 0.4)\ebdot
%\move (0.5 0.5)\bdot\clvec (0.7 0.7)(0.75 0.8)(0.8 0.8)\ebdot
%\move (0.8 1)\bdot \clvec (1 1.2)(1.1 1.4)(1.2 1.4)\ebdot
%\move (1.2 1.5)\bdot \clvec (1.3 1.6)(1.4 1.7)(1.5 1.7)\ebdot
%\move (1.5 1.8)\bdot \clvec (1.7 2)(1.8 1.95)(2 2)
%
%\move(-1 -0)\lvec(3 -0)
%\move(-1 0)\lvec(-1 2)
%
%%\move(1 0.5)\bdot
%
%%\htext (0.2 0.5){$F_N(x)$}
%\htext (1.7 1.2){$F(x)$}
%%
%%\move (0.3 0)\ebdot
%%\move (0.3 0.2)\bdot \lvec(0.6 0.2)\ebdot
%%\move (0.6 0.4)\bdot \lvec(0.7 0.4)\ebdot
%%\move (0.7 0.6)\bdot \lvec(0.85 0.6)\ebdot
%
%\htext (0.4 -0.15){$x_1$}
%\htext (0.7 -0.15){$x_2$}
%\htext (1.1 -0.15){$x_3$}
%\htext (1.4 -0.15){$x_4$}
%
%\htext (-1.4 0.4){$F(x_1)$}
%\htext (-1.4 0.9){$F(x_2)$}
%\htext (-1.4 1.4){$F(x_3)$}
%\htext (-1.4 1.7){$F(x_4)$}
%\htext (-1.1 1.95){1}
%\htext (-1.1 0){0}
%
%%\move (1.1 1.4)\bdot \lvec(1.3 1.4)\ebdot
%%\move (1.3 1.6)\bdot \lvec(1.5 1.6)\ebdot
%%\move (1.5 1.8)\bdot \lvec(1.7 1.8)\ebdot
%%\move (1.7 2)\bdot \lvec(2 2)
%
%\lpatt (0.05 0.05)
%\move(-1 2)\lvec(3 2)
%\move (0.5 0.5)\lvec(0.5 0)
%\move (0.8 1)\lvec(0.8 0)
%\move (1.2 1.5)\lvec(1.2 0)
%\move (1.5 1.8)\lvec(1.5 0)
%
%\move (0.5 0.5)\lvec(-1 0.5)
%\move (0.8 1)\lvec(-1 1)
%\move (1.2 1.5)\lvec(-1 1.5)
%\move (1.5 1.8)\lvec(-1 1.8)
%}

\begin{definition}[degenerate distribution\index{degenerate distribution}, non-degenerate distribution\index{non-degenerate distribution}]
The degenerate distribution or deterministic distribution is the distribution of a random variable which only takes a single value. Otherwise, the distribution function is called non-degenerate distribution.
\end{definition}

Thus, we have following proposition:

\begin{proposition}
Every distribution function $F$ is the unique distribution function $F_X$ of a random variable $X$.
\end{proposition}

\begin{proof}[\bf Proof]
For any distribution function $F$, set $\Omega = (0, 1]$ and $\sF = \sB((0, 1])$. Let $\pro$ denote the restriction of Lebesgue measure to $\sF$. Then $(\Omega, \sF, \pro)$ is a probability space. Let $F$ be any distribution function. Define $X : \Omega \to \R$ by
\be
X(\omega) = \inf\{x : \omega \leq F(x)\}.
\ee

Then, by Theorem \ref{thm:switching_formula_right_continuous_function}, $X$ is a random variable and $X(\omega) \leq x$ if and only if $\omega \leq F(x)$. So
\be
F_X(x) = \pro(X \leq x) = \pro((0, F(x)]) = F(x) - 0 = F(x). \quad\quad(\pro \text{ is Lebesgue measure})
\ee

Alternatively, from Lemma \ref{thm:existence_radon}, we know that there exists a unique Borel measure $\mu_X$ ($F$ implies $X$ and thus implies $\mu_X$) such that
\be
F_X(x) = \underbrace{\mu_X((-\infty, x])}_{dg((a,b])} = \pro(X \leq x) = \underbrace{F(x)}_{g(b)} - \underbrace{\lim_{x\to-\infty}F(x)}_{g(a)} = F(x).
\ee
\end{proof}





\begin{example}
Let $C_n$ denote the $n$th approximation to the Cantor set $C$: thus $C_0=[0,1]$, $C_1=\left[1,\frac 13\right]\cup\left[\frac 23, 1\right]$, $C_2=\left[1,\frac 19\right]\cup\left[\frac 29, \frac 13\right]\cup\left[\frac 23,\frac 79\right]\cup\left[\frac 89, 1\right]$, etc. and $C_n\downarrow C$ as $n\to \infty$. Denote by $F_n$ the distribution function of a random variable uniformly distributed (see Definition \ref{def:uniform_rv}) on $C_n$. Note that $F_n$ is given recursively by
\be
F_{n+1}(x) =\left\{\ba{ll}
\frac 12 F_n(3x) & 0\leq x\leq \frac 13\\
\frac 12 & \frac 13 \leq x \leq \frac 23\\
\frac 12 + \frac 12 F_n(3x-2) \quad\quad & \frac 23 \leq x \leq 1
\ea \right.
\ee

Therefore,
\be
|F_{n+1}(x)-F_n(x)| \leq \frac 12 \sup_x |F_n(x)-F_{n-1}(x)| \leq 2^{-n}.
\ee

Hence $F_n$ converges uniformly with continuous limit $F$. If $x\notin C$ then $x\notin C_n$ for some $n$ and $F=F_n=$ constant in a neightbourhood of $x$, so $F$ is differentiable at $x$ with $F'(x)=0$. The Lebesgue measure of $C$ is zero.
\end{example}

\begin{definition}\label{def:minus_distribution_function}
Let $X$ be a random variable with distribution $F$. Then we denote the distribution of $-X$ by $\supbef{F}{-}$. At points of continuity we have
\be
\supbef{F}{-} (x) = 1 - F(-x)
\ee
and this defines $\supbef{F}{-}$ uniquely\footnote{details needed.}.
\end{definition}

\begin{definition}[symmetric distribution]\label{def:symmetric_distribution_function}
The distribution $F$ is called symmetric if $\supbef{F}{-} = F$ (for all points $x$). When a density $f$ exists this means that $f(-x) = f(x)$.
\end{definition}

\begin{lemma}
If $X_1,X_2$ are independent and identically distributed, then for $t>0$
\be
\pro\brb{\abs{X_1-X_2} > t}\leq 2\pro\brb{\abs{X_1}>\frac 12 t}.
\ee

If $a\geq 0$ is chosen so that $\pro\brb{X_i\leq a}\geq p$ and also $\pro\brb{X_i\geq -a} \geq p$, then
\be
\pro\brb{\abs{X_1-X_2} >t} \geq p\pro\brb{\abs{X_t} > t+a}.
\ee

In particular, if 0 is a median for $X_i$, then
\be
\pro\brb{\abs{X_1-X_2}>t} \geq \frac 12 \pro\brb{\abs{X_1} > t}.
\ee
\end{lemma}


\begin{proof}[\bf Proof]
First,

\be
\bra{\abs{X_1}\leq  \frac 12t} \cap \bra{\abs{X_2}\leq \frac 12t} \subseteq \bra{\abs{X_1-X_2}\leq t}
\ee

\be
\bra{\abs{X_1-X_2}>t} \subseteq \brb{\bra{\abs{X_1}\leq  \frac 12t} \cap \bra{\abs{X_2}\leq \frac 12t} }^c = \bra{\abs{X_1}> \frac 12t} \cup \bra{\abs{X_2}> \frac 12t}
\ee

Therefore,
\beast
\pro\brb{\abs{X_1-X_2}>t} & \leq & \pro\brb{\abs{X_1}> \frac 12t} \cup \bra{\abs{X_2}> \frac 12t} \\
& = & 2\pro\brb{\abs{X_1}> \frac 12t} - \pro\brb{\bra{\abs{X_1}> \frac 12t} \cap \bra{\abs{X_2}> \frac 12t}} \leq 2\pro\brb{\abs{X_1}> \frac 12t}.
\eeast

Second, we have $\bra{X_1>t+a,X_2\leq a} \cap \bra{X_1< -t-a,X_2\geq -a} = \emptyset$ and
\be
\bra{\abs{X_1-X_2}>t} \supseteq \bra{X_1>t+a,X_2\leq a} \cup \bra{X_1< -t-a,X_2\geq -a}.
\ee

Hence,
\beast
\pro\brb{\abs{X_1-X_2}>t} & \geq & \pro\brb{X_1>t+a,X_2\leq a} +  \pro\brb{X_1< -t-a,X_2\geq -a}\\
& = &  \pro\brb{X_1>t+a}\pro\brb{X_2\leq a} +  \pro\brb{X_1< -t-a}\pro\brb{X_2\geq -a} \\
& = & p \pro\brb{X_1>t+a} + p \pro\brb{X_1< -t-a} = p \pro\brb{\abs{X_1}>t+a} .
\eeast
\end{proof}

\begin{lemma}[symmetrization inequalities]\label{lem:distribution_symmetrization_inequalities}
If $X_1,X_2,\dots, X_n$ are independent and have symmetric distributions then $S_n = X_1 + X_2 + \dots + X_n$ has a symmetric distribution and for $t>0$
\be
\pro\brb{\abs{X_1+\dots+X_n} >t} \geq \frac 12 \pro\brb{\max_{1\leq i\leq n}\abs{X_i} > t}.
\ee

If the $X_i$ have a common distribution $F$ then for any continuous point $t>0$ of $F$,
\be
\pro\brb{\abs{X_1+\dots+X_n} >t} \geq \frac 12 \brb{1 - e^{-n\brb{1-F(t) + F(-t)}}}.
\ee
\end{lemma}


\begin{proof}[\bf Proof]
Let the random variable $M = \max_{1\leq i\leq n} \abs{X_i}$ and put $N = S_n - M$. The pair $(M,N)$ is symmetrically distributed in the sense that the four combinations $(\pm M,\pm N)$ have the same distribution\footnote{details needed.}. Clearly,
\be
\pro\brb{M>t} \leq \pro\brb{M>t, T\geq 0} + \pro\brb{M>t,T\leq 0}.
\ee

The two terms on the right have equal probabilities, and so
\be
\pro(S>t) = \pro\brb{M+T>t} \geq \pro\brb{M>t,T\geq 0} \geq \frac 12\pro(M>t)
\ee
as required. Let $F$ be continuous at $t$. Thus $F$ is also continuous at $-t$ since $F$ is symmetric (see Definition \ref{def:minus_distribution_function} and \ref{def:symmetric_distribution_function}) and
\beast
\pro\brb{\max_{1\leq i\leq n}\abs{X_i} \leq t} & = & \brb{ \pro\brb{\abs{X_1} \leq t}}^n = \brb{ \pro\brb{-t \leq X_1 \leq t}}^n = \brb{ \pro\brb{-t < X_1 \leq t}}^n \\
& = & \brb{F(t)  -F(-t)}^n \leq e^{-n\brb{1-F(t) + F(-t)}}
\eeast
since $1-x<e^{-x}$ when $0<x<1$.
\end{proof}




\subsection{Inverse distribution function}

\begin{definition}[inverse distribution function\index{inverse distribution function}]
Let $F$ be a distribution function. Then its inverse distribution function is for any $u\in (0,1)$,
\be
F^*(u):= \inf\bra{x\in \R: F(x) \geq u}.
\ee
\end{definition}



\begin{center}
\psset{yunit=5cm,xunit=7cm}
\begin{pspicture}(-0.1,-0.25)(1.1,1.25)
%\psaxes{->}(0,0)(-0.5,0)(1.1,1.1)%Dy=0.25,dy=0.25

\pstGeonode[PointSymbol=*,PointName=none,dotscale=1.5](0.25,0.2){A1}(0.4,0.4){A2}(0.6,0.7){A3}(0.75,0.8){A4}
\pstGeonode[PointSymbol=o,PointName=none,dotscale=1.5](0.25,0.25){B1}(0.4,0.5){B2}(0.6,0.8){B3}(0.75,0.9){B4}

\psline[linestyle=dashed](1,-0.25)(1,1.25)
\psline(0,-0.25)(0,1.2)
\psline(0,0)(1,0)

\rput[lb](1.05,0.6){$F^*(u)$}
\rput[lb](0.22,-0.08){$u_1$}
\rput[lb](0.37,-0.08){$u_2$}
\rput[lb](0.57,-0.08){$u_3$}
\rput[lb](0.72,-0.08){$u_4$}
\rput[lb](-0.17,0.17){$F^*(u_1)$}
\rput[lb](-0.17,0.4){$F^*(u_2)$}
\rput[lb](-0.17,0.67){$F^*(u_3)$}
\rput[lb](-0.17,0.77){$F^*(u_4)$}
\rput[lb](0.95,-0.1){1}
\rput[lb](-0.05,-0.1){0}
\pstGeonode[PointSymbol=none,PointName=none,CurveType=curve] (0,-0.25){A1}(0.02,0){A2}(0.1,0.15){A3}(0.25,0.2){A4}
\pstGeonode[PointSymbol=none,PointName=none,CurveType=curve] (0.25,0.25){A1}(0.3,0.3){A2}(0.4,0.4){A3}
\pstGeonode[PointSymbol=none,PointName=none,CurveType=curve] (0.4,0.5){A1}(0.5,0.65){A2}(0.6,0.7){A3}
\pstGeonode[PointSymbol=none,PointName=none,CurveType=curve] (0.6,0.8){A1}(0.7,0.8){A2}(0.75,0.8){A3}
\pstGeonode[PointSymbol=none,PointName=none,CurveType=curve] (0.75,0.9){A1}(0.9,1){A2}(1,1.25){A3}

\psline[linestyle=dashed](0.75,0.9)(0.75,0)
\psline[linestyle=dashed](0.6,0.8)(0.6,0)
\psline[linestyle=dashed](0.4,0.5)(0.4,0)
\psline[linestyle=dashed](0.25,0.25)(0.25,0)

\psline[linestyle=dashed](0.6,0.8)(0,0.8)
\psline[linestyle=dashed](0.6,0.7)(0,0.7)
\psline[linestyle=dashed](0.4,0.4)(0,0.4)
\psline[linestyle=dashed](0.25,0.2)(0,0.2)
\end{pspicture}
\end{center}

%\centertexdraw{
%
%\drawdim in
%
%\def\bdot {\fcir f:0 r:0.03 }
%\def\ebdot {\lcir r:0.02 }
%\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
%\linewd 0.01 \setgray 0
%
%
%\move (0 -0.2)\clvec (0 0.2)(0.3 0.4)(0.5 0.4)\bdot
%\move (0.5 0.5)\ebdot\clvec (0.7 0.7)(0.8 0.75)(0.8 0.8)\bdot
%\move (0.8 1)\ebdot \clvec (1 1.3)(1.1 1.6)(1.2 1.6)\bdot
%\move (1.2 1.7)\ebdot \clvec (1.3 1.7)(1.4 1.7)(1.6 1.7)\bdot
%\move (1.6 1.8)\ebdot \clvec (1.7 1.8)(1.9 1.9)(2 2.2)
%
%\move(0 2.2)\lvec(0 -0.2)
%%\move(-1 0)\lvec(-1 2)
%
%%\move(1 0.5)\bdot
%
%%\htext (0.2 0.5){$F_N(x)$}
%\htext (2.1 1.6){$F^*(x)$}
%%
%%\move (0.3 0)\ebdot
%%\move (0.3 0.2)\bdot \lvec(0.6 0.2)\ebdot
%%\move (0.6 0.4)\bdot \lvec(0.7 0.4)\ebdot
%%\move (0.7 0.6)\bdot \lvec(0.85 0.6)\ebdot
%
%\htext (0.4 -0.15){$u_1$}
%\htext (0.7 -0.15){$u_2$}
%\htext (1.1 -0.15){$u_3$}
%\htext (1.5 -0.15){$u_4$}
%
%\htext (-0.45 0.4){$F^*(u_1)$}
%\htext (-0.45 0.9){$F^*(u_2)$}
%\htext (-0.45 1.5){$F^*(u_3)$}
%\htext (-0.45 1.7){$F^*(u_4)$}
%\htext (2.05 -0.05){1}
%\htext (-0.1 -0.05){0}
%
%%\move (1.1 1.4)\bdot \lvec(1.3 1.4)\ebdot
%%\move (1.3 1.6)\bdot \lvec(1.5 1.6)\ebdot
%%\move (1.5 1.8)\bdot \lvec(1.7 1.8)\ebdot
%%\move (1.7 2)\bdot \lvec(2 2)
%
%\lpatt (0.05 0.05)
%\move(2 -0.2)\lvec(2 2.2)
%\move (0.5 0.5)\lvec(0.5 0)
%\move (0.8 1)\lvec(0.8 0)
%\move (1.2 1.5)\lvec(1.2 0)
%\move (1.6 1.8)\lvec(1.6 0)
%
%\move (0.5 0.4)\lvec(0 0.4)
%\move (0.8 0.8)\lvec(0 0.8)
%\move (1.2 1.6)\lvec(0 1.6)
%\move (1.6 1.7)\lvec(0 1.7)
%
%\move(0 0)\lvec(2 0)
%
%}


Therefore if we have a random number generator to generate numbers according to the uniform distribution, we can generate any random variable with a known distribution.

\begin{theorem}[inverse probability transformation (IPT)]\label{thm:inverse_probability_transformation}
Let $F$ be a distribution function (right-continuous) and $U(\omega)$ be a uniform random variable on $[0,1]$ (see Defintion \ref{def:uniform_rv}). If we set
\be
X(\omega) = F^*\circ U(\omega) = F^*( U(\omega)) := \inf\bra{x : U(\omega) \leq F(x)}.
\ee

Then $X$ is a random variable with distribution function $F$, that is, $F^*(U) \sim X$.
\end{theorem}

\begin{proof}[\bf Proof]
By Theorem \ref{thm:switching_formula_right_continuous_function}, we have that $F^*(U(\omega)) \leq x$ if and only if $U(\omega) \leq F(x)$. Thus, by definition of uniform distriubtion,
\be
F_X(x) = \pro\brb{X(\omega)\leq x} = \pro\brb{F^*(U(\omega))\leq x} = \pro\brb{U(\omega) \leq F(x)} = F(x).
\ee

Thus, $X$ has distribution function $F$.
\end{proof}

\begin{remark}
As a consequence, in order to simulate any random variable it is only necessary to use a random number generator to provide a random number uniform in $[0, 1]$ and then use the above procedures in the continuous and discrete cases.

This is important for computer simulation of random variables.
\end{remark}

If $F$ is strictly increasing and continuous, we can define $X= F^{-1}(U)$ and get the corresponding result.\footnote{plot needed here.}


\begin{example}[generating exponential random variables]
The distribution function of an exponential random variable is $F(x) = 1-e^{-\lm x}$ (see Definition \ref{def:exponential_rv}). Then we can generate a exponential random variable $X$ with uniform random variable $U$ by
\be
X = F^*(U) =  F^{-1}(U) = -\frac 1{\lm}\ln\brb{1-U}.
\ee
\end{example}

\begin{remark}
Furthermore, $\mu_X$ is non-zero Radon measure \footnote{need sort out} in $\R$ (since $\pro(X\leq x) \in [0,1]$). Thus, this unique non-zero Radon measure $\mu_X$ is Lebesgue-Stieltjes measure associated with $F$ (by Theorem \ref{thm:existence_radon}).
\end{remark}

\begin{definition}
Let $F^*$ be an inverse distribution function of distribution function $F$. Since $F^*$ is monotone and bounded, we can define
\be
F^*(u^+) := \lim_{v\da u} F^*(v)\quad u\in [0,1).
\ee
\end{definition}


\subsection{Switching formula of distribution functions}

\begin{theorem}[switching formula]\label{thm:switching_formula_distribution_function} %{lem:right_continuous}
Let $F$ be a distribution function and $F^*$ is its inverse distribution function. Then for $u\in (0,1)$, the infimum is attained, that is
\be
[F^*(u),\infty) = \bra{x\in \R: F(x)\geq u}.
\ee

Moreover, for $u\in (0,1)$ and $x\in \R$, we have
\be
F(x) \geq u \ \lra F^*(u) \leq x.
\ee

This is called the switching formula, which has an obvious counterpart:
\be
F(x) < u \ \lra \ F^*(u) > x.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
This is direct result from Theorem \ref{thm:switching_formula_right_continuous_function}.
\end{proof}


\begin{proposition}\label{pro:switching_formula_inverse_distribution_function_open_set_sup}
Let $F$ be a distribution function and $F^*$ is its inverse distribution function. Then for $x\in \R$ and $u\in (0,1)$,
\be
F(x^-) \leq u \ \lra \ F^*(u^+) \geq x.
\ee

Furthermore,
\be
F^*(u^+) = \sup\bra{x:F(x^-)\leq u}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
We have that
\beast
F(x^-) \leq u & \lra & F(w) \leq u\text{ for all }w<x \qquad \text{($F$ is increasing)} \\
& \lra & F(w) < v\text{ for all }w<x, v>u \\%\qquad \text{($F$ is right-continuous)} \\
& \lra & F^*(v) > w\text{ for all }w<x, v>u \qquad \text{(switching formula, Theorem \ref{thm:switching_formula_distribution_function})}\\
& \lra & F^*(v) \geq x\text{ for all }v>u \qquad \text{($F^*$ is left-continuous)} \\
& \lra & F^*(u^+) \geq x. \qquad \text{($F^*$ is increasing)}
\eeast

Thus, if $F^*(u^+) > \sup\bra{x\in \R :F(x^-) \leq u}$, we can find $x'$ such that $F^*(u^+) \geq x'> \sup\bra{x\in \R :F(x^-) \leq u}$. Therefore, $x' \in \bra{x\in \R :F(x^-) \leq u}$ and there is contradiction. Thus, $F^*(u^+) \leq \sup\bra{x\in \R :F(x^-) \leq u}$.

If $F^*(u^+) < \sup\bra{x\in \R :F(x^-) \leq u}$, for any $x'\in (F^*(u^+), \infty]$ such that $F^*(u^+) < x'$ implies that $F(x'^-) > u$ and this is contradiction again. Therefore, $F^*(u^+) = \sup\bra{x:F(x^-)\leq u}$.
\end{proof}


\begin{proposition}[properties of inverse distribution function]\label{pro:inverse_distribution_properties}
Let $F$ be a distribution function and $F^*$ be its inverse distribution function for $u\in (0,1)$. Then
\ben
\item [(i)] $F^*(u)$ is increasing (non-decreasing).
\item [(ii)] $F^*(u)$ is left-continuous.
\item [(iii)] $\lim_{u\da 0} F^*(u) = \inf\bra{x\in \R: F(x)>0}$, $\lim_{u\ua 0}F^*(u) = \sup\bra{x\in\R:F(x)<1}$.
\item [(iv)] For each $u\in (0,1)$ and $x\in \R$ with $0<F(x)<1$,
\beast
F\brb{\brb{F^*(u)}^-} \leq & u & \leq F\brb{F^*(u)} \\
F^*\brb{F(x)} \leq & x & \leq F^*\brb{\brb{F(x)}^+}.
\eeast
\een
\end{proposition}

\begin{remark}
Note that (i) and (ii) can also be proved by Definition \ref{def:left_right_continuous inverses} and Proposition \ref{pro:left_right_continuous_inverse_continuity}, we know that $F^*(u)$ is left-continuous and non-decreasing.
\end{remark}

\begin{proof}[\bf Proof]
\ben
\item [(i)] For $0<u\leq v <1$, $F^*(u) \leq F^*(v)$ by definition of $F^*$. It also follows
\beast
F^*(v)\leq F^*(v) & \ra & v \leq F\brb{F^*(v)}\quad \text{switching formula (Theorem \ref{thm:switching_formula_distribution_function})}\\
& \ra & u\leq F\brb{F^*(v)}\quad \text{(since $u\leq v$)} \\
& \ra &   F^*(u)\leq F^*(v)\quad \text{switching formula (Theorem \ref{thm:switching_formula_distribution_function}) again.}
\eeast
\item [(ii)] Let $u_n \ua u$, then $F(u_n) \ua F(u)$. The assumption is $u_n \leq u_{n+1}$ for all $n$ and $u = \lim_n u_n$. Since $F^*$ is non-decreasing, we have
\be
F^*(u_n) \leq F^*(u_{n+1}) \leq F^*(u)
\ee
for all $n$, so $\ell := \lim_n F^*(u_n)$ exists and satisfies $\ell \leq F^*(u)$. To get the opposite inequality, consider an $x\in \R$ with $F^*(u)>x$. Then
\beast
F^*(u) > x & \ra & u> F(x) \quad \text{(switching formula (Theorem \ref{thm:switching_formula_distribution_function}))} \\
& \ra & u_n > F(x)\quad \text{ for all large $n$ since }u_n\ua u \\
& \ra & F^*(u_n) > x\quad \text{ for all large $n$ (switching formula)} \\
& \ra & \ell > x \quad \text{(since }\ell = \lim_{n\to\infty} F^*(u_n)).
\eeast

Now let $x$ tend up to $F^(u)$ to conclude $\ell \geq F^*(u)$ as desired.

\item [(iii)] Let $\ell =\lim_{u\da 0}F^*(u)$ and $\xi = \inf\bra{x\in \R : F(x) > 0}$\footnote{Note that $\ell$ and $\xi$ may be $-\infty$.}. Suppose that $u_n\da 0$. For each $x\in \bra{x\in \R : F(x) > 0}$, we can find $u_n>0$ such that $F(x)\geq u_n > 0$. Then by switching formula (Theorem \ref{thm:switching_formula_distribution_function}), $F^*(u_n) \leq x$. Therefore, $\ell \leq x$ and thus $\ell \leq \xi$.

For each $u>0$, we have $F^*(u)\in \bra{x\in \R : F(x) > 0}$ as $F\brb{F^*(u)} \geq u > 0 $ by (iii). Therefore, $F^*(u) \geq \xi$ and therefore $\ell \geq \xi$. Hence, $\lim_{u\da 0}F^*(u) = \ell = \xi = \inf\bra{x\in \R : F(x) > 0}$.

The proof of the other equality is similar.
%For $x\in \R$ with $0<F(x)<1$, we have (by switching formula (Theorem ...))
%\be
%F(x) \leq F(x) \ \ra \ F^*\brb{F(x)} \leq x.
%\ee
%
%Now we set $u = F(x)$and $v > u$, thus,
%\be
%v > u =F(x) \ \ra \ F^*(v) > x.
%\ee
%
%Then by letting $v\da u$ shows $F^*(u^+) \geq x$, as desired

\item [(iv)] For $u\in (0,1)$, $F^*(u) \leq F^*(u) \ \ra \ u\leq F\brb{F^*(u)}$ by switching formula (Theorem \ref{thm:switching_formula_distribution_function}). Set $x = F^*(u)$. We need to show $u\geq F(x^-)$. Let $\xi < x$. Then by switching formula
\be
F^*(u) = x > \xi \ \ra \ u> F(\xi).
\ee

Therefore by letting $\xi \ua x$ shows $u\geq F(x^-)$, as desired.

Similarly, for $F(x)\in (0,1)$, $F(x) \leq F(x) \ \ra \ x \geq F^*\brb{F(x)}$ by switching formula (Theorem \ref{thm:switching_formula_distribution_function}).

Set $u = F(x)$. We need to show $x \leq F^*(u^+)$. Let $\mu > u$. Then by switching formula
\be
F(x) = u < \mu \ \ra \ x < F^*(\mu).
\ee

Therefore by letting $\mu \da u$ shows $x \leq F^*(u^+)$, as desired.
\een
\end{proof}

\begin{corollary}\label{cor:continuous_distribution_function_inverse_composition}
Let $F$ be a distribution function and $F^*$ be its inverse distribution function for $u\in (0,1)$. Then
\ben
\item [(i)] $F(F^*(u)) = u$ if and only if $F$ is continuous.
\item [(ii)] $F^*(F(x)) = x$ for all $x\in A:=\bra{x\in \R:0<F(x)<1}$ if and only if $F$ is strictly increasing over $A$.
\een
\end{corollary}

\begin{proof}[\bf Proof]
\ben
\item [(i)] ($\la$). If $F$ is continuous, we have $F(F^*(u)) = u$ by Proposition \ref{pro:inverse_distribution_properties}.(iv).

%\footnote{proof needed.}
%$\forall u\in (0,1)$, we define $x:= F^*(u)$. Thus, from assumption for any small enough positive $\ve$, $\exists \delta >0$ such that $F^*(u) - F^*(u-\delta) < \ve$
%\be
%u -\delta = F(F^*(u-\delta)) \leq F(F^*(u^-)) = F((F^*(u))^-) =  F(F^*(u)) = F(x) = u.
%\ee
%So we have $F(x)$

($\ra$). Let $F(F^*(u)) = u$. Then $\forall x$, if $F$ is not continuous at $x$, we can define that
\be
u = \frac 12 \brb{F(x) + F(x^-)} \in (0,1)
\ee
where $F(x^-)< F(x)$. Therefore, by the definition of inverse distribution function, $F^*(u) =x$. Hence
\be
F(F^*(u)) = F(x) \neq\frac 12 \brb{F(x) + F(x^-)} = u
\ee
which is a contradiction. Thus, $F$ is continuous at all the points.

\item [(ii)] ($\la$). Let $F$ be strictly increasing and $F(x) = u$. Then for any $\delta>0$ we have
\be
F(x-\delta) < u \ \ra\ x - \delta < F^*(u) = F^*\brb{F(x)}
\ee
by switching formula (Theorem \ref{thm:switching_formula_distribution_function}). Then let $\delta \to 0$, we have
\be
x\leq F^*\brb{F(x)} \ \ra\ F^*\brb{F(x)} = x
\ee
by Proposition \ref{pro:inverse_distribution_properties}.(iv) ($F^*\brb{F(x)} \leq x$).

($\ra$). Let $F^*\brb{F(x)} = x$. If $F$ is not strictly increasing, we can find $x\in A$ and $\delta>0$ such that
\be
F(x-\delta) = F(x) \ \ra\ F^*\brb{F(x)} \leq x-\delta < x
\ee
which is a contradiction. Thus, $F$ is strictly increasing on $A$.
\een
\end{proof}


%If $x_1<x_2$, then $F(x_1) < F(x_2)$
%
%Thus for any $\delta>0$, $F(x+\delta) > (F(x))^+$, so $F^*\brb{F(x+\delta) } \geq F^*((F(x))^+)$
%
%Thus,
%\be
%F^*\brb{F(x)} \leq x \leq F^*\brb{\brb{F(x)}^+} \leq F^*\brb{F(x+\delta) }
%\ee
%
%Thus for any $\delta>0$, $F(x-\delta) < (F(x))^+$, so $x-\delta < F^*((F(x))^+)$





\begin{theorem}\label{thm:convergence_distribution_function_and_inverse_distribution_function_imply_each_other}
Let $F_1,F_2,\dots$ and $F$ be distribution functions with corresponding inverse distribution functions $F^*_1,F^*_2,\dots$ and $F^*$. Suppose that
\be
\lim_{n\to \infty} F_n(x) = F(x)\text{ for all continuous points $x\in \R$ of $F$.}\qquad (*)
\ee
\ben
\item [(i)] Suppose that $u\in (0,1)$ and $w$ is a continuous point of $F$ with $F^*(u)>w$. Then $F^*_n(u)>w$ for all large $n$.
\item [(ii)] Suppose that $u\in (0,1)$ and that $y$ is a continuous point of $F$ with $F^*(u)<y$. Then $F^*_n(u)\leq y$ for all large $n$.
\item [(iii)] For each $u\in (0,1)$, one has
\be
F^*(u) \leq \liminf_n F^*_n(u) \leq \limsup F^*_n(u) \leq F^*(u^+).
\ee
\item [(iv)] We have that
\be
\lim_{n\to \infty} F^*_n(u) = F^*(u)\text{ for all continuous points $u\in (0,1)$ of $F^*$.}\qquad (\dag)
\ee
\een

Furthermore, we can have that $(\dag)$ implies $(*)$. Thus, the following two statements are equivalent:
\beast
\text{(i)} & &\lim_{n\to \infty} F_n(x) = F(x)\text{ for all continuous points $x\in \R$ of $F$.} \\
\text{(ii)} & & \lim_{n\to \infty} F^*_n(u) = F^*(u)\text{ for all continuous points $u\in (0,1)$ of $F^*$.}
\eeast
\end{theorem}

\begin{remark}
This plays an important role in the theory of convergence of probability distributions. This is a weaker version of Theorem \ref{thm:convergence_increasing_function_and_its_left_inverse_function_imply_each_other}.
\end{remark}


\begin{proof}[\bf Proof]
\ben
\item [(i)] By switching formula (Theorem \ref{thm:switching_formula_distribution_function}), $F^*(u) > w \ \ra\ F(w) < u$ and we can have $F_n(w) < u$ for large $n$ since $F_n$ is converging to $F$ at $w$. Thus, $F_n^*(u) > w$ by switching formula again.

\item [(ii)] By Proposition \ref{pro:switching_formula_inverse_distribution_function_open_set_sup}, we have $F^*(u^+) <y \ \ra\ u < F(y^-)$. Since $y$ is continuous point of $F$,
\be
u < F(y^-) = F(y) = \lim_{n\to \infty}F_n(y).
\ee

Thus, $F_n(y) > u$ for large $n$. Then $F_n(y) \geq u \ \ra \ F_n^*(u) \leq y$ for large $n$ by switching formula.

\item [(iii)] For any $\ve>0$, we have $F^*(u) > F^*(u)-\ve$. Then by Theorem \ref{thm:non_decreasing_function_continuous_points_dense_set}, we can find $0<\ve' < \ve$ such that $F^*(u)-\ve'$ is a continuous point of $F$. Then by (i), we have that
\be
F^*_n(u) > F^*(u) - \ve' \ \ra\ \inf_{m \geq n} F^*_m(u) \geq F^*(u) \ \ra\ \liminf_n F^*_n(u) \geq F^*(u).
\ee

Similarly, for any $\ve>0$, we have $F^*(u^+) < F^*(u^+) + \ve$. Then by Theorem \ref{thm:non_decreasing_function_continuous_points_dense_set}, we can find $0<\ve' < \ve$ such that $F^*(u^+)+\ve'$ is a continuous point of $F$. Then by (ii), we have that
\be
F^*_n(u) \leq F^*(u^+) + \ve' \ \ra\ \sup_{m \geq n} F^*_m(u) \leq F^*(u^+) \ \ra\ \limsup_n F^*_n(u) \leq F^*(u^+).
\ee

Therefore, we have the required results.

\item [(iv)] By (iii), for all continuous points $u$ of $F^*$, $F^*(u) = F^*(u^+)$. Thus,
\be
\liminf_n F^*_n(u) = \limsup_n F^*_n(u) = \lim_{n\to \infty} F^*_n(u) = F^*(u) .
\ee
\een

Then with the similar argument, we can have $(\dag) \ra (*)$, as required.
\end{proof}




\begin{example}
If $u$ is not a continuous point of $F^*$, then $F^*(u)$ may not converges as $n\to \infty$.

Let $F(x)$ and $F^*(x)$ be the following two distribution functions in the graph. Also, $F^*(u)$ and $F^*_n(u)$ are their inverse distribution function for $u\in (0,1)$.

Then we can easily see that for any continuous point $x$ of $F$, $\lim_{n\to\infty}F_n(x) = F(x)$. However, for the discontinuous point $1/2$ of $F^*$, we have $F^*(1/2) = 1/3$ and $F^*_n(1/2) = 1/2$ for all $n$. Thus, $\lim_{n\to\infty} F^*_n(1/2) \neq F^*(1/2)$.
%
%\centertexdraw{
%
%\drawdim in
%
%\def\bdot {\fcir f:0 r:0.03 }
%\def\ebdot {\lcir r:0.02 }
%\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
%\linewd 0.01 \setgray 0
%
%\move(0 1.7)
%\move (0 0) \lvec(0.5 0.5) \lvec(1 0.5) \lvec(1.5 1)
%\move(0 -0)\lvec(1.5 -0)
%\move(0 0)\lvec(0 1)
%
%\htext (0.9 0.7){$F(x)$}
%
%
%\htext (0.4 -0.15){$1/3$}
%\htext (0.9 -0.15){$2/3$}
%\htext (-0.3 0.4){$1/2$}
%\htext (-0.1 0.95){1}
%\htext (-0.1 0){0}
%
%\move(0 -0)\lvec(1.5 -0)
%\move(0 0)\lvec(0 1)
%
%\htext (2 0.7){$\ra$}
%
%%-----------------
%
%\move (3 0) \lvec(3.5 0.5) \bdot
%\move (3.5 1)\ebdot \lvec(4 1.5)
%
%\move(3 0)\lvec(4 -0)
%\move(3 0)\lvec(3 1.5)
%
%\htext (2.75 0.4){$1/3$}
%\htext (2.75 0.9){$2/3$}
%\htext (3.4 -0.15){$1/2$}
%
%\htext (2.9 1.45){1}
%\htext (2.9 0){0}
%
%\htext (3.6 0.8){$F^*(u)$}
%
%%--------------
%
%\lpatt (0.05 0.05)
%\move(0 1)\lvec(1.5 1)
%\move (0.5 0.5)\lvec(0.5 0)
%\move (1 0.5)\lvec(1 0)
%
%\move (0.5 0.5)\lvec(0 0.5)
%\move (1.5 0)\lvec(1.5 1)
%
%
%%\move (3 0.5)
%
%\move(3.5 1) \lvec(3.5 0)
%\move(3.5 1) \lvec(3 1)
%\move(3.5 0.5) \lvec(3 0.5)
%
%\move(3 1.5) \lvec(4 1.5) \lvec(4 0)
%
%\move(0 -0.4)
%}
%
%\centertexdraw{
%
%\drawdim in
%
%\def\bdot {\fcir f:0 r:0.03 }
%\def\ebdot {\lcir r:0.02 }
%\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
%\linewd 0.01 \setgray 0
%
%\move (0 0) \lvec(0.5 0.4) \lvec(1 0.6) \lvec(1.5 1)
%\move(0 0)\lvec(1.5 0)
%\move(0 0)\lvec(0 1)
%
%\htext (0.85 0.75){$F_n(x)$}
%
%
%\htext (0.4 -0.15){$1/3$}
%\htext (0.9 -0.15){$2/3$}
%%\htext (-0.3 0.4){$1/2$}
%\htext (-0.65 0.5){$1/2+1/n$}
%\htext (-0.65 0.3){$1/2-1/n$}
%\htext (-0.1 0.95){1}
%\htext (-0.1 0){0}
%
%\move(0 -0)\lvec(1.5 -0)
%\move(0 0)\lvec(0 1)
%
%\htext (2 0.7){$\ra$}
%
%%-----------------
%
%\move (3 0) \lvec(3.4 0.5)\lvec(3.6 1)\lvec(4 1.5)
%
%\move(3 0)\lvec(4 -0)
%\move(3 0)\lvec(3 1.5)
%
%\htext (2.75 0.4){$1/3$}
%\htext (2.75 0.9){$2/3$}
%\htext (2.75 0.65){$1/2$}
%\htext (3.4 -0.15){$1/2$}
%
%\htext (2.9 1.45){1}
%\htext (2.9 0){0}
%
%\htext (3.6 0.8){$F^*_n(u)$}
%
%%--------------
%
%\lpatt (0.05 0.05)
%\move(0 1)\lvec(1.5 1)
%\move (0.5 0.4)\lvec(0.5 0)
%\move (1 0.6)\lvec(1 0)
%
%\move (0.75 0.5)\lvec(0 0.5)
%\move (1.5 0)\lvec(1.5 1)
%
%\move (0.5 0.4)\lvec(0 0.4)
%\move (1 0.6)\lvec(0 0.6)
%
%%\move (3 0.5)
%
%\move(3.5 1) \lvec(3.5 0)
%\move(3.6 1) \lvec(3 1)
%\move(3.4 0.5) \lvec(3 0.5)
%\move(3.5 0.75) \lvec(3 0.75)
%
%\move(3 1.5) \lvec(4 1.5) \lvec(4 0)
%}



\begin{center}
\psset{yunit=3cm,xunit=3cm}
\begin{pspicture}(0,-0.2)(4,1.5)
\psline(0,0)(0.5,0.5)(1,0.5)(1.5,1)
\psline(1.5,0)(0,0)(0,1)
\psline[linestyle=dashed](0,1)(1.5,1)(1.5,0)
\psline[linestyle=dashed](0.5,0)(0.5,0.5)(0,0.5)
\psline[linestyle=dashed](1,0.5)(1,0)

\rput[lb](0.9,0.7){$F(x)$}
\rput[lb](0.4,-0.15){$1/3$}
\rput[lb](0.9,-0.15){$2/3$}
\rput[lb](-0.3,0.4){$1/2$}
\rput[lb](-0.1,0.95){1}
\rput[lb](-0.1,0){0}

\rput[lb](2.2,0.7){$\ra$}

\pstGeonode[PointSymbol=*,PointName=none,dotscale=1.2](3.5,0.5){A}
\pstGeonode[PointSymbol=o,PointName=none,dotscale=1.2](3.5,1){B}

\psline(4,0)(3,0)(3,1.5)
\rput[lb](2.75,0.4){$1/3$}
\rput[lb](2.75,0.9){$2/3$}
\rput[lb](3.4,-0.15){$1/2$}

\rput[lb](2.9,1.45){1}
\rput[lb](2.9,0){0}
\rput[lb](3.6,0.8){$F^*(u)$}

\psline(3,0)(3.5,0.5)
\psline(3.5,1)(4,1.5)
\psline[linestyle=dashed](3,1.5)(4,1.5)(4,0)
\psline[linestyle=dashed](3.5,0)(3.5,1)(3,1)
\psline[linestyle=dashed](3.5,0.5)(3,0.5)

\end{pspicture}
\end{center}

\begin{center}
\psset{yunit=3cm,xunit=3cm}
\begin{pspicture}(0,-0.2)(4,1.5)
\psline(0,0)(0.5,0.4)(1,0.6)(1.5,1)
\psline(1.5,0)(0,0)(0,1)
\psline[linestyle=dashed](0,1)(1.5,1)(1.5,0)
\psline[linestyle=dashed](0.5,0)(0.5,0.4)(0,0.4)
\psline[linestyle=dashed](0,0.6)(1,0.6)(1,0)

\rput[lb](0.9,0.7){$F_n(x)$}
\rput[lb](0.4,-0.15){$1/3$}
\rput[lb](0.9,-0.15){$2/3$}
\rput[lb](-0.55,0.55){$1/2+1/n$}
\rput[lb](-0.55,0.35){$1/2-1/n$}
\rput[lb](-0.1,0.95){1}
\rput[lb](-0.1,0){0}

\rput[lb](2.2,0.7){$\ra$}

\psline(4,0)(3,0)(3,1.5)
\psline(3,0)(3.4,0.5)(3.6,1)(4,1.5)
\rput[lb](2.75,0.4){$1/3$}
\rput[lb](2.75,0.9){$2/3$}
\rput[lb](2.75,0.65){$1/2$}
\rput[lb](3.4,-0.15){$1/2$}

\rput[lb](2.9,1.45){1}
\rput[lb](2.9,0){0}
\rput[lb](3.6,0.8){$F^*_n(u)$}

\psline[linestyle=dashed](3,1.5)(4,1.5)(4,0)
\psline[linestyle=dashed](3.5,0)(3.5,1)
\psline[linestyle=dashed](3,1)(3.6,1)
\psline[linestyle=dashed](3.5,0.75)(3,0.75)
\psline[linestyle=dashed](3.4,0.5)(3,0.5)

\end{pspicture}
\end{center}
\end{example}

\subsection{Probability integral transform}

\begin{theorem}[probability integral transform (PIT)]\label{thm:probability_integral_transform}
For any random variable $X$ with distribution function $F$. Then probability integral transform is $Y:=F(X)$ and
\be
\pro\brb{F(X)\leq u} \leq u \quad\forall u\in (0,1).
\ee

Moreover, the transform $Y:=F(X)$ is $[0,1]$ uniformly distributed iff $F$ is continuous, that is,
\be
\pro\brb{Y\leq u} = \pro\brb{F(X)\leq u} = u \quad\forall u\in (0,1) \ \lra\ F\text{ is continuous.}
\ee%In this case, the transform is called probability integral transform.
\end{theorem}

\begin{proof}[\bf Proof]
Let $U$ be $(0,1)$ uniformly distributed and $X\sim F^*(U)$ by Theorem \ref{thm:inverse_probability_transformation}.

%Since $F$ is continuous, by Proposition \ref{pro:switching_formula_inverse_distribution_function_open_set_sup} we can have for $y\in (0,1)$,
%\be
%F(x) = F(x^-)\leq y \ \lra \ F^*(y^+) \geq x\qquad (*)
%\ee
%where $F^*$ is inverse distribution function of $F$. Thus,
%\beast
%F_Y(y) & = & \pro\brb{Y\leq y} = \pro\brb{F(X) \leq y} = \pro\brb{F^*(y^+)\geq X}\\
%& = & \pro\brb{X\leq F^*(y^+)} = F\brb{F^*(y^+)} = y^+ = y
%\eeast
%by Corollary \ref{cor:continuous_distribution_function_inverse_composition}.(i).

Also, we have $F\brb{F^*(U)}\geq U$ by Proposition \ref{pro:inverse_distribution_properties}.(iv) and therefore for all $u\in (0,1)$
\be
\pro\brb{F(X)\leq u} = \pro\brb{F(F^*(U))\leq u} \leq \pro\brb{U\leq u}  = u.
\ee

If $F$ is continuous, then $U = F\brb{F^*(U)}$ by Corollary \ref{cor:continuous_distribution_function_inverse_composition}, so
\be
F(X)\sim F(F^*(U)) = U \sim \ud{0}{1}.
\ee

On the other hand, if $F$ is not continuous, then there exists an $x\in \R$ such that
\be
0< F(x) - F(x^-) = \pro\brb{X = x} \leq \pro\brb{F(X) = F(x)}.
\ee

But $\pro\brb{U = F(x)} = 0$, so $F(X) \nsim U$.
\end{proof}

\subsection{Independence of random variables}


\begin{definition}\label{def:sigma_algebra_generated_by_random_variable}
Recall Definition \ref{def:sigma_algebra_generated_by_measurable_function}, given a countable family of random variables $X_i : \Omega \to G, i \in I$, the $\sigma$-algebra generated by $(X_i : i \in I)$ is
\be
\sigma(X_i, i \in I) = \sigma\brb{X^{-1}_i (A) : A \in \sG, i \in I}.
\ee
\end{definition}

\begin{definition}\label{def:random_variable_independent}
A countable family of random variables $(X_i : i \in I)$ is said to be independent (or $X_i$ are independent) if the $\sigma$-algebras $(\sigma(X_i) : i \in I)$ are independent.
\end{definition}

\begin{proposition}\label{pro:independent_sigma_algebra_random_variable}
If $\sF$ and $\sG$ are independent and random variables $X$ is $\sF$-measurable, $Y$ is $\sG$-measurable, then $X$ and $Y$ are independent.
\end{proposition}

\begin{proof}[\bf Proof]
First we have $\sigma(X) \subseteq \sF$ and $\sigma (Y)\subseteq \sG$. Then $\forall A\in \sigma(X),B\in \sigma(Y)$, we have
\be
\pro(A\cap B) = \pro(A)\pro(B)
\ee
since $A\in \sF,B\in \sG$ and $\sF,\sG$ are independent.
\end{proof}

\begin{proposition}\label{pro:independent_and_measurable}
Let $(\Omega,\sF,\pro)$ be a probability space. Let $X$ and $Y$ be two random variables. If $X$ is independent of $\sF$ and $Y$ is $\sF$-measurable, then $X$ and $Y$ are independent.
\end{proposition}

\begin{proof}[\bf Proof]
$Y$ is $\sF$-measurable, i.e., $\sigma(Y) \subseteq \sF$. Since $X$ is independent of $\sF$, i.e., for any $A\in \sigma(X)$, $B\in \sF$
\be
\pro(A\cap B) = \pro(A)\pro(B).
\ee

Since $\sigma(Y) \subseteq \sF$, the equation holds for all $A\in \sigma(X)$ and $B\in \sigma(Y)$, thus $X$ and $Y$ are independent (by Definition \ref{def:random_variable_independent}).
\end{proof}

\begin{theorem}\label{thm:random_variable_function_indenpence}
Let $(\Omega,\sF,\pro)$ be a probability space. If $(X_{i,j})_{1 \leq i \leq n,1 \leq j \leq m(i)}$ are independent and $f_i : \R^{m(i)} \to \R$ are measurable then $f_i(X_{i,1},\dots,X_{i,m(i)})$ are independent.
\end{theorem}

\begin{proof}[\bf Proof]
Let $\sF_{i,j} = \sigma(X_{i,j})$ and $\sG_i = \sigma \brb{\bigcup_j\sF_{i,j}}$. Then by Theorem \ref{thm:pi_sigma_independent_finite_many_ij}, $\sG_i$ are independent. Since $f_i(X_{i,1}, \dots,X_{i,m(i)})$ is $\sG_i$-measurable (as $f_i$ are measurable\footnote{need details}), we have $f_i(X_{i,1}, \dots,X_{i,m(i)})$ are independent by Proposition \ref{pro:independent_sigma_algebra_random_variable}.
\end{proof}

\begin{remark}
What if $(X_{i,j})_{1\leq j\leq m(i)}$ are not independent?\footnote{need proof}
\end{remark}

\begin{proposition}\label{pro:random_variable_independent}
Let $(X_n : n \in \N)$ be a sequence of real-valued random variables on $(\Omega,\sF,\pro)$ and suppose that for all for all $x_1, \dots, x_n \in \R$ and all $n$,
\be
\pro(X_1 \leq x_1, \dots ,X_n \leq x_n) = \pro(X_1 \leq x_1) \dots \pro(X_n \leq x_n)
\ee
if and only if $X_1,\dots, X_n$ are independent.
\end{proposition}

\begin{remark}
The equation is equivalent to $\pro(X_1 \in dx_1, \dots ,X_n \in dx_n) = \pro(X_1 \in dx_1) \dots \pro(X_n \in dx_n)$.
\end{remark}

\begin{proof}[\bf Proof]
($\la$) It the direct result from Definition \ref{def:independent_set}, \ref{def:sigma_algebra_generated_by_random_variable} and \ref{def:random_variable_independent}.

($\ra$) $X_i$ are independent means that $\sigma(X_i)$ are independent. From \ref{thm:pi_sigma_independent_finite_many}, we know that if $\sA_i$ are $\pi$-systems for which
\be
\pro\lob\bigcap^n_{i =1}A_i\rob = \prod^n_{i=1}\pro(A_i),\quad\quad A_i \in \sA_i
\ee
whenever $A_i\in \sA_i$. Then $\sigma(\sA_i)$ are independent. Clearly, $\{\{\omega: X(\omega)\leq x\},\ x\in\R\}$ is a $\pi$-system since
\be
\left\{\ba{ll}
A:= \{\{\omega: X(\omega)\leq x\},\ x\in\R\} \in \sA\\
B:= \{\{\omega: X(\omega)\leq y\},\ y\in\R\} \in \sA
\ea\right. \ \ra\ \emptyset\in \sA, \ A\cap B = A\text{ or }B \in \sA.
\ee

Thus, It suffices to show that
\be
\{\{\omega: X(\omega)\leq x\},\ x\in\R\}=\{X^{-1}(-\infty,x],\ x\in \R\} \quad\text{ generates }\quad \sigma(X) = \sigma(\{X^{-1}(B), B\in \sB\})
\ee
where $\sB$ is Borel $\sigma$-algebra on $\R$. It is equivalent to show that
\be
\sigma\lob\{X^{-1}(B), B\in \sB\}\rob \subseteq \sigma\lob \{X^{-1}(-\infty,x],\ x\in \R\} \rob
\ee
Then it suffices to show that
\be
\sA := \left\{B: X^{-1}(B) \in \sigma\lob \{X^{-1}(-\infty,x],\ x\in \R\} \rob,\ B\in \sB\right\} \supseteq \sB = \sigma\lob\{ (-\infty,x],\ x\in \R\} \rob
\ee
Then it is obvious that $\{(-\infty,x],\ x\in \R\} \subseteq \sA$.

Thus, it is enough to show that $\sA$ is a $\sigma$-algebra. (This follows by Proposition \ref{pro:inverse_image_preserves_set_operation}.)
\ben
\item [(i)] $B=\emptyset\in \sB$, then $X^{-1}(B)=\emptyset \in \sigma\lob \left\{X^{-1}(-\infty,x],\ x\in \R\right\} \rob$. So $\emptyset\in\sA$.
\item [(ii)] If $B\in \sA$, $X^{-1}(B) \in \sigma\lob \{X^{-1}(-\infty,x],\ x\in \R\} \rob$ and $B\in \sB$. Then
\be
B^c \in \sB,\quad X^{-1}(B^c) = \lob X^{-1}(B)\rob^c\in \sigma\lob \{X^{-1}(-\infty,x],\ x\in \R\} \rob \ \ra \ B^c\in \sA
\ee
\item [(iii)] If $B_n\in\sA \ \ra \ B_n\in \sB, \ \bigcup\limits_nB_n\in\sB$ and
\be
X^{-1}\lob\bigcup\limits_nB_n\rob = \bigcup\limits_n X^{-1}(B_n) \in \sigma\lob \{X^{-1}(-\infty,x],\ x\in \R\} \rob \ \ra \ \bigcup\limits_nB_n\in \sA.
\ee
\een
Hence, $\sA$ is a $\sigma$-algebra.
\end{proof}

\begin{remark}
A sequence of random variables $(X_n : n \geq 0)$ is often regarded as a process\index{process} evolving in time. The $\sigma$-algebra generated by $X_0, \dots,X_n$, $\sF_n = \sigma(X_0, \dots,X_n)$ contains those events depending (measurably) on $X_0, \dots,X_n$ and represents what is known about the process by time $n$.
\end{remark}



\begin{proposition}
Let $X_1,X_2,\dots$ be independent random variables with distribution uniform on $[0,1]$. Let $A_n$ be the event that a record\index{record!probability} occurs at time $n$, that is,
\be
X_n > X_m \quad \text{for all }m<n.
\ee
Then $A_1,A_2,\dots$ are independent. Also, with probability one, infinitely many records occur.
\end{proposition}

\begin{proof}[\bf Proof]
$\pro(A_n)$ means the probability that the maximum of $n$ appears at $n$, then $\pro(A_n) = 1/n$. Also
\be
A_1\cap A_2 \cap \dots \cap A_n \ \ra \ X_1<X_2<\dots< X_n
\ee
which is a increasing order. Then we have $\pro(A_1\cap A_2 \cap A_n) = 1/n!$. Thus, it is obvious that
\be
\pro(A_1\cap A_2 \cap A_n) = \prod_n \pro(A_n) \ \ra \ A_1,A_2,\dots \text{ are independent.}
\ee
With Second Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_2}), we have $\sum_n \pro(A_n)=\sum_n \frac 1n=\infty$, then $\pro(A_n \ \text{i.o.}) = 1$.
\end{proof}

\begin{proposition}\label{pro:measurable_independent_measurable}
Let $X$ be a real-valued $\sG$-measurable random variable. If $\sH$ is independent of $\sigma\brb{\sigma(X),\sG}$, then
\be
X\text{ is $\sigma(\sG,\sH)$-measurable.}
\ee
\end{proposition}

\begin{proof}[\bf Proof]
By theorem , we only need to check the sets in $\pi$-system of $\sB(\R)$. Thus, for any $x \in \R$, $A = \bra{X\leq x}$, thus $X^{-1}(A) = G \in \sigma(X)\subseteq \sG$ by assumption.

Now support that
\be
X^{-1}(A) = G\cap H,\qquad G\in \sG, H\in \sH.
\ee
%Note that $G$ and $H$ must exist as we can take $H =\Omega$.

Therefore, Since $\sH$ is independent of $\sigma(\sigma(X),\sG)$, then
\be
\pro\brb{\omega:X(\omega)\leq x} = \pro\brb{G\cap H} = \pro(G)\pro(H) = \pro\brb{X^{-1}(A)}\pro(H) = \pro\brb{\omega:X(\omega)\leq x}\pro(H)
\ee
which implies that $\pro(H)=1$. Since $\Omega$ is the biggest set satisfying this condition, we can have $H = \Omega$ and thus $X^{-1}(A) = G\cap \Omega = G$ by definition of inverse image. Since $G\in \sG$ and $H=\Omega \in \sH$, $G\cap H \in \sigma(\sG,\sH)$. Thus, $X$ is $\sigma(\sG,\sH)$-measurable.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Rademacher functions}\label{subsec:rademacher_function}

We continue with the particular choice of probability space $(\Omega, \sF, \pro)$ made in the preceding section. Provided that we forbid infinite sequences of 0's, each $\omega \in \Omega$ has a unique binary expansion
\be
\omega = 0.\omega_1\omega_2\omega_3 \dots
\ee

Define random variables $R_n : \Omega \to \{0, 1\}$ by $R_n(\omega) = \omega_n$. Then
\be
R_1 = \ind_{(\frac 12 ,1]},\quad R_2 = \ind_{(\frac 14 ,\frac 12]} + \ind_{(\frac 34 ,1]},\quad R_3 = \ind_{( \frac 18 , \frac 14]} + \ind_{( \frac 38 , \frac 12 ]} + \ind_{(\frac 58 , \frac 34]} + \ind_{( \frac 78 ,1]}.
\ee

These are called the Rademacher functions\index{Rademacher functions}. The random variables $R_1,R_2, \dots$ are independent and Bernoulli\index{Bernoulli}, that is to say
\be
\pro(R_n = 0) = \pro(R_n = 1) = 1/2.
\ee
Let $E = \{0, 1\}^\N$ and let $\sE$ denote the $\sigma$-algebra generated by events of the form
\be
A = \{\omega \in E : \omega_1 = y_1, \dots, \omega_n = y_n\},\ n \in \N,\ y_i \in \{0, 1\}.
\ee
Let $R = (R_1, \dots, ) \in E$. Then $R$ is a random variable in $(E,\sE)$. The distribution $\mu$ of $R$ is then a probability measure on $(E,\sE)$, under which the events $\{\omega_n = y_n\}, n \in \N$, are independent. This is the infinite (fair) coin-toss measure.

We now use a trick involving the Rademacher functions to construct on $\Omega = (0, 1]$, not just one random variable, but an infinite sequence of independent random variables with given distribution functions.

\begin{proposition}
Let $(\Omega, \sF, \pro)$ be the probability space of Lebesgue measure on the Borel subsets of $(0, 1]$. Let $(F_n : n \in \N)$ be a sequence of distribution functions. Then there exists a sequence $(X_n : n \in \N)$ of independent random variables on $(\Omega, \sF, \pro)$ such that $X_n$ has distribution function $F_{X_n} = F_n$ for all $n$.
\end{proposition}

\begin{proof}[\bf Proof]
Choose a bijection $m : \N^2 \to \N$ and set $Y_{k,n} = R_{m(k,n)}$, where $R_m$ is the $m$th Rademacher function. Set
\be
Y_n = \sum^\infty_{k=1} 2^{-k}Y_{k,n}.
\ee
Then $Y_1, Y_2, \dots$ are independent and, for all $n$, for $i2^{-k} = 0.y_1\dots y_k$, we have
\be
\pro(i2^{-k} < Y_n \leq (i + 1)2^{-k}) = \pro(Y_{1,n} = y_1, \dots, Y_{k,n} = y_k) = 2^{-k}
\ee
so $\pro(Y_n \leq x) = x$ for all $x \in (0, 1]$. Set
\be
G_n(y) = \inf\bra{x : y \leq F_n(x)}
\ee%%{lem:right_continuous} \ref{lem:right_continuous}
then, by Theorem \ref{thm:switching_formula_distribution_function}, $G_n$ is Borel and $G_n(y) \leq x$ if and only if $y \leq F_n(x)$. So, if we set $X_n = G_n(Y_n)$, then $X_1,X_2, \dots$ are independent random variables on $\Omega$ and
\be
\pro(X_n \leq x) = \pro(G_n(Y_n) \leq x) = \pro(Y_n \leq F_n(x)) = F_n(x).
\ee
Thus, $X_n$ has distribution function $F_{X_n} = F_n$ for all $n$.
\end{proof}

\subsection{Tail events}

\begin{definition}[tail $\sigma$-algebra\index{tail sigma-algebra@tail $\sigma$-algebra}]\label{def:tail_sigma_algebra}
Let $(X_n : n \in \N)$ be a sequence of random variables. Define
\be
\sT_n = \sigma(X_m:m\geq n+1) = \sigma(X_{n+1},X_{n+2}, \dots),\quad \sT_\infty = \bigcap_n \sT_n.
\ee
\end{definition}

Then $\sT_\infty$ is a $\sigma$-algebra, called the tail $\sigma$-algebra of $(X_n : n \in \N)$. It contains the events which depend only on the limiting behaviour of the sequence.

\begin{theorem}[Kolmogorov's 0-1 law\index{Kolmogorov's 0-1 law}]\label{thm:kolmogorov_0_1}
Suppose that $(X_n : n \in \N)$ is a sequence of independent random variables. Then the tail $\sigma$-algebra $\sT_\infty$ of $(X_n : n \in \N)$ contains only events of probability 0 or 1. Moreover, any $\sT_\infty$-measurable random variable is almost surely constant.
\end{theorem}

\begin{proof}[\bf Proof]
Set $\sF_n = \sigma(X_1, \dots,X_n)$. Then $\sF_n$ is generated by the $\pi$-system of events
\be
A = \{X_1 \leq x_1, \dots ,X_n \leq x_n\}
\ee
whereas $\sT_n$ is generated by the $\pi$-system of events
\be
B = \{X_{n+1} \leq x_{n+1}, \dots ,X_{n+k} \leq x_{n+k}\},\quad k \in \N.
\ee

We have $\pro(A\cap B) = \pro(A)\pro(B)$ for all such $A$ and $B$, by independence. Hence $\sF_n$ and $\sT_n$ are independent, by Theorem \ref{thm:pi_sigma_independent} (Independence). It follows that $\sF_n$, $n\in\N$ and $\sT_\infty$ are independent since $\forall A_n \in \sF_n$, $\forall B\in \sT \subseteq \sT_n$,
\be
\pro\brb{A_n \cap B} = \pro(A_n)\pro(B). \quad\quad (\text{by independence of $\sF_n$ and $\sT_n$})
\ee

Now $\bigcup_n \sF_n$ is a $\pi$-system which generates the $\sigma$-algebra $\sF_\infty = \sigma(X_n : n \in \N)$. So by Theorem \ref{thm:pi_sigma_independent} again, $\sF_\infty$ and $\sT_\infty$ are independent, since $\forall A \in \sF_\infty$, $\forall B\in \sT_\infty$, we can find that $A\in \sF_m$ for some $m\in \N$,
\be
\pro\brb{A \cap B} = \pro(A)\pro(B).\quad\quad (\text{by independence of $\sF_m$ and $\sT_\infty$})
\ee

But $T \subseteq \sF_\infty$. So, if $A \in \sT_\infty$,
\be
\pro(A) = \pro(A \cap A) = \pro(A)\pro(A) \ \ra \ \pro(A) \in \{0, 1\}.
\ee

Finally, if $Y$ is any $\sT_\infty$-measurable random variable, then $F_Y (y) = \pro(Y \leq y)$ takes values in $\{0, 1\}$, so $\pro(Y = c) = 1$, where $c = \inf\{y : F_Y (y) = 1\}$.
\end{proof}


\begin{example}
Let $U$ be a uniform random variable on $[0, 1]$ (see Definition \ref{def:uniform_rv}) and let $R_n$ be its binary expansion. Then $R_n$ are independent and identically distributed (i.i.d.).

The strong law of large numbers (Theorem \ref{thm:slln}) applies here to show that $n^{-1}\sum^n_{i=1} R_i$ converges to 1/2, almost surely. Reformulating this using the canonical probability space $(\Omega, \sF, \pro)$,
\be
\pro\brb{\left\{\omega \in (0, 1]: \frac{|\{k \leq n : \omega_k = 1\}|}n \to \frac 12\right\}} = \pro\brb{\frac{R_1 + \dots+ R_n}n \to \frac 12} = 1.
\ee
This is called Borel's normal number theorem (Theorem \ref{thm:borel_normal}).
\end{example}

\subsection{Large values in sequences of IID random variables}

Consider a sequence $(X_n : n \in \N)$ of independent random variables, all having the same distribution function $F$. Assume that $F(x) < 1$ for all $x \in \R$. Then, almost surely, the sequence $(X_n : n \in \N)$ is unbounded above, so $\limsup_n X_n = \infty$. A way to describe the occurrence of large values in the sequence is to find a function $g : \N \to (0,\infty)$ such that, almost surely,
\be
\limsup_n X_n/g(n) = 1.
\ee

\begin{example}
We now show that $g(n) = \log n$ is the right choice when $F(x) = 1 - e^{-x}$. The same method adapts to other distributions.

Fix $\alpha > 0$ and consider the event $A_n = \{X_n \geq \alpha \log n\}$. Then
\be
\pro(A_n) = e^{-\alpha \log n} = n^{-\alpha},
\ee
so the series $\sum_n \pro(A_n)$ converges if and only if $\alpha > 1$. By the Borel-Cantelli lemmas (Lemma \ref{lem:borel_cantelli_1_measure}, \ref{lem:borel_cantelli_2}), we deduce that, for all $\ve > 0$,
\be
\pro(X_n/ \log n \geq 1 \text{ i.o.}) = 1,\quad\quad \pro(X_n/ \log n \geq 1 + \ve \text{ i.o.}) = 0.
\ee
Hence, almost surely,
\be
\limsup_n X_n/ \log n = 1.
\ee
\end{example}

\begin{example}
Let $X_1,X_2,\dots$ be independent $\sN(0,1)$ random variables. Let
\be
A=\left\{\omega:\lim\sup_n\left\{X_n(\omega)\left/\sqrt{2\log n }\right. \geq 1 \right\}  \right\},\quad B=\left\{\omega:\lim\sup_n\left\{X_n(\omega)\left/\sqrt{2\log n }\right. > 1 \right\}  \right\}
\ee

We want to show that $\pro(A)=1$ and $\pro(B)=0$. First, consider the event $\left\{ X_n(\omega)\left/\sqrt{2\log n }\right. \geq \alpha \right\}$
\beast
\sum_n\pro\lob X_n(\omega)\left/\sqrt{2\log n }\right. \geq \alpha \rob & = & \sum_n \int^\infty_{-\infty}\frac1{\sqrt{2\pi}}e^{-\frac 12 x^2}\ind_{\left\{x\geq \alpha \sqrt{2\log n}\right\}}dx \quad\quad (\text{standard Gaussian density (Definition \ref{def:standard_gaussian_density})})\\
& = & \int^\infty_{-\infty}\frac1{\sqrt{2\pi}}e^{-\frac 12 x^2}\sum_n \ind_{\left\{ n \leq \exp\left\{\frac 12 \frac {x^2}{\alpha^2}\right\}\right\}}dx \quad\quad (\text{by Fubini's theorem (Theorem \ref{thm:fubini})})\\
& = & \int^\infty_{-\infty}\frac1{\sqrt{2\pi}}e^{-\frac 12 x^2}\left\lfloor e^{\frac 12 x^2/\alpha^2} \right\rfloor dx < \int^\infty_{-\infty}\frac1{\sqrt{2\pi}}e^{-\frac 12 x^2\brb{1-\frac 1{\alpha^2}}} dx < \infty \ \Leftrightarrow\ \alpha >1.
\eeast

Thus, let $\alpha = 1$,
\beast
\sum_n\pro\brb{ X_n(\omega)\left/\sqrt{2\log n }\right.} & = & \sum_n \int^\infty_{-\infty}\frac1{\sqrt{2\pi}}e^{-\frac 12 x^2}\ind_{\left\{x\geq \sqrt{2\log n}\right\}}dx = \int^\infty_{-\infty}\frac1{\sqrt{2\pi}}e^{-\frac 12 x^2}\floor{e^{\frac 12 x^2} } dx \\
& > & \int^\infty_{-\infty}\frac1{\sqrt{2\pi}}e^{-\frac 12 x^2}\brb{e^{\frac 12 x^2}-1 } dx = \frac1{\sqrt{2\pi}} \int^\infty_{-\infty}dx -1 =  \infty.
\eeast

With Second Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_2}), we have $\pro(A)=1$.

Also, we have $\alpha =1+1/m,\ m\in \N$, $\sum_n\pro\lob X_n(\omega)\left/\sqrt{2\log n }\right. \geq \alpha \rob < \infty$. Then by First Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_1_measure}),
\be
\pro\lob \left\{\omega: X_n(\omega)\left/\sqrt{2\log n }\right. \geq 1+1/m \right\} \text{ i.o.}\rob = 0
\ee
and since $B = \bigcup_{m\in \N}\lob \left\{\omega: X_n(\omega)\left/\sqrt{2\log n }\right. \geq 1+1/m \right\} \ \text{i.o.}\rob $, we have
\be
\pro(B) \leq \sum_n \pro\lob \left\{\omega: X_n(\omega)\left/\sqrt{2\log n }\right. \geq 1+1/m \right\} \text{ i.o.}\rob = 0 \ \ra \ \pro(B)=0.
\ee

Thus, $\limsup\limits_n(X_n/\sqrt{2\log n})=1$ a.s.
\end{example}



\subsection{Convergence almost surely and convergence in probability}

Recall Definitions \ref{def:convergence_almost_everywhere} and \ref{def:convergence_in_measure}, we have the following two definitions.

\begin{definition}\label{def:convergence_almost_surely}
Let $(\Omega, \sF, \pro)$ be a probability space. We say a sequence of random variables $(X_n : n \in \N)$ converges to $X$ a.s.\index{convergence!almost surely} if
\be
\pro\brb{\{\omega \in \Omega : X_n(\omega) \nrightarrow X(\omega)\}} = 0\quad \lra \quad \pro\brb{\{\omega \in \Omega : X_n(\omega) \to X(\omega)\}} = 1,\quad\quad \text{denoted as $X_n \to X$ a.s.}
\ee
\end{definition}

\begin{definition}\label{def:version_probability}
Let $(\Omega, \sF, \pro)$ be a probability space and $X$ be a random variable. We say a random variable $Y$ is a version of $X$ if $X=Y$ a.s.
\end{definition}

\begin{definition}\label{def:convergence_in_probability}
Let $(\Omega, \sF, \pro)$ be a probability space. For a sequence of random variables $(X_n : n \in \N)$, we have that
\be
\pro\brb{\{\omega \in \Omega : |X_n(\omega) - X(\omega)| > \ve\}} \to 0,\quad \text{ for all }\ve > 0,
\ee
then we say $X_n$ converges to $X$ in probability\index{convergence!in probability!real space}, denoted as $X_n \stackrel{p}{\longrightarrow} X$.
\end{definition}

Recall Theorem \ref{thm:convergence_in_measure}, we have particular theorem for probability.

\begin{theorem}\label{thm:convergence_in_probability}
Let $(\Omega,\sF,\pro)$ be a probability space and $(X_n : n \in \N)$ be a sequence of random variables.
\ben
\item [(i)] If $X_n \to 0$ a.s. then $X_n \stackrel{p}{\longrightarrow} 0$.
\item [(ii)] If $X_n \stackrel{p}{\longrightarrow} 0$ then $X_{n_k}\to 0$ a.s. for some subsequence ($n_k$).
\een
\end{theorem}

For more general case, we define convergence in probability by

\begin{definition}\label{def:convergence_in_probability_metric}
Let $(\Omega, \sF, \pro)$ be a probability space. For a sequence of random variables $(X_n : n \in \N)$ on metric space $(\Omega,d)$, we have that
\be
\pro\brb{\{\omega \in \Omega : d\brb{X_n(\omega),X(\omega)} \geq \ve\}} \to 0,\quad \text{ for all }\ve > 0,
\ee
then we say $X_n$ converges to $X$ in probability\index{convergence!in probability!metric space}, denoted as $X_n \stackrel{p}{\longrightarrow} X$.
\end{definition}

\begin{proposition}
Let $X_n,Y_n$ be two sequences of random variables and $X,Y$ be two random variables such that
\be
X_n \stackrel{p}{\to} X, \quad Y_n \stackrel{p}{\to} Y.
\ee

Then $(X_n,Y_n) \stackrel{p}{\to} (X,Y)$
\end{proposition}

\begin{proof}[\bf Proof]
With triangle inequality of metric ($d\brb{(X_n,Y_n), (X,Y)} \leq d(X_n,X)+d(Y_n,Y)$), we have that for $\ve>0$
\be
\pro\brb{\omega\in \Omega:d\brb{(X_n,Y_n), (X,Y)} \geq \ve } \leq \pro\brb{\omega\in \Omega:d\brb{(X_n,X} \geq \ve/2 } + \pro\brb{\omega\in \Omega:d\brb{(Y_n,Y} \geq \ve/2 } \to 0 + 0 = 0
\ee
as required.
\end{proof}

Recalling Propsoition \ref{pro:uniqueness_limit_convergence_in_measure}, we have

\begin{proposition}[uniqueness of limit of convergence in probability]\label{pro:uniqueness_limit_convergence_in_probability}
Let $(X_n)_{n \in \N}$ be a sequence of random variables and $X,Y$ be random variables. If $X_n \xrightarrow{p} X$ and $X_n \xrightarrow{p} Y$,
\be
X = Y \text{\ a.s.}\quad \text{ i.e., } \pro\brb{X=Y} = 1.
\ee
\end{proposition}


\begin{proposition}[Skorokhod representation of a random variable with prescribed distribution function]\label{pro:skorokhod_representation_distribution_function}
Let $F:\R\to [0,1]$ be distribution function. For probability space $(\Omega,\sF,\pro) = ([0,1],\sB[0,1],\text{Leb})$, define
\beast
X^+(\omega) & := & \inf\bra{x:F(x) > \omega} = \sup\bra{y:F(y)\leq \omega},\\
X^-(\omega) & := & \inf\bra{x:F(x)\geq \omega} = \sup\bra{y:F(y) < \omega}.
\eeast
Then, the random variable $X^-$ has distribution function $F$, and almost surely, $X^+ = X^-$.
\end{proposition}

\begin{proof}[\bf Proof]
By definition of $X^-$, we have
\be
\bra{\omega \leq F(c)} \ \ra \ \bra{X^-(\omega) \leq c}.
\ee

Also,
\be
\bra{x>X^-(\omega)} \ \ra \ \bra{F(x)\geq \omega},
\ee
so, by the right-continuity of $F$, $F(X^-(\omega)) \geq \omega$, and
\be
\bra{X^-(\omega)\leq c} \ \ra \ \bra{\omega \leq F(X^-(\omega)) \leq F(c)}.
\ee

Thus, $\bra{\omega \leq F(c)} \lra \bra{X^-(\omega) \leq c}$, so that
\be
\pro\brb{X^-(\omega)\leq c} = \pro\brb{\omega \leq F(c)}  = F(c),
\ee
i.e., the random variable $X^-$ has distribution function $F$. By definition of $X^+$,
\be
\bra{\omega < F(c)} \ \ra \ \bra{X^+(\omega) \leq c}.
\ee
so that $F(c) \leq \pro(X^+(\omega) \leq c)$. SInce $X^- \leq X^+$, it is clear that
\be
\bra{X^- \neq X^+} = \bigcup_{c\in \Q}\bra{X^-\leq c < X^+}.
\ee

But, for every $c\in \R$,
\be
\pro\brb{\bra{X^- \leq c < X^+}} = \pro\brb{\bra{X^-\leq c}\bs \bra{X^+ \leq c}} \leq F(c) - F(c) = 0.
\ee

Since $\Q$ is countable, the result follows.\footnote{need graph in Williams\cite[P.34]{Williams_1991}}
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Expectation and Convergence in $\sL^p\brb{\Omega,\sF,\pro}$}

\subsection{Expectation}

Recall Definition \ref{def:mu_integral}, in particular,

\begin{definition}\label{def:expectation}
Let $(\Omega, \sF, \pro)$ be a probability space. We shall define, where possible, for a random variable $X : \Omega \to [-\infty,\infty]$, the expectation \index{expectation} of $X$, to be denoted
\be
\E(X) = \int_\Omega X d\pro = \int_\Omega X(\omega)\pro(d\omega).
\ee
\end{definition}

\begin{remark}
For any random variable $X$ denote by $X^+ = \max\bra{X, 0}$, the positive part of $X$, and $X^- = \max\bra{-X, 0}$, the negative part of $X$ are non-negative random variables, for which so that $X = X^+ - X^-$ and $\abs{X} = X^+ + X^-$. Provided not both $\E X^+ = \infty$ and $\E X^- = \infty$, we define the expectation of $X$ to be
\be
\E X = \E X^+ - \E X^-,
\ee
if both $\E X^+$ and $\E X^-$ are infinite then the expectation of $X$ is not defined. In the following, when we write $\E X$ for a random variable $X$, it may be assumed that the expectation of $X$ is well defined (see Definition \ref{def:integral_measurable_function}).
\end{remark}

Recall Definition \ref{def:integral_simple_function}, Theorem \ref{thm:non_negative_measurable_property} and Theorem \ref{thm:lebesgue_integrable_function_property}, we have

\begin{proposition}[properties of expectation]\label{pro:expectation_property}
Let $X,Y$ be random variables and $\E X$, $\E Y$ are well-defined. Then
\ben
\item [(i)] For the indicator of any event $A \subseteq \Omega$ we have $\E\brb{ \ind_A} = \pro(A)$.
\item [(ii)] If $X \geq 0$, then $\E X \geq 0$, and $\E X = 0$ implies that $X = 0$ a.s..
\item [(iii)] If $X,Y \geq 0$ and $a,b\in [0,\infty]$, then $\E(aX + bY) = a\E X+ b\E Y$.
\item [(iv)] If $X,Y$ are integrable and $a, b\in \R$, $\E(aX + bY) = a\E X + b\E Y$.
\een
\end{proposition}

\begin{definition}[moment of random variable\index{moment!random variable}]
For a random variable $X$, the expected values of powers of $X$ are known as moments of $X$, thus $\E (X^r)$ (assuming it is well defined) is the $r$th moment of $X$ and $\E (\abs{X}^r)$ is the $r$th absolute moment of $X$.
\end{definition}

\begin{example}
Let $X_1,X_2,\dots$ be random variables with
\be
X_n=\left\{\ba{ll}
n^2 -1 \quad \quad & \text{with probability }1/n^2\\
-1 \quad \quad & \text{with probability }1-1/n^2
\ea\right.
\ee

Thus,
\be
\E X_n = (n^2-1)\frac 1{n^2} + (-1)\lob 1-\frac 1{n^2}\rob = 0 \ \ra \ \E\lob\frac{X_1 +\dots + X_n}n\rob = 0
\ee

Also, we consider the event
\be
\left\{\frac{X_1 +\dots + X_n}n \nrightarrow -1\right\} = \left\{X_n=n^2-1 \ \ \text{i.o.}\right\}
\ee

However, with First Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_1_probability}), we have
\be
\sum_{n=1}^\infty \pro(X_n = n^2-1)= \sum_{n=1}^\infty \frac 1{n^2} = \frac{\pi^2}6 <\infty \quad\ra\quad \pro\lob X_n=n^2-1 \ \ \text{i.o.}\rob = 0.
\ee
\end{example}

\begin{example}[Another proof of inclusion-exclusion formula.]
For events $A_1,\dots,A_n$, use $\E\brb{\ind_A} = \pro(A)$ (Definition \ref{def:integral_simple_function} and Proposition \ref{pro:expectation_property}), the expression for the product of indicators (Proposition \ref{pro:indicator_function}) and linearity of the expectation (Theorem \ref{thm:lebesgue_integrable_function_property}.(i)),
\beast
\pro(A_1 \cup \dots \cup A_n) & = & \E \brb{\ind_{A_1\cup A_2\cup \dots \cup A_n}} = \E\brb{1 - \prod^n_{i=1} (1 - \ind_{A_i})}\\
& = & \E\brb{\sum_i \ind_{A_i} - \sum_{i_1<i_2} \ind_{A_{i_1}\cap A_{i_2}} + \sum_{i_1<i_2<i_3} \ind_{A_{i_1}\cap A_{i_2} \cap A_{i_3}} - \dots + (-1)^{n-1} \ind_{A_1\cap \dots\cap A_n}}\\
& = & \sum_i \E (\ind_{A_i}) - \sum_{i_1<i_2} \E\brb{\ind_{A_{i_1}\cap A_{i_2}}} + \dots + (-1)^{n-1}\E \brb{\ind_{A_1\cap \dots \cap A_n}}\\
& = & \sum_i \pro(A_i) - \sum_{i_1<i_2} \pro\brb{A_{i_1} \cap A_{i_2}} + \dots + (-1)^{n-1} \pro(A_1 \cap \dots \cap A_n),
\eeast
which is the required expression for the inclusion-exclusion formula.
\end{example}


\begin{example}
Let $X$ be a non-negative integer-valued random variable. We have
\be
\sum^\infty_{n=1}\pro(X\geq n) = \sum^\infty_{n=1} \sum^\infty_{k=n}\pro(X=k)= \sum^\infty_{k=1} \sum^k_{n=1}\pro(X=k)= \sum^\infty_{k=1} k\pro(X=k)=\E(X).
\ee

If $\E(X)=\infty$, let $X_1,X_2,\dots$ is a sequence of independent random variables with the same distribution as $X$, then let $A_n=\{X_n\geq n\}$. Since $\sum^\infty_{n=1}\pro(A_n) = \E(X) =\infty$, By the second Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_2}), we have then
\be
\pro(A_n \text{ i.o.})=\pro(X_n\geq n \text{ i.o.})=1 \ \ra \ \limsup_n (X_n/n)\geq 1 \quad \text{a.s.}
\ee

Hence, if $\E(X)=\infty$, $\E(X/k)=\infty$, with the same argument we have
\be
\limsup_n (X_n/(kn))\geq 1 \quad \text{a.s.} \ \ra \ \limsup_n (X_n/n)\geq k \quad \text{a.s.} \ \ra \ \limsup_n (X_n/n)=\infty \quad \text{a.s.}
\ee

Now suppose that $Y_1,Y_2,\dots$ is any sequence of independent identically distributed random variables with $\E|Y_1|=\infty$. Now let $X_n = |Y_n|$. We have
\be
\limsup_n (|Y_n|/n)=\infty \quad \text{a.s.}
\ee

Also, we know that
\be
|Y_n|\leq |Y_1+\dots+Y_n| + |Y_1+\dots+Y_{n-1}| \ \ra \ \limsup_n (|Y_1+\dots+Y_n|/n) \geq \frac 12\limsup_n (|Y_n|/n) = \infty \quad \text{a.s.}
\ee
\end{example}



\subsection{$\sL^p$ space}

Recalling Definition \ref{def:slp_norm} and \ref{def:essential_sup}, we define the following terms.

\begin{definition}[$\sL^p$ space\index{lp-space-probability@$\sL^p$-space, probability}]\label{def:slp_space_probability}
Let $(\Omega, \sF, \pro)$ be a probability space. For $1 \leq p < \infty$, we denote by $\sL^p = \sL^p(\Omega, \sF, \pro)$ the set of random variables $X$ with finite $\sL^p$-norm:
\be
\dabs{X}_p = \brb{\int_\Omega |X(\omega)|^p d\pro(\omega) }^{1/p} < \infty.
\ee
\end{definition}

\begin{definition}[essential supremum\index{essential supremum, probability}]\label{def:essential_sup_probability}
We denote by $\sL^\infty = \sL^\infty(\Omega, \sF, \pro)$ the set of random variables $X$ with the essential supremum of $X$:
\be
\dabs{X}_\infty := \esssup\brb{X} = \inf\{\lm : |X| \leq \lm \text{ a.s.}\} = \inf\bra{\lm : \pro\brb{\omega:\abs{X(\omega)} > \lm} = 0} < \infty.
\ee
\end{definition}

\begin{proposition}\label{pro:lp_rv_monotonicity}
Let $X$ be a random variable and let $1\leq p<q<\infty$. Then
\be
\E(|X|^p) = \int^\infty_0 p\lm^{p-1}\pro(|X|\geq \lm)d\lm\quad\quad\text{and}\quad\quad X \in\sL^q (\pro) \ \ra \ \pro(|X|\geq \lm) = O(\lm^{-q}) \ \ra \ X\in \sL^p(\pro).
\ee
\end{proposition}

\begin{remark}
We can get the special version of Proposition \ref{pro:lp-norm_monotonicity}.
\end{remark}

\begin{proof}[\bf Proof]
We have
\beast
\int_0 ^\infty p \lambda^{p-1} \mathbb{P}(\abs{X} \geq \lambda) d\lambda  & = & \int_0 ^\infty p\lambda^{p-1} \mathbb{E}(\ind _{\{\abs{X} \geq \lambda\}}) \,\, d\lambda =  \int_0 ^\infty p\lambda^{p-1} \int_\Omega \ind _{\{\omega':\abs{X(\omega')} \geq \lambda \}} (\omega) \,\, \mathbb{P}(d\omega) \, d\lambda \\
& =& \int_\Omega \int_0 ^\infty p\lambda^{p-1} \ind _{\{\omega':\abs{X(\omega')} \geq \lambda \}} (\omega) \,\, d\lambda \, \mathbb{P}(d\omega) \quad\quad \text{(Fubini theorem, Theorem \ref{thm:fubini})} \\
& =& \int_\Omega \int_0 ^{\abs{X(\omega)}} p\lambda^{p-1} \,\, d\lambda \, \mathbb{P}(d\omega) = \int_\Omega \abs{X(\omega)}^p \,\, \mathbb{P} (d\omega) = \mathbb{E}(\abs{X}^p)
\eeast
Now if $X \in L^q (\mathbb{P})$, $\mathbb{E}(\abs{X}^q)<\infty$ by definition. We want to be able to say that since
\[
 \int_0 ^\infty p\lambda^{q-1} \lambda^{-q} \,\, d\lambda = \infty
\]
we must have that $\mathbb{P}(\abs{X} \geq \lambda) = \mathcal{O}(\lambda^{-q})$. To make this argument rigorous, we first note that if $\mathbb{P}(\abs{X} \geq \lambda) \neq \mathcal{O}(\lambda^{-q})$, then  for each $n \in \mathbb{N}$, there exits $\lambda_n$ such that $\mathbb{P}(\abs{X} \geq \lambda_n) > n\lambda_n ^{-q}$. Since $\mathbb{P}(\abs{X} \geq \lambda)$ is non-increasing as a function of $\lambda$, this means that
\[
 \mathbb{P}(\abs{X} \geq \lambda)>\lambda^{-q} \quad \text{for} \quad \lambda \in [\lambda_n ^- , \lambda_n] \quad n=1,2,\ldots
\]
where $\lambda_n ^- := \frac{\lambda_n}{n^{1/q}}$. Thus, for all $n \in \mathbb{N}$, we have that
\[
 \mathbb{E}(\abs{X}^q) > q \int_{\lambda_n ^-} ^{\lambda_n} \lambda^{-1} \, d\lambda = \log(n)
\]
and so $\mathbb{E}(\abs{X}^q) = \infty$. Alternatively, we could have used Chebyshev's inequality.

Now if $\mathbb{P}(\abs{X} \geq \lambda) \leq C\lambda^{-q}$, some $C$, then
\[
 \mathbb{E}(\abs{X}^p) \leq C \int_0 ^\infty p\lambda^{p-1} \lambda^{-q} \,\, d\lambda < \infty.
\]

This is special of Proposition \ref{pro:measure_change_integral}.
\end{proof}


\subsection{Convergence theorems for probability}

From measure theory (Theorem \ref{thm:monotone_convergence_almost_everywhere}, Lemma\ref{lem:fatou_function}, Theorem \ref{thm:dominated_convergence_measure}, Theorem \ref{thm:bounded_convergence_finite_measure} and Lemma \ref{lem:scheffe_measure}), we have the following special theorems:

\begin{theorem}[monotone convergence theorem\index{monotone convergence theorem!probability}]\label{thm:monotone_convergence_probability}
Let $(\Omega,\sF,\pro)$ be a probability space. Let $X$ be a non-negative random variable and $(X_n : n \in \N)$ be a sequence of random variables, with $X_n \geq 0$ a.s. Then
\be
X_n \ua X \text{ a.s.} \ \ra \ \E(X_n) \ua \E(X).
\ee
\end{theorem}

\begin{lemma}[Fatou's lemma\index{Fatou's lemma!probability}]\label{lem:fatou_probability}
Let $(X_n)_{n \in \N}$ be a sequence of non-negative random variables. Then
\be
\E(\liminf X_n) \leq \liminf \E(X_n).
\ee
(inverse Fatou's lemma). Additionally, if there exists a non-negative integrable random variable $Y$ such that $X_n\leq Y$ for all $n$, then
\be
\E(\limsup X_n) \geq \limsup \E(X_n).
\ee
\end{lemma}

\begin{theorem}[dominated convergence theorem\index{dominated convergence theorem!probability}]\label{thm:dominated_convergence_probability}
Let $(\Omega,\sF,\pro)$ be probability space. Let $X$ be a random variable and let $(X_n : n \in \N)$ be a sequence of such random variables. Suppose that $X_n(\omega) \to X(\omega)$ a.s. and that $|X_n| \leq Y$ a.s. for all $n$, for some integrable random variable $Y$. Then $X_n$ and $X$ are integrable, for all $n$ and $\E X_n \to \E X$.
\end{theorem}

\begin{remark}
Actually, dominated convergence theorem also implies that $X_n \to X$ in $\sL^1(\Omega,\sF,\pro)$ by Lemma \ref{lem:scheffe_probability}.
\end{remark}

\begin{example}\label{exa:max_to_zero_in_probability}
Let $(X_n:n\in\N)$ be an identically distributed sequence in $\sL^2(\Omega,\sF,\pro)$. for $\ve>0$,
\be
n\mathbb{P}(\abs{X}>\epsilon \sqrt{n}) = n \mathbb{P}\left( \frac{X_1 ^2}{\epsilon^2} > n \right)  = \mathbb{E}\left( n \ind\brb{\left\{\tfrac{X_1 ^2}{\epsilon^2} > n \right\}} \right) \leq \frac{1}{\epsilon^2}\mathbb{E} (X_1 ^2) < \infty
\ee

Now $n \ind\brb{ \left\{\tfrac{X_1 ^2}{\epsilon^2} > n \right\}} \to 0$ as $n \to \infty$, so by dominated convergence, $n \mathbb{P}(\abs{X}>\epsilon \sqrt{n}) \to 0$.

\be
\mathbb{P}\left( n^{-\frac{1}{2}} \max_{k \leq n} \abs{X_k} > \epsilon \right)  = \mathbb{P} \left( \bigcup_{k \leq n} \{ \abs{X_k} > \epsilon \sqrt{n} \} \right) \leq n \, \mathbb{P}(\abs{X}>\epsilon \sqrt{n}) \to 0 \qquad \brb{n^{-\frac{1}{2}} \max_{k \leq n} \abs{X_k} \stackrel{p}{\to} 0}
\ee
where we have used countable subadditivity (Lemma \ref{lem:countably_subadditive_increasing} since $\pro$ is a measure) and the fact that the $X_n$ are i.i.d. in the final line.
\end{example}


\begin{theorem}[bounded convergence theorem\index{bounded convergence theorem!probability}]\label{thm:bounded_convergence_probability}
Let $(\Omega,\sF,\pro)$ be a measure space. Let $X$ be a random variable and let $(X_n : n \in \N)$ be a sequence of such functions. Suppose $X_n \to X$ in probability and $|X_n| \leq M$ for all $n$, for some constant $M < \infty$. Then $\E\abs{X_n - X}\to 0$, which implies that $\E X_n\to \E X$.
\end{theorem}

\begin{lemma}[Scheff\'e's lemma\index{Scheff\'e's lemma!probability}]\label{lem:scheffe_probability}
let $X$ and $(X_n : n \in \N)$ integrable random variables. Suppose that $X_n(\omega) \to X(\omega)$ a.s., then $\E\abs{X_n -X} \to 0$ if and only if $\E\abs{X_n} \to \E\abs{X}$.
\end{lemma}


Then combining dominated convergence theorem and Scheffe's lemma we have
\begin{corollary}
Let $(\Omega,\sF,\pro)$ be probability space. Let $X$ be a random variable and let $(X_n : n \in \N)$ be a sequence of such random variables. Suppose that $X_n(\omega) \to X(\omega)$ a.s. and that $|X_n| \leq Y$ a.s. for all $n$, for some integrable random variable $Y$. Then $X_n\to X$ in $\sL^1(\Omega,\sF,\pro)$.
\end{corollary}

\begin{proof}[\bf Proof]
Since $X_n\to X$ a.s., we have $\abs{X_n}\to \abs{X}$ a.s. by Lemma \ref{lem:basic_convergence_real}. Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}) we have $\E\abs{X_n} \to \E\abs{X}$. Therefore this implies that $\E\abs{X_n -X} \to 0$ by Scheffe's lemma (Lemma \ref{lem:scheffe_probability}). This is actually $X_n \to X$ in $\sL^1(\Omega,\sF,\pro)$.
\end{proof}


\begin{proposition}\label{pro:expectation_of_independent_product}
For independent random variables $X$ and $Y$, $\E|XY| = \E|X|\E|Y|$ and that if both $X$ and $Y$ are integrable then
\be
\E(XY) = \E(X)\E(Y).
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Let $X$ and $Y$ be random variables. If $X=\ind_A$ and $Y=\ind_B$, then
\be
\E(XY) = \pro(A\cap B) = \pro(A)\pro(B) = \E(X)\E(Y).	
\ee

The identity extends to simple random variables by linearity, if $X=\sum\limits^m_{k=1} a_k {\bf 1}_{A_k}$ and $Y=\sum\limits^n_{j=1} b_j {\bf 1}_{B_j}$ ($a_k,b_j \in [0,\infty]$ and $A_k$ and $B_j$ are disjoint
respectively), \be \E(XY) = \sum\limits^m_{k=1}\sum\limits^n_{j=1} a_kb_j\pro \lob A_k\cap B_j \rob = \sum\limits^m_{k=1}\sum\limits^n_{j=1} a_kb_j \pro(A_k)\pro(B_j) = \E(X)\E(Y). \ee then it extends to non-negative random
variables by monotone convergence theorem (Theorem \ref{thm:monotone_convergence_probability}, by using the approximation $X_n=2^{-n}\left\lfloor 2^nX\right\rfloor$).

For $X$, $Y$ integrable we have $\E(|XY|) = \E(|X|)\E(|Y|) < \infty \ \ra \ XY$ is integrable. Hence, as $X^{\pm}$ and $Y^{\pm}$ are independent, $\E(XY) = \E\lob (X^+-X^-)(Y^+-Y^-)\rob = \lob \E(X^+) - \E (X^-)\rob\lob \E(Y^+) - \E (Y^-)\rob = \E(X)\E(Y)$.
\end{proof}

Then, we have the following proposition by extending to $n$ random variables.

\begin{proposition}\label{pro:expectation_of_independent_product_n}
For independent random variables $X_1,\dots,X_n$, $\E \brb{\prod^n_{i=1} \abs{X_i}} = \prod^n_{i=1} \E \abs{X_i}$ and that if all $X_i$ are integrable then
\be
\E \brb{\prod^n_{i=1} X_i} = \prod^n_{i=1} \E (X_i).
\ee
\end{proposition}




\subsection{Transformations of expectations and density function}

Recall Definition \ref{def:r_random_variable_law}, Proposition \ref{pro:image_measure_function}, \ref{pro:density_function_measure}, we have the following propositions:

\begin{theorem}\label{thm:image_measure_probability}
Let $(\Omega, \sF)$ and $(G, \sG)$ be measurable spaces and let $X : \Omega \to G$ be a random variable. Given a probability $\pro$ on $(\Omega, \sF)$, define $\mu_X = \pro \circ X^{-1}$, the image measure on $(G, \sG)$. Then, for all non-negative measurable functions $g:G\to [0,\infty]$,
\be
\mu_X(g) = \E(g \circ X) = \E(g(X)).
\ee
\end{theorem}

\begin{theorem}[density function\index{density function!probability}]\label{thm:density_function_probability}
Let $(\R^n, \sB(\R^n), \mu)$ be a Lebesgue measure space and let $f_X:\R^n \to [0,\infty]$ be a non-negative $\mu$-integrable function on $\R^n$. For real-valued random variable $X\in \R^n$, define
\be
\mu_X(A) = \pro(X \in A) := \int_{\R^n} \ind_A(x) f_X(x)\mu(dx) = \int_A f_X(x)dx,
\ee
$A \in \sB(\R^n)$ and $\pro(\R^n) =1$. Then $\pro$ is a probablity on $\sB(\R^n)$ and, for all non-negative measurable functions $g:\R^n\to [0,\infty]$,
\be
\mu_X(g) = \int_{\R^n} g(x)f_X(x)dx.
\ee
$f_X$ is called the density function of random variable $X$. As $\pro(\Omega) = 1< \infty$, we have that $f$ is Lebesgue-integrable a.e. by Theorem \ref{thm:uniqueness_density_function}.
\end{theorem}

\begin{remark}
Obviously, $g$ could be extended to integral functions.
\end{remark}

\begin{proposition}\label{pro:density_function_probability}
Combine the above results
\be
\int_\Omega g(X(\omega))\pro(d\omega) = \E(g(X)) = \mu_X(g) = \int_{\R^n} g(x)f_X(x)dx.
\ee
\end{proposition}

\begin{example}
Let $X$ be an exponential random variable. That is, its distribution function satisfies $F(t) = 1 - e^{-t}$, $t \geq 0$. Therefore, by the fundamental theorem of calculus (Theorem \ref{thm:fundamental_theorem_of_calculus_lebesgue}),
\be
\pro(X \leq t) = 1 - e^{-t} = \int^t_0 e^{-x}dx = \int^\infty_0 e^{-x}\ind_{[0,t]}(x)dx.
\ee

For any interval $(a,b]$, the set $\{(a,b],a<b\}$ is $\pi$-system and
\be
\pro(X \in (a,b]) = \underbrace{\pro(X \in (0,b]) - \pro(X \in (0,a])}_{\pro\text{ is a measure}} = F(b) - F(a) = \int^b_a f(x)dx = \int^\infty_0 f(x)\ind_{(a,b]} dx
\ee
where $f_X(x) = e^{-x}\ind_{[0,\infty)}(x)$. Then by uniqueness of extension (Lemma \ref{thm:uniqueness_of_extension_measure}), it follows that for all Borel set $A$,
\be\label{equ:integrate_distribution}
\pro(X \in A) = \int f_X(x)\ind_A(x)dx.
\ee
Note that $F$ need not differentiate to $f$ everywhere (e.g. uniform distribution in $[a,b]$), all that is required is that $f$ integrate properly (equation (\ref{equ:integrate_distribution})). On the other hand, if $F$ does differentiate to $f$ and $f$ is continuous, it follows by the fundamental theorem of calculus (Theorem \ref{thm:fundamental_theorem_of_calculus_lebesgue}) that $f$ is indeed a density for $F$ Billingsley\cite{Billingsley_1995}.

Thus $\mu_X$ has a density function $f_X$. Hence, by Proposition \ref{pro:image_measure_function} and \ref{pro:density_function_measure},
\be
\E(X^2) = \int x^2f_X(x)dx = \int^\infty_0 x^2e^{-x}dx = \Gamma(3) = 2.
\ee
\end{example}

\begin{proposition}\label{pro:density_difficult_to_integrate}
Let $\mu$ and $\nu$ be probability measures on $(E,\sE)$ and suppose that, for some measurable function $f:E\to [0,R]$,
\be
\nu(A) = \int_A fd\mu, \quad A\in\sE.
\ee

Let $(X_n)_{n\in \N}$ be a sequence of independent random variables in $E$ with law $\mu$ and let $(U_n)_{n\in \N}$ be a sequence of independent $U[0,1]$ random variables. Set
\be
T =\min\{n\in \N:RU_n \leq f(X_n)\},\quad Y=X_T.
\ee

Then $Y$ has law $\nu$.
\end{proposition}

\begin{remark}
This is important in applications, where one wishes to simulate a random variable whose density function is difficult to integrate numerically.
\end{remark}

\begin{proof}[\bf Proof]
To make this question work, we shall assume that $f(X_1)$ is positive with positive probability, to ensure that
\[
 \{n \in \mathbb{N}:RU_n \leq f(X_n)\} \neq \emptyset \quad \mathrm{a.s.}
\]
We may define $T$ as taking the value $\infty$ and $Y$ in an arbitrary way on the null set for which equality holds in the above. Then, for any $A \in \mathcal{E}$,
\be
 \mathbb{P}(Y \in A) =  \mathbb{P}(X_T \in A) = \sum_{n \in \mathbb{N}} \mathbb{P}(X_n \in A,T=n) = \sum_{n \in \mathbb{N}} \mathbb{P}(X_n \in A, RU_n \leq f(X_n),T \geq n)
\ee
since $\{RU_n \leq f(X_n)\}=\{T \leq n\}$. But
\[
 \{T \geq n\}=\bigcup_{m < n} \{RU_m > f(X_m)\}
\]
Thus the events $\{X_n \in A, RU_n \leq f(X_n)\}$ and $\{T \geq n\}$ are independent. Therefore, taking $\mathbb{E}(T)=C$, say, we have
\begin{align*}
 \mathbb{P}(Y \in A) =& C \, \mathbb{P}(X_1 \in A, RU_1 \leq f(X_1)) \\
=& C \int_E \int_0 ^1 \ind \{(x',u'):x' \in A, Ru' \leq f(x')\}(x,u) \, du \, \mu (dx) \\
=& \frac{C}{R} \int_E f\ind_A d\mu = \frac{C}{R} \nu(A)
\end{align*}
In particular,
\[
 1=\mathbb{P}(Y \in E) = \frac{C}{R}\nu(E)
\]
But $\nu$ is a probability measure so $\frac{C}{R}=1$, and $\mathbb{P}(Y \in A) = \nu(A)$ as required.
\end{proof}

Now we give the joint distribution function and joint density function. To start with, to keep the notation simpler, consider just the case of two random variables. Then these definitions can be extended to multi-dimensional case.

\begin{definition}[joint distribution function\index{joint distribution function}, joint density function\index{joint density function!probability}, marginal probability density functions\index{marginal density function!probability}]\label{def:joint_distribution_joint_density_marginal_density}
The joint distribution function of $X \in \R^m$ and $Y\in \R^n$ is
\be
F(x,y) = \mu_{X,Y} := \mu = \pro\brb{X \leq x, Y \leq y},\quad x \in \R^m, y \in \R^n,
\ee
so that $F : \R^{m+n} \to [0,1]$. If there exists a Lebesgue-integrable ($\mu$-integrable) function $f(\cdot,\cdot)$ a.e. (by Theorem \ref{thm:density_function_probability}) with
\be
F(x,y) = \int_{Y\leq y} \int_{X\leq x} f(x,y)dx dy,
\ee
so that $dF(x,y) = f(x,y)dxdy := f_{X,Y}(x,y)dxdy$. %f(x_1,\dots, x_n) = \frac{\partial^2F}{\partial x\partial y}
Then $f$ is the joint probability density function of $X$ and $Y$. % Note that, for any region $C \subseteq \R^2$,
%\be
%\pro ((X, Y ) \in C) = {\int\int}_{(x,y)\in C} f(x, y)dxdy.
%\ee

Furthermore,
\be
f_{X}(x) = \int_{\R^n} f_{X,Y}(x,y)dy,\qquad f_{Y}(y) = \int_{\R^m} f_{X,Y}(x,y)dx,
\ee
are the marginal density functions of $X$ and $Y$, respectively. Note that the marginal density function are well-definied and integrable (with respect to Lebesgue-$dx$ or Lebesgue-$dy$ a.e.) by Fubini theorem (Theorem \ref{thm:fubini}.(ii).(a))
\end{definition}


\begin{example}\label{exa:joint_density_8xy}
Consider the joint density for $X$ and $Y$ given by

\begin{center}
\psset{yunit=4cm,xunit=4cm}
\begin{pspicture}(-1.7,-0.2)(1.2,1.2)%[showgrid](-3,-1.5)(3,4)
\psaxes[dx =1,dy=1,labels=none,ticks=none]{->}(0,0)(-0.2,-0.2)(1.2,1.2)%Dx=0.25,Dy=0.25
\psset{algebraic}
\psplot{0}{1}{x}

%\rput[lb](-1.5,0.75){$\max\{X,Y\}-\min\{X,Y\}<\frac 12$}
\rput[lb](-1.7,0.5){$f(x, y) = \left\{\ba{ll} 8xy \quad\quad & 0 \leq x \leq y \leq 1,\\ 0 & \text{otherwise.}\ea\right.$}
\rput[lb](1.1,0.05){$x$}
\rput[lb](0.05,1.1){$y$}
\rput[lb](-0.1,1){1}
\rput[lb](1,-0.1){1}

\pstGeonode[PointSymbol=none,PointName=none](0,0.167){A}(1,0.833){AA}(0.167,0){B}(0.833,1){BB}(0,1){C}(1,1){D}(1,0){E}

\pstLineAB[linestyle=dashed]{C}{D}
\pstLineAB[linestyle=dashed]{D}{E}

\pscustom[fillstyle=solid,fillcolor=red!30]{%linestyle=dashed,
\psline(0,0)(1,1)(0,1)(0,0)
}%
\end{pspicture}
\end{center}


Here, $(X, Y)$ are distributed over the upper half of the unit square as illustrated in the diagram. You should check that this is indeed a joint p.d.f. in that it integrates to 1 over the region.

We now compute the marginal densities of $X$ and $Y$,
\be
f_X(x) = \int^1_x 8xy dy = 4x(1 - x^2) \quad\text{and}\quad f_Y (y) = \int^y_0 8xy dx = 4y^3,
\ee
for $0 \leq x \leq 1$ and $0 \leq y \leq 1$. Calculate that
\be
\E X = \int^1_0 xf_X(x)dx = \int^1_0 4x^2(1 - x^2)dx = 4\brb{\frac 13 - \frac 15} = \frac 8{15}
\ee
and similarly $\E Y = \frac 45$.
\end{example}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{$\sL^p$ Space and Convergence in $\sL^p(\Omega,\sF,\pro)$}

\subsection{Inequalities and orthogonal projection}

Applying Definition \ref{def:slp_norm}, \ref{def:essential_sup}, Theorem \ref{thm:chebyshev_inequality}, \ref{thm:jensen_inequality_measure},\ref{thm:holder_inequality_measure}, \ref{thm:minkowski_inequality_measure} in probability space, we have %, Proposition \ref{pro:lp-norm_monotonicity}

\begin{theorem}[Markov's inequality\index{Markov's inequality}]\label{thm:markov_inequality_probability}
If $X$ is any random variable and $a > 0$, then
\be
\pro\brb{\abs{X} \geq a} \leq \frac{\E\abs{X}}a.
\ee
\end{theorem}

\begin{theorem}[Chebyshev's inequality\index{Chebyshev's inequality}]\label{thm:chebyshev_inequality_probability}
If $X$ is any random variable and $a > 0$, then
\be
\pro\brb{\abs{X-\E(X)} \geq a} \leq \frac{\var(X)}{a^2}.
\ee
\end{theorem}


\begin{theorem}[Jensen's inequality\index{Jensen's inequality!expectation}]\label{thm:jensen_inequality_expectation}
For probability space $(\Omega,\sF,\pro)$, let $X$ be an integrable function with values in $I$ and let $f : I \to \R$ be convex. Then $\E(f(X))$ is well defined and
\be
\E(f(X)) \geq f(\E(X)).
\ee

The equality holds when $f$ is not strictly convex, e.g. when it is a straight line, or when $X$ follows a degenerate distribution, i.e. $X = \E X$ a.s..
\end{theorem}


\begin{corollary}[arithmetic-geometric mean inequality]\label{exa:arithmetic_geometric_mean_inequality_probability_proof}
For positive real numbers $x_1, \dots, x_n$,
\be
\brb{\prod^n_{i=1} x_i}^{1/n} \leq \frac 1n \sum^n_{i=1} x_i.
\ee

The equality holds only for $x_1= x_2 = \dots = x_n$.
\end{corollary}

\begin{proof}[\bf Proof]
This follows by Jensen's inequality (Theorem \ref{thm:jensen_inequality_expectation}) applied to the convex function $f(x) = -\log x$, and the random variable $X$ which takes the value $x_i$ with probability $\frac 1n$, so that
\be
-\frac 1n \sum^n_{i=1} \log x_i = \E (-\log(X)) \geq -\log(\E X) = -\log\brb{\frac 1n \sum^n_{i=1}x_i}
\ee
from which we see that
\be
\log\brb{\brb{\prod^n_{i=1} x_i}^{1/n}} \leq \log\brb{\frac 1n \sum^n_{i=1} x_i},
\ee
which gives the result, since $\log$ is an increasing function. Furthermore, the equality holds for $X = \E X$ a.s. implies that $x_1= x_2 = \dots = x_n$ with equal weight.
\end{proof}



\begin{theorem}[H\"older's inequality\index{H\"older's inequality!expectation}]\label{thm:holder_inequality_expectation}
Let $p, q \in [1,\infty]$ be conjugate indices. Then, for all random variables $X\in \sL^p(\Omega,\sF,\pro)$ and $Y\in \sL^q(\Omega,\sF,\pro)$, we have
\be
\E(|XY|) \leq \brb{\E\abs{X}^p}^{1/p}\brb{\E\abs{Y}^q}^{1/q}.
\ee

\ben
\item [(i)] H\"older's inequality becomes an equality if and only if $\abs{X}^p$ and $\abs{Y}^q$ are linearly dependent in $\sL^1(\Omega,\sF,\pro)$, i.e., for $X\in \sL^p(\Omega,\sF,\pro)$ and $Y \in \sL^q(\Omega,\sF,\pro)$, there exist real number $a,b \geq 0$ such that $a \abs{X}^p = b \abs{Y}^q$ a.s..%\footnote{need proof, see wiki}.
\item [(ii)] For $p,q\in \bra{1}\cup \bra{\infty}$, H\"older's inequality becomes an equality if and only if $p=1$ and $Y = \dabs{Y}_\infty$ a.e. or $q=1$ and $X = \dabs{X}_\infty$ a.s..
\een
\end{theorem}

\begin{theorem}[Cauchy-Schwarz inequality\index{Cauchy-Schwarz inequality!probability}]\label{thm:cauchy_schwarz_inequality_probability}
For all random variables $X,Y\in \sL^2(\Omega,\sF,\pro)$, we have that $XY$ is integrable and
\be
\brb{\E\brb{XY}}^2 \leq \brb{\E\abs{XY}}^2 \leq \E X^2 \E Y^2.
\ee

The equality holds when $aX = bY$ for some $a,b \in \R$ a.s., i.e., $X,Y$ are linearly dependent.
%If $\E(Y^2) > 0$, equality occurs if and only if $X = aY$ for some constant $a \in \R$ (which is implied by equality in H\"older's inequality (Theorem \ref{thm:holder_inequality_expectation})).
\end{theorem}

\begin{remark}
The first inequality is given by Theorem \ref{thm:lebesgue_integrable_function_property}.(iv) and the second is given by Theorem \ref{thm:holder_inequality_expectation}.
\end{remark}


\begin{proposition}\label{pro:lp_subset_decreasing_order}
Let $1 \leq p < q < \infty$. For any random variable $X \in \sL^p(\Omega,\sF,\pro)$,
\be
\brb{\E\abs{X}^p}^{1/p} \leq \brb{\E\abs{Y}^q}^{1/q}.
\ee

Moreover, $\sL^q(\Omega,\sF,\pro)\subseteq \sL^p(\Omega,\sF,\pro)$.
\end{proposition}

\begin{proof}[\bf Proof]
For the conjuate indices
\be
r = q/p,\quad s = \frac {q}{q-p},
\ee
and two random variables $X^p$ and $\ind_{\Omega}$, by H\"older's inequality (Theorem \ref{thm:holder_inequality_expectation})
\be
\E \abs{X}^p = \E\brb{ \abs{X}^p\ind_{\Omega}} \leq \brb{\E \abs{X^{pr}}}^{1/r} \brb{\E \ind_{\Omega}^s}^{1/s} = \brb{\E \abs{X}^q}^{p/q}.
\ee

Thus, we can the required result.
\end{proof}

\begin{theorem}[Minkowski's inequality\index{Minkowski's inequality!expectation}]\label{thm:minkowski_inequality_expectation}
For $p \in [1,\infty]$ and random variables $X,Y\in \sL^p(\Omega,\sF,\pro)$, we have
\be
\brb{\E\abs{X + Y}^p}^{1/p} \leq \brb{\E\abs{X}^p}^{1/p}+\brb{\E\abs{Y}^p}^{1/p}.
\ee

For $p\in (1,\infty)$, the equality holds if and only if $X=cY$ or $Y = cX$ for some $c\geq 0$ a.s..
\end{theorem}

Applying Definition \ref{def:closed_sl2_space} and Theorem \ref{thm:orthogonal_projection_sl2} for probabilty space, we have

\begin{definition}\label{def:closed_sl2_space_probability}
A subset $V \subseteq \sL^2$ is closed\index{closed!$\sL^2(\Omega,\sF,\pro)$ space} if, for every random variable sequence $(X_n : n \in \N)$ in $V$, with $X_n \to X$ in $\sL^2$, we have $X = Y$ a.s., for some $Y \in V$.
\end{definition}

\begin{theorem}[Orthogonal Projection\index{Orthogonal Projection Theorem!probability}]\label{thm:orthogonal_projection_sl2_probability}
Let $\sK$ be a complete subspace of $\sL^2(\Omega,\sF,\pro)$. Then each $X \in \sL^2(\Omega,\sF,\pro)$ has a decomposition $X = Y + Z$, with $Y \in \sK$ and $Z \in \sK^\perp$. Moreover, $\dabs{X - Y}_2 \leq \dabs{X -V}_2$ for all $V \in \sK$, $X-Y \perp Z$ for all $Z\in \sK$ with equality only if $X = Y$ a.s.

The random variable $Y$ is called (a version of) the orthogonal projection\index{orthogonal projection!probability} of $X$ on $E$.
\end{theorem}


\subsection{Bounded in $\sL^p$ and uniform integrability}

Recall Definition \ref{def:bounded_in_slp}, \ref{def:uniformly_integrable}, we give

\begin{definition}\label{def:bounded_in_slp_probability}
For a probability space $(\Omega,\sF,\pro)$, let $\sX$ be a collection of random variables. For $1 \leq p \leq \infty$, we say that $\sX$ is bounded in $\sL^p(\Omega,\sF,\pro)$\index{bounded in $\sL^p$!random variable family} if
\be
\sup_{X\in\sX} \dabs{X}_p < \infty.
\ee
\end{definition}

\begin{proposition}\label{pro:bounded_in_slp_monotonicity}
Let $1\leq p<q < \infty$. If $\sX$ is bounded in $\sL^q(\Omega,\sF,\pro)$, then it is also bounded in $\sL^p(\Omega,\sF,\pro)$.
\end{proposition}

\begin{proof}[\bf Proof]
By Proposition \ref{pro:lp-norm_monotonicity}, we have
\be
\dabs{X}_p \leq \dabs{X}_q \ \ra \ \sup_{X\in\sX} \dabs{X}_p \leq \sup_{X\in\sX} \dabs{X}_q.
\ee

Thus, if $\sX$ is bounded in $\sL^q(\Omega,\sF,\pro)$, then it is also bounded in $\sL^p(\Omega,\sF,\pro)$.
\end{proof}

\begin{definition}\label{def:uniformly_integrable_probability}
Let $(\Omega,\sF,\pro)$ be a probability space. Define
\be
I_\sX(\delta) = \sup\{\E(|X|\ind_A) : X\in \sX ,\forall A \in \sF, \pro(A) \leq \delta\}.
\ee
We say that $\sX$ is uniformly integrable\index{uniformly integrable!random variable family} or UI\index{UI (uniformly integrable)!random variable family} if $\sX$ is bounded in $\sL^1(\Omega,\sF,\pro)$ and
\be
I_\sX(\delta) \da 0\ \text{ as }\ \delta \da 0,\quad\text{i.e.,}\quad \forall \ve>0, \exists \delta >0 \text{ s.t. }I_\sX(\delta) <\ve.
\ee
\end{definition}

Then, from Lemma \ref{lem:uniformly_integrable_finite_measure} and Theorem \ref{thm:ui_equivalent_finite_measure}, we have

\begin{theorem}\label{thm:ui_equivalent_probability}
Let $(\Omega,\sF,\pro)$ be a probability space. A collection of random variables $\sX$ is uniformly integrable, it is equivalent to either of the following two conditions:
\be
\text{(i)}\ \inf\limits_{K\in[0,\infty)}\sup\limits_{X\in \sX} \E\brb{\abs{X}\ind_{\abs{X}>K}} = 0,\quad \text{ i.e., }\quad \sup\limits_{X\in \sX} \E\brb{\abs{X}\ind_{\abs{X}>K}} \to 0 \text{ as }K\to \infty.
\ee

\be
\text{(ii)}\ \inf\limits_{K\in[0,\infty)}\sup\limits_{X\in \sX} \E\brb{\brb{\abs{X}-K}^+} = 0, \quad \text{ i.e., }\quad \sup\limits_{X\in \sX} \E\brb{\brb{\abs{X}-K}^+} \to 0 \text{ as }K\to \infty.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
This is special case for Theorem \ref{thm:ui_equivalent_finite_measure}. %For (i), we prove it in probability sence.\footnote{check pm notes}
\end{proof}


\begin{proposition}\label{pro:finite_integrable_random_variables_implies_ui}
Any single integrable random variable is uniformly integrable. This extends easily to any finite collection of integrable random variables.
\end{proposition}

\begin{proof}[\bf Proof]
See Proposition \ref{pro:finite_integrable_implies_ui}.
\end{proof}

\begin{proposition}\label{pro:sum_of_ui_rv_is_ui}
Let $\sX,\sY$ be UI random variable collections. Then
\be
\text{(i)}\quad \bra{X+Y: X\in \sX,Y\in \sY},\qquad \text{(ii)}\quad  \bra{X-Y: X\in \sX,Y\in \sY},\qquad \text{(iii)}\quad \bra{\abs{X}: X\in \sX}
\ee
are UI as well.
\end{proposition}

\begin{proof}[\bf Proof]
See Proposition \ref{pro:sum_of_ui_is_ui}.
\end{proof}



\begin{proposition}\label{pro:dominated_integrable_random_variable_implies_ui}
For any integrable random variable $Y$, the set
\be
\sX = \bra{X : X \text{ is a random variable}, |X| \leq Y }\quad\text{is UI.}
\ee
\end{proposition}

\begin{proof}[\bf Proof]
See Proposition \ref{pro:dominated_integrable_implies_ui}.
\end{proof}


\begin{proposition}\label{pro:bounded_lp_implies_ui}
If a random variable family $\sX$ is bounded in $\sL^p(\Omega,\sF,\pro)$, for $p\in (1,\infty]$, then it is UI.
\end{proposition}

\begin{proof}[\bf Proof]
%For $X\in \sX$, $X\in \sL^p(\Omega,\sF,\pro)$ thus $X\in \sL^1(\Omega,\sF,\pro)$ (by Jensen inequality (Theorem \ref{thm:jensen_inequality_expectation})). by H\"older inequality (Theorem \ref{thm:holder_inequality_expectation}), for $K >0$.
%\be
%\E\brb{\abs{X}\ind_{\bra{\abs{X} > K}}} \leq \dabs{X}_p\brb{\E\brb{\ind_{\bra{\abs{X}>K}}}}^{1/q} = \dabs{X}_p \brb{\pro\brb{\abs{X}>K}}^{1/q}
%\ee
%where $q = \frac p{p-1}$ and $q\in [1,\infty)$. Since $X$ is integrable, by Markov's inequality (Theorem \ref{thm:markov_inequality_probability})
%\be
%\pro\brb{\abs{X}>K} \leq \frac{\E\abs{X}}K \to 0\text{ as }K \to \infty
%\ee
%Since $\sup\dabs{X}_p < \infty$,
%\be
%\sup_{X\in \sX}\E\brb{\abs{X}\ind_{\bra{\abs{X} > K}}} \leq \sup_{X\in \sX} \dabs{X}_p \sup_{X\in \sX} \brb{\pro\brb{\abs{X}>K}}^{1/q} \to 0.
%\ee%See Theorem \ref{thm:martingale_bounded_lp_as_lp_closed} and Theorem \ref{thm:martingale_ui_as_l1_closed}.
%Thus, $\sX$ is UI.
Direct result of Proposition \ref{pro:slp_implies_ui}.
\end{proof}

\begin{corollary}
If $(X_i)_{i\in I}$ is a family of random variables with
\be
\sup_{i\in I} \abs{\E X_i} < \infty\quad \text{ and }\quad \sup_{i\in I}\var(X_i) < \infty,
\ee
then $(X_i)_{i\in I}$ is UI.
\end{corollary}

\begin{proof}[\bf Proof]
Since $\E X_i^2 = \brb{\E X_i}^2 + \var(X_i)$ for $i\in I$, is bounded, this follows from Proposition \ref{pro:bounded_lp_implies_ui} with $p = 2$.
\end{proof}



\begin{proposition}\label{pro:expected_sup_bounded_implies_ui}
If a random variable family $\sX$ satisfies that
\be
\E\brb{\sup_{X\in \sX}\abs{X}} < \infty,\qquad \text{i.e., }\quad  \sup_{X\in \sX}\abs{X} < \infty \quad\text{a.s..}
\ee

Then $\sX$ is UI.
\end{proposition}

\begin{proof}[\bf Proof]
Since $\sup_{X\in \sX}\abs{X} <\infty$ a.s., we have
\be
\E\brb{\abs{X}\ind_{\abs{X}>K}} \leq \E\brb{\sup_{X\in \sX}\abs{X}\ind_{\sup_{X\in \sX}\abs{X}>K}}
\ee
which is 0 when $K$ is bigger than $\sup_{X\in \sX}\abs{X}$. Thus,
\be
\sup_{X\in \sX} \E\brb{\abs{X}\ind_{\abs{X}>K}} \to 0 \quad \text{as }K\to\infty \ \ra\ \sX \text{ is UI}
\ee
by Theorem \ref{thm:ui_equivalent_probability}.
%Since we know $\esssup X \leq \sup X$\footnote{proof needed.}, we can apply H\"older's inequality (Theorem \ref{thm:holder_inequality_expectation}) for $\pro(A) \to 0$
%\be
%\E\brb{\abs{X}\ind_A} \leq \E\brb{\sup_{X\in \sX}\abs{X}\ind_A}
%\ee
\end{proof}

\begin{remark}
However, the inverse is not true. Note that $\sup_n \E\abs{X_n} \neq \E\brb{\sup_n \abs{X_n}} $ in general.
\end{remark}

\begin{example}%Find a uniformly integrable sequence of random variables $(X_n:n\in\N)$ such that
Take uniformly distributed random variable $X \sim \sU(0,1)$ and consider the sequence \be X_{n,k} = n\ind_{\bra{X \in (k/n^2,(k+1)/n^2]}},\quad k=0,1,\dots,n-1. \ee

The sequence is: $X_{1,0},X_{2,0},X_{2,1},X_{3,0},X_{3,1},X_{3,2},\dots$. Thus, $X_n \in \bra{X_{n,k}}$, $k =0,1,\dots,n-1$ and
\be
\pro\brb{X_{n,k} = n} = \frac 1{n^2} \ \ra \ X_n \to 0\  \text{ a.s. as }n\to \infty
\ee

It is also UI since
\beast
\lim_{K\to \infty} \sup_n \E\brb{\abs{X_n}\ind_{\bra{\abs{X_n}>K}}} & = & \lim_{K\to \infty} \sup_n \E\brb{n \ind_{\bra{X\in (k/n^2,(k+1)/n^2]}}\ind_{\bra{n>K}}} \\
& = & \lim_{K\to \infty} \sup_n n \pro\brb{\bra{X\in (k/n^2,(k+1)/n^2]}}\ind_{\bra{n>K}} \\
& = & \lim_{K\to \infty} \sup_{n>K} n \pro\brb{\bra{X\in (k/n^2,(k+1)/n^2]}}\\
& = & \lim_{K\to \infty} \sup_{n>K} n \cdot \frac 1{n^2} = \lim_{K\to \infty} \sup_{n>K} \frac 1{n} = \lim_{K\to \infty} \frac 1{\ceil{K}} = 0.
\eeast

For $U \in \left(0,\frac 1n\right]$, we have $\abs{X_n} = n$, thus we can find $m$ such that $\forall n\geq m$, $\abs{X_m} \geq \frac 1U$. Then
\be
\E\brb{\sup_n |X_n|} \geq \E \brb{\frac 1U} = \int^1_0 \frac 1u du = \infty.
\ee
\end{example}





\subsection{Convergence in $\sL^p(\Omega,\sF,\pro)$}

Recalling Definition \ref{def:convergence_in_slp_measure}, Proposition \ref{pro:convergence_slp_monotone_measure} and Proposition \ref{pro:convergence_slp_implies_measure}, we have

\begin{definition}\label{def:convergence_in_slp_probability}
Let $(\Omega,\sF,\pro)$ be a probability space. We say that the sequence of random variables $X_n$ converges in $\sL^p$\index{convergence!in $\sL^p(\Omega,\sF,\pro)$} (or in the $p$-th mean) to $X$, for some $p \in [1,\infty)$, if $X,X_n\in \sL^p(\Omega,\sF,\pro)$ for all $n$ and
\be
\E \abs{X_n-X}^p  \to 0 \ \text{ as } n\to \infty.
\ee

If $p = \infty$, we say $X_n$ converges in $\sL^\infty$ to $X$, if $X,X_n \in \sL^\infty(\Omega,\sF,\pro)$ for all $n$ and
\be
\dabs{X_n - X}_\infty \to 0 \ \text{ as } n\to \infty.
\ee
\end{definition}

\begin{proposition}\label{pro:convergence_slp_monotone_probability}
Let $(\Omega,\sF,\pro)$ be a probability space. For $p=[1,\infty)$, if $X_n\stackrel{\sL^p}{\to} X$, then for any $q \in [1,p)$, $X_n \stackrel{\sL^q}{\to} X$.
\end{proposition}

\begin{proposition}\label{pro:convergence_slp_implies_probability}
Let $(\Omega,\sF,\pro)$ be a probability space. For $p=[1,\infty)$, if $X_n\stackrel{\sL^p}{\to} X$, then $X_n \xrightarrow{p} X$.
\end{proposition}

\begin{remark}
Actually, we can release the condition $p \in [1,\infty)$ to $p \in \R^+$ by Markov inequality (Theorem \ref{thm:markov_inequality_probability}).
\end{remark}


\begin{theorem}\label{thm:ui_prob_iff_sl1}
Let $X$ be a random variable and let $(X_n : n \in \N)$ be a sequence of random variables. The following are equivalent:
\ben
\item [(i)] $X_n \in \sL^1(\Omega,\sF,\pro)$ for all $n$, $X \in \sL^1(\Omega,\sF,\pro)$ and $X_n \to X$ in $\sL^1(\Omega,\sF,\pro)$,
\item [(ii)] $\{X_n : n \in \N\}$ is UI and $X_n \to X$ in probability.
\een
\end{theorem}

\begin{remark}
This is the special case of Vitali convergence theorem (Theorem \ref{thm:vitali_convergence})
\end{remark}

\begin{proof}[\bf Proof]
Suppose (i) holds. By Chebyshev's inequality (Theorem \ref{thm:chebyshev_inequality}), for $\ve > 0$, \be \pro(|X_n - X| > \ve) \leq \ve^{-1}\E(|X_n - X|) \to 0 \ee so $X_n \to X$ in probability.
Moreover, given $\ve > 0$, there exists $N$ such that $\E(|X_n - X|) < \ve/2$ whenever $n \geq N$. Then we can find $\delta > 0$ so that $\pro(A) < \delta $ implies $\E(|X|\ind_A) < \ve/2$ by Lemma
\ref{lem:single_integrable_function_ui}.

Then, for $n \geq N$ and $\pro(A) \leq \delta$,
\be
\E(|X_n|\ind_A) \leq \E(|X_n - X|) + \E(|X|\ind_A) < \ve.
\ee
Hence $\{X_n : n \in \N\}$ is UI. We have shown that (i) implies (ii).

Suppose, on the other hand, that (ii) holds. Then there is a subsequence $(n_k)$ such that $X_{n_k} \to X$ a.s. by Theorem \ref{thm:convergence_in_measure} (ii). So, by Fatou's lemma,
\be
\E(|X|) = \E\brb{\liminf_{k} \abs{X_{n_k}}} \leq \underbrace{\liminf_k \E(|X_{n_k}|) < \infty}_{\text{Definition \ref{def:bounded_in_slp}, \ref{def:uniformly_integrable}}} \quad \ra\quad X\in \sL^1(\Omega,\sF,\pro).
\ee

Thus, $X$ is integrable, then by Lemma \ref{lem:single_integrable_function_ui}, $I_X(\delta) \da 0$ as $\delta \da 0$. So $\bra{X_n,n\in \N}\cup X$ is UI.

Now, given $\ve > 0$, there exists $K < \infty$ such that, for all $n$,
\be
\E(|X_n|\ind_{\{|X_n|\geq K\}}) < \ve/3,\quad\quad \E(|X|\ind_{\{|X|\geq K\}}) < \ve/3.\quad\quad\text{(Lemma \ref{lem:uniformly_integrable_finite_measure})}
\ee

Consider the uniformly bounded sequence $X^K_n = (-K) \lor X_n \land K$ and set $X^K = (-K)\lor X \land K$. Then
\be
\bra{\abs{X^K_n-X^K}>\ve} \subseteq \bra{\abs{X_n-X}>\ve}\quad\ra\quad \pro\brb{\abs{X^K_n-X^K}>\ve} \leq \pro\brb{\abs{X_n-X}>\ve} \to 0.
\ee

Thus $X^K_n \to X^K$ in probability, so by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_finite_measure}, $\abs{X^K_n}\leq K$), there exists $N$ such that, for all $n \geq N$, $\E|X^K_n - X^K| < \ve/3$. But then, for all $n \geq N$,
\beast
\E|X_n - X| & \leq & \E|X_n - X^K_n| + \E|X^K_n - X^K| + \E|X - X^K|\\
& \leq & \E(|X_n|\ind_{\{|X_n|\geq K\}}) + \E|X^K_n - X^K| + \E(|X|\ind_{\{|X|\geq K\}}) < \ve.
\eeast
Since $\ve > 0$ was arbitrary, we have shown that (ii) implies (i).
\end{proof}


\begin{proposition}
Let $(X_n)_{n\in\N}$ be an identically distributed sequence in $\sL^2(\Omega,\sF,\pro)$. Then
\be
\E\brb{\left.\max_{k\leq n}|X_k|\right/\sqrt{n}}\to 0 \text{ as }n\to \infty.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
First we have for some constant $M$
\be
\E\brb{\max_{k \leq n} X_k ^2/n} \leq  \frac{1}{n} \mathbb{E} \left( \sum_{k=1} ^n X_k ^2 \right) =  \mathbb{E}(X_1 ^2) \leq M \ \ra \ \bra{\left.\max_{k\leq n}|X_k|\right/\sqrt{n}}_n \ \text{ is bounded in }\sL^2(\Omega,\sF,\pro).
\ee

So it is UI by Proposition \ref{pro:bounded_lp_implies_ui} (as $\sup\dabs{\left.\max_{k\leq n}\abs{X_k}\right/\sqrt{n}}_2 \leq M < \infty$). Also, by Example \ref{exa:max_to_zero_in_probability}, it converges to 0 in probability.

Hence, we can have
\be
\left.\max_{k\leq n}|X_k|\right/\sqrt{n}\to 0\  \text{ in }\sL^1(\Omega,\sF,\pro) \ \ra \ \E\brb{\left.\max_{k\leq n}|X_k|\right/\sqrt{n}}\to 0 \text{ as }n\to \infty
\ee
by Theorem \ref{thm:ui_prob_iff_sl1}.
\end{proof}


Since probability is finite measure, (iii) of Theorem \ref{thm:vitali_convergence} is automatically satisfied, so we have probability version of \ref{thm:vitali_convergence}:

\begin{theorem}\label{thm:slp_iff_probability_ui}
Let $X_1, X_2, \dots$ be $\sL^p(\Omega,\sF,\pro)$-integrable random variables, for $1\leq p<\infty$. Then the sequence $X_n$ converges in $\sL^p(\Omega,\sF,\pro)$ if and and only if
\ben
\item [(i)] the sequence $X_n$ converges in probability,
\item [(ii)] the collection of random variables, $\{\abs{X_n}^p\}$ is uniformly integrable,
\een
\end{theorem}

\begin{proof}[\bf Proof]
We can use Theorem \ref{thm:ui_prob_iff_sl1}. % and Theorem \ref{thm:martingale_ui_as_l1_closed}.
\end{proof}

\begin{remark}
This theorem is Theorem \ref{thm:ui_prob_iff_sl1} when $p=1$. %, Theorem \ref{thm:martingale_bounded_lp_as_lp_closed}
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Variance and covariance}

Now we look at some $\sL^2$ notions relevant to probability.

\begin{definition}[variance\index{variance}]\label{def:variance}
For $X\in \R$ and $X \in \sL^2(\Omega,\sF,\pro)$, with mean $\E X$ (this is well-defined as $\sL^2(\Omega,\sF,\pro) \subseteq \sL^1(\Omega,\sF,\pro)$, see Proposition \ref{pro:lp_rv_monotonicity}), we define variance
\be
\var(X) = \E\brb{(X - \E X)^2}.
\ee
\end{definition}

\begin{remark}
The variance is a measure of how much the distribution of $X$ is spread out around the mean, the smaller the distribution the more the distribution of $X$ is concentrated close to $\E X$. The quantity $\sqrt{\var (X)}$ is known as the standard deviation of $X$. When we use the notation $\var (X)$ we will assume implicitly that it is a finite quantity.
\end{remark}

\begin{proposition}[properties of $\var (X)$]
Let $X\in \R$ and $X\in \sL^2(\Omega,\sF,\pro)$.
\ben
\item [(i)] $\var (X) = \E X^2 - (\E X)^2$.
\item [(ii)] If $c$ is a constant, $\var (cX) = c^2 \var (X)$.
\item [(iii)] If $c$ is a constant, $\var (X + c) = \var (X)$.
\item [(iv)] $\var (X) \geq 0$, and $\var (X) = 0$ if and only if $X = \E X$ a.s..
\item [(v)] The expression $\E (X - c)^2$ is minimized over constants $c$ when $c = \E X$, so that $\E (X - c)^2 \geq \var (X)$, for all $c$, with equality when $c = \E X$.
\een
\end{proposition}

\begin{proof}[\bf Proof]
(i)-(iii) are from definition directly.

\ben
\item [(iv)] By Proposition \ref{pro:expectation_property}.(ii),
\be
(X-\E X)^2 = 0\text{ a.s. }\ \ra \ X = \E X \text{ a.s.}.
\ee
%Thus, let $\E X = c$, we have $X = c$ a.s., for some constant $c$.
\item [(v)] Expand out the expression
\be
\E (X - c)^2 = \E \brb{X^2 - 2cX + c^2} = \E X^2 - 2c\E X + c^2,
\ee
and minimize the right-hand side in $c$ to see that the minimum occurs at $c = \E X$.
\een
\end{proof}

\begin{definition}[covariance\index{covariance}]\label{def:covariance}
For $X,Y \in \R$ and $X, Y \in \sL^2(\Omega,\sF,\pro)$, with means $\E X, \E Y$ (well-defined), covariance of $X$ and $Y$ is
\be
\cov(X, Y) = \E\brb{(X - \E X)(Y - \E Y )}.
\ee
\end{definition}

\begin{remark}
We shall see that this is a measure of the dependence between the random variables $X$ and $Y$.

For a random variable $X = (X_1, \dots,X_n)$ in $\R^n$, we define its covariance matrix
\be
V = \cov(X) = (\cov(X_i,X_j))^n_{i,j=1}, \quad \text{i.e., }\cov(X_i,X_j) = V_{ij}.
\ee
\end{remark}

\begin{proposition}\label{pro:independent_implies_covariance_zero}
For two independent random variables $X, Y \in \sL^2(\Omega,\sF,\pro)$, with means $\E X, \E Y$ (well-defined), we have $\cov(X, Y) = 0$.
\end{proposition}

\begin{proof}[\bf Proof]
This is from Definition \ref{def:covariance} and Proposition \ref{pro:expectation_of_independent_product}.
\end{proof}

\begin{remark}
The converse is generally false. One special case is multivariate Gaussian random variables (see Theorem \ref{thm:multivariate_gaussian_rv_property}.(v)).
\end{remark}

\begin{example}\label{equ:cov_notto_independent} [Covariance equal to 0 does not imply independence]
Suppose that $X$ is a random variable with distribution determined by
\begin{center}
\begin{tabular}{ccccc}
$x$ & 2 & 1 & -1 & -2 \\
\hline
$\pro(X = x)$ & $\frac 14$ & $\frac 14$ & $\frac 14$ & $\frac 14$
\end{tabular}
\end{center}
and let $Y = X^2$. Then $\E X = 0$ and $\E(X^3) = 0$ so that $\cov (X, Y ) = \E(X^3) = 0$, but
\be
\pro(X = 2, Y = 4) = \frac 14 \neq \pro (X = 2) \pro (Y = 4) = \frac 14\times \frac 12,
\ee
so that $X$ and $Y$ are not independent.
\end{example}

\begin{example}
Let $(X,Y)=(\cos\theta,\sin\theta)$ where $\theta=\frac{K\pi}{4}$ and $K$ is a random variable such that $\mathbb{P}(K=r)=1/8,\ r=0,1,\dots,7$.

\begin{center}%\begin{table}[ph]
\begin{tabular}{c|cccccccc}
$K$ & \quad 0 \quad &\quad 1 \quad & \quad 2 \quad & \quad 3 \quad &\quad 4 \quad &\quad 5 \quad & \quad 6 \quad & \quad 7 \quad \\ \hline
$X$ & 1 & $\frac{1}{\sqrt{2}}$ & 0 & $-\frac{1}{\sqrt{2}}$ & -1 & $-\frac{1}{\sqrt{2}}$ & 0 & $\frac{1}{\sqrt{2}}$ \\ \hline
$Y$ & 0 & $\frac{1}{\sqrt{2}}$ & 1 & $\frac{1}{\sqrt{2}}$ & 0 & $-\frac{1}{\sqrt{2}}$ & -1 & $-\frac{1}{\sqrt{2}}$
\end{tabular}
\end{center}%\end{table}

So it is immediate that $\cov(X,Y)=0$. We also have
\begin{equation}
\mathbb{P}\left(X=1/\sqrt{2}\right) = \mathbb{P}\left(Y=1/\sqrt{2}\right) = \frac{1}{4}
\end{equation}
but
\begin{equation}
\mathbb{P}\left(X=1/\sqrt{2},Y=1/\sqrt{2}\right) = \frac{1}{8} \neq \mathbb{P}\left(X=1/\sqrt{2}\right)\mathbb{P}\left(Y=1/\sqrt{2}\right)
\end{equation}
so that $X$ and $Y$ are not independent.
\end{example}


\begin{proposition}[properties of covariance]\label{pro:covariance_property}
Let $X,Y,Z\in\R$ and $X,Y,Z\in \sL^2(\Omega,\sF,\pro)$. Then
\ben
\item [(i)] $\cov (X,Y) = \cov (Y,X)$.
\item [(ii)] $\cov (X,Y ) = \E (XY ) - (\E X) (\E Y)$.
\item [(iii)] $\cov (X,X) = \var (X)$.
\item [(iv)] $\var (X + Y) = \var (X) + \var (Y) + 2\cov (X, Y )$.
\item [(v)] If $c$ is a constant, $\cov (X, c) = 0$.
\item [(vi)] If $c$ is a constant, $\cov (X + c,Y ) = \cov (X, Y )$.
\item [(vii)] If $c$ is a constant, $\cov (cX, Y ) = c\cov (X, Y )$.
\item [(viii)] $\cov (X + Z, Y ) = \cov (X, Y ) + \cov (Z, Y )$. These last two generalize to the case of random variables $X_1,\dots,X_n\in \sL^2(\Omega,\sF,\pro)$ and $Y_1,\dots,Y_n\in \sL^2(\Omega,\sF,\pro)$ and constants $c_1,\dots,c_n, d_1,\dots,d_n \in \R$ to give, by induction,
\be
\cov \brb{\sum^n_{i=1}c_iX_i,\sum^n_{j=1} d_jY_j} = \sum^n_{i=1}\sum^n_{j=1} c_id_j\cov (X_i,Y_j).
\ee

\item [(ix)] Using (iii), we see that a special case of this is
\be%\label{equ:var_cov}
\var\brb{\sum^n_{i=1} X_i} = \sum^n_{i=1} \var (X_i) + \sum^n_{i=1} \sum_{j\neq i} \cov (X_i,X_j),\qquad (*)
\ee
for any random variables $X_1,\dots,X_n$.

\item [(x)] Using (viii), if $X_1,\dots,X_n$ are independent, we have
\be
\var\brb{\sum^n_{i=1} c_iX_i} = \sum^n_{i=1} c_i^2\var (X_i).
\ee
\een
\end{proposition}

\begin{proof}[\bf Proof]
This is obvious from Definitions \ref{def:covariance}.%\beast
%\cov (X,Y) & = & \E (XY - X(\E Y) - Y (\E X) + (\E X)(\E Y ))\\
%& = & \E(XY ) - (\E X)(\E Y ) - (\E X)(\E Y ) + (\E X)(\E Y )\\
%& = & \E (XY ) - (\E X) (\E Y).
%\eeast
%\beast
%\var (X + Y ) & = & \E (X + Y - \E X - \E Y )^2 = \E ((X - \E X) + (Y - \E Y ))^2\\
%& = & \E\brb{(X - \E X)^2 + (Y - \E Y )^2 + 2 (X - \E X) (Y - \E Y)}\\
%& = & \E (X - \E X)^2 + \E (Y - \E Y )^2 + 2\E (X - \E X) (Y - \E Y).
%\eeast
\end{proof}

\begin{definition}[covariance matrix for multi-dimension\index{covariance matrix!multi-dimension}]\label{def:covariance_multidimension}
For random variables $X\in \R^n$ and $X \in \sL^2(\Omega,\sF,\pro)$ with $\E X$ well-defined, we define covariance
\be
\cov(X) = \E\brb{(X-\E X)(X-\E X)^T}.
\ee

For more general case, if $X\in \C^n$ and $X \in \sL^2(\Omega,\sF,\pro)$ with $\E X$ well-defined, we define covariance
\be
\cov(X) = \E\brb{(X-\E X)(X-\E X)^*}
\ee
where $*$ denotes the adjoint matrix (or conjugate transpose matrix).
\end{definition}

%Similarly, we have
%\begin{definition}[covariance matrix for multi-dimension]
%For random variables $X,Y\in \R^n$ and $X,Y\in \sL^2(\Omega,\sF,\pro)$ with $\E X$, $\E Y$ well-defined, we define the covariance of $X$ and $Y$,
%\be
%\cov(X,Y) = \E\brb{(X-\E X)(Y-\E X)^T}.
%\ee
%\end{definition}

\begin{proposition}
Every covariance matrix (which is symmetric or Hermitian) is non-negative definite.
\end{proposition}

\begin{proof}[\bf Proof]
For any $a \in \C^n$ and covariance matrix $\cov\brb{X}$ for $X\in \C^n$, recalling Definition\footnote{need definition here} and Definition \ref{def:covariance_multidimension},
\be
a^*\cov(X)a = \var(a^* X) \geq 0.
\ee

Therefore, $\cov\brb{X}$ is non-negative definite.
\end{proof}

\begin{definition}[correlation\index{correlation}]\label{def:correlation}
For $X, Y \in \sL^2(\Omega,\sF,\pro)$, with means $\E X, \E Y$ (well-defined), then correlation of $X$ and $Y$ is
\be
\corr(X, Y) = \cov(X, Y)/\sqrt{\var(X) \var(Y )}.
\ee

Note that the $\cov(X,Y) \in [-1,1]$. This is implied by Cauchy-Schwarz inequality (Theorem \ref{thm:cauchy_schwarz_inequality_probability}).
\end{definition}

\begin{remark}
It may be further seen that $\abs{\corr (X, Y )} = 1$ if and only if $X = aY +b$ for some constants $a$ and $b$. One property of correlation that we should note is that for constants $a$, $b$, $c$ and $d$ with $ac \neq 0$, we have
\be
\corr (aX + b, cY + d) = \left\{\ba{ll}
\corr(X, Y )\quad\quad & ac > 0,\\
-\corr(X, Y )& ac < 0.
\ea\right.
\ee

This follows easily from the definition of correlation and the properties of the covariance and variance, notice that when $ac = 0$, $\cov (aX + b, cY + d) = 0$, and the correlation is not defined because at least one of $\var (aX + b) = 0$ or $\var (cY + d) = 0$.

Notice that one consequence of this fact is that the correlation between two random variables is scale invariant - if we multiply the observation of $X$ and $Y$ by positive constants we do not alter the correlation.
\end{remark}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conditional Expectation}

\subsection{Elementary conditional probabilities of events}

\begin{definition}\label{def:conditional_probability_elementary}
Let $(\Omega,\sF,\pro)$ be a probability space. Let $A, B \in \sF$ and suppose that $\pro(B) > 0$. The conditional probability\index{conditional probability} of $A$ with respect to $B$ is
\be
\pro(A | B) = \frac{\pro(A\cap B)}{\pro(B)}.
\ee
\end{definition}

\begin{remark}
The conditional probability is interpreted as the probability of event $A$ happening given that event $B$ has happened.
\end{remark}

\begin{proposition}\label{pro:basic_conditional_probability_properties}
The first thing to observe is that $\pro (\cdot|B)$ is a probability distribution on the sample space $B$, because it satisfies the following
\ben
\item [(i)] For $C \subseteq B$, $\pro(C | B) = \pro(C)/\pro(B)$, so that $0 \leq \pro(C | B) \leq 1$.
\item [(ii)] $\pro(B | B) = \pro(B)/\pro(B) = 1$.
\item [(iii)] For countable disjoint events $C_1,C_2,\dots$ in $B$,
\be
\pro\brb{\left.\bigcup^\infty_{i=1} C_i\right|B} = \frac{\pro\brb{\bigcup^\infty_{i=1} C_i \cap B}}{\pro(B)} = \frac{\pro\brb{\bigcup^\infty_{i=1} C_i}}{\pro(B)} = \frac{\sum^\infty_{i=1} \pro(C_i)}{\pro(B)} = \sum^\infty_{i=1} \pro (C_i | B).
\ee

\item [(iv)] The next thing to notice is the multiplication rule that \be \pro(A \cap B) = \pro(A | B)P(B), \ee so that the probability of two events occurring can be broken up into calculating successive probabilities --
    firstly the probability that $B$ occurs and then given that $B$ has occurred the probability that $A$ occurs. This is one of two central procedures for calculating probabilities (the second is the law of total probability (Theorem \ref{thm:law_total_probability})). \een
\end{proposition}

\begin{example}
What is the probability that a Bridge hand contains the ace of hearts given that it contains exactly 5 hearts? Let $A$ be the event that the hand contains the ace of hearts and $B$ the event that the hand contains exactly 5 hearts. Then
\be
\pro(B) = \frac{\binom{13}{5}\binom{39}{8}}{\binom{52}{13}},\quad \pro(A \cap B) = \frac{\binom{12}{4}\binom{39}{8}}{\binom{52}{13}} \quad \ra \quad \pro(A|B) = \frac{\binom{12}{4}}{\binom{13}{5}} = \frac{5}{13}.
\ee
\end{example}

\begin{example}
Suppose that two students are selected, without replacement, from a class of 5 women and 13 men. What is the probability that the first student selected is a man and the second is a woman? Let $B$ be the event that the first is a man and $A$ the event that the second is a woman, then $\pro(A \cap B) = \pro(A | B)\pro(B) = \frac 5{17} \times \frac{13}{18}$.
\end{example}

More generally we can write down the multiplication rule for events $A_1,\dots,A_n$,
\be
\pro(A_1 \cap A_2 \dots \cap A_n) = \pro(A_1)\pro(A_2 |A_1)\pro(A_3 |A_1 \cap A_2) \dots \pro(A_n | A_1 \cap \dots\cap A_{n-1}).
\ee

\begin{example}
In drawing three cards without replacement from a pack what is the probability of three successive aces? Here $A_i$ would be the event that an ace is obtained on draw $i$, $i = 1,2,3$, then
\be
\pro(A_1 \cap A_2 \cap A_3) = \pro(A_1)\pro(A_2 | A_1)\pro(A_3 | A_1 \cap A_2) =  \frac 4{52} \times \frac 3{51} \times \frac 2{50}.
\ee
\end{example}


\begin{example}
For $s>1$ define the zeta function $\zeta(s)=\sum^\infty_{n=1}n^{-s}$. Let $X$ and $Y$ be independent random variables with
\be
\pro(X=n)=\pro(Y=n)=n^{-s}/\zeta(s).
\ee

Let $A_n$ be the event $\left\{n \text{ divides }X\right\}$ then
\be
\pro(A_n)=\pro\lob\bigcup_m \{X=mn\}\rob = \sum_m \pro(X=mn) = \sum_m (mn)^{-s}/\zeta(s) = n^{-s}.
\ee

Let $p_1,\dots,p_k$ be distinct primes
\be
\prod^k_{j=1}p_j = n \ \Leftrightarrow \ p_j \text{ divides $n$ for all }j\leq k \quad\ra\quad \pro\lob \bigcap^k_{j=1} A_{p_j}\rob = \pro\lob A_{(p_1\dots p_k)}\rob = \lob p_1\dots p_k\rob^{-s} = \prod^k_{j=1}\pro(A_{p_j}).
\ee
Thus, the events $\lob A_{p}, p \text{ is prime}\rob$ are independent. Hence the events $\lob A^c_{p}, p \text{ is prime}\rob$ are also independent (from Proposition \ref{pro:independent_event_generated}). Then
\be
\prod_p \lob 1-\frac 1{p^s}\rob = \prod_p \pro\brb{A_p^c} = \pro\lob \bigcap_p A_p^c\rob = \pro\lob \lob \bigcup_p A_p\rob^c\rob = \pro(X\text{ has no prime factors}) = \pro(X=1) = \frac 1{\zeta(s)}.
\ee

which is Euler's formula\index{Euler's formula}. The same argument show that the events $\lob A^c_{p^2}, p \text{ is prime}\rob$ are independent
\be
\pro(X \text{ is square-free}) = \prod_p \lob 1-\frac 1{p^{2s}}\rob = \frac 1{\zeta(2s)}.
\ee

Next, we have the event $\{H=n\} = A_n^X\cap A_n^Y \cap \lob \bigcup\limits_p A_{pn}^X\cap A_{pn}^Y\rob^c$, where $\lob\bigcup\limits_p A_{pn}^X\cap A_{pn}^Y\rob$ means that there exists a prime $p$ s.t. $X$ and $Y$ have a bigger common factor $pn$. Thus,
\be
\pro(\bra{H=n}) = \pro\lob A_n^X\cap A_n^Y \cap \lob \bigcup_p A_{pn}^X\cap A_{pn}^Y\rob^c\rob = \pro\lob A_n^X\cap A_n^Y \cap \lob \bigcap_p \lob A_{pn}^X\cap A_{pn}^Y\rob^c\rob\rob
\ee

Also, we check the event $(A_{pn}|A_n)$ (note that $\pro(A_{kn}|A_n) = k^{-s}$)
\be
\pro\lob \bigcap^k_{j=1}\lob A_{p_jn}|A_n\rob\rob = \pro\lob \left.\bigcap^k_{j=1}A_{p_jn}\right|A_n\rob = \pro\lob \left.A_{(p_1\dots p_jn)}\right|A_n\rob = \lob p_1\dots p_k\rob^{-s} = \prod^k_{j=1}\pro(A_{p_jn}|A_n).
\ee

Thus, we say that $\lob(A_{pn}|A_n), p \text{ is prime}\rob$ are independent. The similar argument for $\lob A_{pn}^X\cap A_{pn}^Y |A_n^X\cap A_n^Y\rob$. Then
\beast
\pro(\bra{H=n}) & = & \pro\lob A_n^X\cap A_n^Y \cap \lob \bigcap_p \lob A_{pn}^X\cap A_{pn}^Y\rob^c\rob\rob = \pro\lob \left.\bigcap_p \lob A_{pn}^X\cap A_{pn}^Y\rob^c \right|A_n^X\cap A_n^Y \rob \pro\lob A_n^X\cap A_n^Y\rob\\
& = & \pro\lob A_n^X\cap A_n^Y\rob \prod_p \pro \lob \lob\left. A_{pn}^X\cap A_{pn}^Y\rob^c \right|A_n^X\cap A_n^Y \rob = \pro\lob A_n^X\cap A_n^Y\rob \prod_p \lob 1 -\pro \lob \lob\left. A_{pn}^X\cap A_{pn}^Y\rob \right|A_n^X\cap A_n^Y \rob\rob \\
& = & \pro\lob A_n^X\rob \pro\lob A_n^Y\rob \prod_p \lob 1 - \pro \lob \left. A_{pn}^X\right|A_n^X\rob \pro\lob \left. A_{pn}^Y \right| A_n^Y \rob\rob = n^{-2s}\prod_p \lob 1 -p^{-2s}\rob = n^{-2s}/\zeta(2s).
\eeast
\end{example}


\begin{theorem}[law of total probability\index{law of total probability}]\label{thm:law_total_probability}
A countable collection $\bra{B_i}^\infty_{i=1}$ of disjoint events for which $\bigcup^{\infty}_{i=1} B_i = \Omega$ is said to be a partition\index{partition} of the sample space $\Omega$. For any partition of the sample space, $\{B_i\}$, and for any event $A$, we may write
\be\label{equ:law_of_total_probability}
\pro(A) = \sum^\infty_{i=1} \pro(A \cap B_i) = \sum_i \pro (A | B_i) \pro(B_i),
\ee
where the second summation extends only over those events $B_i$ in the partition for which $\pro(B_i) > 0$. The identity in (\ref{equ:law_of_total_probability}) is known as the law of total probability.
\end{theorem}

\begin{proof}[\bf Proof]
It follows immediately from \ref{pro:basic_conditional_probability_properties}.(iii),(iv), because the event A may be represented as $A = \bigcup^\infty_{i=1}(A \cap B_i)$.
\end{proof}


\begin{example}
An urn contains $b$ black balls and $r$ red balls from which two balls are drawn without replacement. What is the probability that the second ball drawn is black? Let $A$ represent the event that the second ball is black and $B$ the event that the first ball is black. Then $B$ and $B^c$ form a partition of the sample space (think of the other events in the partition as being $\emptyset$). Then
\be
\pro(A) = \pro (A | B) \pro(B) + \pro (A | B^c) \pro(B^c) = \brb{\frac{b - 1}{b + r - 1}} \brb{\frac{b}{b + r}} + \brb{\frac{b}{b + r - 1}}\brb{\frac r{b + r}}
= \frac b{b + r}.
\ee
This probability may be seen to be the same as the probability that the second ball is black when sampling with replacement.
\end{example}

\begin{example}\label{exa:us_voter}
A survey of US voters shows the following figures for proportions of voters registered with the main parties and the proportions of voters registered for each of the parties who declare an intention to vote for Dubya.
\begin{center}
\begin{tabular}{lcc}
& \quad Registered\quad & \quad Proportion for Bush\quad\\
\hline
Democratic & 45\% & 10\%\\
Republican & 35\% & 60\%\\
Not affiliated\quad & 20\% & 40\%
\end{tabular}
\end{center}
Suppose that a voter is chosen at random, what is the probability (s)he is a Bush voter? Here the partition of the sample space is $D$ (Democrat), $R$ (Republican) and $NA$ (No affiliation), and if $B$ is the event the voter is a Bush supporter then
\be
\pro(B) = \pro(B| D)\pro(D) + \pro(B | R)\pro(R) + \pro(B | NA)\pro(NA) = 0.1 \times 0.45 + 0.6 \times 0.35 + 0.4 \times 0.2 = 0.335.
\ee
\end{example}




\begin{theorem}[Bayes' Theorem\index{Bayes' Theorem}]\label{thm:bayes_theorem_two}
For any events $A$ and $B$, for which $\pro(A) > 0$ and $\pro(B) > 0$, we have
\be
\pro (B |A) = \frac{\pro (A | B) \pro(B)}{\pro(A)}.
\ee
\end{theorem}

\begin{example}
In Example \ref{exa:us_voter}, given that the voter chosen is a Bush supporter, what is the probability that (s)he is a Republican?
\be
\pro(R | B) = \frac{\pro (B | R) \pro(R)}{\pro(B)} = 0.6 \times 0.35/0.335 \approx 0.63.
\ee
\end{example}

Combining the law of total probability (Theorem \ref{thm:bayes_theorem_two})  with the statement in (\ref{thm:bayes_theorem_two}), gives the general statement of Bayes' Theorem:

\begin{theorem}[Bayes' Theorem\index{Bayes' Theorem}]\label{thm:bayes_theorem}
Suppose that $\{B_i\}_i$ is a partition of the sample space and that $A$ is an event for which $\pro(A) > 0$. Then for any event, $B_i$, in the partition with $\pro(B_i) > 0$, we have
\be
\pro(B_i| A) = \frac{\pro (A | B_i) \pro (B_i)}{\sum_j \pro (A | B_j) \pro (B_j)},
\ee
where the summation in the denominator extends over all $j$ for which $\pro(B_j) > 0$.
\end{theorem}



\begin{example}
Consider a diagnostic test for some disease for which the outcome of the test is either positive, +, or negative, -, and which is 99\% accurate, so that if $D$ represents the event that the patient has the disease then
\be
\pro (+ | D) = 0.99 = \pro (- | D^c).
\ee
Suppose that 0.1\% of patients have the disease. A patient is chosen at random and tests positive, what is the probability that (s)he has the disease? Then from Bayes' Theorem
\be
\pro(D | +) = \frac{\pro (+ | D) \pro(D)}{\pro (+ | D) \pro(D) + \pro (+ | D^c) \pro(D^c)} =\frac{0.99 \times 0.001}{0.99 \times 0.001 + 0.01 \times 0.999} \approx 0.09.
\ee
At first sight this conclusion, that the probability that someone testing positive has a somewhat low probability of having the disease, may be counter intuitive. It arises because the probability of a 'false positive', $\pro (+ | D^c)$, is large compared to the probability of the disease in the population, $\pro(D)$. Notice that
\be
\pro(D | +) = \frac 1{1 + (\pro(+ | D^c) \pro(D^c)/\pro (+ | D) \pro(D))}.
\ee
typically $\pro(D^c)$ and $\pro(+ | D)$ will be close to 1 so that $\pro(D | +)$ will be large or small according as the ratio $\pro(+| D^c) =\pro (D)$ is small or large. This becomes less mysterious when you consider a population of 1,000 patients with just one person suffering from the disease. Of the 999 not suffering from the disease there will be about 10 who will test positive so there will be about 11 in the population who will test positive, given that
17 the person selected has tested positive then there is about a 1 in 11 chance that the person is the one suffering from the disease. A similar calculation to the above shows that $\pro(D |-) \approx 0.000001$.
\end{example}

\begin{example}[Simpson's paradox]
One example of conditional probability that appears counter-intuitive when first seen is the following situation which can arise frequently. Consider one individual chosen at random from 50 men and 50 women applicants to a particular College. Figures on the 100 applicants are given in the following table indicating whether they were educated at a state school or at an independent school and whether they were admitted or rejected.
\begin{center}
\begin{tabular}{cccc}
\quad All applicants\quad & \quad Admitted\quad & \quad Rejected \quad &\quad \% Admitted \quad\\
\hline
State & 25 & 25 & 50\% \\
Independent & 28 & 22 & 56\%
\end{tabular}
\end{center}

Note that overall the probability that an applicant is admitted is 0.53, but conditional on the candidate being from an independent school the probability is 0.56 while conditional on being from a state school the probability is lower at 0.50. Suppose that when we break down the figures for men and women we have the following figures.
\begin{center}
\begin{tabular}{cccc}
\quad Men only\quad & \quad Admitted\quad & \quad Rejected \quad &\quad \% Admitted \quad\\
\hline
State & 15 & 22 & 41\%\\
Independent & 5 & 8 & 38\%\\
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{cccc}
\quad Women only \quad & \quad Admitted\quad & \quad Rejected \quad &\quad \% Admitted \quad\\
\hline
State & 10 & 3 & 77\%\\
Independent & 23 & 14 & 62\%\\
\end{tabular}
\end{center}
It may now be seen that now for both men and women the conditional probability of being admitted is higher for state school applicants, at 0.41 and 0.77, respectively. This may seem to be a surprising result in that while the aggregate data suggests that independent school applicants have a better chance of being admitted, the individual tables for both men and women show that in both cases state school applicants have a higher acceptance rate. A 18 result of this type in tables of this sort (called contingency tables in statistics) is known as Simpson's paradox. Strictly, it is not a paradox in that it has an easy explanation. Note that overall women have a much higher acceptance rate (66\%) than men (40\%) whereas the proportion of men from state schools is 74\% with 26\% from independent schools while those proportions are reversed for women. This situation is known in statistics as confounding, it occurs when figures for two separate and different populations are aggregated to give misleading conclusions. It may be difficult or impossible to determine whether data such as that presented in the first table arise from two disparate populations and so confounding is present.

The example shows that if $A, B, C$ are three events it is possible to have the three inequalities
\be\label{equ:conditional_inequality}
\pro (A | B \cap C) > \pro (A | B \cap C^c),\quad\quad \pro (A | B^c \cap C) > \pro (A | B^c \cap C^c),\quad\quad \pro (A | C^c) > \pro (A | C),
\ee
holding simultaneously. In this example, A would be the event 'being admitted', $B$ the event 'being a man' (with $B^c$ being a woman) and $C$ being 'state school' (with $C^c$ being independent school). One situation where the three inequalities in (\ref{equ:conditional_inequality}) cannot hold simultaneously is when
\be\label{equ:conditional_equality}
\pro (B| C) = \pro (B | C^c)
\ee
To see this, suppose that (\ref{equ:conditional_inequality}) and (\ref{equ:conditional_equality}) hold, then we see that
\be
\pro(A \cap B | C) > \pro (A \cap B | C^c),\quad\quad  \pro(A \cap B^c | C) > \pro (A \cap B^c | C^c)
\ee
and then adding and observing that, for example,
\be
\pro (A \cap B | C) + \pro (A \cap B^c | C) = \pro(A | C),
\ee
we obtain $\pro (A | C) > \pro (A | C^c)$, which contradicts (\ref{equ:conditional_inequality}). In this example it would not be possible to engineer that (\ref{equ:conditional_equality}) holds, but consider the situation where a clinical trial of a new drug for some illness is being conducted to test its effectiveness against a standard drug. Then $A$ would be the event that a patient recovers from the illness and $C$ would be the event that the patient receives the new drug ($C^c$ the event that (s)he receives the standard drug), again $B$ or $B^c$ would correspond to the patient being a man or woman,
respectively. Then to avoid the sort of situation described in this example, (\ref{equ:conditional_equality}) would require that the trial be designed so that the relative proportions of men and women that receive the new drug are the same as the relative proportions that receive the standard drug.
\end{example}

\subsection{Elementary conditional probabilities of random variables}

\begin{definition}\label{def:conditional_expectation_elementary_set}
Let $(\Omega,\sF,\pro)$ be a probability space. If $X \in \sL^1(\Omega,\sF,\pro)$ then the conditional expectation\index{conditional expectation!elementary for set} of $X$ given $B$ is
\be
\E\brb{X | B} = \frac{\E(X\ind_B)}{\pro(B)}.
\ee
\end{definition}

Comparing with Definition \ref{def:conditional_expectation_elementary_set}, we define more general conditional expectation:


\begin{definition}\label{def:conditional_expectation_elementary_sigma_algebra}
$(\Omega,\sF,\pro)$ is a probability space. Let $(B_i)_{i\in I}$ be a partition of $\Omega$ i.e., $(B_i)$ are disjoint and $\bigcup_{i\in I}B_i = \Omega$ such that $B_i \in \sF$ for all $i\in I$ where $I$ is a countable
set. Let $\sG = \sigma(B_i , i \in I)$ and for $X \in \sL^1$ define the conditional expectation\index{conditional expectation!elementary for $\sigma$-algebra} of $X$ with respect to $\sG$ to be \be Y := \E\brb{X | \sG} =
\sum_{i\in I} \E\brb{X | B_i}\ind_{B_i}, \quad\quad (\text{i.e., }\E\brb{X | \sG} = \E\brb{X | B_i} \ \lra \ \omega \in B_i) \ee with the convention that $E\brb{X | B_i} = 0$ if $\pro(B_i) = 0$\footnote{If we rule out the event
$B_i$ with $\pro(B_i) = 0$, we can define $\E(X|\sG) = \sum_{i\in I} \E\brb{X | B_i}\ind_{B_i}$ a.s. which is consistent with general definition (see \ref{thm:conditional_expectation_existence_uniqueness}). Note that we can
get a.s. since $I$ is countable}. Also, if $Z$ is a random variable, $\E(X|Z) = \E\brb{X|\sigma(Z)}$.
\end{definition}

\begin{lemma}\label{lem:conditional_expectation_elementary_sigma_algebra}
Let $X \in \sL^1(\Omega,\sF,\pro)$ and $(B_i)_{i\in I}$ be a partition of $\Omega$. The conditional expectation with respect to $\sG = \sigma(B_i , i \in I)$ satisfies
\ben
\item [(i)] $\E\brb{X | \sG}$ is $\sG$-measurable,
\item [(ii)] $\E\brb{X | \sG} \in \sL^1(\Omega,\sG,\pro)$,
\item [(iii)] $\E\brb{X\ind_A} = \E\brb{\E\brb{X | \sG}\ind_A}$ for all $A \in \sG$.
\een
\end{lemma}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Let $f$ be the map $f:\Omega \to I$ with $f(\omega) = i \ \lra \ \omega \in B_i$. For any $J\subseteq I$, we have $f^{-1}(J) = \bigcup_{j\in J} B_j \in \sG$, as $B_j \in \sG, j\in J$. Thus, $f$ is $\sG$-measurable (by Definition \ref{def:measurable_function}). Further, let $g:I \to \R, i \mapsto \E(X|B_i)$. Since $I$ is discrete, $g$ is measurable. Then
\be
\E\brb{X|\sG} = g \circ f(\omega) \quad\text{is $\sG$-measurable (by Proposition \ref{pro:composition_measurable}).}
\ee

\item [(ii)] Let $I' := \bra{i\in I:\pro(B_i) >0}$, by Definition \ref{def:conditional_expectation_elementary_sigma_algebra} and Fubini theorem (Theorem \ref{thm:fubini}),
\beast
\E\brb{\abs{\E\brb{ X | \sG}}} & = & \E\brb{\abs{\sum_{i\in I} \E\brb{X | B_i}\ind_{B_i}}} = \E\brb{\abs{\sum_{i\in I} \frac{\E(X\ind_{B_i})}{\pro(B_i)} \ind_{B_i}}} \leq \E\brb{\sum_{i\in I} \frac{\abs{\E\brb{X\ind_{B_i}}}}{\pro(B_i)} \ind_{B_i}}\\
& \leq & \E\brb{\sum_{i\in I'} \frac{\E\brb{\abs{X}\ind_{B_i}}}{\pro(B_i)} \ind_{B_i}} = \sum_{i\in I'} \frac{\E\brb{\abs{X}\ind_{B_i}}}{\pro(B_i)} \E\brb{\ind_{B_i}} = \E\brb{\abs{X}\ind_{\bra{\bigcup_{i\in I'} B_i}}} \leq \E\brb{\abs{X}} < \infty.
\eeast
\item [(iii)] For every $A\in \sG$, we have $J\subseteq I$ with $A = \bigcup_{j\in J}B_j$. Let $J' := \bra{i\in J:\pro(B_i) >0}$. So, we have $\ind_A = \ind_{\bra{\bigcup_{i\in J'}B_i}}$ a.s., which gives that $X\ind_A = X\ind_{\bra{\bigcup_{i\in J'}B_i}} = X\sum_{i\in J'} \ind_{B_i}$ a.s. Hence,


by Definition \ref{def:conditional_expectation_elementary_sigma_algebra} and Fubini theorem (Theorem \ref{thm:fubini}),
\beast
\E\brb{\E\brb{X | \sG}\ind_A} & = & \int_A \E\brb{X | \sG} d\pro(\omega) = \int_A \brb{\sum_{i\in I} \E\brb{X | B_i}\ind_{B_i}}d\pro(\omega) = \int_A \brb{\sum_{i\in I} \frac{\E(X\ind_{B_i})}{\pro(B_i)} \ind_{B_i}}d\pro(\omega)\\
& = & \int_\Omega \brb{\sum_{i\in I} \frac{\E(X\ind_{B_i})}{\pro(B_i)} \ind_{A\cap B_i}}d\pro(\omega) = \sum_{i\in J'} \frac{\E\brb{X\ind_{B_i}}}{\pro(B_i)} \int_\Omega \ind_{B_i} d\pro(\omega) = \sum_{i\in J'} \E\brb{X\ind_{B_i}} \\
& = &  \E\brb{\sum_{i\in J'}X\ind_{B_i}} = \E\brb{X \sum_{i\in J'}\ind_{B_i}} = \E\brb{X\ind_A}. \quad\quad \text{(Theorem \ref{thm:lebesgue_integrable_function_property})}
\eeast
\een

You can see the proof in Klenke\cite{Klenke_2008}.$P_{172}$.
\end{proof}


\begin{proposition}[law of total expectation]\label{pro:conditional_expectation_elementary_event}
Let $X \in \sL^1(\Omega,\sF,\pro)$ and $(B_i)_{i\in I}$ ($I$ is a countable set) be a partition of $\Omega$. The conditional expectation with respect to $\sG = \sigma(B_i , i \in I)$ satisfies
\be
\E X = \sum_{i\in I} \E\brb{X | B_i}\pro\brb{B_i}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Direct result from Definition \ref{def:conditional_expectation_elementary_sigma_algebra} and the proof of Lemma \ref{lem:conditional_expectation_elementary_sigma_algebra}.(iii).
\end{proof}

\begin{example}
Let $X_1,\dots,X_n$ be $n$ independent random variable with
\be
\pro(X_n =0) = \pro(X_n=1)=\frac 12, \quad\quad S_n = \sum^n_{i=1}X_i,\quad\quad \sG := \sigma(S_n).
\ee

For $0\leq k\leq n$, let $G_k = \{S_n = k\}$, then $\bigcup^n_{k=0}G_k = \Omega$, and $G_k$ are pairwise disjoint. Thus $\sG := \sigma(G_0,\dots,G_n)$. So
\be
\E\brb{X_1|\sG} = \sum^n_{k=0}\E\brb{X_1|G_k}\ind_{G_k}.
\ee

Now, $\E(X_1|G_k) = \pro(X_1=1|S_n = k), \quad\quad \pro(S_n =k) = \binom{n}{k}\brb{\frac 12}^k\brb{\frac12}^{n-k} = \binom{n}{k}2^{-n}$.

Hence
\be
\pro\brb{X_1=1|S_n=k} = \frac{\pro\brb{X_1=1,S_n=k}}{\pro(S_n=k)} = \frac{\frac 12 \binom{n-1}{k-1}\brb{\frac 12}^{n-1}}{\binom{n}{k}2^{-n}} = \frac kn \quad\ra\quad \E\brb{X_1|\sG} = \sum^n_{k=0} \frac kn \ind_{S_n =k} = \frac{S_n}n.
\ee
\end{example}

\begin{example}
Let $X \in \sL^1(\Omega,\sF,\pro)$ and let $Y$ be a r.v. taking values in some countable set $E$. Then $\Omega = \bigcup_{y\in E} \{Y = y\}$ is a partition of $\Omega$ which generates $\sigma(Y)$, and
\be
\E\brb{X| Y} := \E\brb{X | \sigma(Y)} = \sum_{y\in E}\E\brb{X| \{Y = y\}}\ind_{\bra{Y=y}}.
\ee
\end{example}


%%%%%%%%%%%%%%%%%%%%%



\subsection{Conditional expectation}

We now define the conditional expectation with respect to a sub-$\sigma$-algebra so that it satisfies the same properties as conditional expectation with respect to $\sG = \sigma(B_i , i \in I)$ (as Definition \ref{def:conditional_expectation_elementary_sigma_algebra}). The following definition and theorem are due to Kolmogorov.

\begin{theorem}\label{thm:conditional_expectation_existence_uniqueness}
Let $X \in \sL^1(\Omega,\sF,\pro)$ and $\sG \subseteq \sF$ be a sub-$\sigma$-algebra. Then there exists an integrable r.v. $Y$ such that
\ben
\item [(i)] $Y$ is $\sG$-measurable,
\item [(ii)] $Y \in \sL^1(\Omega,\sG,\pro)$,
\item [(iii)] $\E\brb{X\ind_A} = \E\brb{Y\ind_A}$ for all $A \in \sG$.
\een

Moreover, if $\wt{Y}$ is another such r.v. then $Y = \wt{Y}$ a.s. We write $Y = \E\brb{X|\sG}$, a.s.
\end{theorem}

\begin{remark}
Note that if $\sG$ is the trivial $\sigma$-algebra $\{\emptyset, \Omega\}$ (which contains no information), then $\E\brb{X|\sG} = \E(X)$ for all $\omega$.
\end{remark}

\begin{proof}[\bf Proof of Almost sure uniqueness of $\E\brb{X|\sG}$]
Suppose that $X\in \sL^1$ and that $Y$ and $\wt{Y}$ are versions of $\E\brb{X|\sG}$. Then, $Y,\wt{Y} \in \sL^1\brb{\Omega,\sG,\pro}$, and for all $A\in \sG$
\be
\E \brb{Y\ind_A} = \E \brb{X\ind_A} = \E \brb{\wt{Y}\ind_A}\quad\ra\quad \E\brb{\brb{Y-\wt{Y}}\ind_A} = 0. \quad (*)\quad \text{(Theorem \ref{thm:lebesgue_integrable_function_property})}
\ee

Suppose that $Y$ and $\wt{Y}$ are not almost surely equal. We may assume that the labelling is such that $\pro(Y>\wt{Y}) >0$. Since
\be
\bra{Y>\wt{Y} + \frac 1n} \ua \bra{Y>\wt{Y}},
\ee
we see that $\pro\brb{Y-\wt{Y} > \frac 1n} >0$ for some $n$. But the set $\bra{Y-\wt{Y}> \frac 1n}$ is in $\sG$, because $Y$ and $\wt{Y}$ are $\sG$-measurable and
\be
\E\brb{\brb{Y-\wt{Y}}\ind_{Y-\wt{Y}> \frac 1n}} \geq \frac 1n \pro\brb{\bra{Y-\wt{Y}>\frac 1n}} >0,
\ee

a contradiction with $(*)$. Hence $Y=\wt{Y}$ a.s.
\end{proof}

\begin{proof}[\bf Existence of $\E(X|\sG)$ for $X\in \sL^2(\Omega,\sF,\pro)$]
Suppose that $X\in \sL^2(\Omega,\sF,\pro)$. Let $\sG$ be a sub-$\sigma$-algebra of $\sF$, then $\sL^2(\Omega,\sG,\pro)$ is complete by Theorem \ref{thm:completeness_of_slp}. Then by orthogonal projection theorem (Theorem \ref{thm:orthogonal_projection_sl2_probability}, there exists $Y\in \sL^2(\Omega,\sG,\pro)$ such that
\be
\E\brb{\brb{X-Y}^2} = \inf \bra{\E\brb{(X-W)^2}:W\in \sL^2(\Omega,\sG,\pro)},\quad \quad \inner{X-Y}{Z} = 0,\quad Z\in \sL^2(\Omega,\sG,\pro).
\ee

Now, if $A\in \sG$, then $Z:=\ind_A\in \sL^2(\Omega,\sG,\pro)$, we have $\E\brb{(X-Y)Z} = 0\ \ra\ \E\brb{Y\ind_A} = \E\brb{X\ind_A}$. Thus, $Y$ is a version of $\E(X|\sG)$.
\end{proof}

%Before proving the existence for the case that $X\in \sL^1(\Omega,\sF,\pro)$, we have the following lemma:

\begin{proof}[\bf Existence of $\E(X|\sG)$ for $X\in \sL^1(\Omega,\sF,\pro)$]
By splitting $X$ as $X=X^+ - X^-$, we see that it is enough to deal with the case when $X\in \brb{\sL^1(\Omega,\sF,\pro)}^+$. We can now choose bounded variables $X_n = X\land n$ with $0\leq X_n \ua X$. Since $X_n$ is bounded and thus in $\sL^2(\Omega,\sF,\pro)$, we can choose a version $Y_n$ of $\E(X_n|\sG)$.

Since $X_n$ is non-negative bounded random variable, from Lemma \ref{lem:non_negative_bounded_conditional_expectation}, $Y_n \geq 0$ a.s. Also, $X_{n+1} - X_n \geq 0$ implies that $Y_{n+1} \geq Y_n$ a.s.
\be
\bra{0\leq Y_n \ua}^c = \brb{\bigcap_n \bra{Y_{n+1}-Y_n \geq 0}}^c = \bigcup_n \bra{Y_{n+1}-Y_n < 0}
\ee

Thus,
\be
\pro\brb{\bra{0\leq Y_n \ua}^c} = \pro\brb{\bigcup_n \bra{Y_{n+1}-Y_n < 0}} \leq \sum_n \pro \brb{\bra{Y_{n+1}-Y_n < 0}} = 0 \cdot \underbrace{\infty}_{\text{countable}} = 0 \quad\ra\quad 0\leq Y_n \ua \ \text{ a.s.}.
\ee

Then we set $Y(\omega) := \lim_n Y_n(\omega),\ \omega \in \bra{0\leq Y_n \ua}$, thus
\be
Y \text{ is $\sG$-measurable since $Y_n$ is measurable (by Theorem \ref{thm:measurable_function_property}), and $Y_n \ua Y$ a.s.}.
\ee

For any $A \in \sG$, $\E\brb{X_n\ind_A} = \E\brb{Y_n\ind_A}$, also by monotone convergence theorem (Theorem \ref{thm:monotone_convergence_probability}),
\be
\E\brb{X_n\ind_A} \to \E\brb{X\ind_A},\quad\quad \E\brb{Y_n \ind_A} \to \E\brb{Y\ind_A}\quad\ra\quad \E\brb{X\ind_A} = \E\brb{Y\ind_A}.
\ee

Finally, we let $A = \Omega$, $\E\abs{Y} = \E Y = \E\brb{Y \ind_\Omega} = \E\brb{X \ind_\Omega} = \E X < \infty$, thus $Y \in \sL^1(\Omega,\sG,\pro)$.
\end{proof}

\begin{remark}
The condition (iii) in Theorem \ref{thm:conditional_expectation_existence_uniqueness} can be generalized to
\be
\text{(iii')}\quad \E\brb{ZX} = \E\brb{ZY}\ \text{ for every bounded $\sG$-measurable r.v. $Z$}.
\ee
To prove (iii') from (i), (ii) and (iii), first prove it for simple functions $Z$ and then approximate general $Z$.
\end{remark}

\begin{lemma}\label{lem:non_negative_bounded_conditional_expectation}
Let $(\Omega,\sF,\pro)$ be a probability measure and $X$ be a non-negative bounded random variable, then
\be
\E\brb{X|\sG} \geq 0\quad \quad \text{a.s.}
\ee
\end{lemma}

\begin{proof}[\bf Proof]
Since $X$ is non-negative bounded, we have $X\in \sL^2(\Omega,\sF,\pro)$, thus, $\E(X|\sG)$ exists. Let $Y$ be a version of $\E(X|\sG)$. If $\pro(Y<0) >0$, then for some $n$, the set
\be
A := \bra{Y< -\frac 1n}\in \sG \quad\text{has positive probability,} \quad\ra \quad 0 \leq \E\brb{X\ind_A} = \E\brb{Y\ind_A} < - \frac 1n \pro(A) <0.
\ee
The contradiction finishes the proof.
\end{proof}

\begin{proposition}\label{pro:conditional_expectation_basic_property}
Let $(\Omega,\sF,\pro)$ be a probability space and $\sG \subseteq \sF$,
\ben
\item [(i)] If $X \in \sL^1(\Omega,\sF,\pro)$, then $\E\brb{\E\brb{X |\sG}} = \E X$.
\item [(ii)] If $X$ is $\sG$-measurable and $X \in \sL^1(\Omega,\sF,\pro)$ then $\E\brb{X |\sG} = X$ a.s.
\item [(iii)] If $X, Y \in \sL^1(\Omega,\sF,\pro)$ and $a, b \in \R$ then $\E\brb{aX + bY |\sG} = a\E\brb{X |\sG}+ b\E\brb{Y | \sG}$ a.s.
\item [(iv)] If $X \in \sL^1(\Omega,\sF,\pro)$ then $\abs{\E\brb{X |\sG}} \leq \E\brb{\abs{X}|\sG}$ a.s., which implies that $\E\brb{\abs{\E\brb{X |\sG}}} \leq \E\abs{X}$.
\item [(v)] If $X \in \sL^1(\Omega,\sF,\pro)$ and $X\geq 0$ then $\E\brb{X|\sG} \geq 0$ a.s.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Use Theorem \ref{thm:conditional_expectation_existence_uniqueness} and let $\Omega = A\in \sG$. Let $Y$ be a version of $\E\brb{X|\sG}$, $\E\brb{\E\brb{X|\sG}} = \E \brb{Y\ind_\Omega} = \E\brb{X\ind_\Omega} = \E X$.

\item [(ii)] Recall Theorem \ref{thm:conditional_expectation_existence_uniqueness} and let $Y = X$, then $\E\brb{Y\ind_A} = \E\brb{X \ind_A}$ for any $A\in \sG$, thus $X$ is a version of $\E\brb{X|\sG}$, so $X = \E\brb{X|\sG}$ a.s.

\item [(iii)] Let $X'$ be a version of $\E(X|\sG)$, $Y'$ be a version of $\E(Y|\sG)$, for any $A\in \sG$, by Theorem \ref{thm:lebesgue_integrable_function_property} as $X'$ and $Y'$ are integrable,
\be
\E\brb{X'\ind_A} = \E\brb{X\ind A}, \quad \E\brb{Y'\ind_A} = \E\brb{Y\ind_A} \quad\ra\quad \E\brb{\brb{aX' + bY'}\ind_A} = \E\brb{\brb{aX + bY}\ind_A}.
\ee
then $aX' + bY'$ is a version of $\E\brb{aX + bY|\sG}$. Thus, $\E\brb{aX + bY |\sG} = a\E\brb{X |\sG}+ b\E\brb{Y | \sG}$ a.s.

\item [(iv)] We have by (iii) %Let $Y$ be a version of $\E\brb{X|\sG}$, then
\be
\abs{\E\brb{X |\sG}} = \abs{\E\brb{X^+ - X^-|\sG}} \stackrel{\text{a.s.}}{=} \abs{\E\brb{X^+|\sG} - \E\brb{X^-|\sG}} \leq \abs{\E \brb{X^+|\sG}} + \abs{\E\brb{X^-|\sG}}.
\ee

Since $X^+,X^- \geq 0$, we have $\abs{\E \brb{X^+|\sG}} + \abs{\E\brb{X^-|\sG}} \stackrel{\text{a.s.}}{=} \E\brb{X^+ + X^-|\sG} = \E\brb{\abs{X}|\sG}$. so \
\be
\abs{\E\brb{X |\sG}} \leq \E\brb{\abs{X}|\sG} \ \text{ a.s.} \quad \ra \quad \E\brb{\abs{\E\brb{X |\sG}}} \leq \E\brb{\E\brb{\abs{X}|\sG}} = \E\abs{X} \quad \text{(by Theorem \ref{thm:lebesgue_integrable_function_property} and (i))}
\ee

\item [(v)] We can use the same as Lemma \ref{lem:non_negative_bounded_conditional_expectation}.
\een
\end{proof}







\subsection{Non-negative conditional expectation}

With Theorem \ref{thm:conditional_expectation_existence_uniqueness}, we actually built an object $\E(X|\sG)$ as the a.s. increasing limit of $\E(X\land n|\sG)$ for any non-negative random variable $X$, not necessarily integrable. This random variable enjoys similar properties as $\sL^1$ case:


\begin{theorem}\label{thm:conditional_expectation_existence_uniqueness_non_negative}
Let $X$ be a non-negative random variable and $\sG \subseteq \sF$ be a sub-$\sigma$-algebra. Then there exists a r.v. $Y\geq 0$ such that
\ben
\item [(i)] $Y$ is $\sG$-measurable,
\item [(ii)] $\E\brb{ZX} = \E\brb{ZY}$ for all non-negative $\sG$-measurable random variable $Z$.
\een

Moreover, if $\wt{Y}$ is another such r.v. then $Y = \wt{Y}$ a.s. We write $Y = \E\brb{X|\sG}$, a.s.
\end{theorem}

\begin{proof}[\bf Proof]
{\bf Existence}. Let $Y_n = \E(X\land n|\sG)$ a.s. and $Y_n$ is $\sG$-measurable (by Theorem \ref{thm:conditional_expectation_existence_uniqueness} since $X\land n$ is bounded and thus integrable). Also, we know that $Y_n \geq 0$ a.s. by Lemma \ref{lem:non_negative_bounded_conditional_expectation}. Now let
\be
Y = \begin{cases}
\limsup_n Y_n & Y_n \geq 0\\
0 & Y_n < 0
\end{cases} \quad \text{i.e.,}\quad Y = \limsup_n Y_n\ind_{\bigcap_n\bra{Y_n\geq 0}}.
\ee

Thus $Y\geq 0$ is $\sG$-measurable by Theorem \ref{thm:measurable_function_property_infinity}. Then let $Z_n = Z\land n$, we have $Y_nZ \ua Y Z$ a.s. for any non-negative random variable $Z$. Thus, by monontone convergence theorem (Theorem \ref{thm:monotone_convergence_probability})
\be
\E(Y_n Z_n) \ua \E(YZ).
\ee

Also $X_nZ_n \ua XZ$ implies $E(X_nZ_n) \ua \E(XZ)$ by monontone convergence theorem (Theorem \ref{thm:monotone_convergence_probability}). With Theorem \ref{thm:conditional_expectation_existence_uniqueness} (iii'), we have
\be
\E(X_nZ_n) = \E(Y_nZ_n) \quad\ra\quad \E(XZ) = \E(YZ).
\ee

\hspace{-6mm}{\bf Uniqueness}. If $Y$ and $\wt{Y}$ are non-negative and satisfy (i) and (ii). For any $a<b\in \Q^+$, by letting $B = \bra{Y\leq a<b\leq \wt{Y}}\in \sG$ since $Y$ and $\wt{Y}$ are $\sG$-measurable, we obtain
\be
b\pro(B) \leq \E(\wt{Y}\ind_B) = \E(X\ind_B) = \E(Y\ind_B) \leq a\pro(B)
\ee
which entails that $\pro(B) = 0$, so that
\be
\bra{Y<\wt{Y}} = \underbrace{\bigcup_{a<b\in \Q^+} \bra{Y\leq a<b\leq \wt{Y}}}_{\text{countable many}} \quad\ra\quad \pro\brb{Y<\wt{Y}}  = 0
\ee

Similarly, $\pro\brb{\wt{Y}<Y} = 0$, thus $Y = \wt{Y}$ a.s.
\end{proof}

Now we can generalize Proposition \ref{pro:conditional_expectation_basic_property}

\begin{proposition}\label{pro:non_negative_conditional_expectation}
For $(\Omega,\sF,\pro)$ and $\sG \subseteq \sF$ be a sub-$\sigma$-algebra. If $0\leq X\leq Y$, then $0\leq \E(X|\sG) \leq \E(Y|\sG)$ a.s.
\end{proposition}

\begin{proof}[\bf Proof]
First by Theorem \ref{thm:conditional_expectation_existence_uniqueness_non_negative}, $\E(X|\sG)$ and $\E(Y|\sG)$ are well-defined. We can use the same as Lemma \ref{lem:non_negative_bounded_conditional_expectation} to show that $\E(X|\sG)\geq 0$ a.s. and $\E(Y|\sG)\geq 0$ a.s.

%Let $X_n = X\land n$, $Y_n = Y\land n$, we

%If $Y\in \sL^1(\Omega,\sF,\pro)$, then $X\in \sL^1(\Omega,\sF,\pro)$, so $0\leq Y-X\in \sL^1(\Omega,\sF,\pro)$. Thus, by Proposition \ref{pro:conditional_expectation_basic_property} (v), $\E(Y-X|\sG) \geq 0$ a.s. Then by Proposition \ref{pro:conditional_expectation_basic_property} (iii),
%\be
%\E(Y|\sG) - \E(X|\sG) \geq 0 \ \text{a.s.} \quad\ra \quad \E(Y|\sG) \geq \E(X|\sG) \ \text{a.s.}
%\ee

%If $Y$ is not integrable, we have $\E(Y) = \infty$,


Now let $X'$ be a non-negative version of $\E(X|\sG)$ and $Y'$ be a non-negative version of $\E(Y|\sG)$. If $\pro(Y'< X') >0$, then for some $n$, the set
\beast
A := \bra{Y' < X'-\frac 1n} = \bra{Y'+ \frac 1n < X'} \in \sG \quad\text{has positive probability,} %\quad\quad (\text{note that }A\subseteq \bra{Y'<\infty})
\eeast

Then by Theorem \ref{thm:non_negative_measurable_property} (ii), since $Y'\ind_A + \frac 1n \ind_A \leq X'\ind_A$,
\be
\E\brb{Y'\ind_A} + \frac 1n \pro(A) \leq \E\brb{X'\ind_A} = \E\brb{X\ind_A} \leq \E\brb{Y\ind_A} = \E\brb{Y'\ind_A}. %\quad\ra\quad \frac 1n \pro(A) < 0
\ee
The contradiction finishes the proof.
\end{proof}



\subsection{Specific theorems of conditional expectation}


\begin{theorem}[conditional monotone convergence Theorem\index{monotone convergence theorem!conditional expectation}]\label{thm:monotone_convergence_conditional_expectation}
Let $X_n \geq 0 (n \geq 0)$ be such that $X_n \ua X$ a.s. Then
\be
\E\brb{X_n| \sG}\ua \E\brb{X |\sG} \text{ a.s.}
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Note that if $0 \leq X \leq Y$ then $0 \leq \E\brb{X |\sG} \leq \E\brb{Y | \sG}$ a.s. (by Proposition \ref{pro:non_negative_conditional_expectation}). Let $Y_n = \E\brb{X_n | \sG}$ a.s., then $Y_n$ increases with $n$ a.s., so let $Y = \lim_{n\to \infty} Y_n$. Then for any $A \in \sG$, by Theorem \ref{thm:monotone_convergence_probability},
\be
\E\brb{X_n\ind_A} \ua \E\brb{X\ind_A},\quad\quad \E\brb{Y_n\ind_A} \ua \E\brb{Y\ind_A}
\ee
so $\E\brb{Y\ind_A} = \E\brb{X\ind_A}$ and $Y = \E\brb{X| \sG}$ a.s. by uniqueness.
\end{proof}

\begin{theorem}[conditional Fatou's lemma\index{Fatou's lemma!conditional expectation}]\label{thm:fatou_conditional_expectation}
Let $X_n \geq 0$, $n \geq 0$. Then
\be
\E\brb{\left.\liminf_{n\to \infty} X_n\right| \sG} \leq \liminf_{n\to \infty} \E\brb{X_n |\sG}\ \text{ a.s.}
\ee
(inverse conditional Fatou's lemma). Additionally, if there exists a non-negative integrable random variable $Y$ such that $X_n\leq Y$ for all $n$, then
\be
\E\brb{\left.\limsup_n X_n\right|\sG} \geq \limsup_n \E(X_n|\sG)\ \text{ a.s.}
\ee
\end{theorem}

\begin{proof}[\bf Proof]
For $k \geq n$, we have
\be
\underbrace{\inf_{m\geq n} X_m \leq X_k \quad\ra \quad \E\brb{\left.\inf_{m\geq n} X_m\right|\sG} \leq \E(X_k|\sG)\ \text{ a.s.}}_{\text{by Proposition \ref{pro:conditional_expectation_basic_property} (v) since $X_n$ and $\inf X_n$ are non-negative measurable}} \quad \ra \quad \E\brb{\left.\inf_{m\geq n} X_m\right|\sG} \leq \inf_{k\geq n}\E(X_k|\sG) \ \text{ a.s.}
\ee

Then we have
\be
\quad \E\brb{\left.\inf_{m\geq n} X_m\right|\sG} \leq \inf_{k\geq n} \E(X_k |\sG) \leq \sup_n \inf_{k\geq n} \E(X_k|\sG) = \liminf_n \E(X_n|\sG)\ \text{ a.s.}
\ee

But, as $n \to \infty$,
\be
\inf_{m\geq n} X_m \ua \sup_n\brb{\inf_{m\geq n} X_m} = \liminf_n X_n \quad\ra \quad \E\brb{\left.\inf_{m\geq n} X_m\right|\sG} \ua \E\brb{\left.\liminf_n X_n\right|\sG}\ \text{ a.s.}
\ee
by conditional monotone convergence theorem (Theorem \ref{thm:monotone_convergence_conditional_expectation}). Thus, $\E\brb{\left.\liminf_n X_n\right|\sG} \leq \liminf \E(X_n|\sG)$.

For the second inequality, clearly we see that $X_n$ and $\limsup X_n$ are integrable and apply Fatou's lemma to the sequence $Y-X_n$,
\beast
\E\brb{\left.\liminf_n (Y-X_n)\right|\sG} & \leq & \liminf_n \E\brb{ Y-X_n|\sG}\quad \text{a.s.}\\
\E\brb{\left.Y - \limsup X_n\right|\sG} & \leq & \liminf \E(Y|\sG)- \E(X_n|\sG)\quad\text{a.s.}\quad (\text{by Proposition \ref{pro:conditional_expectation_basic_property} (iii)})\\
\E\brb{Y|\sG} - \E\brb{\left.\limsup_n X_n\right|\sG} & \leq & \E(Y|\sG) - \limsup \E(X_n|\sG)\quad\text{a.s.}\quad (\text{by Proposition \ref{pro:conditional_expectation_basic_property} (iii)})\\
\limsup_n \E(X_n|\sG) & \leq & \E\brb{\left.\limsup_n X_n\right|\sG} \quad \text{a.s.} \quad (\E\brb{Y|\sG}\text{ is integrable ($\E\brb{Y|\sG} < \infty$ a.s.) by Theorem \ref{thm:conditional_expectation_existence_uniqueness}})
\eeast
\end{proof}

\begin{theorem}[conditional dominated convergence theorem\index{dominated convergence theorem!conditional expectation}]\label{thm:dominated_convergence_conditional_expectation}
Let $X_n \to X$ a.s., and assume that $\sup\limits_{n\geq 0} \abs{X_n} \leq Y$ for some integral random variable $Y$. Then $\E\brb{X |\sG}$ is integrable and
\be
\E\brb{X_n|\sG} \to \E\brb{X | \sG} \text{ a.s.}
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Consider $Y - X_n$, $Y + X_n \geq 0$. By Proposition \ref{pro:conditional_expectation_basic_property} (iii) and conditional Fatou's lemma (Lemma \ref{thm:fatou_conditional_expectation}),
\be
\E\brb{Y - X |\sG} \leq \liminf_{n\to \infty} (\E\brb{Y |\sG}-\E\brb{X_n | \sG}) \ \text{ a.s.},\quad\quad \E\brb{Y + X | \sG} \leq \liminf_{n\to \infty} (\E\brb{Y | \sG}+\E\brb{X_n |\sG}) \ \text{ a.s.}
\ee

As $\E\brb{Y |\sG}$ is integrable, we get
\vspace{2mm}

$\qquad\qquad\qquad\qquad\liminf\limits_{n\to \infty} \E\brb{X_n | \sG} \geq \E\brb{X | \sG} \geq \limsup\limits_{n\to \infty} \E\brb{X_n | \sG} \ \text{ a.s.}\quad \ra\quad \E(X|\sG) = \lim\limits_{n\to \infty} \E\brb{X_n | \sG}\ \text{ a.s.}$
\end{proof}

\begin{theorem}[conditional Jensen inequality\index{Jensen's inequality!conditional expectation}]\label{thm:jensen_inequality_conditional_expectation}
Let $c : I \to \R$ with $I\subseteq \R$ be a convex function, $X \in \sL^1(\Omega,\sF,\pro)$, and assume that $c(X) \in \sL^1(\Omega,\sF,\pro)$ or $c\geq 0$. Then
\be
c(\E\brb{X | \sG}) \leq \E\brb{c(X) | \sG}\ \text{ a.s.}\footnote{need the condition for equality.}
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Since $c(X)\in \sL^1(\Omega,\sF,\pro)$, $\E\brb{c(X) | \sG}$ is well-defined. From Lemma \ref{lem:jensen_affine}, we have that there exist $a,b\in \R$ such that $c(x) \geq ax + b$ for all $x$. Furthermore, there exists a countable sequence $(a_n,b_n)$ of points in $\R^2$ such that
\be
c(x) \geq a_n x + b_n,\quad\quad c(x) = \sup_n (a_n x + b_n),\ x\in \R.
\ee

For $c(X)\in \sL^1$, $c(x) - a_n x - b_n\geq 0$. Thus, by Proposition \ref{pro:conditional_expectation_basic_property} (v), $\E\brb{c(X)-a_nX -b_n|\sG} \geq 0 \ \text{ a.s.}\ \ra\ \E\brb{c(X)|\sG} \geq \E\brb{a_nX+b_n|\sG}$ a.s. since $X\in \sL^1$.

If $c\geq 0$, there exist $a_n x + b_n \geq 0$ and with Proposition \ref{pro:non_negative_conditional_expectation}, we also have $\E\brb{c(X)|\sG} \geq \E\brb{a_nX+b_n|\sG}$ a.s.

Therefore, by Proposition \ref{pro:conditional_expectation_basic_property}, $\E\brb{a_nX+b_n|\sG} = a_n\E\brb{X|\sG}+b_n$ a.s. Thus,
\be
\E\brb{c(X)|\sG} \geq \sup_n\bra{ a_n\E\brb{X|\sG}+b_n} \ \text{a.s.}
\ee

But $\sup\limits_n\bra{ a_n\E\brb{X|\sG}+b_n} = c(\E\brb{X|\sG})$. So we have $\E\brb{c(X)|\sG} \geq c(\E\brb{X|\sG})$ a.s.
\end{proof}

\begin{corollary}\label{cor:conditional_expectation_norm_smaller}
For probability space $(\Omega,\sF,\pro)$, $\sG \subseteq \sF$ and $p\in [1,\infty)$, if $X\in \sL^p(\Omega,\sF,\pro)$, then%previous version is X\in \sL^1(\Omega,\sF,\pro)$
\be
\dabs{\E\brb{X|\sG}}_p \leq \dabs{X}_p.
\ee
\end{corollary}

\begin{proof}[\bf Proof]
Let $c(x) = \abs{x}^p$. It clear that $c(x)$ is convex. Since $X\notin \sL^p(\Omega,\sF,\pro)$, we have $\dabs{X}_p < \infty$. %, the inequality holds.
If $X\in \sL^1(\Omega,\sF,\pro)$, we use conditional Jensen's inequality (Theorem \ref{thm:jensen_inequality_conditional_expectation}) to get
\be
\abs{\E\brb{X|\sG}}^p \leq \E\brb{\abs{X}^p|\sG} \ \text{a.s.}
\ee
We know that $\E\brb{\abs{X}^p|\sG}\geq 0$ a.s., thus construct the non-negative random variable
\be
Y = \left\{\ba{ll}
\E\brb{\abs{X}^p|\sG} \quad \quad & \E\brb{\abs{X}^p|\sG}\geq 0\\
0 & \E\brb{\abs{X}^p|\sG} < 0
\ea\right. \quad \ra \quad \abs{\E\brb{X|\sG}}^p \leq Y\ \text{ a.s.}
\ee

Thus, by Theorem \ref{thm:non_negative_measurable_property}, we have
\be
\E\brb{\abs{\E\brb{X|\sG}}^p} \leq \E Y = \E\brb{\E\brb{\abs{X}^p|\sG}} = \E\brb{\abs{X}^p} \quad\quad \text{(Proposition \ref{pro:conditional_expectation_basic_property} (i))}
\ee
which gives the inequality $\dabs{\E\brb{X|\sG}}_p \leq \dabs{X}_p$.
\end{proof}

\begin{remark}
We can see that $X \mapsto  \E\brb{X |\sG}$ is a continuous linear operator on $\sL^p$ of (operator) norm (by Definition \ref{def:continuous_rc_2}).
\end{remark}

\begin{proposition}\label{pro:conditional_expectation_tower_independence}
Let $\sG \subseteq \sF$, then
\ben
\item [(i)] (tower property\index{tower property}) If $\sH\subseteq \sG$ and either $X\in \sL^1\brb{\Omega,\sF,\pro}$ or $X\geq 0$, then $\E\brb{\E\brb{X|\sG}|\sH} = \E\brb{X|\sH}$ a.s.
\item [(ii)] ('take out what is known') If $Z$ is $\sG$-measurable and bounded, $X\in \sL^1\brb{\Omega,\sF,\pro}$, then $\E\brb{ZX|\sG} = Z\E\brb{X|\sG}$ a.s.
\item [(iii)] If $p>1$, $\frac 1p + \frac 1q =1$, $X\in \sL^p(\Omega,\sF,\pro)$ and $Z\in \sL^q(\Omega,\sG,\pro)$, then $\E\brb{ZX|\sG} = Z\E\brb{X|\sG}$ a.s.
\item [(iv)] If either $X\in \sL^1\brb{\Omega,\sF,\pro}$ $ZX\in \sL^1\brb{\Omega,\sF,\pro}$ or $X,Z\geq 0$, then if $Z$ is $\sG$-measurable, $\E\brb{ZX|\sG} = Z\E\brb{X|\sG}$ a.s.
\item [(v)] (role of independence) If $\sH$ is independent of $\sigma(\sigma(X),\sG)$ and $X\in \sL^1\brb{\Omega,\sF,\pro}$, then
\be
\E\brb{X|\sigma(\sG,\sH)} = \E\brb{X|\sG} \ \text{ a.s.}
\ee

In particular, if $X$ is independent of $\sH$ then $\E\brb{X | \sH} = \E\brb{X}$ a.s.
\item [(vi)] Let $\sG$ be any $\sigma$-algebra and $\sN$ be set of all the $\pro$-null sets. If $X$ is an integrable random variable, then
\be
\E\brb{X|\sigma(\sG,\sN)} = \E\brb{X|\sG}\ \text{ a.s.}
\ee
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] If $X\in \sL^1$, let $A \in \sH\subseteq \sG$, then by Theorem \ref{thm:conditional_expectation_existence_uniqueness},
\be
\E\brb{\E\brb{X |\sH}\ind_A} = \E\brb{X\ind_A} = \E\brb{\E\brb{X |\sG}\ind_A} = \E\brb{\E\brb{\E\brb{X |\sG} |\sH}\ind_A}
\ee
so both $\E\brb{X |\sH}$ and $\E\brb{\E\brb{X |\sG} |\sH}$ are version of $\E\brb{X |\sH}$, thus $\E\brb{\E\brb{X |\sG} |\sH} = \E\brb{X |\sH}$ a.s. by uniqueness (Theorem \ref{thm:conditional_expectation_existence_uniqueness}).

If $X\geq 0$, let $W$ be any non-negative $\sH$-measurable random variable, by Theorem \ref{thm:conditional_expectation_existence_uniqueness_non_negative},
\be
\E\brb{\E\brb{X |\sH}W} = \E\brb{XW} = \E\brb{\E\brb{X |\sG}W} = \E\brb{\E\brb{\E\brb{X |\sG} |\sH}W}
\ee

\item [(ii)] Since $Z$ is bounded, we have $ZX\in \sL^1(\Omega,\sF,\pro)$, so $\E\brb{ZX|\sG}$ is well-defined. Thus, let $Y$ be a version of $\E(X|\sG)$, it is enough to show that for any $A\in \sG$,
\be
\E\brb{\E\brb{ZX|\sG}\ind_A} = \E \brb{ZX\ind_A} \stackrel{\text{need to show}}{=} \E \brb{ZY\ind_A} = \E\brb{Z\E\brb{X|\sG}\ind_A} \quad \quad(*)
\ee

If $Z = \ind_B$ where $B\in \sG$, we have
\be
\E \brb{ZX\ind_A} = \underbrace{\E \brb{X\ind_{A\cap B}}= \E \brb{Y\ind_{A\cap B}}}_{\text{definition of condition expectation}} = \E \brb{ZY\ind_A}
\ee

If $Z$ is a simple function, we have the same result by Proposition \ref{pro:conditional_expectation_basic_property} (iii). For any non-negative $Z$, we have the same result by letting $Z_n = 2^{-n}\floor{2^n Z}$ thus $Z_n \ua Z$ and apply monotone convergence theorem (Theorem \ref{thm:monotone_convergence_probability}). For general bounded $Z$, we can split $Z = Z^+ - Z^-$ then apply Proposition \ref{pro:conditional_expectation_basic_property} (iii) again.

\item [(iii)] With H\"older's inequality (Theorem \ref{thm:holder_inequality_expectation}), we have
\be
\E(\abs{ZX}) \leq \dabs{Z}_q \dabs{X}_p < \infty \quad\ra\quad ZX \text{ is integrable}.
\ee

Let $Y$ be a version of $\E(X|\sG)$, with the fact that $X\in \sL^p(\Omega,\sF,\pro)$ and $Y\in \sL^p(\Omega,\sG,\pro)$ (by Corollary \ref{cor:conditional_expectation_norm_smaller}, so $YZ$ is integrable by H\"older's inequality (Theorem \ref{thm:holder_inequality_expectation}) and $\E(YZ)$ is well-defined) we can apply the same argument (*) in (ii).

\item [(iv)] Let $Y$ be a version of $\E(X|\sG)$.

With the assumption, we know that $XZ$ is integrable, also and $YZ$ is non-negative a.s. as $Y\geq 0$ a.s. (by Proposition \ref{pro:conditional_expectation_basic_property} (v)). Thus, we can choose any non-negative version of $\E(X|\sG)$, $Y'$ such that $\E(Y'Z)$ is well-defined. Again, we can apply the argument (*) in (ii).

Let $W$ be any non-negative $\sG$-measurable random variable, then $ZW$ is non-negative $\sG$-measurable, by Theorem \ref{thm:conditional_expectation_existence_uniqueness_non_negative},
\be
\E\brb{X(ZW)} = \E\brb{\E\brb{X|\sG}(ZW)} \ \ra\  \E\brb{XZ(W)} = \E\brb{YZ(W)} \ \ra\  \E\brb{XZ|\sG} = Z\E\brb{X|\sG} \text{ a.s.}
\ee
by uniqueness of Theorem \ref{thm:conditional_expectation_existence_uniqueness_non_negative}.

\item [(v)] Without loss of generality, we assume that $X\geq 0$. For $G\in \sG$, $H\in \sH$, $X\ind_G$ and $H$ are independent (by Definition \ref{def:random_variable_independent}), then by Proposition \ref{pro:expectation_of_independent_product},
\be
\E\brb{X\ind_{G\cap H}} = \E\brb{\brb{X\ind_G}\ind_H} = \E(X\ind_G) \pro(H).
\ee

Now let $Y$ be a version of $\E(X|\sG)$. Since $Y$ is $\sG$-measurable, $Y\ind_G$ is independent of $\sH$ so that
\be
\E\brb{(Y\ind_G)\ind_H} = \E\brb{Y\ind_G}\pro(H) \ \ra\ \E\brb{X\ind_{G\cap H}} = \E\brb{Y\ind_{G\cap H}}.
\ee

Thus, consider two finite measures on $(\Omega,\sigma(\sG,\sH))$ where $\sigma(\sG,\sH)$ has $\pi$-system with form $G\cap H$, $G\in \sG,H\in \sH$.
\be
\mu_1(A) := \E\brb{X\ind_A},\quad \mu_2(A) := \E\brb{Y\ind_A},\quad \forall A = G\cap H,\ G\in \sG,\ H\in \sH.
\ee

We know that $\mu_1(\Omega) = \mu_2(\Omega) < \infty$ since $X$ is integrable and two measures agree on the $\pi$-system generating $\sigma(\sG,\sH)$. Thus, by uniqueness of extension (Theorem \ref{thm:uniqueness_of_extension_measure}), we have $\mu_1 = \mu_2$ on $\sigma(\sG,\sH)$. Hence, $\forall A\in \sigma(\sG,\sH)$
\be
\E\brb{\E(X|\sigma(\sG,\sH))\ind_A} = \E\brb{X\ind_A} = \mu_1(A) = \mu_2(A) = \E\brb{Y\ind_A}.
\ee

Then uniqueness of conditional expectation (Theorem \ref{thm:conditional_expectation_existence_uniqueness}) gives that
\be
\E(X|\sigma(\sG,\sH)) = Y \ \text{ a.s.} \ \ra\ \E(X|\sigma(\sG,\sH)) = \E(X|\sG) \ \text{ a.s.}
\ee
Now let $\sG = \bra{\emptyset,\Omega}$, we have $\E(X|\sH) = X$ a.s.

\item [(vi)] It suffices to show that $\forall G\in \sG,N\in \sN$, for $Z= \E(X|\sG)$, $\E(X\ind_{G\cap N}) = \E(Z\ind_{G\cap N})$ (since $G\cap N$ is $\pi$-system of $\sigma(\sG,\sN)$). Since $X$ is integrable and $X\ind_G\ind_N = 0 = Z\ind_G\ind_N$ a.s., we can apply Theorem \ref{thm:lebesgue_integrable_function_property}, % as any set in $\sigma(\sG,\sN)$ can be expressed in the form of union of $G$ and $N$).
\be
\E(X\ind_{G\cap N}) = 0 = \E(Z\ind_{G\cap N}). %\E(X\ind_{G}) - \E(X\ind_{G\cap N^c}) = \E(Z\ind_G) - \E\brb{X\ind_{N^c}\ind_{G}}
\ee
\een
\end{proof}

\begin{remark}
(v) does not hold if we assume that $\sigma(X)$ is independent of $\sH$ and $\sG$ is independent of $\sH$\footnote{see example sheet}.
\end{remark}

\begin{proposition}
If $\sH\subseteq \sG \subseteq \sF$ and $X\in \sL^1\brb{\Omega,\sF,\pro}$, then
\be
\E\brb{\E\brb{X|\sG}|\sH} = \E\brb{\E\brb{X|\sH}|\sG} \text{ a.s.}
\ee
\end{proposition}

\begin{proof}[\bf Proof]
We can get the required result by combining Proposition \ref{pro:conditional_expectation_tower_independence}.(i) and Proposition \ref{pro:conditional_expectation_basic_property}.(ii).
\end{proof}




\subsection{Uniformly integrable property and multivariable conditional expectation}

\begin{theorem}\label{thm:ui_conditional_expectation_implies_ui}
Let $\sX$ be a UI family of random variables on probability space $(\Omega,\sF,\pro)$. Then, the set
\be
\bra{\E\brb{X|\sG}, X\in \sX, \sG\subseteq \sF} \quad \text{is also UI.}
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Recalling Theorem \ref{thm:ui_equivalent_probability}, $\sX$ is UI iff
\be
\sup\limits_{X\in \sX} \E\brb{\abs{X}\ind_{\abs{X}>K}} \to 0 \text{ as }K\to \infty.
\ee

By Definition \ref{def:uniformly_integrable_probability}, we have that if $\sX$ is UI then $X\in \sX$ is bounded in $\sL^1$, so for all $X\in \sX$, we have
$\E\abs{X} < M$ where $M$ is a constant. By Definition \ref{def:uniformly_integrable_probability} again, $\sX$ is UI then for any $\ve>0$, there exists $\delta >0$ such that for any $A\in \sF$ with $\pro(A) <\delta$,
\be
\sup_{X\in \sX}\bra{\E(\abs{X}\ind_A)} < \ve.
\ee

Let $Y= \E\brb{X|\sG}$. If set $c(x) = \abs{x}$, with Jensen's inequality (Theorem \ref{thm:jensen_inequality_conditional_expectation}, as $\abs{X}$ is integrable),
\be
\abs{Y} = \abs{\E\brb{X | \sG}} \leq \E\brb{\abs{X} | \sG}\ \text{ a.s.} \quad (*)\quad\ra\quad  \abs{Y}\ind_{\bra{\abs{Y}>K}} \leq \E\brb{\abs{X} | \sG}\ind_{\bra{\abs{Y}>K}}\ \text{ a.s.} \quad (\dag)
\ee

Now take $K = M/\delta$, let $A = \{\abs{Y} > K\}$, we have (by Chebyshev inequality (Theorem \ref{thm:chebyshev_inequality})),
\be
\pro\brb{A} = \pro\brb{\bra{\abs{Y} > K}} \leq \frac 1K \E\abs{Y} \stackrel{(*)}{\leq} \frac 1K\E\brb{ \E\brb{\abs{X} | \sG}} = \frac 1K\E\abs{X} \leq \frac MK = \delta \quad\ra\quad \sup_{X\in \sX}\bra{\E(\abs{X}\ind_A)}< \ve.
\ee

Also, with $(\dag)$, we have (by Theorem \ref{thm:lebesgue_integrable_function_property}),
\be
\E\brb{\abs{Y}\ind_{\bra{\abs{Y}>K}}} \leq \E\brb{\E\brb{\abs{X} | \sG}\ind_{\bra{\abs{Y}>K}}} = \E\brb{\E\brb{\abs{X}\ind_{\bra{\abs{Y}>K}} | \sG}}
\ee
by Proposition \ref{pro:conditional_expectation_tower_independence} (ii) since $\ind_{\bra{\abs{Y}>K}}$ is $\sG$-measurable and bounded (as $Y = \E\brb{X|\sG}$ is $\sG$-measurable). Then,
\be
\E\brb{\abs{Y}\ind_{\bra{\abs{Y}>K}}} \leq \E\brb{\E\brb{\abs{X}\ind_{\bra{\abs{Y}>K}} | \sG}} = \underbrace{\E\brb{\abs{X}\ind_{\bra{\abs{Y}>K}}}}_{\text{Proposition \ref{pro:conditional_expectation_basic_property} (i)}} < \ve.
\ee
Thus, $Y = \E\brb{X|\sG}$ is UI.
\end{proof}

\begin{corollary}\label{cor:single_integrable_conditional_expectation_implies_ui}
If $X$ is $\sL^1(\Omega,\sF,\pro)$ random variable on a probability space $\Omega$, then the set of all of its condition expectations,
\be
\bra{\E\brb{X|\sG}: \sG \subseteq \sF}\quad \text{is UI.}
\ee
\end{corollary}

\begin{proof}[\bf Proof]
It is the direct result from Proposition \ref{pro:finite_integrable_implies_ui} and Theorem \ref{thm:ui_conditional_expectation_implies_ui}. %Alternative, we give the following proof.
\end{proof}

%\begin{proof}[\bf Proof]
%By Definition \ref{def:uniformly_integrable_probability}, given $\ve > 0$, we can find $\delta > 0$ so that $\E(\abs{X}\ind_A) \leq \ve$ whenever $\pro(A) \leq \delta$. Then choose $K < \infty$ so that $\E(\abs{X}) \leq K\delta$.

%Suppose $Y = \E(X|\sG)$, then $\abs{Y} \leq \E\brb{\abs{X}|\sG}$ (by Proposition \ref{pro:conditional_expectation_basic_property} (iv)). Then
%\be
%\pro(\abs{Y}| \geq K) \leq \frac 1K \E(\abs{Y}) \leq \delta.
%\ee

%Then,
%\be
%\E\brb{\abs{Y}\ind_{\abs{Y} \geq K}} \leq \E\brb{\abs{X}\ind_{\abs{Y}\geq K}} \leq \ve.
%\ee
%Since $K$ was chosen independently of $\sG$, we are done.
%\end{proof}

\begin{proposition}
Let $X$ and $Y$ be random variables and $g$ be a non-negative measurable function with $g(X,Y)\in \sL^1(\Omega,\sF,\pro)$. Let $\sG \subseteq \sF$ be a sub-$\sigma$-algebra, and assume that $Y$ is $\sG$-measurable and $\sigma(X)$ is independent of $\sG$. Then
\be
\E[g(X, Y)|\sG] = \int g(x, Y)\pro_X (d x).
\ee

Here $\pro_X$ is the law of $X$, i.e. $\pro_X (A) := \pro(X \in A)$.
\end{proposition}

\begin{proof}[\bf Proof]
Let $Z$ be $\ind_A$ where $A\in \sG$ thus $Z$ is $\sG$-measurable. Then (by Theorem \ref{thm:image_measure_probability})
\be
\E\brb{Z g(X, Y )} = \int z g(x, y) \pro_{X,Y,Z}(d x, d y, dz)
\ee

But $X$ is independent of $(Z, Y)$ (Proposition \ref{pro:independent_and_measurable}), so (by Proposition \ref{pro:random_variable_independent})
\be
\pro_{X,Y,Z}(d x, d y, dz) = \pro_X(d x) \pro_{Y,Z}(d y, dz).
\ee

By Fubini's Theorem (Theorem \ref{thm:fubini}),
\beast
\int z g(x, y)\pro_{X,Y,Z}(d x, d y, dz) = \int \brb{\int z g(x, y) \pro_{Y,Z}(d y, dz)} \pro_X (d x) = \int \E\brb{Zg(x, Y)}\pro_X (d x) = \E \brb{Z\int g(x, Y )\pro_X (d x)}.
\eeast

Thus,
\be
\E\brb{g(X, Y )\ind_A} = \E \brb{\int g(x, Y )\pro_X (d x)\ind_A}.
\ee

Since $\int g(x, Y )\pro_X (d x)$ is $\sG$-measurable (Lemma \ref{lem:e_1_measurable}) and integrable (Theorem \ref{thm:fubini}), it is $\E\brb{g(X, Y ) |\sG}$ by uniqueness (Theorem \ref{thm:conditional_expectation_existence_uniqueness}).
\end{proof}




\subsection{Conditional probability density function}

\begin{definition}[Conditional density function\index{density function!conditional}]\label{def:density_function_conditional}
Let $f_{X,Y}(x,y)$ be the joint distribution function of $X \in \R^m$ and $Y\in \R^n$ and
\be
f_Y (y) = \int_{\R^m} f_{X,Y}(x, y)d x
\ee
be the marginal density function of $Y$ (see Definition \ref{def:joint_distribution_joint_density_marginal_density}). Note that $f_Y$ is Lebesgue-$dy$-integrable by Fubini theorem (Theorem \ref{thm:fubini}.(ii).(a)).

Define the conditional probability density function\index{density function!conditional probability} of $X$ given $Y$, $X|Y$,\footnote{$X|Y$ is a random variable}
\be
f_{X|Y}(x|y) := \begin{cases}
f_{X,Y}(x,y)/f_Y(y) & f_Y(y) \neq 0\\
0 & \text{otherwise}
\end{cases}.
\ee
\end{definition}

\begin{proposition}\label{pro:conditional_probability_density_function}
Let $X\in \R^m$ and $Y\in \R^n$ be two random variables. Let $f_{X|Y}(x,y)$ be the conditional probability density function of $X$ given $Y$ and $h$ be Borel function on $\R^m$ such that $h(X)$ is integrable, i.e., \be
\E\abs{h(X)} = \int_{\R^m} \abs{h(x)}f_X(x) dx < \infty, \ee where $f_X(x) = \int_{\R} f_{X,Y}(x,y)dy$. Set function $g:\R^n \to \R$, \be g(y) = \int_{\R^m} h(x)f_{X|Y}(x|y) dx. \ee

Then $Z := g(Y)$ is a version of the conditional expectation of $h(X)$ given $\sG:=\sigma(Y)$. That is, \be \E\brb{\left.h(X)\right|\sigma(Y)} = \E\brb{\left.h(X)\right|\sG} \stackrel{\text{a.s.}}{=} Z = \int_\R
h(x)f_{X|Y}(x|Y) dx . \ee
\end{proposition}

\begin{remark}
When $Y = y$, we have \be \E\brb{\left.h(X)\right|Y= y} = \int_{\R^m} h(x)f_{X|Y}(x|y) dx \qquad \text{a.s.}. \ee

Let $\nu(Y, d x)$ be the measure with density $f_{X|Y} (x | Y )$ with respect to Lebesgue measure $dx$. Then $\E\brb{h(X) | Y } = \int_{x\in \R} h(x)\nu(Y, d x)$. When we have such a formula, we say that $\nu(Y, d x)$ is the conditional distribution of $X$ given $Y$. Sometimes $\nu( y, d x)$ is called the conditional distribution of $X$ given $Y = y$. These are defined only up a set of zero measure for $y$. We also say that $f_{X|Y} (x|y)$ is the conditional density function of $X$ given $Y = y$.
\end{remark}

\begin{proof}[\bf Proof]
For any $A\in \sigma(Y)$, for $x\in \R^m$ and $y\in \R^n$,
\be
\E\brb{h(X)\ind_A} = \int h(x) \ind_A(y) f_{X,Y}(x, y)d xd y = \underbrace{\int \brb{ \int h(x) \frac{f_{X,Y}(x, y)}{f_Y ( y)} d x}\ind_A(y) f_Y ( y)d y}_{\text{Fubini theorem (Theorem \ref{thm:fubini}}} = \E\brb{g(Y)\ind_A}.
\ee

Since $\E(h(X)) = \int\int h(X) f_{X,Y}(x,y)dxdy <\infty$, $h(X) f_{X,Y}(x,y)$ is $dx\otimes dy$-integrable, and $f_{X,Y}(x,y)$ is also $dx\otimes dy$-integrable. Thus, by Fubini theorem (Theorem \ref{thm:fubini}) (ii).(a), the following two integrals
\be
\int_{\R^m} h(X) f_{X,Y}(x,Y)dx,\quad\quad \int_{\R^m} f_{X,Y}(x,Y)dx,
\ee
are $\sG$-integrable. Thus, if $f_{X|Y}(x|Y) =0$, then $g(Y)=0$, is $\sG$-measurable. Otherwise, by Theorem \ref{thm:non_negative_measurable_property},
\be
g(Y) = \int_{\R^m} h(x)f_{X|Y}(x|Y) dx = \int_{\R^m} \frac{h(X) f_{X,Y}(x,Y)}{f_Y(Y)} dx = \frac{\int_{\R^m} h(X) f_{X,Y}(x,Y) dx}{f_Y(Y)} = \frac{\int_{\R^m} h(X) f_{X,Y}(x,Y) dx}{\int_{\R^m} f_{X,Y}(x,y)dx}.
\ee

So both of denominator and numerator are $\sG$-measurable, so $g(Y)$ is $\sG$-measurable. Hence $g(Y)$ is a version of $\E\brb{h(X)|\sG}$.
\end{proof}

\begin{example}
Recalling Example \ref{exa:joint_density_8xy}, the conditional densities are \be f_{X|Y} (x | y) = \frac{8xy}{4y^3} = \frac{2x}{y^2},\quad\quad f_{Y|X}(y | x) = \frac{8xy}{4x(1 - x^2)} = \frac{2y}{1 - x^2}, \ee for $0 \leq
x \leq y \leq 1$. We then have \be \E (X | Y = y) = \int^y_0 x\frac{2x}{y^2} dx = \frac {2y}3\text{ a.s.},\quad\quad \E (Y | X = x) = \int^1_x y\frac{2y}{1 - x^2} dy = \frac{2(1 - x^3)}{3(1 - x^2)}\text{ a.s.}. \ee We see
that for $U := X|Y$ and $V:= Y|X$, $\E U  = 2Y/3$ a.s. and $\E V = 2\brb{1- X^3}/\brb{3\brb{1- X^2}}$ a.s.. Then by tower property (Proposition \ref{pro:conditional_expectation_tower_independence}.(i)) and Proposition \ref{pro:conditional_probability_density_function}, we have \beast
& & \E X = \E\brb{\E(X|Y)} = \E\brb{\E U} = \int^1_0 \frac {2y}3f_Y(y) dy = \int^1_0 \frac{8y^4}{3}dy = \frac 8{15},\\
& & \E Y = \E\brb{\E(Y|X)} = \E\brb{\E V} = \int^1_0 \frac{2(1 - x^3)}{3(1 - x^2)} f_X(x) dx = \int^1_0 \frac{8x(1 - x^3)}{3} dx = \frac 4{5} , \eeast which are consistent with results in Example \ref{exa:joint_density_8xy}.
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Characteristic Function and Moment Generating Function}

\subsection{Characteristic function}%Definition}

\begin{definition}[Characteristic Function]\label{def:characteristic_function_n}
For a random variable $X$ in $\R^n$, we define the characteristic function\index{characteristic function}
\be
\phi_X(u) = \E\brb{e^{i\inner{u}{X}}} = \int_\Omega e^{i\inner{u}{X(\omega)}}\pro(d\omega) = \int_{\R^n} e^{i\inner{u}{x}}d\mu_X(x) ,\quad u \in \R^n.
\ee

Thus $\phi_X = \wh{\mu}_X$ (Fourier transform (Definition \ref{def:fourier_transform_borel}) of $\mu_X$ and Theorem \ref{thm:image_measure_probability}), where $\mu_X$ is the law of $X$. Also, $\ol{\phi_X(u)} = \phi_X(-u)$.
\end{definition}

\begin{remark}
\ben
\item [(i)] Actually, $u$ could be in a subset of $\R^n$ rather than $\R^n$.

\item [(ii)] The characteristic function of a real-valued random variable always exists, since it is an integral of a bounded continuous function over a space whose measure is finite.%\item [(iii)] A characteristic function is uniformly continuous on the entire space\footnote{proof needed.}.
\een
\end{remark}


\begin{proposition}
The characteristic function of any random variable is continuous at all points.

Furthermore, the characteristic function of any random variable is uniformly continuous on the entire space.
\end{proposition}

\begin{proof}[\bf Proof]
Let $X$ be a $n$-dimensional random variable and $t_n \to t \in \R^n$. Thus, $e^{i\inner{t_n}{X}} \to e^{i\inner{t}{X}}$ and by dominated convergence theorem,
\be
\phi_X(t_n) = \E\brb{e^{i\inner{t_n}{X}}} \to \E\brb{e^{i\inner{t}{X}}} = \phi_X(t).
\ee

For any $t,h\in \R^n$, we have
\beast
\abs{\phi_X(t+h) - \phi(t)} & = & \abs{\int_{\R^n} e^{i\inner{t+h}{x}}dF_X(x) - \int_{\R^n} e^{i\inner{t}{x}}dF_X(x)} = \abs{\int_{\R^n} \brb{e^{i\inner{h}{x}}-1} e^{i\inner{t}{x}}dF_X(x)} \\
& \leq &  \int_{\R^n} \abs{e^{i\inner{h}{x}}-1} \abs{e^{i\inner{t}{x}}}dF_X(x) = \int_{\R^n} \abs{e^{i\inner{h}{x}}-1} dF_X(x) = \int_{\Omega} \abs{e^{i\inner{h}{X(\omega)}}-1} \pro(d\omega)
\eeast
which is independent of $t$. Let $h_n\to 0$ and then $e^{i\inner{h_n}{X}} \to 1$. Thus, by bounded convergence theorem, we have
\be
\lim_{n\to \infty}\int_{\Omega} \abs{e^{i\inner{h_n}{X(\omega)}}-1} \pro(d\omega) = 0
\ee
which implies that given any $\ve$, we can find large enough $n$ such that
\be
\abs{\phi_X(t+h_n) - \phi(t)} \leq \int_{\Omega} \abs{e^{i\inner{h_n}{X(\omega)}}-1} \pro(d\omega) < \ve.
\ee

Thus, we can pick this $h_n$ as the interval $\delta$ of $x$ in the definition of uniform continuity.
\end{proof}

\begin{definition}\label{def:standard_gaussian_density}
A random variable $X$ in $\R^n$ is standard Gaussian\index{Gaussian random variable!standard} if
\be
\pro(X \in A) = \int_A \frac 1{(2\pi)^{n/2}} e^{-|x|^2/2}dx,\quad A \in\sB(\R^n).
\ee
\end{definition}

\begin{proposition}[The characteristic function of a standard Gaussian random variable $X$ in $\R$]
We have
\be
\phi_X(u) = \int_{\R} e^{iux}\frac 1{\sqrt{2\pi}} e^{-x^2/2}dx = e^{-u^2/2}I\quad\quad \text{where}\quad I = \int_\R \frac 1{\sqrt{2\pi}} e^{-(x-iu)^2/2}dx.
\ee

The integral $I$ can be evaluated by considering the integral of the analytic function $e^{-z^2/2}$ around the rectangular contour with corners $R$, $R - iu$, $-R - iu$, $-R$: by Cauchy's residue theorem (Theorem \ref{thm:cauchy_residue_complex}), the integral round the contour vanishes, as do, in the limit $R \to \infty$, the contributions from the vertical sides of the rectangle. We deduce that
\be
I = \int_\R \frac 1{\sqrt{2\pi}}e^{-x^2/2}dx = 1 \quad\ra\quad \phi_X(u) = e^{-u^2/2}.
\ee
\end{proposition}

%\subsection{Uniqueness and inversion}

We now show that a finite Borel measure is determined uniquely by its Fourier transform and obtain, where possible, an inversion formula by which to compute the measure from its transform.

\begin{definition}[heat kernel\index{heat kernal}]\label{def:heat_kernel}
Define, for $t > 0$ and $x, y \in \R^n$, the heat kernel (or transition density of $n$-dimensional Brownian motion (starts from $x$)),
\be
p_t(x, y) = \frac 1{(2\pi t)^{n/2}} e^{-|y-x|^2/2t} = \prod^n_{k=1} \frac 1{\sqrt{2\pi t}} e^{-|y_k-x_k|^2/2t}.
\ee
\end{definition}

\begin{remark}
This is the fundamental solution for the heat equation $\brb{\partial/\partial t - \frac 12\Delta}p = 0$ in $\R^n$, where $\Delta = \sum^n_{i=1}\frac{\partial^2}{\partial x_i^2}$. But we shall not pursue this property here.\footnote{in sfm}
\end{remark}

\begin{lemma}\label{lem:heat_kernal_density}
Let $Z$ be a standard Gaussian random variable in $\R^n$. Let $x \in \R^n$ and $t \in (0,\infty)$. Then
\ben
\item [(i)] the random variable $Y = x +\sqrt{t}Z$ has density function $p_t(x, \cdot)$ on $\R^n$,
\item [(ii)] for all $y \in \R^n$, we have
\be
p_t(x, y) = \frac 1{(2\pi)^n} \int_{\R^n} e^{i\inner{u}{x}} e^{-|u|^2t/2} e^{-i\inner{u}{y}}du.
\ee
\een
\end{lemma}

\begin{proof}[\bf Proof]
The component random variables $Y_k = x_k + \sqrt{t}Z_k$ are independent Gaussians with mean $x_k$ and variance $t$. So $Y_k$ has density
\be
\frac 1{\sqrt{2\pi t}} e^{-|y_k-x_k|^2/2t}
\ee
and we obtain the claimed density function for $Y$ as the product of the marginal densities.

For $u \in \R$ and $t \in (0,\infty)$, we know that
\be
\int_\R e^{iux}\frac 1{\sqrt{2\pi t}} e^{-|x|^2/2t}dx = \E\brb{e^{iu \sqrt{t}Z_1}} = e^{-u^2t/2}.
\ee

let $u_k = tx$, $x_k - y_k = u/t$, thus $x_k, y_k, u_k \in \R$ and $t \in (0,\infty)$,
\be
\int_\R e^{iu_k(x_k-y_k)} \frac{\sqrt{t}}{\sqrt{2\pi}} e^{-t|u_k|^2/2} du_k = e^{-(x_k-y_k)^2/2t}\quad\ra\quad \frac 1{\sqrt{2\pi t}} e^{-|y_k-x_k|^2/2t} = \frac 1{2\pi} \int_\R e^{iu_k x_k}e^{-u^2_kt/2} e^{-i u_k y_k} du_k.
\ee

On taking the product over $k \in \{1, \dots, n\}$, we obtain the claimed formula for $p_t(x, y)$.
\end{proof}


\begin{theorem}[The inversion formula for densities]\label{thm:inversion_density}
Let $X$ be a random variable in $\R^n$. The law $\mu_X$ of $X$ is uniquely determined by its characteristic function $\phi_X$. Moreover, if $\phi_X$ is integrable, and we define
\be
f_X(x) = \frac 1{(2\pi)^n} \int_{\R^n} \phi_X(u)e^{-i\inner{u}{x}}du,
\ee
then $f_X$ is a bounded, continuous and real, non-negative function, which is a density function for $X$, i.e., if $n=1$
\be
\pro(a<X\leq b) = \int^b_a f_X(x) dx,\quad\quad \text{for all }-\infty<a<b<\infty.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Let $m\in \N$ and let $g_m:\R\to \R$ be the function graphed below:

\begin{center}
\psset{yunit=3cm,xunit=3cm}
\begin{pspicture}(-2,-0.1)(2,0.6)
\psline[linestyle=dashed](0.5,0)(0.5,0.5)
\psline[linestyle=dashed](-0.5,0)(-0.5,0.5)
\psline[linestyle=dashed](-1,0.5)(1,0.5)

\psline(-1.5,0)(1.5,0)
\psline[linewidth=1.5pt](-1.5,0)(-1,0)(-0.5,0.5)(0.5,0.5)(1,0)(1.5,0)

\rput[lb](-1,-0.16){$a$}
\rput[lb](-0.6,-0.2){$a+\tfrac 1m$}
\rput[lb](0.45,-0.16){$b$}
\rput[lb](0.8,-0.2){$b+\tfrac 1m$}
\rput[lb](-1.1,0.4){1}
\rput[lb](1,0.2){$g_m$}
\rput[lb](1.6,-0.1){$\R$}

\end{pspicture}
\end{center}

Let $Z$ be a standard Gaussian random variable in $\R^n$, independent of $X$, and for any $t \in (0,\infty)$. Let $X_t = X + \sqrt{t}Z$, from Lemma \ref{lem:heat_kernal_density}, random variable $x +\sqrt{t}Z$ has density function
\be
p_t(x, y) = \frac 1{(2\pi)^n} \int_{\R^n} e^{i\inner{u}{x}} e^{-\abs{u}^2t/2} e^{-i\inner{u}{y}}du.
\ee

by Fubini's theorem (Theorem \ref{thm:fubini} since $g$ is bounded and thus integrable),
\be
\E(g_m(X + \sqrt{t}Z)) = \frac 1{(2\pi)^n} \int_{\R^n} \int_{\R^n} g_m(x + \sqrt{t}z)e^{-|z|^2/2} dz\mu_X(dx).
\ee

Then we have (by Theorem \ref{thm:density_function_probability})
\beast
\int_{\R^n} g_m(x + \sqrt{t}z)\frac 1{(2\pi)^n} e^{-|z|^2/2} dz & = & \E(g_m(x + \sqrt{t}Z)) = \int_{\R^n} g_m(y)p_t(x, y)dy \\
& = & \frac 1{(2\pi)^n}\int_{\R^n}  \int_{\R^n} e^{i\inner{u}{x}} e^{-\abs{u}^2t/2} e^{-i\inner{u}{y}} du g_m(y)dy,
\eeast
so, by Fubini theorem again,
\beast
\E\brb{g_m(X_t)} & = & \int_{\R^n}\brb{\frac 1{(2\pi)^n} \int_{\R^n} \brb{\int_{\R^n} e^{i\inner{u}{x}} \mu_X(dx)} e^{-\abs{u}^2t/2}e^{-i\inner{u}{y}}du }g_m(y)dy \\
& = & \int_{\R^n}\brb{\frac 1{(2\pi)^n} \int_{\R^n} \phi_X(u)e^{-\abs{u}^2t/2}e^{-i\inner{u}{y}}du }g_m(y)dy.\quad (*)
\eeast

Let $Y$ be another random variable with characteristic function $\phi_Y$. If $\phi_X = \phi_Y$, let $Y_t = Y + \sqrt{t}Z$, thus, with the same argument for ($*$), we have
\beast
\int_{\R^n}\brb{\frac 1{(2\pi)^n} \int_{\R^n} \phi_X(u)e^{-\abs{u}^2t/2}e^{-i\inner{u}{y}}du }g_m(y)dy = \int_{\R^n}\brb{\frac 1{(2\pi)^n} \int_{\R^n} \phi_Y(u)e^{-\abs{u}^2t/2}e^{-i\inner{u}{y}}du }g_m(y)dy = \E\brb{g_m(Y_t)}.
\eeast
%\be
%\E\brb{g_m(X_t) - g_m(Y_t)} = \int_{\R^n}\brb{\frac 1{(2\pi)^n} \int_{\R^n} (\phi_X(u)-\phi_Y(u))e^{-\abs{u}^2t/2}e^{-i\inner{u}{y}}du }g_m(y)dy = 0.
%\ee

Note that the integral (expectation) of left hand side is well defined since $g_m$ is bounded. So by Theorem \ref{thm:image_measure_probability} we have
\be
\mu_{X_t}(g_m) = \E\brb{g_m(X_t)} = \E\brb{g_m(Y_t)} = \mu_{Y_t}(g_m).
\ee

Then by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability}),
\be
\mu_{X_t}(g_m) = \E \brb{g_m(X_t)} \to \E\brb{g(X_t)} \to \E\brb{g(X)} = \mu_X((a,b]). \quad\quad \text{(similar for $Y_t$)}
\ee

Let $a=-\infty$, we have $\mu_{X_t}(g_m) \to \mu_X((-\infty,b])$. Thus $\mu_X((-\infty,b]) = \mu_Y((-\infty,b])$. Since $(-\infty,b]$ forms a $\pi$-system and thus
\be
\mu_X(A) = \mu_Y(A) \quad \forall A\in \sB(\R^n)\quad\ra\quad\text{Thus, $\phi_X$ determines $\mu_X$ uniquely.}
\ee

Now if $\phi_X$ is integrable, $f_X(x)$ exists since
\be
\int_{\R^n} \abs{e^{-i\inner{u}{x}}\phi_X(u)} du = \int_{\R^n} \abs{\phi_X(u)}du < \infty.
\ee

$f_X$ is bounded since
\be
(2\pi)^n \abs{f_X(x)} = \abs{\int_{\R^n} e^{-i\inner{u}{x}}\phi_X(u) du} \leq \int_{\R^n} \abs{\phi_X(u)}du < \infty.
\ee

To show that $f_X$ is continuous, i.e. $f(x_n)\to f(x)$, as $x_n\to x$. That is
\be
\frac 1{(2\pi)^n} \int_{\R^n} \phi_X(u)e^{-i\inner{u}{x_n}}du \to \frac 1{(2\pi)^n} \int_{\R^n} \phi_X(u)e^{-i\inner{u}{x}}du.
\ee

This follows from the dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}), since
\be
\phi_X(u)e^{-i\inner{u}{x_n}} \to \phi_X(u)e^{-i\inner{u}{x}} \quad \quad \text{for all }u\in \R,\quad\quad\text{and }\quad \int_{\R^n} \abs{\phi_X(u)e^{-i\inner{u}{x_n}}} \leq \int_{\R^n} \abs{\phi_X(u)} < \infty.
\ee

Since $\ol{\phi_X(u)} = \phi_X(-u)$, we have $\ol{f}_X = f_X$, so $f_X$ is real-valued. Since $f_X$ is continuous, if it took a negative value anywhere, it would do so on an open interval of positive length, $I$ say. There would exist a continuous function $g$, positive on $I$ and vanishing outside $I$. Then we would have $\E(g(X)) \geq 0$ and $\int_{\R^n} g(x)f_X(x)dx < 0$, a contradiction. Hence, $f_X$ is non-negative.

Now consider the case $n=1$, from ($*$) and Propositions \ref{thm:image_measure_probability},
\be
\mu_{X_t}(g_m) = \E \brb{g_m(X_t)} = \int_\Omega g_m \brb{X_t(\omega)} \pro(d\omega) = \int_\R g_m(x)f_t(x)dx.
\ee
where
\be
f_t(x) := \frac 1{2\pi} \int_\R \phi_X(u)e^{-u^2t/2}e^{-i\inner{u}{x}}du.\quad\quad (\dag)
\ee

We have as $t \to 0$, by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability})
\be
\int_\R g_m \brb{X_t(\omega)} \pro(d\omega) \to \int_\R g_m \brb{X(\omega)} \pro(d\omega) \quad \text{since }\quad g_m \brb{X_t(\omega)}  \to  g_m \brb{X(\omega)} \ \text{and }\  \abs{g_m \brb{X_t(\omega)}} \leq 1.
\ee

Also, by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability})
\be
\int_\R g_m(x)f_t(x)dx  \to \int_\R g_m(x)f(x)dx
\ee
since by ($\dag$), $f_t(x)\to f(x)$,
\beast
g_m(x)f_t(x) \to g_m(x)f(x) \ \text{ and } \int \abs{g_m(x)f_t(x)} dx \leq \abs{\int g_m(x)dx }\sup\abs{f_t(x)} \leq (b+1-a)\int_\R\abs{\phi(u)}du  < \infty.
\eeast

Thus, we get
\be
\E \brb{g_m(X)} = \int_\Omega g_m \brb{X(\omega)} \pro(d\omega) = \int_\R g_m(x)f(x)dx.
\ee

Since $g_m \to g = \ind_{(a,b]}$ pointwise and boundedly, by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability}), we have
\be
\int_\Omega g_m \brb{X(\omega)} \pro(d\omega) \to \int_\Omega g \brb{X(\omega)} \pro(d\omega) \quad  \text{ since }\quad g_m \brb{X(\omega)}  \to g \brb{X(\omega)}  \ \text{ and } \abs{g_m \brb{X(\omega)}} \leq 1.
\ee

Also, by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability})
\be
\int_\R g_m(x)f(x)dx \to \int_\R g(x)f(x)dx \quad\quad\text{since}
\ee
\beast
g_m(x)f(x) \to g(x)f(x) \ \text{ and } \int \abs{g_m(x)f(x)} dx \leq \abs{\int g_m dx} \sup \abs{f(x)} \leq (b+1-a) \int_\R\abs{\phi(u)}du  < \infty.
\eeast

Then, we have
\be
\E \brb{g(X)} = \int_\Omega g \brb{X(\omega)} \pro(d\omega) = \int_\R g(x)f(x)dx = \int_\R \ind_{(a,b]}f(x)dx = \int^b_a f(x)dx.
\ee

The proof of higher dimensional case is similar.
\end{proof}

\begin{remark}[The relation of law, distribution function, characteristic function and density function]
From the previous results, we have the law of random variable (distribution function) determines the characteristic function (Definition \ref{def:characteristic_function_n}). Then characteristic function determines the law (no matter integrable or not) and density function if it is integrable (Theorem \ref{thm:inversion_density}). Since the law is unique, the characteristic function and density function are unique as well.
\end{remark}

\begin{remark}
Binomial, Poisson, $U[0,1]$ distributions do not have continuous density function so cannot have integrable characteristic functions.
\end{remark}


\subsection{Characteristic functions and independence}

\begin{theorem}\label{thm:characteristic_function_independence}% original {thm:characteristic_function_independence}
Let $X = (X_1, \dots,X_n)$ be a random variable in $\R^n$. Then the following are equivalent:
\ben
\item [(i)] $X_1, \dots,X_n$ are independent,
\item [(ii)] $\mu_X = \mu_{X_1} \otimes\dots \otimes \mu_{X_n}$,
\item [(iii)] $\E\brb{\prod_k f_k(X_k)} = \prod_k \E(f_k(X_k))$, for all bounded Borel functions $f_1,\dots, f_n$,
\item [(iv)] $\phi_X(u) = \prod_k \phi_{X_k}(u_k)$, for all $u = (u_1, \dots, u_n) \in \R^n$,
\item [(v)] $f_X(x) = \prod_k f_{X_k}(x_k)$, $\text{Leb}^n$-a.e. for the density functions $f_X,f_{X_1},\dots,f_{X_n}$.
\een
\end{theorem}
\begin{proof}[\bf Proof]
If (i) holds, then by Proposition \ref{pro:random_variable_independent}, for all $x_i\in \R$,
\be
\pro(X_1\leq x_1,\dots, X_n \leq x_n) = \pro(X_1\leq x_1)\dots\pro(X_n \leq x_n).
\ee

Then by Definitions \ref{def:r_random_variable_law}, \ref{def:rn_random_variable_law},
\be
\mu_X(A_1 \times \dots \times A_n) = \prod_k \mu_{X_k}(A_k)
\ee
for all Borel sets $A_1,\dots,A_n$, so (ii) holds, since this formula characterizes the product measure (Theorem \ref{thm:product_measure}).

If (ii) holds, then, for $f_1,\dots, f_n$ bounded Borel functions (thus $\mu_{X_i}$-integrable),
\be
\E\brb{\prod_k f_k(X_k)} \underbrace{= \int_{\R^n} \prod_k f_k(x_k)\mu_X(dx)}_{\text{Theorem \ref{thm:density_function_probability}}} \underbrace{= \prod_k \int_\R f_k(x_k)\mu_{X_k}(dx_k)}_{\text{Fubini's theorem (Theorem \ref{thm:fubini})}} \underbrace{= \prod_k \E(f_k(X_k))}_{\text{Theorem \ref{thm:density_function_probability}}},
\ee
so (iii) holds. Statement (iv) is a special case of (iii) since (by Definition \ref{def:characteristic_function_n})
\be
\phi_X = \E\brb{e^{i\inner{u}{X}}}, \quad\quad \text{where}\quad e^{i\inner{u}{x}} \text{ is bounded}, \ x\in \R^n.
\ee

Suppose that (iv) holds and take independent random variables $\wt{X}_1, \dots, \wt{X}_n$ with $\mu_{\wt{X}_k} = \mu_{X_k}$ for all $k$. Note that such $\wt{X}_i$ exists as it can be defined on different probability space\footnote{theorem needed.}. We know that (i) implies (iv), $\phi_{\wt{X}_k} (u_k) = \phi_{X_k}(u_k)$ so
\be
\phi_{\wt{X}}(u) = \prod_k \phi_{\wt{X}_k} (u_k) = \prod_k \phi_{X_k}(u_k) = \phi_X(u)
\ee
so $\mu_{\wt{X}} = \mu_X$ by Theorem \ref{thm:inversion_density}. Thus, $\forall x_i \in \R,i=1,\dots,n$,
\beast
\pro(X_1\leq x_1,\dots, X_n \leq x_n) & = & \mu_X\brb{\bigotimes^n_{i=1}(-\infty,x_i]} \quad\quad \text{(Definition \ref{def:rn_random_variable_law})}\\
& = & \mu_{\wt{X}} \brb{\bigotimes^n_{i=1}(-\infty,x_i]}  = \mu_{\wt{X}_1}((-\infty,x_1]) \otimes\dots \otimes \mu_{\wt{X}_n}((-\infty,x_n]) \quad\text{by (ii)}\\
& = & \mu_{X_1}((-\infty,x_1]) \otimes\dots \otimes \mu_{X_n}((-\infty,x_n]) \\
& = & \pro(X_1\leq x_1)\dots\pro(X_n \leq x_n). \quad\quad \text{(Definition \ref{def:r_random_variable_law})}
\eeast

Then by Proposition \ref{pro:random_variable_independent}, $X_1,\dots,X_n$ are independent. Hence (i) holds.

Finally, if (ii) holds, we have by Theorem \ref{thm:product_measure},
\be
\mu_X\brb{\bigotimes_k (-\infty,y_k] } = \mu_{X_1}((-\infty,y_1]) \dots \mu_{X_n}((-\infty,y_n])
\ee
which is equivalent to
\be
\int_{\bigotimes_k (-\infty,y_k]} f_X(x)dx = \int^{y_1}_{-\infty} f_{X_1}(x_1)dx_1 \dots \int^{y_n}_{-\infty} f_{X_n}(x_n)dx_n = \int_{\bigotimes_k (-\infty,y_k]} \prod_k f_{X_k} (x_k)dx\quad\quad (*)
\ee
by Theorem \ref{thm:density_function_probability}. Thus since $f$ is non-negative, by Theorem \ref{thm:non_negative_measurable_property}, it holds if $f_X(x) = \prod\limits_k f_{X_k}(x_k)$ a.e. (with respect to Lebesgue measure). If ($*$) holds, then $f_X(x) = \prod\limits_k f_{X_k}(x_k)$ a.e. by uniqueness of density function (Theorem \ref{thm:uniqueness_density_function}) as $f_X(x)$ and $\prod\limits_k f_{X_k}(x_k)$ are density function. %\footnote{this can only prove if part, need further proof.}.
Hence (v) holds.
\end{proof}

\begin{example}
We will show that there do not exist independent identically distributed random variables $X$, $Y$ such that
\be
X-Y \sim \sU[-1,1]\footnote{see Definition \ref{def:uniform_rv}}.
\ee

Using the fact that $X$ and $Y$ are i.i.d., we have by Definition \ref{def:characteristic_function_n} and Theorem \ref{thm:characteristic_function_independence},
\be
\phi_{X-Y} = \phi_X \phi_{-Y} = \phi_X  \overline{\phi_Y} = \phi_X \overline{\phi_X} = \abs{\phi_X}^2 \geq 0
\ee

However,
\be
\phi_{X-Y}(u) = \E\brb{e^{iu(X-Y)}} = \frac 12 \int^1_{-1}e^{iux}dx = \frac{\sin u}{u}.
\ee

When $u = 3\pi/2$ the right hand side is negative. So the characteristic function of $\sU[-1,1]$ is not non-negative everywhere.
\end{example}

%\section{Properties of Specific Random Variables}

\begin{proposition}\label{pro:sigma_algebra_random_variable_independence}
Let $(\Omega,\sF,\pro)$ be probability space and $\sE$ be a $\sigma$-algebra. The the random variable vector $X = (X_1,\dots,X_k)$ is independent of $\sE$ if and only if for any $A\in \sE$ and any continuous bounded function $f:\R^n \to \R$,
\be
\E\brb{\ind_A f(X_1,\dots,X_k)} = \pro(A)\E\brb{f(X_1,\dots,X_k)}\quad\quad (*).
\ee
\end{proposition}

\begin{proof}[\bf Proof]
%If $X$ is independent of $\sE$, we have that for any $A\in \sE$ and $B\in \sigma(X_1,\dots,X_k)$,
%\be
%\pro\brb{A\cap B} = \pro(A)\pro(B).
%\ee
Since $A\in \sE$, $\ind_A$ is $\sE$-measurable, so we have $\ind_A$ and $X$ are independent by Proposition \ref{pro:independent_and_measurable}. Then by Theorem \ref{thm:characteristic_function_independence}.(iii), for any bounded continuous function $f$,
\be
\E\brb{\ind_A f(X_1,\dots,X_k)} = \E\brb{\ind_A} \E(f(X_1,\dots,X_k)) = \pro(A) \E \brb{f(X_1,\dots,X_k)}.
\ee

Now suppose for any $A\in \sF$, $(*)$ holds. Let $B\in \sA$ where $\sA$ is the $\pi$-system generating $\sigma(X_1,\dots,X_k)$. Thus, $B$ is of the form $\bigcap^k_{i=1} B_i = \bigcap^k_{i=1} \bra{\omega:X_i(\omega) \leq y_i}$. Thus, we define continuous bounded functions $f_n$,
\be
f_n(x_1,\dots, x_k) = \prod^k_{i=1} f_{n,i} ,\qquad \text{where } f_{n,i} = \left\{\ba{ll}
1 \quad\quad & x_i \leq y_i\\
\frac{x_i - y_i}{\frac 1n} \quad\quad & y_i \leq x_i \leq y_i + \frac 1n\\
0 & x_i \geq y_i + \frac 1n
\ea\right.,\quad i =1,\dots,k.
\ee

Then by the assumption, we have
\be
\E\brb{\ind_A f_n(X_1,\dots,X_k)} = \pro(A)\E\brb{f_n(X_1,\dots,X_k)}
\ee

Also, we can see that $f_n(X_1,\dots,X_1) \to \ind_{B}$ as $n\to \infty$. Thus, by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability}), we have
\be
\E\brb{\ind_A \ind_B} = \pro(A)\E\brb{\ind_B} = \pro(A)\pro(B).
\ee

Since $B$ is arbitary set in the $\pi$-system generating $\sigma(X_1,\dots,X_k)$, we have for any $A\in \sE$ and $B\in \sigma(X_1,\dots,X_k)$ (by Theorem \ref{thm:pi_sigma_independent}), $\E\brb{\ind_A \ind_B} = \pro(A)\pro(B)$. This means that $X$ is independent of $\sE$.
\end{proof}

\begin{corollary}\label{cor:sigma_algebra_random_variable_independence_by_conditional_expectation}
Let $(\Omega,\sF,\pro)$ be probability space and $\sG$ be a $\sigma$-algebra. The the random variable vector $X = (X_1,\dots,X_k)$ is independent of $\sG$ if and only if for any continuous bounded Borel function $f:\R^n \to \R$,
\be
\E\brb{f(X_1,\dots,X_k)|\sG} = \E\brb{f(X_1,\dots,X_k)} \text{ a.s.}.
\ee
\end{corollary}

\begin{remark}
Note that when $f$ is continuous bounded Borel function, the statement is actually the inverse of Proposition \ref{pro:conditional_expectation_tower_independence}.(v).
\end{remark}

\begin{proof}[\bf Proof]
Let $Y = \E\brb{f(X_1,\dots,X_k)|\sG}$ a.s.. Then by Definition of conditional expectation, for any $A\in \sG$ (as $f(X_1,\dots,X_k)$ is bounded and thus integrable),
\be
\E\brb{Y\ind_A} = \E\brb{f(X_1,\dots,X_k)\ind_A} \underbrace{= \pro(A)\E\brb{f(X_1,\dots,X_k)}}_{\text{Proposition \ref{pro:sigma_algebra_random_variable_independence}}} = \E\brb{\ind_A \E\brb{f(X_1,\dots,X_k)}}.
\ee

This equality holds if and only if $Y = \E\brb{f(X_1,\dots,X_k)}$ a.s. by Theorem \ref{thm:conditional_expectation_existence_uniqueness}.
\end{proof}

\begin{theorem}\label{thm:sigma_algebra_random_variable_independence_by_conditional_expectation}
Let $(\Omega,\sF,\pro)$ be probability space and $\sG$ be a $\sigma$-algebra. The the random variable vector $X = (X_1,\dots,X_k)^T$ is independent of $\sG$ if and only if% for the characteristic function $\phi_X$,
\be
\E\brb{\left.e^{i\bsa{u,X}}\right|\sG} = \E\brb{e^{i\bsa{u,X}}} \text{ a.s.}.\quad (*)
\ee
\end{theorem}

\begin{proof}[\bf Proof]
The only if part is implied by Proposition \ref{pro:conditional_expectation_tower_independence}.(v).

Now we prove the if part. For any $A\in \sG$, we have $u\in \R^{k}$ and $v = \brb{u^T,u_{k+1}}^T$ where $u_{k+1} \in \R$, and $Y = \brb{X^T,\ind_A}$
\beast
\phi_Y(v) & = & \E\brb{e^{i\bsa{v,Y}}} = \E\brb{e^{i\bsa{u,X}}e^{iu_{k+1}\ind_A}} = \E\brb{e^{i\bsa{u,X}}e^{iu_{k+1}}\ind_A}\\
& = & e^{iu_{k+1}} \E\brb{e^{i\bsa{u,X}}\ind_A} = e^{iu_{k+1}} \E\brb{e^{i\bsa{u,X}}} \E\brb{\ind_A} \qquad (\text{by ($*$)}) \\
& = & \E\brb{e^{i\bsa{u,X}}} \E\brb{e^{iu_{k+1}}\ind_A} = \E\brb{e^{i\bsa{u,X}}} \E\brb{e^{iu_{k+1}\ind_A}} = \phi_X(u)\phi_{\ind_A}(u_{k+1}).
\eeast

Thus, we can see that $X$ is independent of $\ind_A$ by Theorem \ref{thm:characteristic_function_independence}. Then we can say that $X$ is independent of $\sG$.
\end{proof}



\subsection{Moment generating function}

\begin{definition}[moment generating function\index{moment generating function}]\label{def:mgf_probability}
The moment generating function $M_X$ of a real-valued random variable $X$ is defined by
\be
M_X(\theta)=\E\brb{e^{\theta X}},\quad \theta\in \R.
\ee
\end{definition}

\begin{example}
The set $I=\bra{\theta:M_X(\theta)<\infty}$ is an interval (i.e., $\R$, $\{0\}$ and $(-\infty,1)$). %Assume for simplicity that $X\geq 0$.

Suppose $M_X(a)$ and $M_X(b)$ are both finite with $a<b$. Then for all $c \in (a,b)$,
\be
M_X(c) \leq \E\brb{\max\bra{e^{aX},e^{bX}}} < M_X(a) + M_X(b) < \infty.
\ee

\ben
\item [(i)] For $I=\mathbb{R}$, we can take $X$ with $X(\omega)=1$ for all $\omega$ as $M_X(\theta) = e^{\theta } < \infty$, $\forall \theta \in \R$.

\item [(ii)] For $I=\{0\}$, we can take $X$ with a Cauchy distribution (i.e. density $1/(\pi(1+x^2)$, see Definition \ref{def:cauchy_random_variable}) since for $\theta \neq 0$,
\beast
M_X(\theta) & = & \int^\infty_{-\infty} \frac{e^{\theta x}}{\pi(1+x^2)}dx = \int^\infty_0 \frac{e^{\theta x}+ e^{-\theta x}}{\pi(1+x^2)}dx \geq \int^\infty_0 \frac{e^{\abs{\theta} x}}{\pi(1+x^2)}dx \geq \int^\infty_0 \frac{e^{\abs{\theta} x}}{\pi(1+x)^2}dx\\
& \geq & e^{-\abs{\theta}}\int^\infty_1 \frac{e^{\abs{\theta} x}}{\pi x^2} dx = \frac{e^{-\abs{\theta}}}{\pi}\int^\infty_1 \brb{\frac 1{x^2} + \frac 1{x} + \frac 12 + \frac {x}{3!} + \frac {x^2}{4!} + \dots}dx = \infty
\eeast
and $M_X(0) = 1<\infty$.

\item [(iii)] For $I=(-\infty,1)$, we can take $X \sim \sE(1)$ (i.e. density $e^{-x}$ on $[0, \infty)$) since $\theta \neq 1$ ($M_X(1) = \infty$ when $\theta =1$),
\beast
M_X(\theta) & = & \int^\infty_0 e^{\theta x}e^{-x}dx = \int^\infty_0 e^{(\theta-1) x}dx = \frac 1{\theta -1} \left.e^{(\theta -1)x}\right|^\infty_0 < \infty \ \text{ only if $\theta < 1$.}
\eeast
\een
\end{example}

\begin{proposition}\label{pro:mgf_finite_moment}
Let $\theta \in I = \bra{\theta:M_X(\theta)<\infty}$ and $I$ contains a neighbourhood of 0 (i.e., an open set contains 0) then $X$ has finite moments of all orders given by
\be
\E X^n = \left.\brb{\frac {d^n M_X(\theta)}{d\theta^n}}\right|_{\theta=0}.
\ee%Find a necessary and sufficient condition on the sequence of moments $m_n = \E(X^n)$ for $I$ to contain a neighbourhood of 0. Furthermore, for an event $A$ ($A\in \sF$), we have \be \E \brb{X^n\ind_A} =  \ee
\end{proposition}

\begin{proof}[\bf Proof]
For all $\theta$, we have that
\be
\sum_{n=0} ^N \frac{(\theta X)^n}{n!} \to e^{\theta X} \text{ pointwise as } N \to \infty.
\ee

Now take $\ve \in I$ with $\ve >0$ and $M_X(-\ve) < \infty$. Then for all $\theta \in (-\ve,\ve)$, we have that
\be
\abs{\sum_{n=0} ^N \frac{(\theta X)^n}{n!}} \leq \sum_{n=0}^N \frac{\abs{\theta X}^n}{n!} \leq e^{\abs{\ve X}} \leq e^{\ve X} +  e^{-\ve X}
\ee
for all $N$. Thus,
\be
\E \abs{\sum_{n=0} ^N \frac{(\theta X)^n}{n!}} \leq \E\brb{e^{\ve X} + e^{-\ve X} } < \infty \ \ra \ \sum_{n=0} ^N \frac{(\theta X)^n}{n!}\text{ is integrable.}
\ee

Hence by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}), $e^{\theta X} $ is integrable and
\be
M_X(\theta) = \E(e^{\theta X}) = \E\brb{\lim_{N\to \infty} \sum_{n=0} ^N \frac{(\theta X)^n}{n!} } = \lim_{N \to \infty} \E \brb{ \sum_{n=0} ^N \frac{(\theta X)^n}{n!} } = \sum_{n=0} ^\infty \frac{\theta^n \E(X^n)}{n!} .\quad (*)
\ee
by Fubini theorem (Theorem \ref{thm:fubini}). Also, for each $n$
\be
\frac{\theta^n \E(\abs{X}^n)}{n!} < \infty \ \ra \ \E\brb{\abs{X}^n} < \infty.
\ee
%Thus $\mathbb{E}(X^n) < \infty$ for all $n$.

%If $I$ contains a neighbourhood of 0, then ($\dag$) must have positive radius of convergence. Conversely, if ($\dag$) has positive radius of convergence $R$, say, then for $0<r<R$,
%\[
% \mathbb{E}(e^{rX}) = \sum_{n=0} ^\infty \frac{r ^n \mathbb{E}(X^n)}{n!} <\infty
%\]
%by monotone convergence. Also, as $X \geq0$, $\phi(-r) \leq \phi(r) < \infty$. Thus $I$ contains a neighbourhood of 0. Using results from power series, ($\dag$) has positive radius of convergence if and only if
%\[
% \limsup_{n \to \infty} \abs{\frac{m_n}{n!}}^\frac{1}{n} < \infty,
%\]
%for example.

Using standard results for power series,
\begin{equation}
\sum_{n=0} ^\infty \frac{\theta^n \E(X^n)}{n!}\quad (\dag)
\end{equation}
is infinitely differentiable for all $\theta$ within its radius of convergence\footnote{details needed.}, so in particular for all $\theta \in (-\ve,\ve)$. The $n$th derivative at 0 is given by $\E X^n$. By the relationship in ($*$), this also equals $\left.\brb{\frac{d^nM_X(\theta)}{d\theta^n}}\right|_{\theta = 0}$.
\end{proof}

%\begin{proposition}\label{pro:mgf_finite_moment_positive_rv}
%Let $X$ be a positive random variable and $\theta \in I = \bra{\theta:M_X(\theta)<\infty}$ where $I$ contains an interval $(-\ve,0]$ then $X$ has moments of all orders given by
%\be
%\E X^n = \left.\brb{\frac {d^n M_X(\theta)}{d\theta^n}}\right|_{\theta=0}.
%\ee

%Note that $X$ may not have finite moments in this case. The $n$th finite %moments exist as long as
%\end{proposition}

%\begin{proof}[\bf Proof]
%We apply the similar argument in Proposition \ref{pro:mgf_finite_moment} and get
%\be
%\E \abs{\sum_{n=0} ^N \frac{(\theta X)^n}{n!}} \leq \E\brb{e^{\abs{\ve X}}  } \leq 1 < \infty \ \ra \ \sum_{n=0} ^N \frac{(\theta X)^n}{n!}\text{ is integrable.}
%\ee
%\end{proof}



\begin{theorem}\label{thm:mgf_uniquely_determine_law}
Let $X$ and $Y$ be two random variables. If their moment generatring functions exist, say $M_X(\theta)$ and $M_Y(\theta)$ with $M_X(\theta) =M_Y(\theta) < \infty$ for any $\theta \in \R$. Then we have that $X$ and $Y$ have the same distribution. In other words, $M_X$ determines the law $\mu_X$ uniquely.
\end{theorem}

\begin{proof}[\bf Proof]
Apply the same argument in proof of Theorem \ref{thm:inversion_density}.
\end{proof}

\begin{corollary}\label{cor:mgf_indicator_expectation_independent}
Let $A$ be an event and $X$ be a random variable satisfies that
\be
\E\brb{e^{\theta X}\ind_A} = \E\brb{e^{\theta X}}\pro(A).
\ee
where $e^{\theta X}$ is integrable. Then $X$ is independent of $A$.
\end{corollary}

\begin{proof}[\bf Proof]
Let $X'$ be a random variable independent of $A$ with
\be
\pro\brb{X\leq x, \ind_A \leq a} = \pro\brb{X'\leq x,\ind_A\leq a}.
\ee

In other words, they have the same joint law. Therefore,
\be
\E\brb{e^{\theta X} \ind_A} = \E\brb{e^{\theta X'} \ind_A} = \E\brb{e^{\theta X'}}\pro(A)
\ee
by Theorem \ref{thm:characteristic_function_independence}.(iii) since $X'$ is independent of $A$. Also, we know that by assumption
\be
\E\brb{e^{\theta X}}\pro(A) = \E\brb{e^{\theta X} \ind_A} \ \ra \  \E\brb{e^{\theta X}}\pro(A) = \E\brb{e^{\theta X'}}\pro(A).
\ee

Hence, we have that $M_X(\theta) = \E\brb{e^{\theta X}} = \E\brb{e^{\theta X'}} = M_{X'}(\theta) < \infty$ (for the case $\pro(A) >0$\footnote{For $\pro(A) =0$, $X$ is of course independent of $A$ by definition}) as $e^{\theta X}$ is integrable. Then by Theorem \ref{thm:mgf_uniquely_determine_law}, we have that $X\sim X'$. Therefore,
\be
\pro\brb{X\leq x,\ind_A\leq a} = \pro\brb{X'\leq x,\ind_A\leq a} = \pro\brb{X'\leq x} \pro\brb{\ind_A\leq a} = \pro\brb{X\leq x} \pro\brb{\ind_A\leq a}
\ee
by Proposition \ref{pro:random_variable_independent} and the fact that $X\sim X'$. Hence, we have that $X$ is independent of $A$ by Proposition \ref{pro:random_variable_independent}.
\end{proof}

\begin{proposition}\label{pro:independent_mgf}
Let $X_1,X_2,\dots,X_n$ be random variables with $\theta \in \R^n$ ($\theta$ can just be in a subset of $\R^n$) and $e^{\theta_1 X_1},\dots, e^{\theta_n X_n}$ are integrable. Then $X_1,\dots,X_n$ are independent if and only if for moment generating functions $M_{X_1},\dots,M_{X_n}$,
\be%M_{X_1+\dots+X_n}(\theta) = M_X(\theta)M_Y(\theta).
\E\brb{e^{\theta_1X_1 + \dots + \theta_nX_n}} = M_X = M_{X_1}\dots M_{X_n} = \E\brb{e^{\theta_1X_1}}\dots \E\brb{e^{\theta_n X_n}}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Apply the same argument in proof of Theorem \ref{thm:inversion_density}.%Direct result from Theorem \ref{thm:characteristic_function_independence}
\end{proof}


\begin{definition}[skewness\index{skewness}]\label{def:skewness}
The skewness of a random variable $X$ with mean $\mu$ and variance $\sigma^2$ is defined as
\be
\skewness(X) = \E\brb{\brb{\frac{X-\mu}{\sigma}}^3} = \frac{\E(X-\mu)^3}{\brb{\E (X-\mu)^2 }^{3/2}} .
\ee
\end{definition}

\begin{remark}
If skewness is positive, the distribution tends to lean to left.
\end{remark}

\begin{definition}[kurtosis\index{kurtosis}]\label{def:kurtosis}
The kurtosis of a random variable $X$ with mean $\mu$ and variance $\sigma^2$ is defined as \be \kurt(X) = \frac{\E (X-\mu)^4}{\brb{\E (X-\mu)^2 }^2} = \frac{\E (X-\mu)^4}{\sigma^4}. \ee

Also, the excess kurtosis is defined by $\ekurt(X) = \kurt(X)-3$.
\end{definition}

\begin{remark}
If excess kurtosis is positive, it means that the distribution has 'heavy tail'\index{heavy tail} (or 'fat tail').
\end{remark}

For non-negative random variable $X$, we have the following speical version theorem of moment related to Laplace transform.

\begin{theorem}[Uniqueness of Laplace transform of random variables]\label{thm:laplace_uniquely_determine_law}
Let $X$ be a non-negative random variable and $s>0$. Also, define
\be
\sL(s) = \E\brb{e^{-s X}}.
\ee

Then for non-negative random variables $X$ and $Y$ with $\sL_X(s) =\sL_Y(s) $ for any $s >0$. Then we have that $X$ and $Y$ have the same distribution. In other words, $\sL_X(s)$ determines the law $\mu_X$ uniquely.
\end{theorem}

\begin{remark}
Note that the Laplace form is guaranteed to be finite.
\end{remark}

\begin{proof}[\bf Proof]
\footnote{proof needed. see \cite{Feller_1968_v2}.$P_{430}$}
\end{proof}

\begin{proposition}\label{pro:laplace_finite_moment}
Let $X$ be a non-negative random variable and $\theta>0$. Also, define
\be
\sL(\theta) = \E\brb{e^{-\theta X}}.
\ee

Given $r\in \R$ with $r =n-\alpha$ where $n$ is a non-negative integer and $\alpha >0$. %If $ \sL^{(n)}(\theta)\theta^{\alpha-1}$, then
If $\E X^n < \infty$, then
\be
\E X^r = \frac{(-1)^n}{\Gamma(\alpha)}\int^\infty_{0} \sL^{(n)}(\theta) \theta^{\alpha-1} d\theta
\ee
with $\Gamma$ the usual Gamma function.

In particular, we have
\be
\E X^{n} = \left.(-1)^n \sL^{(n)}(\theta)\right|_{\theta = 0}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Since $\E X^n < \infty$, we can see that $\E X^m < \infty$ for all integer $m \leq n$ by Proposition \ref{pro:lp_subset_decreasing_order} and Theorem \ref{thm:differentiation_under_integral_sign},
\be
\sL^{(n)}(\theta) = (-1)^n\E \brb{X^n e^{-\theta X}}\qquad (*)
\ee

Since $X^ne^{-\theta X}$ is non-negative, the integral
\be
\int^\infty_0 \sL^{(n)}(\theta) \theta^{\alpha -1}d\theta
\ee
is well defined. Then by Fubini theorem (Theorem \ref{thm:fubini}), the integral is
\beast
(-1)^n\int^\infty_0 \E \brb{X^n e^{-\theta X}} \theta^{\alpha -1}d\theta & = & (-1)^n \E\brb{X^n \int^\infty_0 e^{-\theta X}\theta^{\alpha -1}d\theta} \\
& = & (-1)^n \E\brb{X^{n-\alpha} \int^\infty_0 e^{-\theta X}(\theta X)^{\alpha -1}d(\theta X)} \\
& = & (-1)^n \E\brb{X^{n-\alpha} \int^\infty_0 e^{-\theta} \theta^{\alpha -1}d\theta} = (-1)^n \Gamma(\alpha) \E X^{n-\alpha}
\eeast
as required. Since $X^ne^{-\theta X} \ua X^n$ as $\theta \to 0$ for ($*$), we can have
\be
\E X^{n} = \left.(-1)^n \sL^{(n)}(\theta)\right|_{\theta = 0}.
\ee
by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}).
\end{proof}

\section{Convergence in Distribution and Weak Convergence}

\subsection{Convergence in distribution for random variables}

\begin{definition}
Let $F$ be a distribution function on $\R$ and let $(F_n : n \in \N)$ be a sequence of such distribution functions. We say that $F_n \to F$ as distribution
functions\index{convergence!as distribution function} if $F_n(x) \to F(x)$ at every point $x \in \R$ where $F$ is continuous.
\end{definition}

\begin{definition}\label{def:convergence_in_distribution}
Let $X$ be a random variable and let $(X_n : n \in \N)$ be a sequence of random variables. We say that $X_n \to X$ in distribution\index{convergence!in distribution} (denoted by $X_n \stackrel{d}{\to}X$) if $F_{X_n} \to F_X$.
\end{definition}


\begin{theorem}[Skorokhod representation theorem\index{Skorokhod representation theorem}]\label{thm:skorokhod_representation}
Suppose that $(F_n:n\in\N)$ is a sequence of distribution function on $\R$, and $F$ is a distribution function on $\R$ that $F_n(x) \to F(x)$ at every point $x$ of continuity of $F$.

Then there exists a probability triple $(\Omega,\sF,\pro)$ carring a sequence $(X_n)$ of random variables and also a random variable $X$ such that
\be
F_n = F_{X_n},\quad F = F_X,\quad\quad \text{and }\quad X_n \to X \text{ a.s.}
\ee
\end{theorem}

\begin{remark}
This is a kind of `converse' to Lemma \ref{lem:weak_convergence_implies_in_distribution}.
\end{remark}

\begin{proof}[\bf Proof]
Take $(\Omega,\sF,\pro) = ([0,1],\sB([0,1]),\text{Leb})$, define
\be
X^+(\omega) := \inf\bra{x :F(x) > \omega},\quad  X^-(\omega) := \inf\bra{x :F(x) \geq \omega},
\ee
similarly,
\be
X_n^+(\omega) := \inf\bra{x :F_n(x) > \omega},\quad  X_n^-(\omega) := \inf\bra{x :F_n(x) \geq \omega}.
\ee

Thus, from Proposition \ref{pro:skorokhod_representation_distribution_function}, we have $\pro(X^+ = X^-) = 1$.

Now fix $\omega$. Let $x$ be a non-atom of $F$ with $x > X^+(\omega)$. Then $F(z) > \omega$ and hence, for large $n$, $F_n(x) > \omega$ (since $F_n \to F$ as distributio function), so that $X^+_n(\omega) \leq x$. So $\limsup_n X^+_n(\omega) \leq x$. Also we can choose $x\da X^+(\omega)$ (remark of Definition \ref{def:atom_distribution}). Hence
\be
\limsup_n X^+_n(\omega) \leq X^+(\omega),
\ee
and by similar arguments,
\be
\limsup_n X^-_n(\omega) \geq X^-(\omega).
\ee

Since $X^-_n \leq X^+_n$ and $\pro(X^+ = X^-) =1$, we have
\be
\limsup_n X^+_n = \liminf_n X^+_n = \lim_n X^+_n = X^+ \ \text{ a.s.}, \quad\quad \limsup_n X^-_n = \liminf_n X^-_n = \lim_n X^-_n = X^- \ \text{ a.s.},
\ee

Thus, $\pro(X^+_n = X^-_n) = 1$. Define
\be
X_n = \left\{\ba{ll}
X_n^- \quad\quad & X_n^+ = X_n^-\\
0 & \text{otherwise}
\ea\right.,\quad\quad X = \left\{\ba{ll}
X^+\quad\quad & X^+ = X^-\\
0 & \text{otherwise}
\ea\right.
\ee
we have $F_n(x) = \pro(X_n\leq x)$ and $F(x) = \pro\brb{X\leq x}$ (since $X_n^- = X_n$ a.s. and $X^- = X$ a.s.) and $\pro\brb{\lim_n X_n \to X} = 1$ since $X^- = X$ a.s., $X_n^- = X_n$ a.s. and $X_n^- \to X^-$ a.s.
\end{proof}

\subsection{Weak convergence}

\begin{definition}[weak convergence]
Let $\mu$ be a Borel probability measure on $(\R,\sB(\R))$ and let $(\mu_n : n \in \N)$ be a sequence of such measures. We say that $\mu_n \to \mu$ weakly\index{convergence!weakly!probability measure} or $\mu_n \stackrel{w}{\to} \mu$ if $\mu_n(f) \to \mu(f)$ for all continuous bounded functions $f$ on $\R$.
\end{definition}

%Recall the definition of compact support (Definition \ref{def:compact_support}),
%
%\begin{definition}
%Let $\mu$ be a Borel probability measure on $(\R,\sB(\R)$ and let $(\mu_n : n \in \N)$ be a sequence of such measures. We say that $\mu_n \to \mu$ vaguely\index{convergence!vaguely} or $\mu_n \stackrel{v}{\to} \mu$ if $\mu_n(f) \to \mu(f)$ for all continuous functions $f$ with compact support on $\R$.
%\end{definition}

\begin{lemma}\label{lem:convergence_in_distribution_implies_weak_convergence}
Suppose $X$, $(X_n:n\in \N)$ are random variables such that $X_n \to X$ in probability. Then $\mu$ and $(\mu_n : n \in \N)$ are the law (Borel probability measures on $\R$) corresponding to distribution function $F_X$ and $F_{X_n}$, i.e.
\be
\mu((-\infty,x]) = F_X(x),\quad\quad \mu_n((-\infty,x]) = F_{X_n}(x).
\ee
Then we have $\mu_n \stackrel{w}{\to} \mu$.
\end{lemma}

\begin{proof}[\bf Proof]
Suppose that $X_n \to X$ in probability, for any bounded continuous function $f$ on $\R$, $\forall \ve>0$, there exists $\delta >0$, s.t. $\abs{X_n - X}< \delta$, $\abs{f(X_n) - f(X)} < \ve$ (by definition of continuity (Definition \ref{def:continuous_real_2})). Let
\be
A := \{\abs{f(X_n) - f(X)} < \ve\}, \quad B := \{\abs{X_n - X}< \delta\} \quad \ra\quad B\subseteq A \ \ra\ A^c \subseteq B^c.
\ee

Thus, $\pro\brb{\bra{\abs{f(X_n) - f(X)} \geq \ve}} = \pro(A^c) \leq \pro(B^c) = \pro\brb{\bra{\abs{X_n - X} \geq \delta}} \to 0$. So $f(X_n) \to f(X)$ in probability. Then by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability}) and Theorem \ref{thm:image_measure_probability}, we have
\vspace{2mm}

$\qquad\qquad\qquad\qquad\mu_n(f) = \E(f(X_n)) \to \E(f(X)) = \mu(f)\quad\ra\quad \mu_n \stackrel{w}{\to} \mu$.
\end{proof}

We check the fact that $F_n$ does not converge to $F$ at discontinuous points of $F$.

\begin{example}
Suppose that $X_n = \frac 1n$, $X=0$ ($F$ is not continuous at 0). Let $\mu_n$ be the law of $X_n$, so that $\mu_n$ is the unite mass at $\frac 1n$, and let $\mu$ be the law of $X$. Then, for any bounded continuous function on $\R$, $f\in C_b(\R)$,
\be
\mu_n(f) = \underbrace{f\brb{\frac 1n} \to f(0) }_{\text{by continuity}} = \mu(f) \quad \ra\quad \mu_n \stackrel{w}{\to} \mu.
\ee

However, $F_n(0) = 0 \nrightarrow 1 = F(0)$.
\end{example}


\begin{lemma}\label{lem:weak_convergence_implies_in_distribution}
If $\mu_n\stackrel{w}{\to} \mu$, then for the corresponding distribution functions $F_n(x) = \mu_n((-\infty,x])$ and $F = \mu((-\infty,x])$,
\be
\lim_n F_n (x) = F(x)
\ee
for every non-atom (that is, every point of continuity) $x$ of $F$.
\end{lemma}

\begin{proof}[\bf Proof]
Suppose that $\mu_n\stackrel{w}{\to} \mu$, let $x\in \R$, and let $\delta >0$. Construct bounded continuous function $f$
\be
f(y) := \left\{\ba{ll}
1& y\leq x\\
1- \delta^{-1}(y-x) \quad\quad & x< y < x + \delta\\
0 & y\geq x + \delta
\ea\right.
\ee

Then $\mu_n (f) \to \mu(f)$. Now,
\be
F_n(x) = \mu_n((-\infty,x]) \leq \mu_n (h),\quad\mu(h) \leq \mu((-\infty,x+\delta]) = F(x+\delta) \quad \ra\quad \limsup_n F_n(x) \leq F(x+\delta).
\ee

However, $F$ is right-continuous, so we may let $\delta \da 0$ to obtain
\be
\limsup_n F_n (x) \leq F(x),\quad \forall x\in \R.\quad (*)
\ee

In similar fashion, working with $y\to h(y+\delta)$, we find that for $x\in \R$ and
\be
\liminf F_n(x) \leq F(x-\delta) \quad \ra\quad \liminf F_n(x) \leq F(x-), \quad \forall x\in \R. \quad (\dag)
\ee

If $F$ is continuous at $x$, then we have $F(x^-) =F(x)$, which gives us that
\be
\limsup_n F_n(x) \leq \liminf_n F_n(x) \quad \ \ra\ \lim_n F_n(x) = F(x),
\ee
as required.
\end{proof}

Now we can merge Lemma \ref{lem:convergence_in_distribution_implies_weak_convergence} and \ref{lem:weak_convergence_implies_in_distribution},

\begin{theorem}\label{thm:weak_convergence_iff_convergence_in_distribution}
Let $\mu_n$, $\mu$ be probability measures on $\R$. $F_n$ and $F$ are corresponding distribution functions $F_n(x) = \mu_n((-\infty,x])$ and $F = \mu((-\infty,x])$,. The following are equivalent.
\ben
\item [(i)] $\mu_n \to \mu$ weakly.
\item [(ii)] $F_n(x)\to F(x)$ for every $x \in\R$ that is a continuity point of $F$.
\een
\end{theorem}

\begin{remark}
Of course, this theorem can be proved by previous argument, but we give the proof implied by the results in measure theory.
\end{remark}

\begin{proof}[\bf Proof]
(i) $\ra$ (ii): This is (i) $\ra$ (iv) in Theorem \ref{thm:portmanteau} ($E=\R$). Indeed, $\partial (-\infty, x] = \{x\}$, so if $F$ continuous at $x$ then $\mu(\{x\}) = 0$.

(ii) $\ra$ (i): Let $G$ be open in $\R$. We may write $G = \bigcup_{k\geq0}(a_k, b_k)$ for pairwise disjoint intervals $(a_k, b_k)$. Then $\mu_n(G) =
\sum_{k\geq0} \mu_n(a_k, b_k)$ by countable additivity. Now,
\be
\mu_n(a_k, b_k) = F_n (b_k^-)- F_n (a_k) \geq F_n (b)- F_n (a)
\ee
for every $a_k < a < b < b_k$. Choosing $a$ and $b$ to be continuity points of $F$ (such points are dense in $\R$ (given $x\in \R$, there exists a sequence $(y_n)$ of reals with $y_n \da x$ such that every $y_n$ is a non-atom of $\mu$) by Proposition \ref{pro:non_atoms_dense}), we obtain
\be
\liminf\mu_n(a_k, b_k) \geq \liminf \brb{F_n (b)- F_n (a)} = F(b)-F(a).\quad\quad (\text{by (ii)})
\ee

Letting $a \da a_k$ and $b \ua b_k$ along continuity points of $F$ (since the set of continuity points of $F$ is dense), we obtain
\be
\liminf\mu_n(a_k, b_k) \geq \underbrace{F(b_k^-)}_{\text{$F$ is increasing}}- \underbrace{F(a_k)}_{\text{$F$ is right-continuous}} = \mu(a_k, b_k). \quad\quad (*)
\ee

%%%%%% the following is found wrong
%Finally, applying the idea in proof of Fatou's lemma (Lemma \ref{lem:fatou_function}),
%\beast
%\inf_{m\geq n} \mu_m (a_k,b_k) \leq \mu_m (a_k,b_k) & \ra & \sum_{k\geq 0}\inf_{m\geq n} \mu_m (a_k,b_k) \leq \sum_{k\geq 0}\mu_m (a_k,b_k)\\
%& \ra & \sum_{k\geq 0}\inf_{m\geq n}  \mu_m (a_k,b_k) \leq \inf_{m\geq n}\sum_{k\geq 0}\mu_m (a_k,b_k). \quad\quad (**)
%\eeast

%Clearly,
%\be
%\sum_{k\geq 0}\sup_n\brb{\inf_{m\geq n}  \mu_m (a_k,b_k)} \leq \sup_n\sum_{k\geq 0}\brb{\inf_{m\geq n}  \mu_m (a_k,b_k)}. \quad\quad(\dag)
%\ee

%Thus,
%\beast
%\liminf\limits_n \mu_n(G) & = & \liminf\limits_n \sum\limits_{k\geq0}  \mu_n(a_k, b_k) = \sup_n \inf_{m\geq n} \sum\limits_{k\geq0}  \mu_m(a_k, b_k) \stackrel{(**)}{\geq} \sup_n \sum\limits_{k\geq0}  \inf_{m\geq n}  \mu_m(a_k, b_k) \\
%& \stackrel{(\dag)}{\geq } & \sum\limits_{k\geq0}  \sup_n \inf_{m\geq n}  \mu_m(a_k, b_k) =  \sum\limits_{k\geq0} \liminf\limits_n \mu_n(a_k, b_k) \stackrel{(*)}{\geq} \sum\limits_{k\geq0}\mu(a_k, b_k) = \mu(G).
%\eeast

Finally, applying Fatou's lemma (Lemma \ref{lem:fatou_function}, we consider $\sum_{k\geq 0}\ind_{(k,k+1]}$ a measure and $\mu_n$ a function sequence),
\beast
\liminf\limits_n \mu_n(G) & = & \liminf\limits_n \sum\limits_{k\geq0}  \mu_n(a_k, b_k) \geq \sum\limits_{k\geq 0} \liminf\limits_n \mu_n(a_k, b_k) \stackrel{(*)}{\geq} \sum\limits_{k\geq0}\mu(a_k, b_k) = \mu(G).
\eeast


This is actually (ii) in Theorem \ref{thm:portmanteau}, which is equivalent to (i).
\end{proof}


Then by Definition \ref{def:convergence_in_distribution}, Theorem \ref{thm:weak_convergence_iff_convergence_in_distribution} and Theorem \ref{thm:image_measure_probability},

\begin{corollary}\label{cor:convergence_in_distribution_iff_expectation}
$X_n \stackrel{d}{\to}  X$ if and only if $\E\brb{ f (X_n)}\to \E\brb{ f (X)}$ for all bounded continous functions, $f \in C_b(E)$.
\end{corollary}

\begin{example}
\ben
\item [(i)] If $X_n = x_n$ is constant and $x_n \to x$ then $X_n \stackrel{d}{\to}  X$ since $\delta_{x_n} \stackrel{w}{\to} \delta_x$.
\item [(ii)] Let $U$ be a uniform r.v. (see Definition \ref{def:uniform_rv}) in $[0, 1]$, and write $X_n = n^{-1}\floor{U}$. Then $X_n \to U$ a.s., so $X_n \stackrel{d}{\to} U$. Indeed, $\mu_{X_n} = \frac 1n \sum^{n-1}_{k=0} \delta_{\frac kn} \stackrel{w}{\to} d x = \mu_X(U)$.
\item [(iii)] Central Limit Theorem (see Theorem \ref{thm:central_limit}). Let $X_1, X_2, \dots$ be i.i.d. r.v.'s in $\sL^2(\R)$. If $\mu := \E[X]$ and $\sigma^2 := \var(X) = \E[X^2]$ then for all $a < b$,
\be
\pro\brb{a <\frac{X_1 +\dots + X_n - n\mu}{\sigma\sqrt{n}} < b} \to \int^b_a \frac{e^{- \frac {x^2}2}}{\sqrt{2\pi}}d x
\ee
which is saying that $\frac{X_1+\dots+X_n-n\mu}{\sigma\sqrt{n}}\stackrel{d}{\to}\sN(0,1)$ in distribution, where $\sN(0,1)$ is a Gaussian random variable with mean 0 and variance 1.
\een
\end{example}


\subsection{Properties of convergence in distribution}%of random variables}

\begin{lemma}\label{lem:convergence_in_probability_implies_in_distribution}
If $X_n \to X$ in probability then $X_n \to X$ in distribution.
\end{lemma}

\begin{proof}[\bf Proof]
Suppose $X_n \stackrel{p}{\to} X$ and write
\be
F_n(x) = \pro(X_n \leq x), \quad\quad F(x) = \pro(X\leq x),
\ee
for the distribution functions of $X_n$ and $X$ respectively. $\forall \ve>0$,
\be
F_n(x) = \pro(X_n\leq x) = \pro(X_n\leq x,X\leq x+\ve) + \pro(X_n \leq x,X> x+\ve) \leq F(x+\ve) + \pro\brb{\abs{X_n -X}>\ve}.
\ee

Similarly,
\be
F(x-\ve) = \pro(X \leq x -\ve) = \pro(X\leq x-\ve,X_n\leq x) + \pro(X\leq x-\ve,X_n> x) \leq F_n(x) + \pro\brb{\abs{X_n -X}>\ve}.
\ee

Thus,
\be
F(x-\ve)- \pro\brb{\abs{X_n -X}>\ve} \leq F_n(x) \leq F(x+\ve) + \pro\brb{\abs{X_n -X}>\ve}
\ee

Let $n\to \infty$ to obtain (as $\pro\brb{\abs{X_n -X}>\ve}\to 0$)
\be
F(x-\ve) \leq \liminf_{n\to \infty }F_n(x) \leq \limsup_{n\to \infty }F_n(x) \leq F(x+\ve),
\ee

If $F$ is continuous at $x$ then
\be
F(x-\ve) \ua F(x),\quad\quad F(x+\ve) \da F(x) \quad\text{as }\ve \da 0.
\ee

Thus, $\lim F_n(x) = F(x)$ for all continuous points, known as $X_n \to X$ in distribution.
\end{proof}

\begin{remark}
The inverse of above lemma is not true.
\end{remark}

\begin{example}
Let $X$ be a Bernoulli variable taking values 0 and 1 with equal probability $\frac 12$. Let $X_1,X_2,\dots$ be indentical random variables given by $X_n= X$ for all $n$. The $X_n$ are certainly not independent, but $X_n \stackrel{d}{\to} X$. Let $Y=1-X$. Clearly $X_n \stackrel{d}{\to} Y$, since $X$ and $Y$ have the same distribution. However, $X_n$ cannot converge to $Y$ in any other mode because $\abs{X_n - Y} =1$ always.
\end{example}

However, we have the following proposition:

%\begin{proposition}\label{pro:convergence_constant_distribution_implies_probability}
%Let $X_n$, $X$ be r.v.'s. If $X_n \to  X$ in probability, i.e.
%\be
%\lim_{n\to \infty} \pro(d(X_n, X) > \ve)\to 0\text{ for all }\ve > 0
%\ee
%then $X_n \to X$ in distribution. %If $X_n \to c$ in distribution for some constant $c$ then $X_n \to c$ in probability.
%\end{proposition}

%\begin{proof}[\bf Proof]
%\footnote{need proof}
%\end{proof}

\begin{proposition}\label{pro:convergence_in_distribution_constant_implies_convergence_in_probability}
If $X_n \to c$ in distribution for some constant $c$, then $X_n \to c$ in probability. That is,
\be
X_n \stackrel{d}{\to}c \ \lra\ X_n \stackrel{p}{\to}c.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
For any $\ve>0$, let $B_{\ve}(c)$ be the open ball of radius $\ve$ around point $c$, and $B^c_{\ve}(c)$ its complement. Then
\be
\pro\brb{\abs{X_n - c} \geq \ve} = \pro\brb{X_n \in B^c_{\ve}(c)}.
\ee

Thus,
\be
\lim_{n\to \infty} \pro\brb{\abs{X_n - c} \geq \ve} \leq \limsup_{n\to \infty} \pro\brb{\abs{X_n - c} \geq \ve}  = \limsup_{n\to \infty}\pro\brb{X_n \in B^c_{\ve}(c)}
\ee

If $X_n$ converges in distribution to $c$, then $F_{X_n}(x) \to F_c(x)$ for every continuous point $x$ of $F$. Then by weak convergence theorem (Theorem \ref{thm:weak_convergence_iff_convergence_in_distribution}), $\mu_n \stackrel{w}{\to} \mu$ where $\mu_n = \pro\circ X_n^{-1}$ and $\mu = \pro\circ c^{-1}$. Then by Portmanteau theorem (Theorem \ref{thm:portmanteau}), we have that for closed set $B^c_{\ve}(c)$,
\be
\limsup_{n\to \infty}\pro\brb{X_n \in B^c_{\ve}(c)} = \limsup_{n\to \infty}\mu_n\brb{B^c_{\ve}(c)} \leq \mu\brb{B^c_{\ve}(c)} = \mu\brb{c\in B^c_{\ve}(c)} = 0
\ee
which implies that
\be
\lim_{n\to \infty} \pro\brb{\abs{X_n - c} \geq \ve} = 0 \ \lra\  X_n \stackrel{p}{\to }c.
\ee
\end{proof}


\begin{proposition}\label{pro:convergence_in_probability_with_convergence_in_distribution_implies_convergence_in_distribution}
Let $X_n,Y_n$ be random variable sequence and $X$ be random variable such that
\be
\abs{Y_n - X_n} \stackrel{p}{\to}0,\qquad X_n \stackrel{d}{\to} X.
\ee

Then $Y_n \stackrel{d}{\to} X$.
\end{proposition}

\begin{proof}[\bf Proof]
Consider any bounded function $f$ (i.e., $\abs{f(x)} \leq M$) which is also Lipschitz continuous: $\exists K>0,\quad \forall x,y$,
\be
\abs{f(x) -f(y)} \leq K\abs{x-y}.
\ee

For any $\ve>0$ we have
\beast
\abs{\E f(Y_n) - \E f(X_n)} & \leq & \E\abs{f(Y_n) - f(X_n)} \\
& = & \E\brb{\abs{f(Y_n) - f(X_n)} \ind_{\bra{\abs{Y_n - X_n}<\ve}}} + \E\brb{\abs{f(Y_n) - f(X_n)} \ind_{\bra{\abs{Y_n - X_n}\geq \ve}}} \\
& \leq & \E\brb{K\abs{Y_n - X_n} \ind_{\bra{\abs{Y_n - X_n}<\ve}}} + \E\brb{2M\ind_{\bra{\abs{Y_n - X_n}\geq \ve}}} \\
& \leq & K\ve \pro\brb{\abs{Y_n - X_n}<\ve} + 2M\pro\brb{\abs{Y_n - X_n}\geq \ve} \\
& \leq & K\ve + 2M\pro\brb{\abs{Y_n - X_n}\geq \ve}.
\eeast

Therefore,
\beast
\abs{\E f(Y_n) - \E f(X)} & \leq &  \abs{\E f(Y_n) - \E f(X_n)} + \abs{\E f(X_n) - \E f(X)} \\
& \leq & K\ve + 2M\pro\brb{\abs{Y_n - X_n}\geq \ve} + \abs{\E f(X_n) - \E f(X)}.
\eeast

Then since $\abs{Y_n - X_n} \stackrel{p}{\to}0$ and $X_n \stackrel{d}{\to} X$ (so $\lim_{n\to \infty}\abs{\E f(X_n) - \E f(X)}$ by Portmanteau theorem (Theorem \ref{thm:portmanteau}),
\beast
\lim_{n\to\infty}\abs{\E f(Y_n) - \E f(X)} & \leq & K\ve +  2M \lim_{n\to \infty}\pro\brb{\abs{Y_n - X_n}\geq \ve} + \lim_{n\to \infty}\abs{\E f(X_n) - \E f(X)}\\
& = & K\ve + 0 + 0 = K\ve.
\eeast

Since $\ve$ was arbitrary, we can conclude that the limit must in fact be equal to zero, and therefore, $\E f(Y_n) \to \E f(X)$, which again by Portmanteau theorem (Theorem \ref{thm:portmanteau}) imples that $Y_n$ converges to $X$ in distribution.
\end{proof}

\begin{proposition}\label{pro:convergence_in_distribution_and_constant_in_probability_implies_convergence_in_distribution}
Let $X_n,Y_n$ be two sequences of random variables, $X$ a random variable and $c$ a constant.

If $X_n \stackrel{d}{\to }X$ and $Y_n \stackrel{d}{\to}c$, then $X_nY_n \stackrel{d}{\to} cX$.
\end{proposition}

\begin{remark}
This is a special case of Slutsky theorem (Theorem \ref{thm:slutsky_convergence_in_distribution}).
\end{remark}

\begin{proof}[\bf Proof]
First, $Y_n \stackrel{d}{\to}c$ implies that $Y_n \stackrel{p}{\to}c$ by Proposition \ref{pro:convergence_in_distribution_constant_implies_convergence_in_probability}.

Consider $\mu_n = \pro\circ X_n^{-1}$ and $\mu = \pro\circ X^{-1}$. Then $\forall \ve,\delta>0$
\be
\pro\brb{\abs{X_n(Y_n - c)}\geq \ve} \leq \pro\brb{\abs{X_n} \geq \delta} + \pro\brb{\abs{Y_n-c} \geq \ve/\delta} = \mu_n\brb{B^c_\delta(0)} + \pro\brb{\abs{Y_n-c} \geq \ve/\delta}.
\ee

Then \footnote{subadditivity of limsup needed.}
\beast
\limsup_{n\to \infty}\pro\brb{\abs{X_n(Y_n - c)}\geq \ve} & \leq & \limsup_{n\to\infty}\mu_n\brb{B^c_\delta(0)} + \limsup_{n\to\infty}\pro\brb{\abs{Y_n-c} \geq \ve/\delta}\\
& \leq & \mu\brb{B^c_\delta(0)} + \limsup_{n\to\infty}\pro\brb{\abs{Y_n-c} \geq \ve/\delta} \\
& = & \mu\brb{B^c_\delta(0)} = \pro\brb{\abs{X}\geq \delta}
\eeast
by Portmanteau theorem (Theorem \ref{thm:portmanteau}) since $B^c_\delta(0)$ is a closed set and $Y_n \stackrel{p}{\to}c$. Since $\delta$ was also arbitrary, we have that $\pro\brb{\abs{X}\geq \delta} = 0$ and thus $\limsup_{n\to \infty}\pro\brb{\abs{X_n(Y_n - c)}\geq \ve} = 0$ which implies that $X_n(Y_n - c) \stackrel{p}{\to} 0$.

Then for every bounded $\abs{f(x)}\leq M$ Lipschitz continuous function ($\exists K>0,\quad \forall x,y$, $\abs{f(x) -f(y)} \leq K\abs{x-y}$)
\beast
\abs{\E f(X_nY_n) - \E f(cX)} & \leq & \E \abs{f(X_nY_n) - f(cX_n)} + \abs{\E f(cX_n) - \E f(cX)} \\
& \leq & \E\brb{K\abs{X_n(Y_n - c)} \ind_{\bra{\abs{X_n(Y_n - c)} <\ve}}} + \E\brb{2M\ind_{\bra{\abs{X_n(Y_n - c)}\geq \ve}}} + \abs{\E f(cX_n) - \E f(cX)} \\
& \leq & K \ve \pro\brb{ \abs{X_n(Y_n -  c)}<\ve} + 2M \pro\brb{ \abs{X_n(Y_n -  c)}\geq \ve} + \abs{\E f(cX_n) - \E f(cX)}\\
& \leq & K \ve + 2M \pro\brb{ \abs{X_n(Y_n -  c)}\geq \ve} + \abs{\E f(cX_n) - \E f(cX)}
\eeast

Then
\beast
\limsup_{n\to \infty}\abs{\E f(X_nY_n) - \E f(cX)} & \leq & K \ve + 2M \limsup_{n\to \infty}\pro\brb{ \abs{X_n(Y_n -  c)}\geq \ve} + \limsup_{n\to \infty}\abs{\E f(cX_n) - \E f(cX)} \\
& = & K\ve + 2M \cdot 0 + 0 = K\ve.
\eeast

Note that the last term is due to $X_n\stackrel{d}{\to} X$ ($f\circ c$ is also bounded Lipschitz function). Furthermore, since $\ve$ was arbitrary, we can conclude that the limit must in fact be equal to zero, and therefore, $\E f(X_nY_n) \to \E f(cX)$, which again by Portmanteau theorem (Theorem \ref{thm:portmanteau}) imples that $X_nY_n$ converges to $cX$ in distribution.
\end{proof}

Generally, $X_n \stackrel{d}{\to }X$ and $Y_n \stackrel{d}{\to }Y$ do not imply $(X_n,Y_n) \stackrel{d}{\to }(X,Y)$. However, this is the case when $Y = c$ for constant $c$.


\begin{proposition}[joint convergence in distribution\index{convergence in distribution!joint}]\label{pro:joint_convergence_in_distribution_constant}
Let $X_n,Y_n$ be two sequences of random variables with $X_n \stackrel{d}{\to }X$ and $Y_n \stackrel{d}{\to}c$ where $X$ a random variable and $c$ a constant. Then
\be
\brb{X_n, Y_n} \stackrel{d}{\to} \brb{X,c}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]%\footnote{proof needed. see van der Vaart, A.W. (2000), Asymptotic Statistics, Cambridge University Press.}
First note that for any $\ve>0$,
\be
d\brb{(X_n,Y_n) - (X_n,c)} \leq d\brb{X_n,X_n} + d\brb{Y_n,c} = d\brb{Y_n,c}
\ee
and thus
\be
\pro\brb{d\brb{(X_n,Y_n), (X_n,c)}\geq \ve} \leq \pro\brb{d\brb{Y_n, c}\geq \ve} \to 0 \ \ra\  d\brb{(X_n,Y_n), (X_n,c)} \stackrel{p}{\to} 0.
\ee

Thus, according to Proposition \ref{pro:convergence_in_probability_with_convergence_in_distribution_implies_convergence_in_distribution},
it suffices to show that $\brb{X_n,c}\stackrel{d}{\to}(X,c)$. For every bounded continuous function $f: (x,y)\mapsto f(x,y)$, the function $g:x\mapsto f(x,c)$ is continuous and bounded. Thus since $X_n \stackrel{d}{\to} X$,
\be
\E f\brb{X_n,c} = \E\brb{g(X_n)} \to \E\brb{g(X)} = \E f(X,c) \ \ra\ \brb{X_n,c}\stackrel{d}{\to}(X,c)
\ee
as required.
\end{proof}





\subsection{Continuous mapping theorem}


\begin{theorem}[continuous mapping theorem\index{continuous mapping theorem}]\label{thm:continuous_mapping_probability}
Let $X_n,X$ be random elements defined on a metric space $\sS$. Suppose a function $g: \sS\to \sS'$ (where $\sS'$ is another metric space) has the set of discontinuity points $\sD_g$ such that $\pro\brb{X\in \sD_g} = 0$. Then
\ben
\item [(i)]  $X_{n} \stackrel{a.s.}{\to} X \ \ra\  g(X_{n})\stackrel{a.s.}{\to} g(X)$.
\item [(ii)] $X_{n} \stackrel{p}{\to} X \ \ra\  g(X_{n})\stackrel{p}{\to} g(X)$.
\item [(iii)] $X_{n} \stackrel{d}{\to} X \ \ra\  g(X_{n})\stackrel{d}{\to} g(X)$.
\een
\end{theorem}

\begin{proof}[\bf Proof]
Spaces $\sS$ and $\sS'$ are equipped with certain metrics. For simplicity we will denote both of these metrics using the $|x-y|$ notation, even though the metrics may be arbitrary and not necessarily Euclidean.
\ben
\item [(i)] {\bf convergence almost surely.} By definition of the continuity of the function $g$,
\be
\lim _{{n\to \infty }}X_{n}(\omega )=X(\omega )\ \ra\ \lim _{{n\to \infty }}g(X_{n}(\omega ))=g(X(\omega ))
\ee
at each point $X(\omega)$ where $g$ is continuous. Therefore
\beast
\pro\brb{\lim _{n\to \infty }g(X_{n})=g(X)} &\geq & \pro\brb{\lim _{n\to \infty }g(X_{n})=g(X),\ X\notin \sD_{g}}\\
& \geq & \pro\brb{\lim _{n\to \infty }X_{n}=X,\ X\notin \sD_{g}}=1
\eeast
because the intersection of two almost sure events is almost sure. By definition, we conclude that $g(X_n)$ converges to $g(X)$ almost surely.

\item [(ii)] {\bf Convergence in probability.} Fix an arbitrary $\ve > 0$. Then for any $\delta > 0$ consider the set $B_{\delta}$ defined as
\be
B_\delta = \big\{x\in S : x\notin D_g, \exists y\in S:\ |x-y|<\delta,\, |g(x)-g(y)|>\varepsilon\big\}.
\ee

This is the set of continuity points $x$ of the function $g$ for which it is possible to find, within the $\delta$-neighborhood of $x$, a point which maps outside the $\ve$-neighborhood of $g(x)$. By definition of continuity, this set shrinks as $\delta$ goes to zero, so that
\be
\lim_{\delta \to 0}  B_{\delta} = \emptyset.
\ee

Now suppose that $\abs{g(X) - g(X_n)} > \ve$. This implies that at least one of the following is true: either $\abs{X-X_n} \geq \delta$, or $X \in \sD_g$, or $X\in B_{\delta}$. In terms of probabilities this can be written as
\be
\pro\brb{\abs{g(X_{n})-g(X)}\geq \ve} \leq \pro\brb{\abs{X_{n}-X}\geq \delta } + \pro\brb{X\in B_{\delta}}+\pro\brb{X\in \sD_{g}}.
\ee

On the right-hand side, the first term converges to zero as $n \to \infty$ for any fixed $\delta$, by the definition of convergence in probability of the sequence $\bra{X_n}$. The second term converges to zero as $\delta \to 0$, since the set $B_\delta$ shrinks to an empty set. And the last term is identically equal to zero by assumption of the theorem. Therefore the conclusion is that
\be
\lim_{n\to\infty}\pro\brb{\abs{g(X_n)-g(X)}\geq \varepsilon} = 0,
\ee
which means that $g(X_n)$ converges to $g(X)$ in probability.

\item [(iii)] {\bf Convergence in distribution.} We will need a particular statement from the Portmanteau theorem (Theorem \ref{thm:portmanteau}) that convergence in distribution $X_{n}\stackrel{d}{\to} X$ is equivalent to
\be
\limsup _{{n\to \infty }}\pro\brb{X_{n}\in F}\leq \pro\brb{X\in F}\quad \text{ for every closed set }F.
\ee

Fix an arbitrary closed set $F\subseteq S'$. Denote by $g^{-1}(F)$ the pre-image of $F$ under the mapping $g$: the set of all points $x \in S$ such that $g(x)\in F$. Consider a sequence $\bra{x_k}$ such that $g(x_k) \in F$ and $x_k \to x$. Then this sequence lies in $g^{-1}(F)$, and its limit point $x$ belongs to the closure of this set, $\ol{g^{-1}(F)}$ (by definition of the closure). The point $x$ may be either:
\bit
\item a continuity point of $g$, in which case $g(x_k) \to g(x)$, and hence $g(x)\in F$ because $F$ is a closed set, and therefore in this case $x$ belongs to the pre-image of $F$,
\item a discontinuity point of $g$, so that $x \in \sD_g$.
\eit

Thus the following relationship holds:
\be
\overline{g^{{-1}}(F)}\ \subseteq \ g^{{-1}}(F)\cup \sD_{g} .
\ee

Consider the event $\bra{g(X_n)\in F}$. The probability of this event can be estimated as
\be
\pro\brb{g(X_{n})\in F}=\pro\brb{X_{n}\in g^{{-1}}(F)}\leq \pro\brb{X_{n}\in \overline{g^{{-1}}(F)}},
\ee
and by the Portmanteau theorem (Theorem \ref{thm:portmanteau}) the $\limsup$ of the last expression is less than or equal to $\pro\brb{X \in g^{-1}(F)}$. Using the formula we derived in the previous paragraph, this can be written as
\beast
\pro \brb{X\in {\overline {g^{-1}(F)}}} & \leq &\pro\brb{X\in g^{-1}(F)\cup \sD_{g}} \\
& \leq & \pro\brb{X\in g^{-1}(F)}+\pro\brb{X\in \sD_{g}}=\pro\brb{g(X)\in F}+0.
\eeast

On plugging this back into the original expression, it can be seen that
\be
\limsup_{n\to\infty} \pro\brb{g(X_n)\in F}\leq \pro\brb{g(X) \in F},
\ee
which, by Portmanteau theorem (Theorem \ref{thm:portmanteau}), implies that $g(X_n)$ converges to $g(X)$ in distribution.
\een
\end{proof}


\begin{theorem}[Slutsky's theorem\index{Slutsky's theorem}]\label{thm:slutsky_convergence_in_distribution}
Let $X_n,Y_n$ be two sequences of random variables, $X$ a random variable and $c$ a constant. Let $X_n \stackrel{d}{\to }X$ and $Y_n \stackrel{d}{\to}c$. Then for a continuous function $g(x,y)$, we have
\be
g\brb{X_n,Y_n} \stackrel{d}{\to} g(X,c).
\ee
\end{theorem}

\begin{remark}
The following conclusions are the special cases.
\ben
\item [(i)] $X_n + Y_n \stackrel{d}{\to} X+c$.
\item [(ii)] $X_nY_n \stackrel{d}{\to} cX$.
\item [(iii)] $X_n/Y_n \stackrel{d}{\to} X/c$, provided that $c$ is invertible.
\een
\end{remark}

\begin{proof}[\bf Proof]
The couple $(X_n,Y_n)$ is jointly convergent in distribution to $(X,c)$ by the joint convergence proposition (Proposition \ref{pro:joint_convergence_in_distribution_constant}). Therefore, by continuous mapping theorem (Theorem \ref{thm:continuous_mapping_probability}), the fact that $g(x,y)$ is continuous implies that $g(X_n,Y_n)$ converges in distribution to $g(X,c)$.
\end{proof}





\subsection{Tightness and \levy's convergence theorem}

Recall Definition \ref{def:tight_measure},

\begin{definition}\label{def:tight_probability}
Let $(\mu_i)_{i\in I}$ be a family of probability measures on $(E, d)$. We say that $(\mu_i)_{i\in I}$ is tight if for all $\ve > 0$ there is $K \subseteq E$ compact such that $\sup\limits_{i\in I} \mu_i(E \bs K) < \ve$.
\end{definition}

\begin{theorem}[Prokhorov's Theorem]\label{thm:prokhorov_probability}
Let $(\mu_n, n \geq 0)$ be probability measures on $(E, d)$. %\ben\item [(i)] If $\mu_n \stackrel{w}{\to} \mu$ for some probability measure $\mu$, then $(\mu_n, n \geq 0)$ is tight.\item [(ii)]
If $(\mu_n, n \geq 0)$ is tight. Then there is a subsequence $(\mu_{n_k} , k \geq 0)$ and a probability measure $\mu$ such that $\mu_{n_k} \stackrel{w}{\to} \mu$ as $k \to \infty$.
%\een
\end{theorem}

To motivate this theorem, notice that for $(\delta_{x_n})$, tightness is exactly the requirement that the $x_n$ are contained in a compact set, so Prokhorov's Theorem may (loosely) be seen as a generalization of the Bolzano-Weierstrass Theorem.

\begin{proof}[\bf Proof]
We prove the case $E = \R$. Consider $(F_n (q), n \geq 0) \subseteq [0, 1]$ for $q$ a rational number. There exists a subsequence $n^q_k$ such that $F_{n^q_k}(q)\to  F(q)$ (i.e. the subsequence has a limit in $[0, 1]$ since it is bounded by Bolzano-Weierstrass theorem (Theorem \ref{thm:bolzano_weierstrass_r})). Since $\Q$ is countable, by a diagonal procedure\footnote{need to check}, we can find $(n_k, k \geq 0)$ such that $F_{n_k}(q)\to F(q)$ for all $q \in \Q$. Then $F$ is non-decreasing on $\Q$, so extend it to $\R$ via $F(t) = \lim\limits_{q\da t,q\in \Q} F(q)$ and $F$ is non-decreasing on $\R$. Thus, $F$ is \cadlag (it is right-continuous by definition of $F(t)$ and its left limit exists since $F$ is bounded and non-decreasing.)

Check that for every continuity point $t$ of $F$, $F_{n_k} (t) \to  F(t)$ as $k \to \infty$. $\forall \ve > 0$, and pick $\eta > 0$ such that $F(t + \eta) \leq F(t) + \ve$ and $t + \eta \in \Q$ (since $F$ is countinous at point $t$). Then $F_{n_k} (t +\eta)\to  F(t +\eta) \leq F(t)+\ve$. Thus, since $\limsup F_{n_k}(q) = \lim F_{n_k}(q)$ for all $q\in \Q$,
\beast
\limsup_k F_{n_k} (t) \leq \limsup_k F_{n_k} (t +\eta) = F(t +\eta) \leq F(t)+\ve\quad\ra\quad \limsup F_{n_k} (t) \leq F(t),
\eeast%{def:infimum}
by definition of infimum (Definition \ref{def:limsup_liminf_real}). Similarly we get that $F(t) \leq \liminf F_{n_k}(t)$. Thus, $\lim\limits_{k\to \infty} F_{n_k}(t) \to F(t),\ (*)$ by Definition \ref{def:limsup_liminf_real}.%{def:number_limsup_liminf}.

We want to show that $F$ is the corresponding distribution function of some probability measure $\mu$. It is sufficient to check that $F(t) \to  0, 1$ as $t \to  -\infty,+\infty$ (since we alrealy know that $F$ is non-decreasing and \cadlag).

By tightness given $\ve$, we can find compact set $K = [A, B]$ such that $\mu_n((B,\infty)) < \ve - \mu_n((-\infty,A))$ for all $n \geq 0$. But $\ve > \mu_{n_k} ((B,\infty)) = 1- F_{n_k} (B)$ and $\ve > \mu_{n_k} ((-\infty,A)) = F_{n_k} (A^-)$. Then we can pick continuous points $A'$ and $B'$ of $F$ such that $A'<A<B<B'$. Thus, by $(*)$,
\be
\ve > 1- F_{n_k} (B) > 1- F_{n_k} (B') \to 1 - F(B'),\quad\quad \ve > F_{n_k} (A^-) \geq F_{n_k}(A') \to F(A').
\ee
as required. Therefore, $F_{n_k} \to  F$ at every continuity point of $F$. Then by Theorem \ref{thm:weak_convergence_iff_convergence_in_distribution}, $\mu_{n_k} \to \mu$ weakly.
\end{proof}


\begin{lemma}\label{lem:levy_convergence_lemma}
Let $X$ be a random variable with values in $\R^n$. Then for any norm $\dabs{\cdot}$ on $\R^n$, there exists a constant $C>0$ (depending on $n$ and on the choice of the norm but not depending on $X$) such that
\be
\pro\brb{\dabs{X}\geq K} \leq CK^n \int_{\bsb{0,\frac 1K}^n} (1-\Re \phi_X(u))du.
\ee
\end{lemma}

\begin{proof}[\bf Proof]
Indeed, $\Re\phi_X (u) = \Re \E\brb{e^{i\inner{u}{X}}} = \E\brb{\cos(uX)}$. By Fubini's Theorem,
\beast
K^n \int_{\bsb{0,\frac 1K}^n} (1-\Re\phi_X (u))du & = & \E\brb{1- K^n\int_{\bsb{0,\frac 1K}^n} \prod^n_{i=1} \cos (u_iX_i) du } = \E\brb{1-\prod^n_{i=1}\frac{\sin( \frac {X_i}K )}{\frac {X_i}K}}.\quad\quad (*)
\eeast

Define the norm $\dabs{u} = \sup_{1\leq i\leq n}\abs{u_i}$, $u\in \R^n$. Then the continuous function $\sinc: t\in \R\to \frac 1t \sin t$ is such that there exists $0<c<1$ such that $\abs{\sinc t} \leq c$ for every $t\geq 1$, so that function $f:u\in \R^d \mapsto \prod^n_{i=1}\sin u_i/u_i$ satisfies that $\abs{f(u)}\leq c$ if any $u_i\geq 1$, i.e., we need $\dabs{u}\geq 1$ to have that $1-\abs{f(u)}\geq 1-c$. Then take $C = (1-c)^{-1}$, we have $\ind_{\dabs{u}\geq 1} \leq C(1- f(u))$. Then let $u = \frac XK$, since $\ind_{\dabs{u}\geq 1}$ and $C(1- f(u))$ are non-negative, by Theorem \ref{thm:non_negative_measurable_property}, we have
\be
\pro(\dabs{X} \geq K) = \E\brb{\ind_{\dabs{\frac XK}\geq1}} \leq C \E\brb{1-\frac{\sin( \frac XK )}{\frac XK}} \stackrel{(*)}{=} CK^n \int_{\bsb{0,\frac 1K}^n} (1-\Re\phi_X (u))du.
\ee

Then this result holds for any norm on $\R^n$ by the equivalence of norms in finite-dimensional vector spaces. Note that $C$ does not depend on the law of $X$.
\end{proof}

\begin{theorem}[\levy's continuity theorem\index{\levy's continuity theorem}, \levy's convergence theorem\index{\levy's convergence theorem}]\label{thm:levy_continuity}
\ben
\item [(i)] Let $(X_n)_{n \in \N}$, $X$ be r.v.'s such that $X_n \stackrel{d}{\to} X$. Then $\phi_{X_n} (u) \to  \phi_X (u)$ for all $u \in \R^d$.
\item [(ii)] Let $(X_n)_{n \in \N}$ be r.v.'s such that $\phi_{X_n} (u) \to \phi(u)$ for all $u \in \R^d$ for some function $\phi : \R^d \to\C$ that is continuous at 0. There is a r.v. $X$ such that $\phi = \phi_X$ and $X_n\stackrel{d}{\to} X$.
\een
\end{theorem}

\begin{remark}
In probability theory, \levy��s continuity theorem, named after the French mathematician Paul \levy, connects convergence in distribution of the sequence of random variables with pointwise convergence of their characteristic functions. An alternative name sometimes used is \levy��s convergence theorem (see \cite{Williams_1991}, sec 18.1).

Note that there a Laplace transform version of \levy's continuity theorem in \cite{Feller_1948}\footnote{details needed.}.
\end{remark}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Recall Corollary \ref{cor:convergence_in_distribution_iff_expectation}, $X_n \stackrel{d}{\to} X$ if and only if $\E[ f (X_n)]\to \E[ f (X)]$ for all $f \in C_b(\R^d )$. Since, $e^{i\inner{u}{X_n}}\in C_b(\R^d)$,
\be
\phi_{X_n}(u) = \E \brb{e^{i\inner{u}{X_n}}} \to \E \brb{e^{i\inner{u}{X}}}= \phi_X(u)\quad \forall u \in \R^d.
\ee

\item [(ii)] From Lemma \ref{lem:levy_convergence_lemma}, we know that for any norm on $\R^d$, there exists a universal constant $C > 0$ such that for all r.v. $X_n$ taking values in $\R^d$,
\be
\pro(\dabs{X_n} \geq K) \leq CK^d \int_{\bsb{0,\frac 1K}^d} (1-\Re\phi_{X_n} (u))du.
\ee

Thus, since $\phi_{X_n} (u) \to  \phi(u)$, $\Re \phi_{X_n} (u) \to  \Re \phi(u)$ and $1-\Re \phi_{X_n}(u)$ is bounded. By dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}),
\be
\pro(\dabs{X_n} \geq K) \leq CK^d \int_{\bsb{0,\frac 1K}^d} (1-\Re\phi_{X_n} (u))du \to CK^d \int_{\bsb{0,\frac 1K}^d} (1-\Re\phi (u))du
\ee

Since $\phi$ is continuous at zero and $\phi(0) = 1$, for any $\ve > 0$ there is $K$ large enough such that $\abs{1-\Re\phi(u)} \leq \frac{\ve}{2C}$ for any $u\in [0, \frac 1K ]^d$. Thus, for such $K$,
\be
\limsup_n\pro(\dabs{X_n} \geq K) \leq \frac{\ve}2.
\ee

Thus there is $N_\ve$ such that for all $n > N_\ve$, $\pro(\dabs{X_n} \geq K) \leq \ve$. Up to taking $K$ even larger, we may assume that
\be
\sup_{1\leq n\leq N_\ve} \pro(\dabs{X_n} \geq K) \leq \ve.
\ee

Now for such $K$, $\sup_n \pro(\dabs{X_n} \geq K) \leq \ve$ (for all $n$), but we know that $\bra{\sup_{1\leq i\leq d}X_n^i \geq K} = \bra{X_n \in (-K,K)^d}^c$
\be
\sup_n \pro(\dabs{X_n} \geq K) \leq \ve\quad \lra\quad \sup_n \mu_n\brb{\bra{(-K, K)^d}^c} \leq \ve \quad\ra\quad \sup_n \mu_n\brb{\bra{[-K, K]^d}^c}\leq \ve
\ee
where $\mu_n$ is the law of $X_n$. Since $[-K, K]^d$ is a compact set, then $\brb{\mu_n,n\geq1}$ is a tight family of measures.

By Prokhorov's theorem (Theorem \ref{thm:prokhorov_probability}), there is a subsequence such that $\mu_{n_k} \stackrel{w}{\to} \mu$, for some probability measure $\mu$ on $\R$. Hence $X_{n_k} \stackrel{d}{\to} X$ for some r.v. $X$ (by Theorem \ref{thm:weak_convergence_iff_convergence_in_distribution}).

But by part (i), $\phi_{X_{n_k}} \to \phi_X$ pointwise, so $\phi = \phi_X$ is the characteristic function of a r.v. Let us assume that $X_n$ does not converge in distribution to $X$. Then we could find $f \in C_b(\R)$ such that $\E[ f (X_n)]$ does not converge to $\E[ f (X)]$ (by Corollary \ref{cor:convergence_in_distribution_iff_expectation}). Thus we could find an extraction $n_k$ and $\ve > 0$ such that $\abs{\E\brb{ f (X_{n_k})} - \E\brb{ f (X)}} \geq \ve$.

But $(\mu_{n_k}, k \geq 1)$ is tight (since it is contained in a tight family), so we can further extract a subsequence such that $X_{n_{k_r}} \to X'$ in distribution for some r.v. $X'$. Similarly, $\phi_X = \phi_{X'}$, so $X$ and $X'$ have the same distribution (by Theorem \ref{thm:inversion_density}), i.e., $\mu_X = \mu_{X'}$, Then $\E\brb{ f(X_{n_{k_r}})}\to \E\brb{f(X')} = \mu_{X'}(f)=\mu_X(f) = \E[ f (X)]$ by Theorem \ref{thm:image_measure_probability} since $f\in C_b(\R)$. Contradiction.
\een
\end{proof}

Now combine Theorem \ref{thm:weak_convergence_iff_convergence_in_distribution}, \ref{thm:skorokhod_representation} and \ref{thm:levy_continuity}, we have:

\begin{theorem}[equivalent modes of convergence]\label{thm:equivalent_modes_of_convergence}
Suppose we are given a sequence of Borel probability measures $(\mu_n)_{n \in \N}$ on $\R$ and a further such measure $\mu$. Write $F_n$ and $F$ for the corresponding distribution functions, \be F_n(x) = \mu_n ((-\infty,x])
,\quad\quad F(x) = \mu ((-\infty,x]) \ee and write $\phi_n$ and $\phi$ for the corresponding Fourier transforms (Definition \ref{def:fourier_transform_borel}) \be \phi_n(u) = \int e^{iux}\mu_n(dx),\quad\quad \phi(u) = \int
e^{iux}\mu(dx). \ee

On the probability space $([0, 1),\sB([0, 1)), dx)$ (with Lebesgue measure), define random variables
\be
X_n(\omega) = \inf\{x \in \R : \omega \leq F_n(x)\},\quad\quad X(\omega) = \inf\{x \in \R : \omega \leq F(x)\},
\ee
Recall that $X_n \sim \mu_n$ and $X \sim \mu$. The following are equivalent:
\ben
\item [(i)] $\mu_n \to \mu$ weakly,
\item [(ii)] $F_n \to F$ as distribution functions,
\item [(iii)] $\phi_n(u) \to \phi(u)$ for all $u \in \R$,
\item [(iv)] $X_n \to X$ a.s.
\een
\end{theorem}

%\begin{proof}[\bf Proof]
%Recall Definition \ref{def:fourier_transform_borel},
%\end{proof}

%We do not prove this result in full, but note how to see certain of the implications. First, (iii) is a special case of (i), and (iv) implies (i) by bounded convergence, using $\mu_n(f) = \E(f(X_n)) \to \E(f(X)) = \mu(f)$. A proof that (ii) implies (iv) follows, with some care, from the definition of $X_n$ and $X$. The fact that (iii) implies (ii) is a consequence of the following famous result, which we do not prove here.

Furthermore, we have special version continuity theorem for Laplace transform.

\begin{theorem}[continuity theorem for Laplace transform]\label{thm:levy_continuity_laplace}
For a probability distribution $F$ of $X\in [0,\infty)$, the Laplace transform of $F$ is
\be
\sL(s) = \int^\infty_0 e^{-sx}F(dx)
\ee
for $s\geq 0$. Let $F_n$ be probability distributions of $X_n\in [0,\infty)$ with Laplace transform $\sL_n(s)$.

If $X_n \to X$ in distribution where $F$ is possibly defective, then $\sL_n(s) \to \sL(s)$ for $s>0$.

If $\sL_n(s)$ converges for each $s>0$ to a limit $\vp(s)$, then $\vp(s)$ is the transform of a possibly defective distribution $F$ of random variable $X$, say $\sL(s)$, and $X_n\to X$ in distribution.

Note that $F$ is not defective iff $\sL(s) \to 1$ as $s \to 0$.
\end{theorem}

\begin{proof}[\bf Proof]
\footnote{proof needed. proof needed. see \cite{Feller_1968_v2}.$P_{431}$.}
\end{proof}

In general, $X_n\stackrel{d}{\to} X$ and $Y_n\stackrel{d}{\to} Y$ does not imply that
\be
\brb{X_n,Y_n} \stackrel{d}{\to} (X,Y).
\ee

However, if $X_n$ and $Y_n$ are independent, the joint sequence converges in distribution.

\begin{theorem}\label{thm:joint_distribution_of_independent_sequences_converges_in_distribution}
Let $X_n$ and $Y_n$ be independent sequences and $X_n \stackrel{d}{\to} X$ and $Y_n \stackrel{d}{\to} Y$ where $X$ and $Y$ are random variables. Then there exist independent random variables $U$ and $V$ such that $U\sim X$ and $V\sim Y$ and%$X_n+Y_n \stackrel{d}{\to} X + Y$.
\be
\brb{X_n,Y_n}\stackrel{d}{\to} (U,V).
\ee
\end{theorem}

\begin{proof}[\bf Proof]
For $X_n \stackrel{d}{\to} X$ and $Y_n \stackrel{d}{\to} Y$, we can always find\footnote{theorem needed. Heinz Bauer, Probability Theory, De Gruyter, 1996, Theorem 9.5.} independent random variables $U$ and $V$ such that
\be
X_n\stackrel{d}{\to}U,\qquad Y_n\stackrel{d}{\to}V,\qquad X\sim U\qquad Y\sim V.
\ee

Thus, $\phi_{X_n}(t) \to \phi_U(t)$ and $\phi_{Y_n}(t) \to \phi_V(t)$ for all $t\in \R$ by \levy's continuity theorem (Theorem \ref{thm:levy_continuity}).

Also, since $X_n$ and $Y_n$ are independent, we have $Z_n := (X_n,Y_n)^T$ and $t = (t_1,t_2)^T$ for any $t_1,t_2\in \R$,
\be
\phi_{Z_n}(t) = \E\brb{e^{i\inner{t}{Z_n}}} = \E\brb{e^{i\brb{t_1X_n + t_2Y_n}}} = \E\brb{e^{it_1X_n}}\E\brb{e^{it_2Y_n}} = \phi_{X_n}(t_1)\phi_{Y_n}(t_2)
\ee
by Theorem \ref{thm:characteristic_function_independence}. Therefore, for $W = (U,V)$,
\be
\phi_{Z_n}(t) \to \phi_{U}(t_1)\phi_{V}(t_2) = \E\brb{e^{it_1 U}}\E\brb{e^{it_1 V}} = \E\brb{e^{i\brb{t_1 U + t_2 V}}} = \E\brb{e^{i\inner{t}{W}}} = \phi_W(t)
\ee
since $U$ and $V$ are independent (see Theorem \ref{thm:characteristic_function_independence}). $\phi_W(t)$ is a characteristic function, so it is continuous at 0\footnote{proposition needed.}. Thus, we have that
\be
(X_n,Y_n) = Z_n \stackrel{d}{\to} W = (U,V)
\ee
by \levy's continuity theorem (Theorem \ref{thm:levy_continuity}).
\end{proof}


\begin{corollary}\label{cor:independent_sequences_continuous_mapping_converges_in_distribution_independent_limits}
Let $X_n,Y_n$ be two independent sequences of random variables with $X_n \stackrel{d}{\to }X$ and $Y_n \stackrel{d}{\to} Y$ where $X,Y$ are random variable. Then for a continuous function $g(x,y)$, we have
\be
g\brb{X_n,Y_n} \stackrel{d}{\to} g(U,V)
\ee
where $U$ and $V$ are independent random variables with $U\sim X$ and $V \sim Y$.
\end{corollary}

\begin{proof}[\bf Proof]
Direct result of Theorem \ref{thm:joint_distribution_of_independent_sequences_converges_in_distribution} and continuous mapping theorem (Theorem \ref{thm:continuous_mapping_probability}).
\end{proof}

%--------------------

\section{Discrete Random Variables}

\subsection{Discrete random variable and probability generating function}

\begin{definition}[discrete random variable\index{random variable!discrete}, discrete probability distribution\index{probability distribution!discrete}, probability mass function\index{probability mass function}]\label{def:random_variable_discrete}
A discrete probability distribution shall be understood as a probability distribution characterized by a probability mass function. Thus, the distribution of a random variable $X$ is discrete, and X is then called a discrete random variable, if
\be
\sum_{u_k} \pro(X=u_k) = 1
\ee
as $u_k$ ($\bra{u_k}_k$ is countable) runs through the set of all possible values of $X$. $\pro(X=u_k)$ is called probability mass function. It follows that such a random variable can assume only a countable (finite or countably infinite) number of values.
\end{definition}

%Terminology.

\begin{remark}
If the probability distribution of $X$ is a standard distribution such as the binomial distribution (or Poisson, or geometric), we say that $X$ is a binomial (respectively, Poisson, or geometric) random variable.
\end{remark}

\begin{example}
Suppose that two standard dice are rolled so that the sample space is $\Omega = \bra{(i, j) : 1 \leq i, j \leq 6}$, and we are interested in the sum of the numbers shown so that the random variable $X : \Omega\to \R$ is given by $X(i, j) = i + j$. The probability of each point in $\Omega$ is $\frac 1{36}$ with the set of possible values taken on by $X$ being $\Omega_X = \{2,3,\dots,12\}$ and, for example,
\be
\pro(X = 6) = \pro (\{(1, 5), (2, 4), (3, 3), (4, 2), (5, 1)\}) = \frac{5}{36}.
\ee
If we set $p_j = \pro(X = j)$, for $j = 2,\dots, 12$, then the table
\begin{center}
\begin{tabular}{ccccccccccccc}
$j$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12\\
\hline
$p_j$ & $\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ & $\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ & $\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$ \\
\end{tabular}
\end{center}
gives the full probability distribution of the random variable $X$.
\end{example}

\begin{definition}[probability generating function\index{probability generating function}]\label{def:pgf_probability}
If $X$ is a discrete random variable taking values in the non-negative integers $\bra{0,1, \dots}$, then the probability generating function of $X$ is defined by a continuous function\footnote{need to prove it's continuous}

\be G_X(z) = \E \brb{z^X} = \sum_{k=0}^{\infty}\pro(X=k)z^k = \sum_{k=0}^\infty p_k z^k,\quad z\in [0,1]. \ee
\end{definition}

\begin{remark}
Note that $G_X(z) \in [0,1]$ as $G_X(1) = \sum_{k=0}^{\infty}\pro(X=k) = 1$.
\end{remark}

\begin{theorem}\label{thm:pgf_unique}
The probability generating function of $X$, $G(z)$, $0 \leq z \leq 1$, determines the probability distribution of $X$ uniquely.
\end{theorem}

\begin{remark}
We can prove this by letting $\theta = \log z$ and applying Theorem \ref{thm:mgf_uniquely_determine_law}.
\end{remark}

\begin{proof}[\bf Proof]
Suppose that
\be
G(z) = \sum^\infty_{r=0} p_rz^r = \sum^\infty_{r=0} q_rz^r, \text{ for all }0 \leq z \leq 1,
\ee
where $p_r \geq 0$, and $q_r \geq 0$ for each $r$, and $\sum^\infty_{r=0} p_r = 1 = \sum^\infty_{r=0} q_r$. We will show by induction on $n$ that $p_n = q_n$ for all $n$. First see, by setting $z = 0$, that $p_0 = q_0$. Now assume that $p_i = q_i$ for $0 \leq i \leq n$, then for $0 < z < 1$
\be
\sum^\infty_{r=n+1} p_rz^r = \sum^\infty_{r=n+1} q_rz^r.
\ee

Divide through both sides by $z^{n+1}$ and let $z \downarrow 0$ (as $z^{n}$ is continuous at 0) to see that $p_{n+1} = q_{n+1}$ to complete the induction.
\end{proof}

In addition to determining the distribution uniquely, the probability generating function may be used to compute moments of the random variable by evaluating derivatives of the function.

\begin{proposition}\label{pro:independent_pgf}
Let $X_1,\dots,X_n$ be independent random variable taking values in $\bra{0,1,\dots}$. Then for $z_1,\dots,z_n \in [0,1]$,
\be
H(z) := \E\brb{\prod^n_{i=1}z_i^{X_i}} = \prod^n_{i=1}G_{X_i}(z_i).
\ee

In particular, for $X = X_1 + \dots + X_n$, we have $G_X(z) = \prod^n_{i=1}G_{X_i}(z)$.
\end{proposition}

\begin{proof}[\bf Proof]
Direct result from Proposition \ref{pro:independent_mgf} by letting $\theta_i = \log z_i$ as $H(z)$ is bounded.
\end{proof}

\begin{theorem}\label{thm:pgf_probability}
Let $X$ be a random variable with probability generating function $G(z)$, then% the mean of $X$ is
\be
\pro(X=k) = \frac{G^{(k)}(0)}{k!}.
\ee
\end{theorem}

\begin{proof}[\bf Proof]%\footnote{need proof.}
We know that $G(z) = \sum_{k=0}^\infty p_k z^k$ is convergent in $z\in [0,1]$. Then by Lemma \ref{lem:radius} and Theorem \ref{thm:power_series_differentiation}, for $z\in (0,1)$, we have%By Theorem \ref{thm:differentiation_under_integral_sign}, we can have that for $z\in (0,1)$%\footnote{need proof.}%From the definition, we have that
\be
G^{(k)}(z) = \sum^\infty_{n=k} p_n \frac{n!z^{n-k}}{(n-k)!} \ \ra \ G^{(k)}(0) = \lim_{z\da 0} G^{(k)}(z) = p_k k! \ \ra \ \pro(X=k) = p_k = \frac{G^{(k)}(0)}{k!}.
\ee
as $z^{n}$ is continuous at 0.
\end{proof}

\begin{theorem}\label{thm:pgf_moment}
Let $X$ be a random variable with probability generating function $G(z)$, then% the mean of $X$ is
\be
\E\brb{\frac{X!}{(X-k)!}} = \lim_{z\ua 1} G^{(k)}(z) = G^{(k)}(1^-).
\ee

In particular,
\be
\E X = \lim_{z\ua 1} G'(z) = G'(1^-), \qquad \var (X) = G''(1^-) + G'(1^-) - (G'(1^-))^2.
\ee
\end{theorem}

\begin{proof}[\bf Proof]%\footnote{need proof.}
First we see that $\E\brb{\frac{X!}{(X-k)!}}$ is well-defined as it is non-negative.

First assume that $\E\brb{\frac{X!}{(X-k)!}} < \infty$. For $0 < z < 1$, by Lemma \ref{lem:radius} and Theorem \ref{thm:power_series_differentiation}
%as $\frac{X!}{(X-k)!} z^{X-k}$ is non-negative and
%\be
%\E\brb{\frac{X!}{(X-k)!} z^{X-k}} < \E\brb{\frac{X!}{(X-k)!}} < \infty,
%\ee
%then by Theorem \ref{thm:differentiation_under_integral_sign}, we have
%\be
%G^{(k)}(z) = \E\brb{\frac{X!}{(X-k)!} z^{X-k}}
%\ee

\be
G^{(k)}(z) = \sum^\infty_{r=1} \frac{r!}{(r-k)!}p_rz^{r-k} \leq \sum^\infty_{r=1} \frac{r!}{(r-k)!} p_r = \E\brb{\frac{X!}{(X-k)!}}.
\ee

We see that $G^{(k)}(z)$ is non-decreasing in $z$ so that $\lim_{z\uparrow 1} G^{(k)}(z) \leq \E\brb{\frac{X!}{(X-k)!}}$.

Take $\ve > 0$, and choose $N$ so that $\sum^N_{r=1} \frac{r!}{(r-k)!} p_r \geq \E\brb{\frac{X!}{(X-k)!}} -\ve$. Then
\be
\lim_{z\uparrow 1} G^{(k)}(z) \geq \lim_{z\ua 1} \sum^N_{r=1} \frac{r!}{(r-k)!} p_r z^{r-k} = \sum^N_{r=1} \frac{r!}{(r-k)!} p_r \geq \E X - \ve.
\ee

Note that ($*$) holds because it takes the limit of the sum of finite items, having the form of $z^n$. This is true for each $\ve > 0$, whence $\lim_{z\uparrow 1} G^{(k)}(z) \geq \E X$ and it follows that $G^{(k)}(1^-) = \lim_{z\ua 1} G^{(k)}(z) = \E\brb{\frac{X!}{(X-k)!}}$.

If $\E\brb{\frac{X!}{(X-k)!}} = \infty$, then for any $M > 0$ choose $N$ so that $\sum^N_{r=1} \frac{r!}{(r-k)!} p_r \geq M$, and, as above, see that
\be
\lim_{z\uparrow 1} G^{(k)}(z) \geq \lim_{z\uparrow 1} \sum^N_{r=1} \frac{r!}{(r-k)!} p_r z^{r-1} = \sum^N_{r=1} \frac{r!}{(r-k)!} p_r \geq M,
\ee
this is true for any $M$, whence $G^{(k)}(1^-) = \lim_{z\uparrow 1} G^{(k)}(z) = \infty$ and $\E\brb{\frac{X!}{(X-k)!}} = G^{(k)}(1^-)$.

Accordingly, we have $\E X = G'(1^-)$ and
\be
\var(X) = \E\brb{X^2} - \brb{\E X}^2 = \E\brb{X(X-1)} + \E X - \brb{\E X}^2 = G''(1^-) + G'(1^-) - (G'(1^-))^2,
\ee
as required.
\end{proof}


\begin{remark}
Here we cannot write $G(1)$ as $z^{n}$ is NOT continuous at 1 when $n\to \infty$.
\end{remark}


\subsection{Bernoulli random variables}

\begin{definition}[Bernoulli random variable]\label{def:bernoulli_rv}
A random variable $X$ in $\bra{0,1}$ is Bernoulli distributed\index{Bernoulli random variable!$\R$} if, for some $p\in [0,1]$ and $q = 1-p$, has probability mass function
\be
\pro(X=k) = \begin{cases}  p & k =1  \\ q & k = 0   \end{cases}
\ee

We write $X \sim \berd(p)$. The cumulative distribution function of Bernoulli distribution is given by
\be
F(x) =  \begin{cases}    0 & x<0 \\ q & 0\leq x<1 \\ 1 & x\geq 1    \end{cases}
\ee
\end{definition}

\begin{example}[Bernoulli distribution]
Consider a sample space with just two points $\Omega =\{H,T\}$, where the two points may be thought of as $H$ = "heads", and $T$ = "tails", so we are modelling a coin toss where we will take $p$ as the probability of heads and $1- p$ as the probability of tails. Then $p = \pro(H)$ and $1 - p = \pro (T)$, with $0 \leq p \leq 1$.
\end{example}

\begin{proposition}\label{pro:pgf_bernoulli}
Suppose $X\sim \berd(p)$. Then for $\theta,t \in \R$,
\be
G_X(z) = (1-p) + pz,\qquad M_X(\theta) = (1-p) + pe^\theta,\qquad \phi_X(t) = (1-p) + pe^{it}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
From Definition \ref{def:pgf_probability}, \ref{def:mgf_probability},
\beast
G_X(z) & = & \E\brb{z^X} = \pro(X=1) z^1 + \pro(X=0)z^0 = p z + q,\\
M_X(\theta) & = & \E\brb{e^{\theta X}} = \pro(X=1) e^\theta + \pro(X=0)\cdot 1 = pe^\theta + q,\\
\phi_X(t) & = & \E\brb{e^{it X}} = \pro(X=1) e^{it} + \pro(X=0)\cdot 1 = pe^{it} + q.
\eeast
\end{proof}

\begin{proposition}
Suppose $X\sim \berd(p)$. Then \be \text{(i)}\ \E X = p,\quad\quad \text{(ii)}\ \var X = p(1-p),\quad\quad\text{(iii)}\ \skewness(X) = \frac{1-2p}{\sqrt{p(1-p)}},\quad\quad\text{(iv)}\ \ekurt(X) = \frac{1-6p(1-p)}{p(1-p)}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Let $q=1-p$,
\be
\E X= \sum^1_{k=0}\pro(X=k) k = p,\qquad \E X^2 = \sum^1_{k=0}\pro(X=k) k^2 = p \ \ra \ \var X = p - p^2 = pq.
\ee

\be
\E(X-p)^3 = \sum^1_{k=0}\pro(X=k) (k-p)^3 = p(1-p)^3 + q(0-p)^3 = pq^3 - qp^3 = pq(q-p).
\ee

\be
\skewness(X) = \frac{pq(q-p)}{(pq)^{3/2}} = \frac{q-p}{\sqrt{pq}}.
\ee

\beast
\E(X-p)^4 & = & \sum^1_{k=0}\pro(X=k) (k-p)^4 = p(1-p)^4 + q(0-p)^4 = pq^4 + qp^4 \\
& = & pq(p^2 - pq + q^2) = pq(p^2 + 2pq + q^2 -3pq) = pq(1-3pq).
\eeast

\be \ekurt(X) = \frac{pq(1-3pq)}{p^2q^2} - 3 = \frac{1-3pq}{pq} - 3 = \frac{1-6pq}{pq}. \ee

We can also apply Theorem \ref{thm:pgf_moment},
\be
\E X = G'(1^-) = p, \quad \var(X) = G''(1^-) + G'(1^-) - \brb{G'(1^-)}^2 = 0 + p - p^2 = p(1-p) = pq.
\ee
\end{proof}


\subsection{Binomial random variables}

\begin{definition}[Binomial random variable]\label{def:binomial_rv}
A random variable $X$ in $\bra{0,1,\dots,n}$ is Binomial distributed\index{Binomial random variable!$\R$} if, for some $n\in \Z^+$ and $p\in [0,1]$, has probability mass function
\be
\pro(X=k) = {n\choose k}p^k(1-p)^{n-k},\quad\quad k\in \bra{0,1,\dots,n}.
\ee

We write $X \sim \bd(n,p)$. %The cumulative distribution function of Bernoulli distribution is given by
%\be
%F(x) =  \begin{cases}    0 & x<0 \\ q & 0\leq x<1 \\ 1 & x\geq 1    \end{cases}
%\ee
\end{definition}

\begin{remark}
This models the number of heads obtained in $n$ successive tosses of the coin. Note that if $n=1$, $X\sim \berd(p)$.
\end{remark}

\begin{proposition}\label{pro:pgf_binomial}
Suppose $X\sim \bd(n,p)$. Then for $\theta,t \in \R$,
\be
G_X(z) = \brb{(1-p)+ pz}^n,\qquad M_X(\theta) = \brb{(1-p)+ pe^{\theta}}^n,\qquad \phi_X(t) = \brb{(1-p)+ pe^{it}}^n.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
From Definition \ref{def:pgf_probability}, \ref{def:mgf_probability},
\beast
G_X(z) & = & \E\brb{z^X} = \sum^n_{k=0}\pro(X=k) z^k = \sum^n_{k=0} {n\choose k}(zp)^k(1-p)^{n-k} = \brb{(1-p)+ pz}^n,\\
M_X(\theta) & = & \E\brb{e^{\theta X}} = \sum^n_{k=0}\pro(X=k) e^{\theta k} = \sum^n_{k=0} {n\choose k}(e^\theta p)^k(1-p)^{n-k} = \brb{(1-p)+ pe^\theta}^n.%\phi_X(t) & = & \E\brb{e^{it X}} = \sum^n_{k=0}\pro(X=k) e^{it k} = \sum^n_{k=0} {n\choose k}(e^{it} p)^k(1-p)^{n-k} = \brb{(1-p)+ pe^{it}}^n.
\eeast
and similarly for $\phi_X(t)$.
\end{proof}

\begin{remark}
By Proposition \ref{pro:independent_pgf}, we can see that $X\sim \bd(n,p)$ is sum of $X_1,\dots,X_n \stackrel{\text{i.i.d.}}{\sim} \berd(p)$ by assuming $\theta = \log z$.
\end{remark}

\begin{proposition}
Suppose $X\sim \bd(n,p)$. Then \be \text{(i)}\ \E X = np,\quad\quad \text{(ii)}\ \var X = np(1-p),\quad\quad\text{(iii)}\ \skewness(X) = \frac{1-2p}{\sqrt{np(1-p)}},\quad\quad\text{(iv)}\ \ekurt(X) =
\frac{1-6p(1-p)}{np(1-p)}. \ee
\end{proposition}

\begin{proof}[\bf Proof]
\be
\E X= \sum^n_{k=0}\pro(X=k) k = \sum^n_{k=1} {n\choose k}p^k(1-p)^{n-k} k = np\sum^n_{k=1} {n-1\choose k-1}p^{k-1}(1-p)^{n-k} = np.
\ee

\beast
\E X^2 & = & \sum^n_{k=0}\pro(X=k) k^2 = \sum^n_{k=1} {n\choose k}p^k(1-p)^{n-k} (k^2-k + k) \\
& = & n(n-1)p^2 \sum^n_{k=2} {n-2\choose k-2}p^{k-2}(1-p)^{n-k} + np = n(n-1)p^2 + np.
\eeast

Then $\var X = n(n-1)p^2 + np - n^2p^2 = np - np^2 = np(1-p)$. Furthermore,

\beast
\E(X-np)^3 & = & \sum^n_{k=0}\pro(X=k) (k-np)^3 = \sum^n_{k=0}\pro(X=k) k^3 - 3np\sum^n_{k=0}\pro(X=k) k^2  + 3n^2p^2 \sum^n_{k=0} \pro(X=k) k - n^3p^3\\
& = & \sum^n_{k=0}\pro(X=k) \brb{k(k-1)(k-2) + 3k^2 - 2k} - 3np\sum^n_{k=0}\pro(X=k) k^2  + 3n^2p^2 \sum^n_{k=0} \pro(X=k) k - n^3p^3\\
& = & n(n-1)(n-2)p^3 + \brb{3 - 3np}\brb{n(n-1)p^2 + np} + \brb{-2 + 3n^2p^2} np - n^3 p^3\\
& = & np(1-2p)(1-p).
\eeast
and
\be
\E X^3 = n(n-1)(n-2)p^3 + 3\brb{n(n-1)p^2 + np} - 2np = np\brb{(n-1)(n-2)p^2 + 3(n-1)p + 1}.
\ee

Therefore,
\be
\skewness(X) = \frac{np(1-2p)(1-p)}{\brb{np(1-p)}^{3/2}} = \frac{1-2p}{\sqrt{np(1-p)}}.
\ee

\beast
\E(X-np)^4 =  \sum^n_{k=0}\pro(X=k) k^4 - 4np\sum^n_{k=0}\pro(X=k) k^3  + 6n^2p^2 \sum^n_{k=0} \pro(X=k) k^2 - 4n^3p^3\sum^n_{k=0} \pro(X=k) k + n^4p^4
\eeast
and $\E X^4 = \sum^n_{k=0}\pro(X=k) \brb{k(k-1)(k-2)(k-3) + 6k^3 - 11k^2 + 6k}$. Therefore,
\beast
\E(X-np)^4 & = & n(n-1)(n-2)(n-3)p^4 + (6 - 4np)np\brb{(n-1)(n-2)p^2 + 3(n-1)p + 1} \\
& & \qquad + (-11+ 6n^2p^2) \brb{n(n-1)p^2 + np} + (6-4n^3p^3)np + n^4p^4\\
& = & np\brb{(3n-6)p^3 + (-6n+12)p^2 + (3n - 7)p + 1} = np (1-p)\brb{(3n-6)p(1-p) + 1}.
\eeast

Thus, \be \ekurt(X) = \frac{np (1-p)\brb{(3n-6)p(1-p) + 1}}{n^2p^2(1-p)^2} - 3 = \frac{(3n-6)p(1-p) + 1}{np(1-p)} - 3 = \frac{1-6p(1-p)}{np(1-p)}. \ee
\end{proof}

\subsection{Geometric random variables}

\begin{definition}[geometric random variable]\label{def:geometric_rv}
A random variable $X$ in $\bra{0,1,\dots}$ is geometric distributed\index{geometric random variable!$\R$} if, for some $p\in [0,1]$, has probability mass function
\be
\pro(X=k) = (1-p)^k p,\quad k\in \bra{0,1,\dots}
\ee

We write $X \sim \geo(p)$. The cumulative distribution function of geometric distribution is given by
\be
F(x) = 1-(1 - p)^{x+1}.
\ee
\end{definition}

\begin{proposition}\label{pro:pgf_geometric}
Suppose $X\sim \geo(p)$. Then for $\theta,t \in \R$,
\be
G_X(z) = \frac{p}{1-(1-p)z},\qquad M_X(\theta) = \frac{p}{1-(1-p)e^\theta},\qquad \phi_X(t) = \frac{p}{1-(1-p)e^{it}}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
From Definition \ref{def:pgf_probability}, \ref{def:mgf_probability},
\beast
G_X(z) & = & \E\brb{z^X} = \sum^\infty_{k=0}\pro(X=k) z^k = \sum^\infty_{k=0} (1-p)^k p z^k = p\sum^\infty_{k=0} (1-p)^kz^k = \frac p{1-(1-p)z},\\
M_X(\theta) & = & \E\brb{e^{\theta X}} = \sum^\infty_{k=0}\pro(X=k) e^{\theta k} = p\sum^\infty_{k=0} (1-p)^k \brb{e^\theta}^k = \frac p{1-(1-p)e^\theta}.
\eeast
and similarly for $\phi_X(t)$.
\end{proof}

\begin{proposition}
Suppose $X\sim \geo(p)$. Then \be \text{(i)}\ \E X = \frac{1-p}p,\quad\quad \text{(ii)}\ \var X = \frac{1-p}{p^2},\quad\quad\text{(iii)}\ \skewness(X) = \frac{2-p}{\sqrt{1-p}},\quad\quad\text{(iv)}\ \ekurt(X) = 6 +
\frac{p^2}{1-p}. \ee
\end{proposition}

\begin{proof}[\bf Proof]
\be
\E X= \sum^\infty_{k=0}\pro(X=k) k = \sum^\infty_{k=0} (1-p)^k p k = \sum^\infty_{k=0} (1-p)^k p (k+1-1) = -p\brb{\frac{1-p}{p}}' - 1= \frac{1-p}p.
\ee

\beast
\E X^2 & = & \sum^\infty_{k=0}\pro(X=k) k^2 = \sum^\infty_{k=0} (1-p)^k p k^2 = \sum^\infty_{k=0} (1-p)^k p (k^2 + 3k + 2 - 3k-2)\\
& = & p\brb{\frac{(1-p)^2}{p}}'' - 3\frac{1-p}p - 2 = \frac{(1-p)(2-p)}{p^2}.
\eeast

Therefore,
\be
\var X = \E X^2 - (\E X)^2 = \frac{(1-p)(2-p)}{p^2} - \brb{\frac{1-p}p}^2 = \frac{1-p}{p^2}.
\ee

\beast
\E X^3 & = & \sum^\infty_{k=0}\pro(X=k) k^3 = \sum^\infty_{k=0} (1-p)^k p k^3 = \sum^\infty_{k=0} (1-p)^k p (k^3 + 6k^2 + 11k + 6 - 6k^2 - 11k - 6)\\
& = & -p\brb{\frac{(1-p)^3}{p}}''' - 6\frac{(1-p)(2-p)}{p^2} - 11\frac{1-p}p - 6 = \frac{(1-p)(6-6p+p^2)}{p^3}.
\eeast

\be
\E\brb{X-\frac{1-p}p}^3 = \frac{(1-p)(6-6p+p^2)}{p^3} - 3 \frac{1-p}p\frac{(1-p)(2-p)}{p^2} + 2\brb{\frac{1-p}{p}}^3 =  \frac{(1-p)(2-p)}{p^3}.
\ee

\be
\skewness(X) = \frac{(1-p)(2-p)/p^3}{(1-p)^{3/2}/p^3} = \frac{2-p}{\sqrt{1-p}}.
\ee

Furthermore,
\beast
\E X^4 & = & \sum^\infty_{k=0}\pro(X=k) k^4 = \sum^\infty_{k=0} (1-p)^k p k^4 = \sum^\infty_{k=0} (1-p)^k p (k^4 + 10k^3 + 35k^2 + 50k + 24 - 10k^3 - 35k^2 - 50k - 24)\\
& = & p\brb{\frac{(1-p)^4}{p}}'''' - 10\frac{(1-p)(6-6p+p^2)}{p^3} - 35\frac{(1-p)(2-p)}{p^2} - 50\frac{1-p}p - 24 = \frac{(1-p)(2-p)(12-12p+p^2)}{p^4}.
\eeast


\beast
\E\brb{X-\frac{1-p}p}^4 & = &  \frac{(1-p)(2-p)(12-12p+p^2)}{p^4} - 4 \frac{1-p}p \frac{(1-p)(6-6p+p^2)}{p^3} + 6 \brb{\frac{1-p}p}^2\frac{(1-p)(2-p)}{p^2} - 3\brb{\frac{1-p}{p}}^4\\
& = &  \frac{(1-p)(9-9p + p^2)}{p^4}.
\eeast

\be \ekurt(X) = \frac{(1-p)(9-9p + p^2)/p^4}{(1-p)^2/p^4} - 3 = \frac{p^2}{1-p} + 9 - 3 = 6 + \frac{p^2}{1-p} . \ee
\end{proof}

\subsection{Negative binomial random variables}

Suppose there is a sequence of independent Bernoulli trials, each trial having two potential outcomes called ``success" and ``failure". In each trial the probability of failure is $p$ and of success is $1 - p$. We are observing this sequence until a predefined number $r$ of failures has occurred. Then the random number of successes we have seen, $X$, will have the negative binomial (or Pascal) distribution.

\begin{definition}[negative binomial random variable]\label{def:negative_binomial_rv}
A random variable $X$ in $\bra{0,1,\dots}$ is negative binomial distributed\index{negative binomial random variable!$\R$} if, for some $p\in [0,1]$ and $r\in \R^+$ (extended version), has probability mass function
\be
\pro(X=k) = {k+r-1 \choose k}\cdot (1-p)^k p^r,\quad k\in \bra{0,1,\dots}
\ee

We write $\nbd(r,p)$.% The cumulative distribution function of negative binomial distribution is given by
%\be
%F(x) = 1-(1 - p)^{x+1}.
%\ee
\end{definition}

\begin{remark}
The probability mass function makes sence
\be
\sum^\infty_{k=0}\pro(X=k) = \sum^\infty_{k=0}{k+r-1 \choose k}\cdot (1-p)^k p^r = 1
\ee
by Proposition \ref{pro:negative_binomial_pmf}.
\end{remark}

\begin{proposition}\label{pro:pgf_negative_binomial}
Suppose $X\sim \nbd(r,p)$. Then for $\abs{z} < \frac 1p$, $\theta<-\log p$ and $t\in \R$,
\be
G_X(z) = \brb{\frac{p}{1 - (1-p) z}}^r,\qquad M_X(\theta) = \brb{\frac{p}{1 - (1-p) e^\theta}}^r,\qquad \phi_X(t) = \brb{\frac{p}{1 - (1-p) e^{it}}}^r.
\ee
\end{proposition}

\begin{remark}
By Proposition \ref{pro:independent_pgf}, we can see that $X\sim \nbd(n,p)$ is sum of $X_1,\dots,X_n \stackrel{\text{i.i.d.}}{\sim} \geo(p)$ by assuming $\theta = \log z$.
\end{remark}

\begin{proof}[\bf Proof]
From Definition \ref{def:pgf_probability},% \ref{def:mgf_probability},
\beast
G_X(z) & = & \E\brb{z^X} = \sum^\infty_{k=0}\pro(X=k) z^k = \sum^\infty_{k=0} {k+r-1 \choose k}\cdot (1-p)^k p^r z^k \\
& = & p^r\sum^\infty_{k=0} {k+r-1 \choose k}\cdot ((1-p)z)^k = \brb{\frac{p}{1 - (1-p) z}}^r.
%M_X(z) & = & \E\brb{e^{\theta X}} = \sum^\infty_{k=0}\pro(X=k) e^{\theta k} = p\sum^\infty_{k=0} (1-p)^k \brb{e^\theta}^k = \frac p{1-(1-p)e^\theta}.
\eeast
and similarly for $M_X(\theta)$ and $\phi_X(t)$.
\end{proof}

\begin{proposition}
Suppose $X\sim \nbd(r,p)$. Then \beast \text{(i)}\ \E X = \frac{r(1-p)}{p},\quad\quad \text{(ii)}\ \var X = \frac{r(1-p)}{p^2},\quad\quad\text{(iii)}\ \skewness(X) = \frac{2-p}{\sqrt{r(1-p)}},\quad\quad\text{(iv)}\
\ekurt(X) = \frac 6r + \frac{p^2}{r(1-p)}. \eeast
\end{proposition}

\begin{proof}[\bf Proof]
From proof of Proposition \ref{pro:negative_binomial_pmf},
\beast
{k+r-1 \choose k} k & = & (-1)^k \frac{(-r)(-r-1)(-r-2)\cdots(-r-k+1)}{(k-1)!} \\
& = & r (-1)^{k-1}\frac{(-r-1)(-r-2)\cdots(-r-k+1)}{(k-1)!} = r {k+r-1 \choose k-1}.
\eeast

Thus,

\be
\E X= \sum^\infty_{k=0} {k+r-1 \choose k}\cdot (1-p)^k p^r k = \frac{r(1-p)}p \sum^\infty_{k=1} {k+r-1 \choose k-1}\cdot (1-p)^{k-1}p^{r+1}   = \frac{r(1-p)}p.
\ee

\beast
\E X^2 & = & \sum^\infty_{k=0} {k+r-1 \choose k}\cdot (1-p)^k p^r k^2 = \sum^\infty_{k=0} {k+r-1 \choose k}\cdot (1-p)^k p^r (k^2 -k +k)  \\
& = & r(r+1)\brb{\frac{1-p}{p}}^2\sum^\infty_{k=2} {k+r-1 \choose k-2}\cdot (1-p)^{k-2} p^{r+2} + \sum^\infty_{k=0} {k+r-1 \choose k}\cdot (1-p)^k p^rk \\
& = &  r(r+1)\brb{\frac{1-p}{p}}^2 + \frac{r(1-p)}p = \frac{r(1-p)}p \brb{\frac{r+1-rp}p}.
\eeast

Therefore,
\be
\var X = \E X^2 - (\E X)^2 = \frac{r(1-p)}p \brb{\frac{r+1-rp}p} - \brb{\frac{r(1-p)}p}^2 = \frac{r(1-p)}{p^2}.
\ee

\beast
\E X^3 & = & \sum^\infty_{k=0} {k+r-1 \choose k}\cdot (1-p)^k p^r k^3 = \sum^\infty_{k=0}  {k+r-1 \choose k}\cdot  (1-p)^k p^r (k^3 - 3k^2 + 2k + 3k^2 - 2k)\\
& = & r(r+1)(r+2)\brb{\frac{1-p}{p}}^3 + 3 \frac{r(1-p)}p \brb{\frac{r+1-rp}p} - 2\frac{r(1-p)}p \\
& = & \frac{r(1-p)}p \frac{r^2p^2 - (r+1)(2r+1)p + (r+1)(r+2)}{p^2}.%\sum^\infty_{k=3} {k+r-1 \choose k-3}\cdot (1-p)^{k-3} p^{r+3}
\eeast

\beast
\E\brb{X-\frac{r(1-p)}p}^3 & = & \frac{r(1-p)}p \frac{r^2p^2 - (r+1)(2r+1)p + (r+1)(r+2)}{p^2} - 3 \frac{r(1-p)}p\frac{r(1-p)}p \brb{\frac{r+1-rp}p}  + 2\brb{\frac{r(1-p)}{p}}^3 \\
& = & \frac{r(1-p)}p \frac{(2-p)}{p^2} = \frac{r(1-p)(2-p)}{p^3}.
\eeast

\be
\skewness(X) = \frac{r(1-p)(2-p)/p^3}{r^{3/2}(1-p)^{3/2}/p^3} = \frac{2-p}{\sqrt{r(1-p)}}.
\ee

Furthermore,
\beast
\E X^4 & = & \sum^\infty_{k=0} {k+r-1 \choose k}\cdot (1-p)^k p^r k^4 = \sum^\infty_{k=0} {k+r-1 \choose k}\cdot (1-p)^k p^r (k^4 -6k^3 + 11k^2 - 6k + 6k^3 - 11k^2 + 6k)\\
& = & \frac{r(1-p)}{p}\brb{(r+1)(r+2)(r+3)\brb{\frac{1-p}{p}}^3 + 6 \frac{r^2p^2 - (r+1)(2r+1)p + (r+1)(r+2)}{p^2} - 11 \brb{\frac{r+1-rp}p} + 6} \\
& = & \frac{r(1-p)}{p}\frac{(r+1)(r+2)(r+3) - 3(r+1)^2(r+2)p + (r+1)(3r^2 + 3r + 1)p^2 -r^3 p^3 }{p^3}.
\eeast


\beast
\E\brb{X-\frac{r(1-p)}p}^4 & = & \frac{r(1-p)}{p}\left(\frac{(r+1)(r+2)(r+3) - 3(r+1)^2(r+2)p + (r+1)(3r^2 + 3r + 1)p^2 -r^3 p^3 }{p^3}\right.\\
& & \qquad\qquad\qquad - 4\brb{\frac{r(1-p)}{p}}\frac{r^2p^2 - (r+1)(2r+1)p + (r+1)(r+2)}{p^2} \\
& & \left.\qquad\qquad\qquad + 6\brb{\frac{r(1-p)}{p}}^2\brb{\frac{r+1-rp}p} - 3\brb{\frac{r(1-p)}{p}}^3\right)\\
& = &  \frac{r(1-p)}{p}\brb{\frac{3(r+2)-\brb{3r + 6}p + p^2}{p^3}} .
\eeast

\be \ekurt(X) = \frac{r(1-p)(3(r+2)-\brb{3r + 6}p+ p^2)/p^4}{r^2(1-p)^2/p^4} - 3 = \frac 6r + \frac{p^2}{r(1-p)} . \ee
\end{proof}

\subsection{Poisson random variables}

\begin{definition}[Poisson random variable]\label{def:poisson_rv}
A random variable $X$ in $\bra{0,1,\dots}$ is poisson distributed\index{Poisson random variable!$\R$} if for some $\lm >0$ it has probability mass function
\be
\pro(X=k) = \frac{\lm^k e^{-\lm}}{k!},\qquad  k\in \bra{0,1,\dots}.
\ee

We write $\pd(\lm)$.% The cumulative distribution function of negative binomial distribution is given by
%\be
%F(x) = 1-(1 - p)^{x+1}.
%\ee
\end{definition}

\begin{proposition}\label{pro:pgf_poisson}
Suppose $X\sim \pd(\lm)$. Then for $z,\theta,t\in \R$,
\be
G_X(z) = \exp(\lambda (z-1)),\qquad M_X(\theta) = \exp(\lambda (e^\theta-1)),\qquad \phi_X(t) = \exp(\lambda (e^{it}-1)).
\ee
\end{proposition}

\begin{proof}[\bf Proof]
From Definition \ref{def:pgf_probability},% \ref{def:mgf_probability},
\be
G_X(z) = \E\brb{z^X} = \sum^\infty_{k=0}\pro(X=k) z^k = \sum^\infty_{k=0} \frac{\lm^k e^{-\lm}}{k!} z^k = e^{\lm z - \lm}\sum^\infty_{k=0} \frac{(\lm z)^k e^{-\lm z}}{k!} = e^{\lm (z - 1)}.%M_X(z) & = & \E\brb{e^{\theta X}} = \sum^\infty_{k=0}\pro(X=k) e^{\theta k} = p\sum^\infty_{k=0} (1-p)^k \brb{e^\theta}^k = \frac p{1-(1-p)e^\theta}.
\ee
and similarly for $M_X(\theta)$ and $\phi_X(t)$.
\end{proof}

\begin{proposition}\label{pro:moments_poisson}
Suppose $X\sim \pd(\lm)$. Then \beast \text{(i)}\ \E X = \lm,\quad\quad \text{(ii)}\ \var X = \lm,\quad\quad\text{(iii)}\ \skewness(X) = \lm^{-1/2},\quad\quad\text{(iv)}\ \ekurt(X) = \lm^{-1}. \eeast
\end{proposition}

\begin{proof}[\bf Proof]
\be
\E X= \sum^\infty_{k=0} \frac{\lm^k e^{-\lm}}{k!} k = \lm \sum^\infty_{k=1}  \frac{\lm^{k-1} e^{-\lm}}{(k-1)!}  = \lm.
\ee

\be
\E X^2 = \sum^\infty_{k=0} \frac{\lm^k e^{-\lm}}{k!} k^2 = \sum^\infty_{k=0} \frac{\lm^k e^{-\lm}}{k!} (k^2 -k +k) = \lm^2 \sum^\infty_{k=2} \frac{\lm^{k-2} e^{-\lm}}{(k-2)!} + \lm = \lm^2 + \lm.
\ee

Therefore, $\var X = \E X^2 - (\E X)^2 = \lm^2 + \lm - \lm^2 = \lm$.

\be
\E X^3 = \sum^\infty_{k=0} \frac{\lm^k e^{-\lm}}{k!} k^3 = \sum^\infty_{k=0}  \frac{\lm^k e^{-\lm}}{k!} (k^3 - 3k^2 + 2k + 3k^2 - 2k) = \lm^3 + 3(\lm^2 + \lm) - 2\lm = \lm^3 + 3\lm^2 + \lm.
\ee

\be
\E\brb{X-\lm}^3 = \lm^3 + 3\lm^2 + \lm - 3\lm(\lm^2 + \lm) + 2\lm^3 = \lm \ \ra \ \skewness(X) = \frac{\lm}{\lm^{3/2}} = \lm^{-1/2}.
\ee

Furthermore,
\beast
\E X^4 & = & \sum^\infty_{k=0} \frac{\lm^k e^{-\lm}}{k!} k^4 = \sum^\infty_{k=0} \frac{\lm^k e^{-\lm}}{k!} (k^4 -6k^3 + 11k^2 - 6k + 6k^3 - 11k^2 + 6k)\\
& = & \lm^4 + 6\brb{\lm^3 + 3\lm^2 + \lm} -  11\brb{\lm^2 + \lm} + 6\lm = \lm^4 + 6\lm^3 + 7\lm^2 + \lm.
\eeast


\be
\E\brb{X-\lm}^4 = \lm^4 + 6\lm^3 + 7\lm^2 + \lm - 4\lm\brb{\lm^3 + 3\lm^2 + \lm} + 6\lm^2\brb{\lm^2 + \lm} - 3\lm^4 = 3\lm^2 + \lm.
\ee

\be \ekurt(X) = \frac{\lm(3\lm + 1)}{\lm^2} - 3 = \lm^{-1}. \ee
\end{proof}

\begin{proposition}[sums of independent Poisson random variables]\label{pro:poisson_sum}
Let $X_1,\dots,X_n$ with $X_i \sim \pd(\lm_i)$ where $\lm_i > 0$ be independent random variables. Then
\be
X := X_1 + \dots + X_n \sim \pd\brb{\sum^n_{i=1}\lm_i}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
The probability generating function of $X_i$ is $e^{-\lm_i(1-z)}$. Then by Proposition \ref{pro:independent_pgf},
\be
G_X(z) = \prod^n_{i=1} G_{X_i}(z) = \prod^n_{i=1} e^{-\lm_i(1-z)} = e^{-(1-z)\sum^n_{i=1}\lm_i}.
\ee

Thus, $X$ is a Poisson random variables with parameter $\sum^n_{i=1}\lm_i$.
\end{proof}

\begin{proposition}[sums of independent Poisson random variables for fixed number]\label{pro:poisson_sum_fixed_number}
Let $X \sim \pd(\lm)$ and $Y \sim \pd(\mu)$ where $\lm,\mu > 0$ be independent random variables. Then given that $X+Y = n$, for $0\leq k\leq n$,
\be
X_{X+Y =n} \sim \bd\brb{n,\frac{\lm}{\lm + \mu}}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
By Proposition \ref{pro:poisson_sum}, $X+Y \sim \pd(\lm+\mu)$. For $0\leq k\leq n$,
\beast
\pro(X=k|X+Y=n) & = & \frac{\pro(X=k,X+Y=n)}{\pro(X+Y=n)} = \frac{\pro(X=k,Y=n-k)}{\pro(X+Y=n)} \\
 & = & \frac{\pro(X=k)\pro(Y=n-k)}{\pro(X+Y=n)} = \frac{e^{-\lm}\frac{\lm^k}{k!}e^{-\mu}\frac{\mu^{n-k}}{(n-k)!}}{e^{-(\lm+\mu)}\frac{(\lm+\mu)^n}{n!}} \\
& = & \binom{n}{k} \brb{\frac{\lm}{\lm+\mu}}^k\brb{1-\frac{\lm}{\lm+\mu}}^{n-k}
\eeast
as required.
\end{proof}

%\begin{definition}
%The discrete random variable $X$ has negative binomial distribution if,
%\be
%\pro(X = k) = {k+r-1 \choose k} (1-p)^r p^k \quad\text{for }k = 0, 1, 2, \dots
%\ee
%\end{definition}


\section{Continuous Random Variables}

\subsection{Continuous random variable}

%\begin{definition}[continuous random variable]
%\footnote{need definition.}
%\end{definition}

\begin{definition}[continuous probability distribution\index{continuous probability distribution}]\label{def:continuous_probability_distribution}
A continuous probability distribution is a probability distribution that is continuous and it is generated by having a probability density function.\footnote{The absolute continuity of cdf guarantees the existence of pdf. details needed.}
\end{definition}

\begin{definition}[absolutely continuous probability distribution\index{absolutely continuous probability distribution}]\label{def:absolutely_continuous_distribution}
A probability distribution with probability density function is called absolutely continuous, since their cumulative distribution function is absolutely continuous with respect to the Lebesgue measure $\mu$.\footnote{details needed for absolute continuous.}
\end{definition}

\begin{definition}[continuous random variable\index{continuous random variable}]\label{def:continuous_random_variable}
If the distribution function of random variable $X$ is continuous, then $X$ is called a continuous random variable. This means that its probability density function $f(x)$ exists and
\be
\pro\brb{X\in A} = \int_A f d\mu. %\pro(X\leq [a,b]) = \int^b_a f(x) dx.
\ee

Continuous random variable is also called absolutely continuous random variable.\footnote{details needed.}
\end{definition}


\begin{example}
$X$ has density function
\be
f_X(x) = \left\{\ba{ll}
4x^3 \quad\quad & x\in [0,1] \\
0 & \text{otherwise}
\ea\right..
\ee

Then the probability between 1/2 and 1 can be calculated as follows
\be
\pro\brb{X\in \bsb{\frac 12,1}} = \int^1_{1/2} f_X(x)dx = \int^1_{1/2} 4x^3 dx = \left.x^4\right|^1_{1/2} = 1 - \brb{\frac 12}^4 = \frac{15}{16}.
\ee
\end{example}


\begin{definition}[median\index{median!random variable}]\label{def:median_rv}
The median $m$ of a continuous random variable $X$ with density function $f$ is the point which satisfies
\be
\pro (X \geq m) = \int^\infty_m f(x)dx = \int^m_{-\infty} f(x)dx = \pro (X \leq m) = \frac 12.
\ee

Thus half the distribution lies on one side of $m$ and half on the other. For a discrete random variable, $X$, a median $m$ is a point satisfying
\be
\pro (X \geq m) \geq \frac 12 \quad \text{and}\quad \pro (X \leq m) \geq \frac 12.
\ee
\end{definition}

\begin{remark}
Note that for the normal distribution $\sN(\mu,\sigma^2)$ (see Definition \ref{def:gaussian_rv}) the mean is equal to the median (and this is true for any symmetric distribution).
\end{remark}

\begin{definition}[mode\index{mode!random variable}]\label{def:mode_rv}
A mode of a continuous random variable, with density function $f$, is a point $m$ for which $f(m) \geq f(x)$ for all $x$, that is the density function is maximized at a mode.

For a discrete random variable a mode is a point, $m$, for which $\pro(X = m) \geq \pro (X = x)$ for all possible values $x$.
\end{definition}

\begin{remark}
In the case of the normal distribution function (see Definition \ref{def:gaussian_rv}), the mean and median are also the mode. For example, for the $\sE(\lm)$ distribution with density function $\lm e^{-\lm x}$ for $x > 0$, we have seen that the mean is $1/\lm$, it is easy to check that the median is $\log 2/\lm$ and the mode is 0.
\end{remark}

\subsection{Uniform random variables}

\begin{definition}[uniform random variable]\label{def:uniform_rv}
A random variable $X$ in $\R$ is uniformly distributed\index{uniform random variable!$\R$} if, for some $a<b \in \R$, $X$ has density function
\be
f_X(x) = \begin{cases}  \frac{1}{b - a} & \text{for } x \in [a,b]  \\  0 & \text{otherwise}   \end{cases}
\ee

We write $X \sim \ud{a}{b}$. The cumulative distribution function of uniform distribution is given by
\be
F(x) = \begin{cases} 0  & \text{for } x < a \\  \frac{x-a}{b-a} & \text{for } x \in [a,b) \\ 1 & \text{for } x \ge b \end{cases}
\ee
\end{definition}

%A result that is important for computer simulation of random variables is the following.

%\begin{theorem}
%Suppose that $U \sim \sU[0, 1]$, then for any continuous distribution function $F$, the random variable $X = F^{-1}(U)$ has distribution function $F$.
%\end{theorem}

%\begin{proof}[\bf Proof]
%Note that for $u \in [0, 1]$, $\pro (U \leq u) = u$, so we have
%\be
%\pro (X \leq x) = \pro\brb{F^{-1} (U) \leq x} = \pro(U \leq F(x)) = F(x),
%\ee
%which gives the result.
%\end{proof}




\begin{proposition}\label{pro:mgf_uniform}
Suppose $X \sim \ud{a}{b}$. Then for $\theta,t\in \R$,
\be
M_X(\theta) = \frac{e^{\theta b} - e^{\theta a}}{\theta(b-a)},\quad\quad \phi_X(t) = \frac{e^{it b} - e^{it a}}{it(b-a)}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
From Definition \ref{def:mgf_probability},
\be
M_X(\theta) = \E\brb{e^{\theta X}} = \int^b_a e^{\theta x} \frac 1{b-a} dx = \frac 1{\theta(b-a)}  \int^b_a de^{\theta x} = \frac{e^{\theta b} - e^{\theta a}}{\theta(b-a)}.
\ee

\be
\phi_X(t) = \E\brb{e^{it X}} = \int^b_a e^{it x} \frac 1{b-a} dx = \frac 1{it(b-a)}  \int^b_a de^{it x} = \frac{e^{it b} - e^{it a}}{it(b-a)}.
\ee
\end{proof}

\begin{proposition}\label{pro:moments_uniform}
Suppose $X \sim \ud{a}{b}$. Then
\be
\text{(i)}\ \E X = \frac 12(b+a),\quad\quad \text{(ii)}\ \var X = \frac 1{12}(b-a)^2,\quad\quad\text{(iii)}\ \skewness(X) = 0,\quad\quad\text{(iv)}\ \ekurt(X) = -\frac 65.
\ee
\end{proposition}

\begin{proof}[\bf Proof]%(i) and (ii) can be done using integration by parts. Or
With Propositions \ref{pro:mgf_uniform}, \ref{pro:mgf_finite_moment}, with L'H\^opital Rule\footnote{need theorem}
\beast
\E X = \left.\frac {d M_X(\theta)}{d\theta}\right|_{\theta=0} = \left.  \frac{(\theta b -1)e^{\theta b} - (\theta a -1)e^{\theta a}}{\theta^2(b-a)} \right|_{\theta=0}  =  \left.  \frac{\theta b^2 e^{\theta b} - \theta a^2e^{\theta a}}{2\theta(b-a)} \right|_{\theta=0} = \left. \frac{b^2 e^{\theta b} - a^2e^{\theta a}}{2(b-a)}  \right|_{\theta=0}=\frac 12(b+a).
\eeast

\beast
\E X^2 & = & \left.\frac {d^2 M_X(\theta)}{d\theta^2}\right|_{\theta=0} = \left.  \frac{\theta^2\brb{\theta b^2 e^{\theta b} - \theta a^2e^{\theta a}} - 2\theta\brb{(\theta b -1)e^{\theta b} - (\theta a -1)e^{\theta a}} }{\theta^4(b-a)}  \right|_{\theta=0} \\
& = & \left.  \frac{\brb{\theta^2 b^2 - 2\theta b + 2}e^{\theta b} - \brb{\theta^2 a^2 - 2\theta a + 2}e^{\theta a}}{\theta^3(b-a)}  \right|_{\theta=0} = \left.  \frac{\theta^2 b^3 e^{\theta b} - \theta^2 a^3 e^{\theta a}}{3\theta^2(b-a)}  \right|_{\theta=0}\\
& = & \frac{b^3 - a^3}{3(b-a)} = \frac 13 \brb{b^2 + ab + a^2}.
\eeast

Thus, $\var X = \E X^2 - \brb{\E X}^2 = \frac 13 \brb{b^2 + ab + a^2} - \frac 14 (b^2+ 2ab+ a^2) = \frac{1}{12}(b-a)^2$. Also,
\beast
\E X^3 & = & \left.\frac {d^3 M_X(\theta)}{d\theta^3}\right|_{\theta=0} = \left. \frac{\theta^3\brb{\theta^2 b^3 e^{\theta b} - \theta^2 a^3 e^{\theta a}}- 3\theta^2 \brb{\brb{\theta^2 b^2 - 2\theta b + 2}e^{\theta b} - \brb{\theta^2 a^2 - 2\theta a + 2}e^{\theta a}}}{\theta^6(b-a)}   \right|_{\theta=0} \\
& = & \left. \frac{\brb{\theta^3 b^3 - 3\theta^2 b^2 + 6\theta b - 6}e^{\theta b} - \brb{\theta^3 a^3 - 3\theta^2 a^2 + 6\theta a - 6}e^{\theta a}}{\theta^4(b-a)}   \right|_{\theta=0} = \left. \frac{\theta^3 b^4 e^{\theta b} - \theta^3 a^4 e^{\theta a}}{4\theta^3(b-a)}   \right|_{\theta=0} \\
& = &  \frac{b^4 - a^4}{4(b-a)} = \frac 14 \brb{b^3 + b^2 a + ba^2 + a^3}.
\eeast

Thus,
\beast
\E\brb{X-\mu}^3 = \E X^3 - 3\mu\E X^2 + 3\mu^2 \E X - \mu^3 = \frac 14 \brb{b^3 + b^2 a + ba^2 + a^3} - \frac 32 (b+a) \frac 13 \brb{b^2 + ab + a^2} + \frac 28(b+a)^3 = 0.
\eeast

By Definition \ref{def:skewness}, $\skewness(X) = 0$. Furthermore,
\beast
\E X^4 & = & \left.\frac {d^4 M_X(\theta)}{d\theta^4}\right|_{\theta=0} = \left. \frac{\theta^4\brb{\theta^3 b^4 e^{\theta b} - \theta^3 a^4 e^{\theta a}}- 4\theta^3 \brb{\brb{\theta^3 b^3 - 3\theta^2 b^2 + 6\theta b - 6}e^{\theta b} - \brb{\theta^3 a^3 - 3\theta^2 a^2 + 6\theta a - 6}e^{\theta a}}}{\theta^8(b-a)}   \right|_{\theta=0} \\
& = &  \left. \frac{\brb{\theta^4 b^4 - 4\theta^3 b^3 + 12\theta^2b^2 - 24\theta b + 24}e^{\theta b} - \brb{\theta^4 a^4 - 4\theta^3 a^3 + 12\theta^2a^2 - 24\theta a + 24}e^{\theta a}}{\theta^5(b-a)}   \right|_{\theta=0} \\
& = & \left. \frac{\theta^4 b^5 e^{\theta b} - \theta^4 a^5 e^{\theta a}}{5\theta^4(b-a)}   \right|_{\theta=0} = \frac{b^5 - a^5}{5(b-a)} = \frac 15 \brb{b^4 + b^3a + b^2a^2 + ba^3 + a^4}.
\eeast

Thus,
\beast
& & \E\brb{X-\mu}^4 = \E X^4 - 4\mu\E X^3 + 6\mu^2 \E X^2 - 4\mu^3\E X + \mu^4 \\
& = & \frac 15 \brb{b^4 + b^3a + b^2a^2 + ba^3 + a^4} - \frac 42(b+a)\frac 14 \brb{b^3 + b^2 a + ba^2 + a^3} + \frac 64(b+a)^2 \frac 13 \brb{b^2 + ab + a^2}- \frac 4{16}(b+a)^4 + \frac 1{16}(b+a)^4 \\
& = & \frac 1{80}(b-a)^4
\eeast

By Definition \ref{def:kurtosis}, we have $\ekurt(X) = \frac{(b-a)^4/80}{(b-a)^4/144} -3 = -\frac 65$.
\end{proof}

In deed, we can have more general result.

\begin{proposition}\label{pro:moments_general_uniform}
Suppose $X \sim \ud{a}{b}$. Then with $\mu = \frac 12 (a+b)$, for $n\in \N$
\be
\E X^n = \frac {b^{n+1}- a^{n+1}}{(n+1)(b-a)},\quad\quad \E (X-\mu)^n = \begin{cases}  \frac {(b-a)^n}{(n+1)2^n} & n\text{ is even} \\  0 & n\text{ is odd}   \end{cases}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
We know that $e^x = \sum^\infty_{n=0} \frac {x^n}{n!}$ with Taylor expansion. Thus, by Proposition \ref{pro:mgf_uniform}, \ref{pro:mgf_finite_moment},
\beast%e^{\theta a} = \E\brb{\sum^\infty_{n=0} \frac{(\theta a)^n}{n!}} \ \ra \
\E X^n & = & \left.\frac {d^n M_X(\theta)}{d\theta^n}\right|_{\theta=0} = \left.\brb{\frac {d^n }{d\theta^n}\brb{\frac{e^{\theta b} - e^{\theta a}}{\theta(b-a)}}}\right|_{\theta=0} = \frac 1{b-a}\left.\brb{\frac {d^n }{d\theta^n}\brb{\sum^\infty_{n=0}\frac{(\theta b)^n - (\theta a)^n}{\theta n!}}}\right|_{\theta=0}  \\
& = & \frac 1{b-a} \frac{n!(b^{n+1}-a^{n+1})}{(n+1)!} = \frac {b^{n+1}- a^{n+1}}{(n+1)(b-a)}.
\eeast

\beast
\E (X-\mu)^n & = & \sum^n_{k=0} C_n^k \E X^{n-k} (-\mu)^k = \sum^n_{k=0} \frac{n!}{(n-k)!k!} \frac {b^{n-k+1}- a^{n-k+1}}{(n-k+1)(b-a)} (-\mu)^k = \sum^n_{k=0} \frac{(n+1)!}{(n-k+1)!k!} \frac {b^{n-k+1}- a^{n-k+1}}{(n+1)(b-a)} (-\mu)^k \\
& = & \frac 1{(n+1)(b-a)} \brb{\sum^n_{k=0} C^{k}_{n+1} b^{n-k+1} (-\mu)^k - \sum^n_{k=0} C^{k}_{n+1} a^{n-k+1} (-\mu)^k} \\
& = & \frac 1{(n+1)(b-a)} \brb{\brb{b -\mu}^{n+1} - (-\mu)^{n+1} - \brb{a-\mu}^{n+1} + (-\mu)^{n+1}} = \frac 1{(n+1)(b-a)} \brb{\brb{b -\mu}^{n+1} - \brb{a-\mu}^{n+1} }
\eeast

Since $\mu = \frac 12(a+b)$, then $\E (X-\mu)^n = 0$ if $n$ is odd. If $n$ is even,
\be
\E (X-\mu)^n = \frac 1{(n+1)(b-a)} \brb{\brb{\frac{b -a}2}^{n+1} - \brb{-\frac{b-a}2}^{n+1} } = \frac {(b-a)^n}{(n+1)2^n}.
\ee

Thus, Proposition \ref{pro:moments_uniform} can be derived easily.
\end{proof}

\begin{proposition}\label{pro:uniform_max_min}
Let $X_1,\dots,X_n \stackrel{\text{i.i.d.}}{\sim} \ud{0}{1}$. Then define
\be
Y := \max\bra{X_1,\dots,X_n},\qquad Z := \min\bra{X_1,\dots,X_n}.
\ee

Then
\be
f_Y(y) = ny^{n-1},\quad y\in [0,1]\qquad f_Z(z) = n(1-z)^{n-1}, \quad z\in [0,1]
\ee
and
\beast
M_Y(\theta) & = & n!e^{\theta} \brb{\frac{1}{\theta(n-1)!} - \frac{1}{\theta^2(n-2)!} + \dots  + \frac{\brb{-1}^{n-1}}{\theta^{n}} } + \frac {(-1)^{n}n!}{\theta^{n}},\\
M_Z(\theta) & = & \frac {n!}{\theta^{n}}e^\theta -n! \brb{\frac{1}{\theta(n-1)!} + \frac{1}{\theta^2(n-2)!} + \dots  + \frac{1}{\theta^{n}} }
\eeast

In particular,
\be
\E Y^m = \frac{n}{n+m},\qquad \E Z^m = \frac{n!m!}{(n+m)!}.
\ee

Also,
\be
\cov(Y,Z) = \frac 1{(n+1)^2(n+2)},\quad \corr(Y,Z) = \frac 1n.
\ee
\end{proposition}

\begin{remark}
That $Y$ and $Z$ have positive correlation make sense since when $Z$ becomes large, $Y$ tends to become large as well ($Y\geq Z$).
\end{remark}

\begin{proof}[\bf Proof]
First, we have (as $X_1,\dots,X_n$ are i.i.d.)
\beast
\pro\brb{Y\leq y} & = & \pro\brb{\max\bra{X_1,\dots,X_n}\leq y} = \pro\brb{X_1\leq y, \dots,X_n \leq y} \\
& = & \pro\brb{X_1\leq y}\dots \pro\brb{X_n\leq y} = y^n,
\eeast

and
\beast
\pro\brb{Z\leq z} & = & \pro\brb{\min\bra{X_1,\dots,X_n}\leq z} = 1 - \pro\brb{X_1\geq z, \dots,X_n \geq z} \\
& = & 1 - \pro\brb{X_1\geq z}\dots \pro\brb{X_n\geq z} = 1 - (1-z)^n.
\eeast

Taking differentiation,
\be
f_Y(y) = ny^{n-1},\qquad f_Z(z) = n(1-z)^{n-1}.
\ee

Therefore, for $\theta \neq 0$. Let $I(n) = \int^1_0 e^{\theta y} y^n dy$ and thus $M_Y(\theta) = nI(n-1)$. Then $I(0) = \frac 1{\theta}\brb{e^{\theta} -1}$ and
\be
I(1) = \int^1_0 e^{\theta y} y dy = \frac 1{\theta} \int^1_0 y de^{\theta y} = \frac 1{\theta}\brb{e^{\theta} - I(0)} = e^\theta \brb{\frac 1{\theta} - \frac 1{\theta^2}} + \frac 1{\theta^2}
\ee
\beast
I(n) & = & \int^1_0 e^{\theta y} y^{n} dy = \frac{1}{\theta} \int^1_0 y^{n} de^{\theta y} = \frac 1{\theta }\brb{\left.y^{n} e^{\theta y}\right|^1_0 - n \int^1_0 e^{\theta y} y^{n-1} dy} \\
& = & \frac 1{\theta }\brb{e^{\theta } - n \int^1_0 e^{\theta y} y^{n-1} dy} = \frac 1{\theta}\brb{e^{\theta } - nI(n-1)}.
\eeast

Let $J(n) := \theta^nI(n)/n!$. Then
\be
J(n) = \frac{\theta^{n-1}e^{\theta}}{n!} - J(n-1).
\ee

Therefore,
\beast
J(n) & = & \frac{\theta^{n-1}e^{\theta}}{n!} - \frac{\theta^{n-2}e^{\theta}}{(n-1)!} + \dots + (-1)^nJ(0)\\
\theta^nI(n)/n! & = & \frac{\theta^{n-1}e^{\theta}}{n!} - \frac{\theta^{n-2}e^{\theta}}{(n-1)!} + \dots + (-1)^nI(0) = \frac{\theta^{n-1}e^{\theta}}{n!} - \frac{\theta^{n-2}e^{\theta}}{(n-1)!} + \dots + (-1)^n\frac 1{\theta}\brb{e^{\theta} -1}\\
I(n) & = & n!e^{\theta} \brb{\frac{1}{\theta n!} - \frac{1}{\theta^{2}(n-1)!} + \dots + \frac{(-1)^n}{\theta^{n+1}} } + \frac {(-1)^{n+1} n!}{\theta^{n+1}}\\
I(n) & = & \frac {n!e^{\theta}}{\theta^{n+1}} \brb{\frac{\theta^{n}}{n!} - \frac{\theta^{n-1}}{(n-1)!} + \dots + (-1)^n } + \frac {(-1)^{n+1} n!}{\theta^{n+1}}\\
I(n) & = & \frac {(-1)^nn!e^{\theta}}{\theta^{n+1}} \brb{1 + \frac{\brb{-\theta}}{1!} + \dots + \frac{\brb{-\theta}^{n-1}}{(n-1)!} + \frac{\brb{-\theta}^{n}}{n!}} + \frac {(-1)^{n+1}n!}{\theta^{n+1}}%I(n) & = & \frac {(-1)^nn!e^{\theta}}{\theta^{n+1}} \brb{e^{-\theta} - \frac{(-1)^{n+1}\theta^{n+1}}{(n+1)!} - \text{other following Taylor expansion}} + \frac {(-1)^n}{\theta^{n+1}}\\%I(n) & = & \frac {(-1)^{n+1}n!e^{\theta}}{\theta^{n+1}} \brb{\frac{(-1)^{n+1}\theta^{n+1}}{(n+1)!} + \text{other following Taylor expansion}}
\eeast

Therefore,
\beast
M_Y(\theta) = nI(n-1) & = & \frac {(-1)^{n-1}n!e^{\theta}}{\theta^{n}} \brb{1 + \frac{\brb{-\theta}}{1!} + \dots + \frac{\brb{-\theta}^{n-1}}{(n-1)!} } + \frac {(-1)^{n}n!}{\theta^{n}}\\
& = & n!e^{\theta} \brb{\frac{1}{\theta(n-1)!} - \frac{1}{\theta^2(n-2)!} + \dots  + \frac{\brb{-1}^{n-1}}{\theta^{n}} } + \frac {(-1)^{n}n!}{\theta^{n}}\qquad (*)
\eeast

%\be
%I(n) = ne^{\theta}\brb{\frac{1}{\theta n!} - \frac{1}{\theta^{2}(n-1)!} + \dots + (-1)^{n-1}\frac{1}{\theta^{n}}} -  \frac n{\theta^{n+1}}\brb{e^{\theta} -1}.
%\ee

Then taking differentiation of ($*$),
\beast
\frac{dM_Y(\theta)}{d\theta} & = & n!e^{\theta} \brb{\frac{1}{\theta(n-1)!} - \frac{1}{\theta^2(n-2)!} + \dots  + \frac{\brb{-1}^{n-1}}{\theta^{n}} + \frac{-1}{\theta^2(n-1)!}  + \dots  + \frac{\brb{-1}^{n-1}(n-1)}{\theta^n} + \frac{\brb{-1}^{n}n}{\theta^{n+1}} } + \frac {n(-1)^{n+1}n!}{\theta^{n+1}}\\
& = & n!e^{\theta} \brb{\frac{n}{\theta n!} - \frac{n}{\theta^2(n-1)!} + \dots  + \frac{n\brb{-1}^{n-1}}{\theta^{n}1!} + \frac{n\brb{-1}^{n}}{\theta^{n+1}} } + \frac {n(-1)^{n+1}n!}{\theta^{n+1}}\qquad (**)\\
& = & \frac {n(-1)^n n!e^{\theta}}{\theta^{n+1}} \brb{1 + \frac{\brb{-\theta}}{1!} + \frac{\brb{-\theta}^2}{2!} + \dots + \frac{\brb{-\theta}^n}{n!}} + \frac {n(-1)^{n+1}n!}{\theta^{n+1}}\\
& = & \frac {n(-1)^n n!e^{\theta}}{\theta^{n+1}} \brb{e^{-\theta} - \frac{(-1)^{n+1}\theta^{n+1}}{(n+1)!} - \text{other following Taylor expansions}} + \frac {n(-1)^{n+1}n!}{\theta^{n+1}}\\
& = & -n(-1)^n n! \frac{(-1)^{n+1}}{(n+1)!} + O\brb{\theta} = \frac{n!n}{(n+1)!} + O\brb{\theta} = \frac{n}{n+1} + O\brb{\theta}.
\eeast

Letting $\theta = 0$, we have
\be
\E Y = \left.\frac{dM_Y(\theta)}{d\theta}\right|_{\theta = 0} = \frac{n}{n+1}.
\ee
%\beast
%M_Y(\theta) & = & \E\brb{e^{\theta Y}} = \int^1_0 e^{\theta y} ny^{n-1} dy = \frac{1}{\theta} \int^1_0 ny^{n-1} de^{\theta y} \\
%& = & \frac 1{\theta }\brb{\left.ny^{n-1} e^{\theta y}\right|^1_0 - n (n-1) \int^1_0 e^{\theta y} y^{n-2} dy} = \frac n{\theta }\brb{e^{\theta } - (n-1) \int^1_0 e^{\theta y} y^{n-2} dy}.
%\eeast
%By symmetry, we have $Z = 1-Y$

Taking differentiation of ($**$),
\beast
\frac{d^2M_Y(\theta)}{d\theta^2} & = & n!e^{\theta} n \brb{\frac{1}{\theta n!} - \frac{1}{\theta^2(n-1)!} + \dots  + \frac{\brb{-1}^{n-1}}{\theta^{n}1!} + \frac{\brb{-1}^{n}}{\theta^{n+1}} -\frac{1}{\theta^2 n!} + \frac{2}{\theta^3(n-1)!} - \dots  + \frac{n\brb{-1}^{n}}{\theta^{n+1}1!} + \frac{(n+1)\brb{-1}^{n+1}}{\theta^{n+2}}  } \\
& & \qquad + \frac {n(n+1)(-1)^{n+2}n!}{\theta^{n+2}} \\
& = & n!e^{\theta}n \brb{\frac{n+1}{\theta (n+1)!} - \frac{n+1}{\theta^2 n!} + \dots  + \frac{(n+1)\brb{-1}^{n}}{\theta^{n+1}1!} + \frac{(n+1)\brb{-1}^{n+1}}{\theta^{n+2}} } + \frac {n(n+1)(-1)^{n+2}n!}{\theta^{n+2}} \\
& = & \frac{n(n+1) (-1)^{n+1} n!e^{\theta}}{\theta^{n+2}} \brb{1 + \frac{(-\theta)}{1!} + \frac{(-\theta)^2}{2!} + \dots  + \frac{\brb{-\theta}^{n+1}}{(n+1)!} } + \frac {n(n+1)(-1)^{n+2}n!}{\theta^{n+2}} \\
& = & \frac{n(n+1) (-1)^{n+1} n!e^{\theta}}{\theta^{n+2}} \brb{e^{-\theta} - \frac{(-1)^{n+2}\theta^{n+2}}{(n+2)!} - \text{other following Taylor expansions}}  + \frac {n(n+1)(-1)^{n+2}n!}{\theta^{n+2}}  \\
& = & -n(n+1) (-1)^{n+1} n! \frac{(-1)^{n+2}}{(n+2)!}  + O\brb{\theta} = \frac{n(n+1)n!}{(n+2)!} + O\brb{\theta} = \frac{n}{n+2} + O\brb{\theta}.
\eeast

Letting $\theta = 0$, we have
\be
\E Y^2 = \left.\frac{d^2M_Y(\theta)}{d\theta^2}\right|_{\theta = 0} = \frac{n}{n+2} \ \ra \ \var Y = \frac{n}{n+2} - \brb{\frac{n}{n+1}}^2 = \frac{n}{(n+1)^2(n+2)}.
\ee

Similarly, we can see that
\be
\E Y^m = \frac{n}{n+m} = \int^1_0 nx^{n+m-1}dx = \int^1_0 x^m nx^{n-1}dx.
\ee

For $Z$, we have
\beast
M_Z(\theta) = \E\brb{e^{\theta Z}} & = & \int^1_0 e^{\theta z} n(1-z)^{n-1}dz = \int^1_0 e^{\theta(1-z)} nz^{n-1}dz = e^\theta \int^1_0 e^{-\theta z}nz^{n-1}dz = e^{\theta}M_Y(-\theta)\\
& = & n! \brb{\frac{1}{(-\theta)(n-1)!} - \frac{1}{\brb{-\theta}^2(n-2)!} + \dots  + \frac{\brb{-1}^{n-1}}{\brb{-\theta}^{n}} } + \frac {(-1)^{n}n!}{\brb{-\theta}^{n}} e^\theta \\
& = & -n! \brb{\frac{1}{\theta(n-1)!} + \frac{1}{\theta^2(n-2)!} + \dots  + \frac{1}{\theta^{n}} } + \frac {n!}{\theta^{n}}e^\theta \qquad (\dag)
\eeast

Then taking differentiation of ($\dag$),
\beast
\frac{dM_Z(\theta)}{d\theta} & = & n! \brb{\frac{1}{\theta^2 (n-1)!} + \frac{2}{\theta^3(n-2)!} + \dots  + \frac{n}{\theta^{n+1}} } + n!e^\theta \brb{\frac{1}{\theta^n} -\frac{n}{\theta^{n+1}}} \\
& = & \frac{n!}{\theta^{n+1}} \brb{\frac{\theta^{n-1}}{ (n-1)!} + \frac{2\theta^{n-2}}{(n-2)!} + \dots  + n } \\
& & \qquad + n!\brb{1+ \theta + \frac{\theta^2}2 + \dots + \frac{\theta^n}{n!} + \frac{\theta^{n+1}}{(n+1)!} + \text{other following Taylor expansions} } \frac{\theta - n}{\theta^{n+1}} \\
& = & \frac{n!}{\theta^{n+1}} \brb{\frac{\theta^{n-1}}{ (n-1)!} + \frac{2\theta^{n-2}}{(n-2)!} + \dots  + n + \theta + \theta^2 + \frac{\theta^3}2 + \dots + \frac{\theta^{n+1}}{n!} -n - n\theta - \frac{n\theta^2}2 - \dots - \frac{n\theta^{n+1}}{(n+1)!} + O(\theta^{n+2}) }\\
& = & \frac{n!}{\theta^{n+1}} \brb{\frac{\theta^{n}}{(n-1)!} + \frac{\theta^{n+1}}{n!} - \frac{n\theta^n}{n!} - \frac{n\theta^{n+1}}{(n+1)!} + O(\theta^{n+2}) } = \frac{1}{n+1} + O\brb{\theta}.
\eeast

Letting $\theta = 0$, we have
\be
\E Z = \left.\frac{dM_Z(\theta)}{d\theta}\right|_{\theta = 0} = \frac{1}{n+1}.
\ee

Similarly, by Proposition \ref {pro:beta_gamma_relation}
\be
\E Z^m = \int^1_0 z^m n(1-z)^{n-1} dz = n\int^1_0 z^m (1-z)^{n-1} dz = n\frac{\Gamma(m+1)\Gamma(n)}{\Gamma(m+n+1)} = \frac{ m!n!}{(m+n)!}.
\ee

In particular, $\var Z = \frac{n!2}{(n+2)!} - \brb{\frac{1}{n+1}}^2  = \frac{2(n+1)-(n+2)}{(n+1)^2(n+2)} = \frac n{(n+1)^2(n+2)}$.

Second, we have for $z\leq y$,
\be
\pro\brb{Y\leq y,Z\geq z} = \pro\brb{z\leq X_1\leq y,\dots,z\leq X_n\geq y} = \pro\brb{z\leq X_1\leq y}\dots \pro\brb{z\leq X_n\leq y} = (y-z)^n
\ee
and
\be
\pro\brb{Y\leq y,Z\leq z} = \pro\brb{Y\leq y} - \pro\brb{Y\leq y,Z\geq z} = y^n - (y-z)^n
\ee

Thus,
\be
f_{Y,Z}(y,z) = \frac{\partial^2 \brb{y^n - (y-z)^n}}{\partial y\partial z} = n(n-1)(y-z)^{n-2}.
\ee

Thus, by Definition \ref{def:beta_function} and Proposition \ref {pro:beta_gamma_relation}
\beast
\E(YZ) & = & \int^1_0 \int^1_z yz n(n-1)(y-z)^{n-2} dydz = \int^1_0 \int^1_z (y-z+z)z n(n-1)(y-z)^{n-2} dydz \\
& = & \int^1_0 \int^1_z n(n-1)(y-z)^{n-1}z dydz + \int^1_0 \int^1_z n(n-1)(y-z)^{n-2}z^2 dydz \\
& = & \int^1_0 (n-1)(1-z)^{n}z dz + \int^1_0 n(1-z)^{n-1}z^2 dz =  (n-1) \int^1_0(1-z)^{n-1}z dz + \int^1_0 (1-z)^{n-1}z^2 dz \\ %& = & \int^1_0 n\left.y(y-z)^{n-1}\right|^1_z z dz = \int^1_0 n (1-z)^{n-1} dz = (1-z)^n
& = & (n-1) B(n,2) + B(n,3) = (n-1)\frac{\Gamma(n)\Gamma(2)}{\Gamma(n+2)} + \frac{\Gamma(n)\Gamma(3)}{\Gamma(n+3)} \\
& = & \frac{(n-1)(n-1)!}{(n+1)!} + \frac{(n-1)!2!}{(n+2)!} = \frac{(n-1)}{n(n+1)} + \frac{2}{n(n+1)(n+2)} = \frac{(n-1)(n+2) + 2}{n(n+1)(n+2)} = \frac{1}{n+2}.
\eeast

Thus,
\be
\cov(Y,Z) = \E(YZ) - \E Y \E Z = \frac{1}{n+2} - \frac{1}{n+1}\frac{n}{n+1} = \frac{1}{(n+1)^2(n+2)}.
\ee

Thus
\be
\corr(Y,Z) = \frac{\cov(Y,Z)}{\sqrt{\var Y\var Z}} = \frac{\frac{1}{(n+1)^2(n+2)}}{\frac{n}{(n+1)^2(n+2)}} = \frac 1n.
\ee
\end{proof}




\subsection{Exponential random variables}

\begin{definition}[exponential random variable]\label{def:exponential_rv}
A random variable $X$ in $\R$ is exponential distributed\index{exponential random variable!$\R$} if, for some $\lm > 0, \lm \in \R$, $X$ has density function
\be
f_X(x) = \begin{cases} \lm e^{-\lm x}, & x \ge 0, \\ 0, & x < 0. \end{cases}
\ee

We write $X \sim \sE(\lm)$. The cumulative distribution function of exponential distribution is given by
\be
F(x) = \begin{cases} 1-e^{-\lm x}, & x \ge 0, \\ 0, & x < 0. \end{cases}
\ee
\end{definition}


\begin{proposition}\label{pro:mgf_exponential}
Suppose $X \sim \sE(\lm)$. Then for $\theta < \lm$ and $t\in \R$,
\be
M_X(\theta) = \frac{\lm}{\lm - \theta},\quad\quad \phi_X(t) = \frac{\lm}{\lm - it}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
From Definition \ref{def:mgf_probability},
\be
M_X(\theta) = \E\brb{e^{\theta X}} = \int^\infty_0 \lm e^{\theta x} e^{-\lm x} dx = \frac{\lm}{\theta - \lm} \int^\infty_0 de^{(\theta -\lm)x} = \frac{\lm}{\lm - \theta}.
\ee

\be
\phi_X(t) = \E\brb{e^{it X}} = \int^\infty_0 \lm e^{it x} e^{-\lm x} dx = \int^\infty_0 \lm e^{-\lm x}\brb{\cos tx + i\sin tx} dx .
\ee

Therefore,
\beast
I = \int^\infty_0 \lm e^{-\lm x}\brb{\cos tx}dx & = & - \brb{\left.e^{-\lm x}\cos tx\right|^\infty_0 + t \int^\infty_0 e^{-\lm x} \sin tx dx} = - \brb{-1 - \frac t{\lm} \int^\infty_0  \sin tx de^{-\lm x}}\\
& = & - \brb{-1 - \frac t{\lm} \brb{\left.\sin tx e^{-\lm x}\right|^\infty_0 - t\int^\infty_0  e^{-\lm x}\cos tx  dx}} = 1 - \frac {t^2}{\lm^2} I.
\eeast

Thus, $\lm^2 I = \lm^2 - t^2 I \ \ra \ I = \frac{\lm^2}{\lm^2 + t^2}$. Similarly,
\beast
J = \int^\infty_0 \lm e^{-\lm x}\brb{\sin tx}dx & = & - \brb{\left.e^{-\lm x}\sin tx\right|^\infty_0 - t \int^\infty_0 e^{-\lm x} \cos tx dx} = - \brb{\frac t{\lm} \int^\infty_0  \cos tx de^{-\lm x}}\\
& = & - \brb{\frac t{\lm} \brb{\left.\cos tx e^{-\lm x}\right|^\infty_0 + t\int^\infty_0  e^{-\lm x}\sin tx  dx}} = \frac t{\lm}\brb{1 - \frac t{\lm} J}.
\eeast

Thus, $\lm^2 J = \lm t - t^2 J \ \ra \ J = \frac{\lm t}{\lm^2 + t^2}$. Thus,
\be
\phi_X(t) = \frac{\lm^2}{\lm + t^2} + \frac{i\lm t}{\lm + t^2} = \frac{\lm^2(\lm + it)}{\lm^2 + t^2} = \frac{\lm}{\lm - it}.
\ee
\end{proof}

\begin{proposition}\label{pro:moments_exponential}
Suppose $X \sim \sE(\lm)$. Then \be \text{(i)}\ \E X = \frac 1{\lm},\quad\quad \text{(ii)}\ \var X = \frac 1{\lm^2},\quad\quad\text{(iii)}\ \skewness(X) = 2,\quad\quad\text{(iv)}\ \ekurt(X) = 6,\quad\quad \text{(v)} \E X^n
= \frac {n!}{\lm^n}. \ee
\end{proposition}

\begin{proof}[\bf Proof]%(i) and (ii) can be done using integration by parts. Or
With Propositions \ref{pro:mgf_exponential}, \ref{pro:mgf_finite_moment},
\beast
\E X = \left.\frac {d M_X(\theta)}{d\theta}\right|_{\theta=0} = \left. \frac{\lm}{(\lm -\theta)^2}\right|_{\theta=0} = \frac {\lm}{\lm^2} = \frac 1{\lm},\qquad \E X^2 = \left.\frac {d^2 M_X(\theta)}{d\theta^2}\right|_{\theta=0} = \left. \frac{2\lm}{(\lm -\theta)^3} \right|_{\theta=0} = \frac{2}{\lm^2}.
\eeast

Thus, $\var X = \E X^2 - \brb{\E X}^2 = \frac{2}{\lm^2} - \frac{1}{\lm^2} = \frac{1}{\lm^2}$. Also,
\be
\E X^3 = \left.\frac {d^3 M_X(\theta)}{d\theta^3}\right|_{\theta=0} = \left. \frac{6\lm}{(\lm -\theta)^4}  \right|_{\theta=0} = \frac{6}{\lm^3}.
\ee

Thus,
\be
\E\brb{X-\mu}^3 = \E X^3 - 3\mu\E X^2 + 3\mu^2 \E X - \mu^3 = \frac{6}{\lm^3} - 3\frac{1}{\lm}  \frac{2}{\lm^2} + 3 \frac{1}{\lm^2} \frac{1}{\lm^2} - \frac{1}{\lm^3} = \frac{2}{\lm^3}.
\ee

By Definition \ref{def:skewness}, $\skewness(X) = \frac{2/\lm^3}{1/\lm^3} = 2$. Furthermore,
\be
\E X^4 = \left.\frac {d^4 M_X(\theta)}{d\theta^4}\right|_{\theta=0} = \left. \frac{24\lm}{(\lm -\theta)^5}  \right|_{\theta=0}  = \frac{24}{\lm^4}.
\ee

Thus,
\be
\E\brb{X-\mu}^4 = \E X^4 - 4\mu\E X^3 + 6\mu^2 \E X^2 - 4\mu^3\E X + \mu^4 = \frac{24}{\lm^4} - 4\frac{1}{\lm}\frac{6}{\lm^3} + 6\frac{1}{\lm^2}\frac{2}{\lm^2}- 4\frac{1}{\lm^3}\frac{1}{\lm} + \frac{1}{\lm^4} = \frac{9}{\lm^4} .
\ee

By Definition \ref{def:kurtosis}, we have $\ekurt(X) = \frac{9/\lm^4}{1/\lm^4} -3 = 6$.

For (v), we know that $\frac {d^n M_X(\theta)}{d\theta^n} = \lm n!/(\lm -\theta)^{n+1}$,
\be
\E X^n = \left.\frac {d^n M_X(\theta)}{d\theta^n}\right|_{\theta=0} = \frac{\lm n!}{\lm^{n+1}} = \frac{n!}{\lm^n}.
\ee
\end{proof}

The exponential distribution is sometimes used to model the lifetime of a component. %If $X$ is the lifetime and $X \sim \sE(\lm)$, then the probability that the component survives a length of time $x > 0$ is $\pro (X > x) = e^{-\lm x}$.

\begin{proposition}[memoryless property of the exponential distribution\index{memoryless property!exponential distribution}]\label{pro:memoryless_exponential_rv}
Let $X\sim \sE(\lm)$. Then for $x > 0$ and $y > 0$,
\be
\pro (X > x + y | X > y) = e^{-\lm x}.
\ee
\end{proposition}

\begin{remark}
So that, given the component has survived a length of time $y$ the probability that it will survive a further time $x$ is the same as if it has just been installed. This property, which is crucial to the study of stochastic processes, is known as the lack of memory property of the exponential distribution.
\end{remark}

\begin{proof}[\bf Proof]
We have that
\be
\pro (X > x + y | X > y) = \frac{\pro (X > x + y,X > y)}{\pro (X > y)} = \frac{\pro (X > x + y)}{\pro (X > y)} = \frac{e^{-\lm(x+y)}}{e^{-\lm y}} = e^{-\lm x},
\ee
as required.
\end{proof}

\begin{proposition}[minimum of independent exponentials is exponential]\label{pro:exponential_minimum}
Suppose that $X_i \sim \sE(\lm_i)$, $i = 1,\dots,n$ are independent. Then $\min\brb{X_1,\dots,X_n} \sim \sE\brb{\sum_i \lm_i}$.
\end{proposition}

\begin{proof}[\bf Proof]
Using the independence, we see that for $x \geq 0$,
\beast
\pro \brb{\min_i X_i \leq x} & = & 1 - \pro \brb{\min_i X_i > x} = 1 - \pro \brb{X_1> x,\dots,X_n > x} \\
& = & 1 - \prod_{i=1}^n \pro \brb{X_i > x} = 1 - \prod^n_{i=1} e^{-\lm_i x} = 1 - e^{-x\sum_i \lm_i},
\eeast
so that $\min\brb{X_1,\dots,X_n} \sim \sE\brb{\sum_i \lm_i}$.
\end{proof}



\begin{proposition}
Let $X_1,X_2,\dots \stackrel{\text{i.i.d.}}{\sim} \sE(\lm)$ and $Y=\max\bra{n:X_1+X_2+\cdots+X_n\leq 1}$. Then $Y \sim \pd(\lm)$.
\end{proposition}

\begin{proof}[\bf Proof]
We define $Z=X_1+\cdots+X_n\sim \Gamma(n,\lambda)$. Then
\beast
\mathbb{P}(Y=n) & = & \mathbb{P}(X_1+\cdots+X_n\leq 1, X_1+\cdots+X_n+X_{n+1}>1) = \mathbb{P}(Z\leq 1,Z+X_{n+1}>1)\\
& = & \int^1_0 \mathbb{P}(X>1-z)f_Z(z)dz = \int^1_0 e^{-\lambda(1-z)}\frac{\lambda^n}{e^{-\lambda z}z^{n-1}}{(n-1)!}dz \\
& = & \int^1_0 \frac{\lambda^n}{e^{-\lambda}z^{n-1}}{(n-1)!}dz = \frac{\lambda^n e^{-\lambda}}{n!}
\eeast
as required.
\end{proof}


\subsection{Gaussian random variables}

\begin{definition}[Gaussian random variable]\label{def:gaussian_rv}
A random variable $X$ in $\R$ is Gaussian distributed\index{Gaussian random variable!$\R$} if, for some $\mu \in \R$ and some $\sigma^2 \in (0,\infty)$, $X$ has density function
\be
f_X(x) = \frac 1{\sqrt{2\pi}\sigma} e^{-(x-\mu)^2/2\sigma^2}.
\ee

We also admit as Gaussian any random variable $X$ with $X = \mu$ a.s., this degenerate case corresponding to taking $\sigma^2 = 0$. We write $X \sim \sN(\mu, \sigma^2)$.
\end{definition}

%\begin{center}
%\begin{pspicture}(-5,-3)(5,5.5)
%\psaxes{->}(0,0)(-5,-2)(5,4.5)
%\psset{plotpoints=500,linewidth=1pt}
%\psFourier[cosCoeff=2, linecolor=green]{-4.5}{4.5}
%\psFourier[cosCoeff=0 0 2, linecolor=magenta]{-4.5}{4.5}
%\psFourier[cosCoeff=2 0 2, linecolor=red]{-4.5}{4.5}
%\end{pspicture}
%\end{center}

\begin{center}
\psset{yunit=4cm,xunit=3}
\begin{pspicture}(-2,-0.2)(2,1.4)
% \psgrid[griddots=10,gridlabels=0pt, subgriddiv=0]
\psaxes[Dy=0.25]{->}(0,0)(-2,0)(2,1.25)
\uput[-90](6,0){$x$}\uput[0](0,1){$y$}
\rput[lb](1.5,0.75){\textcolor{red}{$\sigma =0.5$}}
\rput[lb](1.5,0.5){\textcolor{blue}{$\sigma =1$}}
\rput[lb](-2,0.5){$f(x)=\dfrac{1}{\sigma\sqrt{2\pi}}\,e^{-\dfrac{(x-\mu)^2}{2\sigma{}^2}}$}
\rput[lb](1.5,1.05){$F(x)$}
\psGauss[linecolor=red, linewidth=2pt]{-1.75}{1.75}%
\psGaussI[linewidth=1pt]{-2}{2}%
\psGauss[linecolor=green, mue=0.6, linewidth=2pt]{-1.75}{1.75}%cyan
\psGauss[sigma=1, linecolor=blue, linewidth=2pt]{-1.75}{1.75}
\end{pspicture}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{proposition}\label{pro:mgf_gaussian}
Suppose $X \sim \sN(\mu, \sigma^2)$. Then
\be
M_X(\theta) = \exp\brb{\mu \theta + \sigma^2\theta^2/2},\qquad \phi_{X}(t) = \exp\brb{it\mu-t^2\sigma^2/2}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
From Definition \ref{def:mgf_probability}, we have
\beast
M_X(\theta) = \E\brb{e^{\theta X}} & = & \frac 1{\sqrt{2\pi}\sigma} \int^\infty_{-\infty} e^{\theta x} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx = \frac 1{\sqrt{2\pi}\sigma} \int^\infty_{-\infty}  e^{-\frac{x^2-2\mu x + \mu^2}{2\sigma^2}} e^{\frac{2\sigma^2\theta x}{2\sigma^2}}dx \\
& = & \frac 1{\sqrt{2\pi}\sigma} \int^\infty_{-\infty}  e^{-\frac{x^2- 2(\sigma^2\theta +\mu) x + \mu^2}{2\sigma^2}} dx  = \frac 1{\sqrt{2\pi}\sigma} \int^\infty_{-\infty}  e^{-\frac{(x-\sigma^2\theta -\mu)^2 + \mu^2 - (\sigma^2\theta +\mu)^2}{2\sigma^2}} dx\\ & = &  e^{-\frac{\mu^2 - (\sigma^2\theta +\mu)^2}{2\sigma^2}} \frac 1{\sqrt{2\pi}\sigma} \int^\infty_{-\infty}  e^{-\frac{(x-\sigma^2-\mu)^2}{2\sigma^2}} dx = e^{\frac{(\sigma^2\theta +\mu)^2}{2\sigma^2}-\mu^2 } = e^{\mu \theta + \sigma^2\theta^2/2}.
\eeast

Similarly, we have the result for characteristic function.
\end{proof}

\begin{proposition}\label{pro:moments_gaussian}
Suppose $X \sim \sN(\mu, \sigma^2)$. Then \be \text{(i)}\ \E X = \mu,\quad\quad \text{(ii)}\ \var X = \sigma^2,\quad\quad\text{(iii)}\ \skewness(X) = 0,\quad\quad\text{(iv)}\ \ekurt(X) = 0. \ee
\end{proposition}

\begin{proof}[\bf Proof]%(i) and (ii) can be done using integration by parts. Or
With Propositions \ref{pro:mgf_gaussian}, \ref{pro:mgf_finite_moment},
\be
\E X = \left.\frac {d M_X(\theta)}{d\theta}\right|_{\theta=0} = \left.\brb{\mu + \sigma^2 \theta} e^{\mu \theta + \sigma^2\theta^2/2} \right|_{\theta=0} = \mu.
\ee

\be
\E X^2 = \left.\frac {d^2 M_X(\theta)}{d\theta^2}\right|_{\theta=0} = \left.\brb{\brb{\mu + \sigma^2 \theta}^2 + \sigma^2} e^{\mu \theta + \sigma^2\theta^2/2} \right|_{\theta=0} = \mu^2 + \sigma^2.
\ee

Thus, $\var X = \E X^2 - \brb{\E X}^2 = \mu^2 + \sigma^2 - \mu^2 = \sigma^2$. Also,
\be
\E X^3 = \left.\frac {d^3 M_X(\theta)}{d\theta^3}\right|_{\theta=0} = \left.\brb{\brb{\mu + \sigma^2 \theta}^3 + 3\sigma^2\brb{\mu + \sigma^2 \theta}} e^{\mu \theta + \sigma^2\theta^2/2} \right|_{\theta=0} = \mu^3 + 3\mu\sigma^2.
\ee

Thus,
\be
\E\brb{X-\mu}^3 = \E X^3 - 3\mu\E X^2 + 3\mu^2 \E X - \mu^3 = \mu^3 + 3\mu\sigma^2 - 3\mu\brb{\mu^2 + \sigma^2} + 3\mu^3 - \mu^3 = 0.
\ee

By Definition \ref{def:skewness}, $\skewness(X) = 0$. Furthermore,
\be
\E X^4 = \left.\frac {d^4 M_X(\theta)}{d\theta^4}\right|_{\theta=0} = \left.\brb{\brb{\mu + \sigma^2 \theta}^4 + 6\sigma^2\brb{\mu + \sigma^2 \theta}^2 + 3\sigma^4} e^{\mu \theta + \sigma^2\theta^2/2} \right|_{\theta=0} = \mu^4 + 6\mu^2\sigma^2 + 3\sigma^4.
\ee

Thus,
\beast
\E\brb{X-\mu}^4 & = & \E X^4 - 4\mu\E X^3 + 6\mu^2 \E X^2 - 4\mu^3\E X + \mu^4 \\
& = & \mu^4 + 6\mu^2\sigma^2 + 3\sigma^4 - 4\mu\brb{\mu^3 + 3\mu\sigma^2} + 6\mu^2\brb{\mu^2 + \sigma^2} - 3\mu^4 = 3\sigma^4 .
\eeast

By Definition \ref{def:kurtosis}, we have $\ekurt(X) = \frac{3\sigma^4}{\sigma^4} -3 = 0$.
\end{proof}


\begin{proposition}
Let $k$ be a positive integer and let $X\sim \sN(0,\sigma^2)$. Then
\be
\E X^k = \left\{\ba{ll}
0 & k \text{ is odd}\\
\frac{k!\sigma^k}{2^{k/2}(k/2)!}\quad\quad & k \text{ is even}
\ea\right.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
We have
\be
\E X^k=\frac{1}{\sqrt{2\pi}\sigma}\int^\infty_{-\infty}x^k\exp\brb{-\frac{x^2}{2\sigma^2}}dx
\ee

If $k$ is odd, then $\E X^k=0$. If $k$ is even,
\beast
\E X^k & = & \left[\frac{\sigma^2}{\sqrt{2\pi}\sigma}\left(-x^{k-1}\right)\exp\brb{-\frac{x^2}{2\sigma^2}}\right]^\infty_{-\infty} + \frac{\sigma^2}{\sqrt{2\pi}\sigma}\int^\infty_{-\infty}(k-1)x^{k-2}\exp\brb{-\frac{x^2}{2\sigma^2}}dx \\
& = & \sigma^2(k-1)\mathbb{E}(X^{k-2}) = \sigma^2(k-1)\sigma^2(k-3)\cdots \E X^0 = \frac{k!\sigma^k}{2^{k/2}(k/2)!}
\eeast
as required.
\end{proof}



\begin{proposition}\label{pro:linear_gaussian}
Suppose $X \sim \sN(\mu, \sigma^2)$ and $a,b\in \R$. Then
\be
aX + b \sim \sN\brb{a\mu + b, a^2\sigma^2}.%,\quad\quad \text{(ii)}\ \phi_{X}(u) = e^{iu\mu-u^2\sigma^2/2}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
For (i), note that
\[
 F_{aX + b} (x) = \pro(aX + b \leq x) = \pro\left(X \leq \frac{x - b}{a} \right) = F_X \left( \frac{x - b}{a} \right)
\]
so
\[
 f_{aX +b} (x) = \frac{d}{dx} F_{aX + b} (x) = \frac{1}{a} f_X \left( \frac{x - b}{a} \right) = \frac{1}{\sqrt{2 \pi (a\sigma)^2}} \exp \left(-\frac{\brb{x - (a\mu + b)}^2}{2(a\sigma)^2} \right).
\]
Thus $aX + b \sim \sN(a\mu + b, a\sigma)$.     %For part (ii), note that from (i), we have that if $Z \sim \sN(0,1)$, then $X$ and $\mu + \sigma Z$ have the same distribution. Thus \vspace{2mm} $\qquad\qquad\qquad\qquad\qquad\qquad\E(e^{iuX})=e^{iu\mu} \E(e^{iu\sigma Z}) = e^{iu\mu} \phi_Z (u\sigma) = e^{iu\mu - u^2\sigma^2 / 2}$.
\end{proof}

\begin{proposition}\label{pro:abs_gaussian}
Suppose $X\sim \sN\brb{\mu,\sigma^2}$, Then
\be
\E\abs{X} = \sqrt{\frac 2{\pi}} \sigma \exp\brb{-\frac{\mu^2}{2\sigma^2}} + \mu\brb{2\Phi\brb{\frac{\mu}{\sigma}} -1},\quad\quad \text{where}\quad\Phi(x) = \int^x_{-\infty} \frac 1{\sqrt{2\pi}}\exp\brb{-\frac{x^2}2}dx.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
We know the density function is $\frac 1{\sqrt{2\pi}\sigma} \exp\brb{-\frac{(x-\mu)^2}{2\sigma^2}}$, thus
\beast
\E\abs{X} & = & \frac 1{\sqrt{2\pi}\sigma}  \int^\infty_{-\infty} \abs{x}\exp\brb{-\frac{(x-\mu)^2}{2\sigma^2}}dx = \frac 1{\sqrt{2\pi}\sigma}\brb{\int^\infty_0 x\exp\brb{-\frac{(x+\mu)^2}{2\sigma^2}}dx + \int^\infty_0 x\exp\brb{-\frac{(x-\mu)^2}{2\sigma^2}}dx}\\
& = & \frac 1{\sqrt{2\pi}\sigma}\lob\int^\infty_0 (x+\mu)\exp\brb{-\frac{(x+\mu)^2}{2\sigma^2}}dx + \int^\infty_0 (x-\mu)\exp\brb{-\frac{(x-\mu)^2}{2\sigma^2}}dx \right. \\
& & \qquad\qquad\qquad\qquad \qquad\qquad \qquad\qquad  \left. + \mu \int^\infty_0 \brb{\exp\brb{-\frac{(x-\mu)^2}{2\sigma^2}}-\exp\brb{-\frac{(x+\mu)^2}{2\sigma^2}}}dx\rob
\eeast

Thus,
\beast
\E\abs{X} & = & \frac {\sigma}{\sqrt{2\pi}}\left.\brb{-\exp\brb{-\frac{(x+\mu)^2}{2\sigma^2}} - \exp\brb{-\frac{(x-\mu)^2}{2\sigma^2}}}\right|^\infty_0 + \frac{\mu}{\sqrt{2\pi}\sigma} \int^\infty_0 \brb{\exp\brb{-\frac{(x-\mu)^2}{2\sigma^2}}-\exp\brb{-\frac{(x+\mu)^2}{2\sigma^2}}}dx\\
& = & \sqrt{\frac 2{\pi}} \sigma \exp\brb{-\frac{\mu^2}{2\sigma^2}} + \frac{\mu}{\sqrt{2\pi}\sigma} \int^\infty_0 \brb{\exp\brb{-\frac{(x-\mu)^2}{2\sigma^2}}-\exp\brb{-\frac{(x+\mu)^2}{2\sigma^2}}}dx\\%& = & \sqrt{\frac 2{\pi}} \sigma \exp\brb{-\frac{\mu^2}{2\sigma^2}} + \frac{\mu}{\sqrt{2\pi}\sigma} \brb{\int^\infty_{-\mu} \exp\brb{-\frac{x^2}{2\sigma^2}}dx - \int^\infty_\mu \exp\brb{-\frac{x^2}{2\sigma^2}}dx}\\
& = & \sqrt{\frac 2{\pi}} \sigma \exp\brb{-\frac{\mu^2}{2\sigma^2}} + \frac{\mu}{\sqrt{2\pi}} \brb{\int^\infty_{-\frac{\mu}{\sigma}} \exp\brb{-\frac{x^2}2}dx - \int^\infty_{\frac{\mu}{\sigma}} \exp\brb{-\frac{x^2}2}dx}\\
& = & \sqrt{\frac 2{\pi}} \sigma \exp\brb{-\frac{\mu^2}{2\sigma^2}} + \mu \brb{\Phi\brb{\frac{\mu}{\sigma}}- \Phi\brb{-\frac{\mu}{\sigma}}} = \sqrt{\frac 2{\pi}} \sigma \exp\brb{-\frac{\mu^2}{2\sigma^2}} + \mu\brb{2\Phi\brb{\frac{\mu}{\sigma}} -1}
\eeast
as required.
\end{proof}

\begin{proposition}\label{pro:bound_of_gaussian_law}
Let $X$ be a standard Gaussian random variable, i.e. $X\sim \sN(0,1)$. Then for any $x\in \R^+$,%\R\bs \bra{0}$,
\be
\frac{x^{-1} - x^{-3}}{\sqrt{2\pi}} e^{- x^2/2} \leq \pro\brb{X > x} \leq \frac{x^{-1} }{\sqrt{2\pi}} e^{-x^2/2}
\ee
\end{proposition}

\begin{proof}[\bf Proof]
It is equivalent to prove that
\be
\brb{x^{-1} - x^{-3}} e^{- x^2/2} \leq \int^\infty_x e^{-t^2/2} dt \leq x^{-1} e^{-x^2/2}.
\ee

First, we have
\beast
\int^\infty_x e^{-t^2/2}dt & = & \int^\infty_x e^{-x^2/2} e^{-(t-x)^2/2} e^{-(t-x)x}dt = e^{-x^2/2} \int^\infty_0 e^{-t^2/2 - tx} dt = x^{-1} e^{-x^2/2} \int^\infty_0 -e^{-t^2/2 } de^{-tx} \\
& = & x^{-1} e^{-x^2/2} \brb{\left.-e^{-t^2/2 } e^{-tx}\right|^\infty_0 - \int^\infty_0 e^{-tx} e^{-t^2/2} t dt } = x^{-1} e^{-x^2/2} \brb{1 - \int^\infty_0 t e^{-tx} e^{-t^2/2} dt } \geq x^{-1} e^{-x^2/2} \quad \quad (*)
\eeast
as the integrand is positive. If we do the integral by parts again, then the second part of the above equation is
\beast
x^{-1} e^{-x^2/2} \int^\infty_0 t e^{-tx} e^{-t^2/2} dt & = & x^{-2} e^{-x^2/2} \int^\infty_0 -t e^{-t^2/2} de^{-tx} =  x^{-2} e^{-x^2/2} \brb{\left.-t e^{-t^2/2} e^{-tx}\right|^\infty_0 + \int^\infty_0 e^{-tx} d\brb{t e^{-t^2/2}}}\\ %& = & x^{-2} e^{-x^2/2} \brb{\left.-t e^{-t^2/2} e^{-tx}\right|^\infty_0 + \int^\infty_0 e^{-tx} d\brb{t e^{-t^2/2}}} \\
& = & x^{-2} e^{-x^2/2} \brb{0 -\int^\infty_0 t^2 e^{-tx} e^{-t^2/2}dt  + \int^\infty_0 e^{-tx} e^{-t^2/2} dt }\\          %& = & -x^{-3} e^{-x^2/2} \int^\infty_0 e^{-t^2/2}de^{-tx} -  x^{-2} e^{-x^2/2} \int^\infty_0 t^2 e^{-tx} e^{-t^2/2}dt\\
& = & x^{-3} e^{-x^2/2} \brb{-\left.e^{-t^2/2}e^{-tx}\right|^\infty_0 - \int^\infty_0 te^{-tx}e^{-t^2/2}dt}-  x^{-2} e^{-x^2/2} \int^\infty_0 t^2 e^{-tx} e^{-t^2/2}dt\\
& = & x^{-3} e^{-x^2/2} -  x^{-3} e^{-x^2/2}  \int^\infty_0 te^{-tx}e^{-t^2/2}dt -  x^{-2} e^{-x^2/2} \int^\infty_0 t^2 e^{-tx} e^{-t^2/2}dt\\
& \leq &  x^{-3} e^{-x^2/2}          %& = & x^{-3} e^{-x^2/2} \int^\infty_0 t^2 e^{-t^2/2}de^{-tx}+ x^{-2} e^{-x^2/2} \int^\infty_0 e^{-tx} e^{-t^2/2} dt  \\
%& = & x^{-3} e^{-x^2/2} \brb{\left.t^2 e^{-t^2/2}e^{-tx}\right|^\infty_0 - \int^\infty_0 e^{-tx}  dt^2 e^{-t^2/2}} + x^{-2} e^{-x^2/2} \int^\infty_0 e^{-tx} e^{-t^2/2} dt  \\%- \left. e^{-tx}e^{-t^2/2}\right|^\infty_0   + \int^\infty_0  e^{-tx}de^{-t^2/2} }\\
%& = & x^{-3} e^{-x^2/2} \brb{ 1 + \int^\infty_0  e^{-tx}de^{-t^2/2} - \int^\infty_0 2t e^{-tx} e^{-t^2/2}dt + \int^\infty_0 e^{-tx}  t^3 e^{-t^2/2}dt }
\eeast
as the integrands are positive. Thus, we have
\be
\int^\infty_x e^{-t^2/2} dt \geq \brb{x^{-1} - x^{-3}} e^{- x^2/2}.\quad\quad (\dag)
\ee

Combining ($*$) and ($\dag$), we have the required result.
\end{proof}

\begin{proposition}\label{pro:gaussian_sequence_converges_to_gaussian}
Let $(X_n)_{n\in \N}$ be a sequence of Gaussian random variables with bounded means $(\mu_n)_{n\in \N}$ and variances $(\sigma^2_n)_{n\in \N}$. If $X_n \stackrel{d}{\to} X$. Then $X$ is also a Gaussian random variable.
\end{proposition}

\begin{proof}[\bf Proof]
Since $(\mu_n)_{n\in \N}$ and $(\sigma^2_n)_{n\in \N}$ are bounded, we can apply Bolzano-Weierstrass theorem \ref{thm:bolzano_weierstrass_r} to pick converging subsequence of $n_k$ such that $(\mu_{n_k})$ and $(\sigma^2_{n_k})$ are converging to $\mu$ and $\sigma^2$.

By \levy\ continuity theorem (Theorem \ref{thm:levy_continuity}), we have characteristic functions $\phi_{X_n}(t)\to \phi_X(t)$. Then we have
\be
\abs{\phi_{X_{n_k}}(t)} \to \abs{\phi(t)} \ \ra \ \abs{e^{i\mu_{n_k}t - \sigma^2_{n_k}t^2/2}} = \abs{e^{- \sigma^2_{n_k}t^2/2}} \to \abs{\phi(t)}.
\ee

Thus, $e^{-\sigma^2t^2/2} = \abs{\phi(t)}$ and therefore, $e^{- \sigma^2/2} = \abs{\phi(1)}$. Hence, $\sigma^2$ is uniquely determined. Then
\be
e^{i\mu_{n_k}t - \sigma^2_{n_k}t^2/2} \to \phi_X(t) \ \ra \ e^{i\mu t } = \phi_X(t) e^{\sigma^2 t^2/2},\ \forall t\in \R.
\ee

Thus, $\mu$ is also uniquely determined. Thus, we have that $ \phi_X(t) = e^{i\mu t - \sigma^2 t^2/2}$ and $X$ is a Gaussian.
\end{proof}


\begin{definition}[multivariate Gaussian random variables\index{Gaussian random variables!multivariate}]
A random variable $X= (X_1,\dots,X_n)$ in $\R^n$ is Gaussian if $\inner{u}{X}$ is Gaussian, for all $u \in \R^n$ and $X_1,\dots,X_n$ are multivariate Gaussian random variables. That is, any linear combination of multivariate Gaussian random variables is still a Gaussian random variable.
\end{definition}

\begin{definition}[non-degenerate Gaussian distribution\index{non-degenerate!multivariate normal distribution}]\label{def:non_degenerate_multivariate_gaussian}
The multivariate Gaussian distribution is said to be non-degenerate when the symmetric covariance matrix $\Sigma$ is positive definite. In this case the distribution has density
\be
f_{X}(x_1,\dots,x_n) = \frac{1}{\sqrt{(2\pi)^n\abs{\Sigma}}} \exp\left(-\frac{1}{2}(x-\mu)^T{\Sigma}^{-1}(x-\mu) \right),
\ee
where $\abs{\Sigma}$ is the determinant of $\Sigma$. Note how the equation above reduces to that of the univariate normal distribution if $\Sigma$ is a $1 \times 1$ matrix (i.e. a real number).
\end{definition}

\begin{remark}
The integral of density function is 1\footnote{details needed}.
\end{remark}

\begin{example}
An example of such a random variable is provided by $X = (X_1, \dots,X_n)$, where $X_1, \dots,X_n$ are independent $\sN(0, 1)$ random variables. To see this, we note that
\be
\E e^{i\inner{u}{X}} = \E \prod_k e^{iu_kX_k} = e^{-|u|^2/2}
\ee
so $\inner{u}{X}$ is $\sN(0, |u|^2)$ for all $u \in \R^n$.
\end{example}


\begin{example}[bivariate Gaussian random variables\index{Gaussian random variables!bivariate}]\label{exa:bivariate_gaussian}
In the 2-dimensional nonsingular case ($n = \rank(\Sigma) = 2$), the probability density function of a vector $[X,Y]^T$ is
\beast
f_{X,Y}(x,y) & = & \frac{1}{2 \pi \sigma_x \sigma_y \sqrt{1-\rho^2}} \exp\left( -\frac{1}{2(1-\rho^2)}\left[ \frac{(x-\mu_x)^2}{\sigma_x^2} + \frac{(y-\mu_y)^2}{\sigma_y^2} - \frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x \sigma_y} \right] \right)\\
& = & \frac{1}{2 \pi \sigma_x \sigma_y \sqrt{1-\rho^2}} \exp\left( -\frac{1}{2(1-\rho^2)} \brb{\frac{x-\mu_x}{\sigma_x} - \frac{\rho(y-\mu_y)}{\sigma_y}}^2  - \frac{(y-\mu_y)^2}{2\sigma_y^2} \right)
\eeast
where $\rho$ is the correlation between $X$ and $Y$ and where $\sigma_x>0$ and $\sigma_y>0$ with $\cov(X,Y) = \rho \sigma_x\sigma_y$. In this case,
\be
\mu = \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}, \quad \Sigma = \begin{pmatrix} \sigma_x^2 & \rho \sigma_x \sigma_y \\ \rho \sigma_x \sigma_y & \sigma_y^2 \end{pmatrix}.
\ee

Then the conditional density function is
\be
f_{X|Y}(x,y) = f_{X,Y}(x,y)/f_Y(x,y) = \frac{1}{\sqrt{2 \pi} \sigma_x \sqrt{1-\rho^2}} \exp\left( -\frac{1}{2(1-\rho^2)} \brb{\frac{x-\mu_x}{\sigma_x} - \frac{\rho(y-\mu_y)}{\sigma_y}}^2 \right)
\ee
where $f_Y(y)\neq 0$.
\end{example}

\begin{theorem}\label{thm:multivariate_gaussian_rv_property}
Let $X = (X_1,\dots,X_n)$ be a Gaussian random variable in $\R^n$ with mean $\mu$ and variance $\Sigma$, i.e., $X_1,\dots,X_n$ are multivariate Gaussian random variables. Then
\ben
\item [(i)] $AX + b$ is a Gaussian random variable in $\R^m$, for all $m\times n$ matrices $A$ and all $b \in \R^m$. That is, $AX +b \sim \sN(A\mu+b, A\Sigma A^T)$\footnote{need proof}.
\item [(ii)] $X \in \sL^2(\Omega,\sF,\pro)$ and its distribution is determined by its mean $\mu$ and its covariance matrix $V$. %In particular, $\var(X_1+X_2) = V_{1}$
\item [(iii)] $\phi_X(u) = e^{i\inner{u}{\mu}-\inner{u}{\Sigma u}/2}$, $M_X(\theta) = e^{\inner{\theta}{\mu}+\inner{\theta}{\Sigma \theta}/2}$.
\item [(iv)] if $V$ is invertible, then $X$ has a density function on $\R^n$, given by
\be
f_X(x) = (2\pi)^{-n/2}(\det \Sigma )^{-1/2} \exp\brb{-\inner{x - \mu}{\Sigma^{-1}(x - \mu)}/2}.
\ee
\item [(v)] suppose $X = (X_1,X_2)$, with $X_1$ in $\R^{n_1}$ and $X_2$ in $\R^{n_2}$, then
\be
\cov(X_1,X_2) = 0 \quad \ra\quad X_1,\ X_2 \text{ are independent}.
\ee
\een
\end{theorem}

\begin{remark}
Note that (v) is only true for multivariate Gaussian random variables.
\end{remark}

\begin{proof}[\bf Proof]
For $u \in \R^n$, we have $\inner{u}{AX+b} = \inner{A^Tu}{X}+ \inner{u}{b}$ so $\inner{u}{AX+b}$ is Gaussian, by Proposition \ref{pro:linear_gaussian}. This proves (i).

Each component $X_k$ is Gaussian, so $X \in \sL^2(\Omega,\sF,\pro)$. Set $\mu = \E(X)$ and $\Sigma = \var(X)$. For $u \in \R^n$ we have $\E(\inner{u}{X}) = \inner{u}{\mu}$ and \be \var(\inner{u}{X}) =
\cov(\inner{u}{X},\inner{u}{X}) = \inner{u}{\Sigma u}. \ee Since $\inner{u}{X}$ is Gaussian, by Proposition \ref{pro:linear_gaussian}, we must have \be \inner{u}{X} \sim \sN\brb{\inner{u}{\mu}, \inner{u}{\Sigma u}},\quad\quad
\phi_X(u) = \E e^{i\inner{u}{X}} = e^{i\inner{u}{\mu}-\inner{u}{\Sigma u}/2}. \ee This is (iii) and (ii) follows by uniqueness of characteristic functions. Similarly, we get the moment generating function.

Let $Y_1, \dots, Y_n$ be independent $\sN(0, 1)$ random variables. Then $Y = (Y_1,\dots, Y_n)$ has density
\be
f_Y (y) = (2\pi)^{-n/2} \exp\brb{-|y|^2/2}.
\ee

Set $\wt{X} = \Sigma^{1/2}Y +\mu$, then $\wt{X}$ is Gaussian, with $\E(\wt{X}) = \mu$ and $\var(\wt{X} ) = \Sigma$, so $\wt{X} \sim X$. If $\Sigma$ is invertible, then $\wt{X}$ and hence $X$ has the density claimed in (iv), by a linear change of variables in $\R^n$.

Finally, if $X = (X_1,X_2)$ with $\cov(X_1,X_2) = 0$, then, for $u = (u_1, u_2)$, we have
\be
\inner{u}{\Sigma u} = \inner{u_1}{\Sigma_{11}u_1} + \inner{u_2}{\Sigma_{22}u_2},
\ee
where $\Sigma_{11} = \var(X_1)$ and $\Sigma_{22} = \var(X_2)$. Then $\phi_X(u) = \phi_{X_1}(u_1)\phi_{X_2}(u_2)$ so $X_1$ and $X_2$ are independent by Theorem \ref{thm:characteristic_function_independence}.
\end{proof}

\begin{example}
Let $(X, Y)$ be a Gaussian random vector in $\R^2$ (so $\lm X + \mu Y$ is Gaussian for all $\lm,\mu \in \R$). Let $X' = aY + b$, where $a$ and $b$ are chosen so that
\be
\text{(i)}\ \cov(X - X', Y ) = 0,\quad\quad \text{(ii)}\ \E X' = \E X.
\ee

In this case $X - X'$ and $Y$ are independent (by Theorem \ref{thm:multivariate_gaussian_rv_property}.(v), this is a property of Gaussian random vectors). Then by Proposition \ref{pro:expectation_of_independent_product},
\be
\E\brb{(X - X')\ind_A(Y )} = \E\brb{X - X'}\pro(Y \in A) = 0,
\ee
so $X' = \E\brb{X | Y}$ since $X\in \sL^2(\Omega,\sF,\pro)$ by Theorem \ref{thm:multivariate_gaussian_rv_property} (ii). Notice that
\be
\cov(X,Y) = \cov(aY+b,Y) = a\var Y \quad\ra\quad a = \frac{\cov(X,Y )}{\var Y},
\ee

Then, $\E X = \E X' = a\E Y + b \quad\ra\quad b = \E X - a\E Y$, thus
\be
\E\brb{X | Y} = \E X+ \frac{\cov(X, Y )}{\var(Y )} (Y -\E Y).
\ee
\end{example}

%\begin{problem}
%What is the conditional distribution of $X$ given $Y$? (Hint:
%\be
%\E[h(X) | Y ] = \E[h(X -\E[X | Y ]+\E[X | Y ]) | Y ],
%\ee
%and $X -\E[X | Y ]$ and $\E[X | Y ]$ are independent.)
%\end{problem}


\begin{example}
Suppose that $X_1, \dots,X_n$ are jointly Gaussian random variables with
\be
\E(X_i) = \mu_i, \cov (X_i,X_j) = \Sigma_{ij}
\ee
and that the matrix $\Sigma = (\Sigma_{ij})$ is invertible. Set $Y = \Sigma^{-\frac 12} (X - \mu)$. %. Show that we can write $X_2$ in the form $X_2 = aX_1 + Z$ where $Z$ is independent of $X_1$ and determine the distribution of $Z$.

Then
\beast
\E Y & = & \Sigma^{-1/2}\E (X - \mu) = 0\\
\var Y & = & \Sigma^{-1/2} \var\brb{X} \Sigma^{-1/2} = I.
\eeast

Hence $\cov(Y_i,Y_j) = \delta_{ij}$, so $Y_1, \dots, Y_n$ are independent $\sN(0,1)$ random variables.

If $Z = X_2 - aX_1$ then $(X_1,Z)$ is Gaussian, so $Z$ and $X_1$ willbe independent provided
\be
0 = \cov(Z,X_1) = \Sigma_{12} - a\Sigma_{11}.
\ee

If $\Sigma_{11} \neq 0$, take $a = \Sigma_{12} /\Sigma_{11}$. If $\Sigma_{11} = 0$, then $X_1=\mu_1$ a.s. so $X_1$ and $X_2$ are independent and we can take $a=0$. Then $Z$ is Gaussian and
\be
\E Z = \mu_2 - a\mu_1,\quad \var Z = \Sigma_{22} - 2a\Sigma_{12} + a^2 \Sigma_{11}.
\ee
\end{example}

\begin{example}
Let $X_1, \dots,X_n$ be independent $\sN(0, 1)$ random variables. We will show that
\be
\brb{\ol{X},\ \sum^n_{m=1} (X_m - X)^2},\quad \quad \brb{X_n/\sqrt{n},\ \sum^{n-1}_{m=1} X^2_m}
\ee
have the same distribution, where $\ol{X} = (X_1 + \dots + X_n)/n$.

%Let $A$ be an $n \times n$ orthogonal matrix with $\left( \frac{1}{\sqrt{n}}, \ldots, \frac{1}{\sqrt{n}} \right)^T$ as the $n^\mathrm{th}$ row. Then
%\[
% A \left( \begin{array}{c}
%X_1 \\
% \vdots \\
%X_n
%\end{array} \right) = \left( \begin{array}{c} Y_1 \\
%\vdots \\
%Y_{n-1} \\
%\sqrt{n}\ol{X}
%\end{array} \right)
%\]
% where $Y_1,\ldots, Y_{n-1}, \sqrt{n} \ol{X}$ are i.i.d. $N(0,1)$ (to see this check the mean and variance of the RHS). As $A$ is orthogonal,
%\[
% \sum_{i=1}^n X_i ^2 = \sum_{i=1} ^{n-1} Y_i ^2 + n\ol{X}^2.
%\]
%Then
%\[
% \sum_{i=1} ^n (X_i - \ol{X})^2 = \sum_{i=1} ^{n-1} Y_i ^2.
%\]
%Thus $\ol{X}$ and $\sum_{i=1} ^n (X_i - \ol{X})^2$ are independent. It is easy to check that the distribution of $\ol{X}$ is the same as that of $X_n / \sqrt{n}$. The result then follows.
%\end{solution}

Both $\ol{X}$ and $X_n/\sqrt{n}$ are $\sN(0,1)$. Since $\E\brb{\ol{X}(X_m - \ol{X})} = 0$ for all $m$, $\ol{X}$ and $(X_1 - \ol{X},\dots,X_n - \ol{X})$ are independent, so $\ol{X}$ and $\sum^n_{m=1}\brb{X_m - \ol{X}}^2$ are independent. Set $\phi(u) = \E\brb{e^{iuX_1^2}}$ and note that
\be
n\ol{X}^2 + \sum^n_{m=1} \brb{X_m - \ol{X}}^2 = \sum^n_{m=1} X_m^2 \ \ra \ \phi(u) \E\brb{\exp\brb{iu \sum^n_{m=1}(X_m - \ol{X})^2}} = \phi(u)^n.
\ee

Thus, $\sum^n_{m=1} (X_m - \ol{X})^2$ and $\sum^{n-1}_{m=1}X_m^2$ have the same characteristic function and hence the same distribution.
\end{example}

%\begin{proposition}\label{pro:moments_multivariate_gaussian}
%Suppose $X$ is a Gaussian random variable in $\R^n$ and $X \sim \sN(\mu, \Sigma)$. Then
%\be
%\text{(i)}\ \E X = \mu,\quad\quad \text{(ii)}\ \E X^2 = \Sigma ,\quad\quad\text{(iii)}\ \E X^3 = ,\quad\quad\text{(iv)}\ \E X^4 =.
%\ee
%\end{proposition}

%\begin{proof}[\bf Proof]
%By Theorem \ref{thm:multivariate_gaussian_rv_property}, we have
%\be
%M_X(\theta) = \exp\brb{\theta^T\mu + \frac 12 \theta^T \Sigma \theta}.
%\ee
%\end{proof}



\subsection{Cauchy random variables}

\begin{definition}\label{def:cauchy_random_variable}
A random variable $X$ is called standard Cauchy random variable\index{Cauchy random variable!standard} if its density function is for $d>0$,
\be
f_X(x) = \frac d{\pi(d^2+x^2)}, \quad x \in \R.
\ee

The distribution is called Cauchy distribution, denoted by $\sC(d)$\index{distribution!Cauchy}. Furthermore, $
sC(1)$ is called standard Cauchy distribution.

For the non-central Cauchy distribution, we use $\sC(\mu,d)$ with density function
\be
f_X(x) = \frac d{\pi(d^2+(x-\mu)^2)}.
\ee
\end{definition}

\begin{remark}
Note that the moment generating function, mean, variance, etc. are undefined.
\end{remark}

\begin{center}
\psset{xunit=2,yunit=3cm}
 \begin{pspicture*}(-2.8,-0.3)(3.1,2.1)
 \psset{linewidth=1pt}
 \multido{\rb=0.1+0.2,\rm=0.0+0.2}{4}{%
 \psCauchy[b=\rb,m=\rm,linecolor=red]{-2.5}{2.5}
 \psCauchyI[b=\rb,m=\rm,linecolor=blue]{-2.5}{2.5}}
 \psaxes[Dy=0.4,dy=0.4,Dx=0.5,dx=0.5]{->}(0,0)(-3,0)(3,2)
 \end{pspicture*}
\end{center}

\begin{proposition}
The characteristic function of central Cauchy random variable $X\sim \sC(d)$ is
\be
\phi_X(t) = e^{-|dt|},\quad t\in \R.
\ee

Furthermore, the characteristic function of non-central Cauchy random variable $X\sim \sC(\mu,d)$ is given by
\be
\phi_X(t) = e^{i\mu t - \abs{dt}},\qquad t\in\R.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Since $e^{-\abs{t}}$ is integrable, by Theorem \ref{thm:inversion_density} we can compute the inverse Fourier transform
\be
\frac 1{2\pi}\int_\R e^{-itx}e^{-\abs{dt}} dt = \frac 1{2\pi}\brb{\int^\infty_0 e^{-(d+ix)t} dt + \int^0_{-\infty}e^{(d-ix)t} dt } = \frac 1{2\pi}\brb{ \frac 1{d+ix} + \frac 1{d-ix}} = \frac d{\pi(d^2+x^2)}.
\ee

A direct computation of the transform of $d/\pi(d^2+x^2)$ can be made by Cauchy's residue theorem (Theorem \ref{thm:cauchy_residue_complex})\footnote{need details}.
\end{proof}
%Computing the characteristic function directly will require the use of the residue theorem from complex analysis. Alternatively, note that $\varphi$ is integrable, so we may compute the inverse Fourier transform. It is easy to show that this equals the given density.

\begin{proposition}
If $X_1,\dots,X_n$ are independent Cauchy random variables, then $\brb{X_1+\dots+X_n}/n$ is also Cauchy.
\end{proposition}

\begin{proof}[\bf Proof]
By Theorem \ref{thm:characteristic_function_independence}.(iii),
\be
\E \brb{e^{it(X_1+\dots+X_n)/n}} = \brb{\E\brb{e^{i(t/n)X_n}}}^n = \brb{e^{i\mu t/n - \abs{dt}/n}}^n = e^{i\mu t-\abs{dt}}.
\ee

Hence, $(X_1+\dots+X_n)/n$ is also Cauchy.
\end{proof}

\begin{remark}
Neither the strong law of large numbers nor the central limit theorem apply here, as the mean of a Cauchy random variable is not well-defined. %Comment on this in the light of the strong law of large numbers and central limit theorem.

Note that we can not apply Proposition \ref{pro:independent_mgf} as $e^{\theta X_i}$ is not integrable. %bounded.
\end{remark}

\begin{proposition}\label{pro:two_independent_standard_gaussian_quotient_implies_cauchy}
Let $X$ and $Y$ be two independent standard Gaussian random variables. Then $X/Y$ is standard Cauchy distributed.
\end{proposition}

\begin{proof}[\bf Proof]
We see that by symmetry of Gaussian distribution (see Definition \ref{def:gaussian_rv}), for any $k \geq 0$
\be
\pro\brb{\frac{X}{Y} \leq k} = \frac 2{2\pi} \int^\infty_0 \int^{ky}_0 e^{-\frac {x^2}2} e^{-\frac {y^2}2} dx dy.
\ee

Let $X = r\sin \theta$ and $Y = r\cos \theta$, thus,
\be
\pro\brb{\frac{X}{Y} \leq k} = \frac 2{2\pi} \int^\infty_0 \int^{\arctan k}_{-\pi/2} e^{-r^2/2} rdr d\theta = \frac 12 +\frac 1{\pi}\arctan k.
\ee

Then taking differentiation wrt $k$, we have the density function is
\be
f(k) = \brb{\frac 12 + \frac 1{\pi} \arctan k}' = \frac 1{\pi(1+k^2)}.
\ee

Similarly, we have the same result for $k \leq 0$
\be
\pro\brb{\frac{X}{Y} \leq k} = \frac 2{2\pi} \int^\infty_0 \int^{ky}_{-\infty} e^{-\frac {x^2}2} e^{-\frac {y^2}2} dx dy = \frac 2{2\pi} \int^\infty_0 \int^{\arctan k}_{-\pi/2} e^{-r^2/2} rdr d\theta = \frac 12 + \frac 1{\pi}\arctan k.
\ee
%(but with probability $\pro\brb{\frac{X}{Y} \leq k} = \frac 1{\pi}\arctan \abs{k}$).

Taking differentiation wrt $k$, we have the same density function. Thus, $X/Y$ is standard Cauchy distributed.
\end{proof}

\begin{proposition}
Let $X\sim \sC(1)$. Then $X^{-1}\sim X$.
\end{proposition}

\begin{proof}[\bf Proof]%\footnote{proof needed.}
For $x>0$,
\beast
F_{X^{-1}}(x) & = & \pro\brb{X^{-1}\leq x} = \frac 12 + \pro\brb{0<X^{-1}\leq x} \\
& = & \frac 12 + \pro\brb{X\geq 1/x} = \frac 32 - F_{X}(1/x).
\eeast

Then differentiating both sides we have
\be
p_{X^{-1}}(x) = \frac 1{x^2} p_X(1/x) = \frac 1{x^2}\frac{d}{\pi\brb{d^2+ \brb{\frac 1x}^2 }}  = p_X(x).
\ee

Thus we have the required result by symmetry.
\end{proof}


\subsection{Gamma random variables}

\begin{definition}[Gamma random variable]\label{def:gamma_rv}
A random variable $X$ in $\R^+$ is called Gamma-distributed\index{Gamma-distributed random variable} if, for some $k >0$ and $\lm >0$, its density function is
\be
f_X(x) = \frac{1}{\Gamma(k)} \lm^k x^{k - 1} e^{-\lm x},
\ee
where $k$ is shape parameter, $\lm$ is rate parameter and $\Gamma(\cdot)$ is Gamma function (see Theorem \ref{thm:infinite_integral_of_gamma_function_for_positive_values}). We write $X \sim \Gamma(k, \lm)$.
\end{definition}

\begin{remark}
If $k=1$, Gamma distribution is exponential distribution with parameter $\lm$, i.e., $\Gamma(1,\lm) \sim \sE(\lm)$.

If $k\in \Z^+$, then Gamma distributed random variable $X\sim \Gamma(k,\lm)$ is the summation of $k$ independent exponential distributed random variables $\sE(\lm)$ with the same parameter $\lm$ (see Proposition \ref{pro:gamma_sum_of_exponential}).
\end{remark}

\begin{center}
\psset{xunit=1.2cm,yunit=10cm,plotpoints=200}
\begin{pspicture*}(-0.75,-0.1)(9.5,0.6)
\rput[lb](5,0.5){\textcolor{red}{$k=0.3$, $\lambda =0.5$}}
  \rput[lb](5,0.4){\textcolor{blue}{$k=1.2$, $\lambda =0.5$}}
  \rput[lb](5,0.3){\textcolor{black}{$k=2$, $\lambda =0.5$}}
 \psGammaDist[linewidth=1pt,linecolor=red,alpha=0.3,beta=0.5]{0.01}{9}
 \psGammaDist[linewidth=1pt,linecolor=blue,alpha=1.2,beta=0.5]{0.01}{9}
  \psGammaDist[linewidth=1pt,linecolor=black,alpha=2,beta=0.5]{0.01}{9}
 \psaxes[Dy=0.1]{->}(0,0)(9.5,.6)
 \end{pspicture*}
\end{center}

\begin{proposition}\label{pro:mgf_gamma}
Suppose $X \sim \Gamma(k,\lm)$. Then for $\theta < \lm$ and $t\in \R$,
\be
M_X(\theta) = \brb{\frac{\lm}{\lm - \theta}}^k,\quad\quad \phi_X(t) = \brb{\frac{\lm}{\lm - it}}^k.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
From Definition \ref{def:mgf_probability},
\be
M_X(\theta) = \E\brb{e^{\theta X}} = \int^\infty_0 e^{\theta x} \frac{1}{\Gamma(k)} \lm^k x^{k - 1} e^{-\lm x} dx = \frac{\lm^k}{(\lm - \theta)^k} \int^\infty_0 \frac{1}{\Gamma(k)} (\lm - \theta )^kx^{k-1} e^{-(\lm - \theta)x} dx = \brb{\frac{\lm}{\lm - \theta}}^k.
\ee

For $\phi_X(t)$, we have $e^{itx} = \sum_{n=0}^{+\infty} \frac {(itx)^n} {n!}$ and thus
\be
\phi_X(t) = \E\brb{e^{it X}} = \int^\infty_0 e^{itx} \frac{1}{\Gamma(k)} \lm^k x^{k - 1} e^{-\lm x} dx = \int^\infty_0 \frac{1}{\Gamma(k)} \lm^k x^{k - 1} e^{-\lm x} \sum_{n=0}^{\infty} \frac {(itx)^n} {n!} dx.
\ee

Therefore,
\beast
\phi_X(t) = \int^\infty_0 \sum_{n=0}^{\infty} \frac{1}{\Gamma(k)} \lm^k x^{k - 1} e^{-\lm x} \frac {(itx)^n} {n!} dx = \sum_{n=0}^{\infty} \frac {(it)^n} {n!}  \int^\infty_0  \frac{1}{\Gamma(k)} \lm^k x^{n+k - 1} e^{-\lm x} dx
\eeast
by Fubini Theorem (Theorem \ref{thm:fubini}) since $\abs{\E\brb{e^{it X}}} \leq \E\abs{e^{itX}} = 1 < \infty$. Thus,
\be
\phi_X(t) = \sum_{n=0}^{\infty} \frac {(it)^n} {n!} \frac{\Gamma(n+k)}{\lm^n\Gamma(k)} \int^\infty_0  \frac{1}{\Gamma(n+k)} \lm^{n+k} x^{n+k -1} e^{-\lm x} dx  = \sum_{n=0}^{\infty} \frac {(it)^n} {\lm^n n!} \frac{\Gamma(n+k)}{\Gamma(k)} = \sum_{n=0}^{\infty}  \binom {n+k-1} {n} \brb{\frac{it}{\lm}}^n
\ee

Then by Proposition \ref{pro:negative_binomial_pmf},
\be
\phi_X(t) = \frac {1} {(1 - it/\lm)^k} = \brb{\frac{\lm}{\lm - it}}^k.
\ee
\end{proof}

%\be \int_0^{+\infty} e^{itx} \frac {x^{k-1}} {\Gamma(k)\theta^k} \exp\left\{-\frac {x} {\theta}\right\} dx = \sum_{n=0}^{+\infty} \binom {n+k-1} {n} (i\theta t)^n = \frac {1} {(1 - i\theta t)^k} \ee
%$\phi_X(t) = ?$\footnote{probably need Cauchy theorem from complex analysis.}
%\be
%\phi_X(t) = \E\brb{e^{it X}} = \int^\infty_0  e^{it x} \frac{1}{\Gamma(k)} \lm^k x^{k - 1}  e^{-\lm x} dx = \int^\infty_0 \brb{\cos tx + i\sin tx} \frac{1}{\Gamma(k)} \lm^k x^{k - 1}  e^{-\lm x} dx .
%\ee
%Therefore,
%\beast
%I = \int^\infty_0 \lm e^{-\lm x}\brb{\cos tx}dx & = & - \brb{\left.e^{-\lm x}\cos tx\right|^\infty_0 + t \int^\infty_0 e^{-\lm x} \sin tx dx} = - \brb{-1 - \frac t{\lm} \int^\infty_0  \sin tx de^{-\lm x}}\\
%& = & - \brb{-1 - \frac t{\lm} \brb{\left.\sin tx de^{-\lm x}\right|^\infty_0 - t\int^\infty_0  e^{-\lm x}\cos tx  dx}} = 1 - \frac {t^2}{\lm^2} I.
%\eeast
%Thus, $\lm^2 I = \lm^2 - t^2 I \ \ra \ I = \frac{\lm^2}{\lm^2 + t^2}$. Similarly,
%\beast
%J = \int^\infty_0 \lm e^{-\lm x}\brb{\sin tx}dx & = & - \brb{\left.e^{-\lm x}\sin tx\right|^\infty_0 - t \int^\infty_0 e^{-\lm x} \cos tx dx} = - \brb{\frac t{\lm} \int^\infty_0  \cos tx de^{-\lm x}}\\
%& = & - \brb{\frac t{\lm} \brb{\left.\cos tx de^{-\lm x}\right|^\infty_0 + t\int^\infty_0  e^{-\lm x}\sin tx  dx}} = \frac t{\lm}\brb{1 - \frac t{\lm} J}.
%\eeast
%Thus, $\lm^2 J = \lm t - t^2 J \ \ra \ J = \frac{\lm t}{\lm^2 + t^2}$. Thus,
%\be
%\phi_X(t) = \frac{\lm^2}{\lm + t^2} + \frac{i\lm t}{\lm + t^2} = \frac{\lm^2(\lm + it)}{\lm^2 + t^2} = \frac{\lm}{\lm - it}.
%\ee

\begin{proposition}\label{pro:gamma_sum_of_exponential}
Let $X_i \sim \sE(\lm)$ be independently distributed for $i=1,\dots,k$ with $k\in \Z^+$. Then $\sum^k_{i=1}X_i$ is a Gamma distributed random variable with parameters $k$ and $\lm$, i.e., $\sum^k_{i=1}X_i\sim \Gamma(k,\lm)$.
\end{proposition}

\begin{proof}[\bf Proof]
Direct result from Proposition \ref{pro:mgf_gamma}, Proposition \ref{pro:mgf_exponential} and Theorem \ref{thm:mgf_uniquely_determine_law}.
\end{proof}

\begin{proposition}\label{pro:moments_gamma}
Suppose $X \sim \Gamma(k,\lm)$. Then \beast \text{(i)}\ \E X = \frac k{\lm},\quad \text{(ii)}\ \var X = \frac k{\lm^2},\quad\text{(iii)}\ \skewness(X) = \frac{2}{\sqrt{k}},\quad\text{(iv)}\ \ekurt(X) = \frac 6k,\quad \text{(v)}\ \E X^n = \frac {k(k+1)\dots (n+k-1)}{\lm^n}.
\eeast
\end{proposition}

\begin{proof}[\bf Proof]%(i) and (ii) can be done using integration by parts. Or
With Propositions \ref{pro:mgf_gamma}, \ref{pro:mgf_finite_moment},
\beast
\E X = \left.\frac {d M_X(\theta)}{d\theta}\right|_{\theta=0} = \left. \frac{k\lm^k}{(\lm -\theta)^{k+1}}\right|_{\theta=0} = \frac {k\lm^k}{\lm^{k+1}} = \frac k{\lm},\qquad \E X^2 = \left.\frac {d^2 M_X(\theta)}{d\theta^2}\right|_{\theta=0} = \left. \frac{k(k+1)\lm^k}{(\lm -\theta)^{k+2}} \right|_{\theta=0} = \frac{k(k+1)}{\lm^2}.
\eeast

Thus, $\var X = \E X^2 - \brb{\E X}^2 = \frac{k(k+1)}{\lm^2} - \frac{k^2}{\lm^2} = \frac{k}{\lm^2}$. Also,
\be
\E X^3 = \left.\frac {d^3 M_X(\theta)}{d\theta^3}\right|_{\theta=0} = \left. \frac{k(k+1)(k+2)\lm^k}{(\lm -\theta)^{k+3}}  \right|_{\theta=0} = \frac{k(k+1)(k+2)}{\lm^3}.
\ee

Thus,
\be
\E\brb{X-\mu}^3 = \E X^3 - 3\mu\E X^2 + 3\mu^2 \E X - \mu^3 =  \frac{k(k+1)(k+2)}{\lm^3} - 3\frac{k}{\lm} \frac{k(k+1)}{\lm^2} + 3 \frac{k^3}{\lm^3}  - \frac{k^3}{\lm^3} = \frac{2k}{\lm^3}.
\ee

By Definition \ref{def:skewness}, $\skewness(X) = \frac{2k/\lm^3}{k^{3/2}/\lm^3} = \frac 2{\sqrt{k}}$. Furthermore,
\be
\E X^4 = \left.\frac {d^4 M_X(\theta)}{d\theta^4}\right|_{\theta=0} = \left. \frac{k(k+1)(k+2)(k+3)\lm^k}{(\lm -\theta)^{k+4}}  \right|_{\theta=0}  = \frac{k(k+1)(k+2)(k+3)}{\lm^4}.
\ee

Thus,
\beast
\E\brb{X-\mu}^4 & = & \E X^4 - 4\mu\E X^3 + 6\mu^2 \E X^2 - 4\mu^3\E X + \mu^4 \\
& = & \frac{k(k+1)(k+2)(k+3)}{\lm^4} - 4\frac{k}{\lm}\frac{k(k+1)(k+2)}{\lm^3} + 6\frac{k^2}{\lm^2}\frac{k(k+1)}{\lm^2}- 4\frac{k^3}{\lm^3}\frac{k}{\lm} + \frac{k^4}{\lm^4} = \frac{3k^2 + 6k}{\lm^4} .
\eeast

By Definition \ref{def:kurtosis}, we have $\ekurt(X) = \frac{3k(k+2)/\lm^4}{k^2/\lm^4} -3 = \frac 6k$.

For (v), we know that $\frac {d^n M_X(\theta)}{d\theta^n} = \lm^k k(k+1)\dots (n+k-1)/(\lm -\theta)^{n+k}$,
\be
\E X^n = \left.\frac {d^n M_X(\theta)}{d\theta^n}\right|_{\theta=0} = \frac {k(k+1)\dots (n+k-1)}{\lm^n}.
\ee
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sums of independent random variables}

\subsection{Weak Law of large numbers}

\begin{theorem}[weak law of large numbers\index{weak law of large numbers!finite variance}]\label{thm:wlln_finite_variance}
Let $X_1,X_2, \dots$ be independent, identically distributed random variables with $\E X_1 = \mu$ and $\var (X_1) < \infty$. Then
\be
\frac{X_1 + \dots X_n}{n} \stackrel{p}{\to } \mu,\quad \text{ as }n \to \infty.
\ee

That is, for any constant $\ve > 0$,
\be
\pro\brb{\abs{\frac{X_1 + \dots + X_n}n - \mu} > \ve} \to 0,\quad \text{ as }n \to \infty.
\ee
\end{theorem}



\begin{proof}[\bf Proof]
By Chebyshev's inequality (Theorem \ref{thm:chebyshev_inequality_probability}) we have
\beast
\pro\brb{\abs{\frac{X_1 + \dots + X_n}n - \mu} > \ve} & \leq & \frac 1{\ve^2} \E\brb{\frac{X_1 + \dots + X_n}n - \mu}^2 = \frac 1{n^2\ve^2} \E (X_1 + \dots + X_n - n\mu)^2\\
& = & \frac 1{n^2\ve^2} \var (X_1 + \dots + X_n) = \frac{n}{n^2\ve^2}\var (X_1) = \frac 1{n\ve^2} \var (X_1) \to 0,
\eeast
as required.
\end{proof}

\begin{remark}\label{rem:wlln}
\ben
\item [(i)] This is proved first by Jacob Bernoulli\footnote{need mathematician link} in 1713.
\item [(ii)] The statement in Theorem \ref{thm:wlln_finite_variance} is normally referred to by saying that the random variable $(X_1 + \dots+ X_n)/n$ `converges in probability' to $\mu$, written
\be
\frac{X_1 + \dots + X_n}n \stackrel{p}{\to} \mu \text{ as }n \to \infty.
\ee
\item [(iii)] This result should be distinguished from the strong law of large numbers which states that
\be
\pro \brb{\frac{X_1 + \dots + X_n}n \to \mu \text{ as }n \to \infty} = 1.
\ee
As the name implies, the strong law of large numbers implies the weak law (by Theorem \ref{thm:convergence_in_probability}). The mode of convergence in the strong law is referred to as `convergence with probability one' or `almost sure convergence'.

\item [(iv)] Notice that the requirement that the random variables in the weak law be independent is not one that we may dispense with. For example, suppose that $\Omega = \bra{\omega_1, \omega_2}$ has just two points and let $X_n(\omega_1) = 1$ and $X_n(\omega_2) = 0$ for each $n$, so that the random variables are identically distributed, but not of course independent. Let $p = \pro(\{\omega_1\}) = 1 -\pro(\{\omega_2\})$, where $0 < p < 1$, then $\E X_1 = p$, and we have
\be
\frac{X_1(\omega_1) + \dots + X_n(\omega_1)}n = 1,\quad\quad \frac{X_1(\omega_2) + \dots + X_n(\omega_2)}n = 0,\quad  \text{for all }n,
\ee
so that the conclusion of Theorem \ref{thm:wlln_finite_variance} cannot hold.

\item [(v)] It should be noticed that the weak law of large numbers is `distribution free' in that the particular distribution of the summands $\{X_i\}$ only influences the result through the mean, $\mu$, (and, in the form we have stated it, through the fact that the variance is finite) but otherwise the underlying distribution does not enter the conclusion of the Theorem.
\een
\end{remark}

\begin{example}
The weak law of large numbers underlies the `frequentist' interpretation of probability. Suppose that we have independent repetitions of an experiment, and we set $X_i = 1$ if a particular outcome occurs on the ith repetition (e.g., 'Heads'), and $X_i = 0$, otherwise (e.g., `Tails'). Then $\E X_i = p$, say, where $p = \pro(X_i = 1)$ is the probability of the outcome. Then $(X_1 +\dots+X_n)/n$ is the average number of occurrences of the outcome in $n$ repetitions and this converges in the above sense to $\E X_i = p$, thus the probability $p$ is  the long-run proportion of times that the outcome occurs.
\end{example}

By giving a more refined argument it is possible to dispense with the requirement in the statement of the Theorem that $\var (X_1) < \infty$. The conclusion still holds provided $\E \abs{X_1} < \infty$.

\begin{theorem}[weak law of large numbers\index{weak law of large numbers!finite absolute moment}]\label{thm:wlln_finite_absolute_moment}
Let $X_1,X_2, \dots$ be independent, identically distributed random variables with $\E X_1 = \mu$ and $\E\abs{X_1} < \infty$. For any constant $\ve > 0$,
\be
\pro\brb{\abs{\frac{X_1 + \dots + X_n}n - \mu} > \ve} \to 0,\quad \text{ as }n \to \infty.
\ee
\end{theorem}

%We can use Markov' inequality (Theorem \ref{thm:markov_inequality_probability})
%\beast
%\pro\brb{\abs{\frac{X_1 + \dots + X_n}n - \mu} > \ve} & \leq & \frac 1{\ve} \E\abs{\frac{X_1 + \dots + X_n}n - \mu} = \frac 1{n\ve} \E \brb{X_1 + \dots + X_n - n\mu}\\
%& \leq & \frac 1{n\ve} \sum^n_{i=1}\E\abs{X_i-\mu} = \frac{n}{n^2\ve^2}\var (X_1) = \frac 1{n\ve^2} \var (X_1) \to 0,
%\eeast

\begin{proof}[\bf Proof]%Proof using convergence of characteristic functions
By Taylor's theorem for complex functions\footnote{need link}, the characteristic function of any random variable, $X$, with finite mean $\mu$, can be written as
\be
\phi_X(t) = 1 + it\mu + o(t), \quad t \to 0.
\ee

All $X_1, X_2, \dots$ have the same characteristic function, so we will simply denote this $\phi_X$. Among the basic properties of characteristic functions there are
\be
\phi_{\frac 1 n X}(t)= \phi_X\brb{\frac t n},\quad \phi_{X+Y}(t)=\phi_X(t) \phi_Y(t) \quad \text{if $X$ and $Y$ are independent.}
\ee

These rules can be used to calculate the characteristic function of $\ol{X}_n := \frac 1n\sum^n_{i=1}X_i$ in terms of $\phi_X$:
\be
\phi_{\ol{X}_n}(t)= \bsb{\phi_X\brb{\frac tn}}^n = \bsb{1 + i\mu\frac tn + o\brb{\frac tn}}^n \to e^{it\mu}, \quad \text{as } n \to \infty.
\ee

The limit  $e^{it\mu}$  is the characteristic function of the constant random variable $\mu$, and hence by \levy's convergence theorem (Theorem \ref{thm:levy_continuity}), $\ol{X}_n$ converges in distribution to $\mu$:
\be
\ol{X}_n \xrightarrow{d} \mu\ \text{ as } n \to \infty.
\ee

$\mu$ is a constant, which implies that convergence in distribution to $\mu$ and convergence in probability to $\mu$ are equivalent (Proposition \ref{pro:convergence_in_distribution_constant_implies_convergence_in_probability}) Therefore,
\be
\ol{X}_n  \xrightarrow{p} \mu\ \text{ as } n \to \infty.
\ee%{pro:convergence_constant_distribution_implies_probability}
\end{proof}%This shows that the sample mean converges in probability to the derivative of the characteristic function at the origin, as long as the latter exists.


\subsection{Strong law of large numbers for finite fourth moment}

The result we obtain in this section will be largely superseded in the next. We include it because its proof is much more elementary than that needed for the definitive version of the strong law which follows.

\begin{theorem}[strong law of large numbers\index{strong law of large numbers!4th moment}]\label{thm:slln_4th}
Let $(X_n : n \in \N)$ be a sequence of independent random variables such that, for some constants $\mu \in \R$ and $M < \infty$,
\be
\E(X_n) = \mu, \quad \E(X^4_n) \leq M\quad \text{for all }n.
\ee

Set $S_n = X_1 + \dots + X_n$. Then
\be
S_n/n \to \mu \ \text{ a.s.,\quad as }n \to \infty.
\ee
\end{theorem}
\begin{proof}[\bf Proof]
Consider $Y_n = X_n - \mu$. Then $Y^4_n \leq 2^4(X^4_n + \mu^4)$, so
\be
\E(Y^4_n ) \leq 16(M + \mu^4)
\ee
and it suffices to show that $(Y_1 + \dots +Y_n)/n \to 0$ a.s. So we are reduced to the case where $\mu = 0$.

Note that $X_n,X^2_n,X^3_n$ are all integrable since $X^4_n$ is. Since $\mu = 0$, by independence,
\be
\E(X_iX^3_j) = \E(X_iX_jX^2_k) = \E(X_iX_jX_kX_l) = 0
\ee
for distinct indices $i, j, k, l$. Hence
\be
\E(S^4_n ) = \E\brb{\sum_{1\leq i\leq n} X^4_k + 6\sum_{1\leq i<j\leq n} X^2_i X^2_j}.
\ee

Now for $i < j$, by independence and the Cauchy-Schwarz inequality (Theorem \ref{thm:cauchy_schwarz_inequality_probability}),
\be
\E(X^2_i X^2_j) = \E(X^2_i)\E(X^2_j) \leq \E(X^4_i)^{1/2}\E(X^4_j)^{1/2} \leq M.
\ee

So we get the bound
\be
\E(S^4_n) \leq nM + 3n(n - 1)M \leq 3n^2M.
\ee

Thus
\be
\E\brb{\sum_n (S_n/n)^4} \leq 3M \sum_n 1/n^2 < \infty \quad\ra\quad \sum_n (S_n/n)^4 < \infty \text{ a.s.}
\ee
and hence $S_n/n \to 0$ a.s..
\end{proof}

\begin{proposition}
Let $(X_n : n \in \N)$ be a sequence of independent random variables such that, for some constants $\mu \in \R$ and $M < \infty$,
\be
\E(X_n) = \mu, \quad \E(X^4_n) \leq M\quad \text{for all }n.
\ee

Set $S_n = X_1X_2 + X_2X_3 + \dots + X_{n-1}X_n$. Then
\be
P_n/n \to \mu^2 \ \text{ a.s.,\quad as }n \to \infty.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Consider
\be
Y_n = X_{2n-1}X_{2n},\quad Z_n = X_{2n}X_{2n+1}.
\ee

Then since $(X_n : n \in \N)$ are independent, $Y_n$ and $Z_n$ are independent and
\be
\E(Y_n) = \E\brb{X_{2n-1}}\E\brb{X_{2n}} = \mu^2,\quad \E(Z_n) = \E\brb{X_{2n}}\E\brb{X_{2n+1}} = \mu^2,\quad \E(Y_n^4) ,\E(Z_n^4) \leq M^2.
\ee

Set $S_n = Y_1+\dots + Y_n$ and $T_n = Z_1+\dots + Z_n$. By Theorem \ref{thm:slln_4th}, we have
\be
S_n/n \to \mu^2,\quad T_n/n \to \mu^2 \ \text{ a.s. }.
\ee

Then
\be
\frac{P_{2n}}{2n} = \frac{S_n + T_{n-1}}{2n} \to \mu^2 ,\quad \frac{P_{2n+1}}{2n+1} = \frac{S_n + T_{n}}{2n+1} \to \mu^2 ,\ \text{ a.s..}
\ee

Thus, $P_{n}/n \to \mu^2$ a.s..
\end{proof}




\subsection{Canonical probability space}%Bernoulli shifts}

\begin{definition}[canonical probability space\index{canonical probability space!random variable sequence}]\label{def:canonical_probability_space_random_variable_sequence}
Consider a sequence of random variable $(X_n : n \in \N)$ and the infinite product space
\be
E = \R^\N = \{x = (x_n : n \in \N) : x_n \in \R \text{ for all }n\}
\ee
and the $\sigma$-algebra $\sE$ on $E$ generated by the coordinate functions $f_n(x) = x_n$
\be
\sE = \sigma(f_n : n \in \N) = \sigma\brb{f_n^{-1}(A):A\in \sB(\R),n\in \N} = \sigma\brb{\bigotimes^{n-1}_{k=1} \R \times A \times \bigotimes_{k>n}\R : A\in \sB(\R),n\in \N}.
\ee

Note that $\sE$ is also generated by the $\pi$-system
\be
\sA = \left\{ \bigotimes_{n \in\N} A_n : A_n \in \sB \text{ for all }n,\ A_n = \R \text{ for sufficiently large }n\right\}.
\ee

Define $X : \Omega \to E$ by $X (\omega) = (X_n(\omega) : n \in \N)$
\begin{align*}
X: (\Omega, \sF, \pro) & \longrightarrow (E, \sE) \\
\omega & \longmapsto (X_1(\omega), X_2(\omega), \ldots )
\end{align*}

Then for any set in $\pi$-system, $A = \bigotimes_{n\in \N}A_n \in \sA$, $A_n \in \sB(\R)$. Thus, $B_n = X_n^{-1}(A_n) \in \sF$ since $X_n$ are random variables (measurable). Thus, $X^{-1}(A) = \bigcap_n B_n \in \sF$, so by Theorem \ref{thm:pi_system_measurable}, $X$ is measurable.

Also the image measure $\mu = \pro \circ X^{-1}$. By uniqueness of extension (Theorem \ref{thm:uniqueness_of_extension_measure}), $\mu$ is the unique measure on $\sE$ having this property.

%Under the probability measure $\mu$, the coordinate maps $(X_n : n \in \N)$ are themselves a sequence of independent random variables with law $m$.

The probability space $(E, \sE, \mu)$ is called the canonical probability space for such sequences.
\end{definition}

\begin{definition}[shift map\index{shift map}]\label{def:shift_map}
Define the shift map $\theta : E \to E$ by
\be
\theta(x_1, x_2, \dots ) = (x_2, x_3, \dots ).
\ee
\end{definition}

\begin{theorem}\label{thm:measure_preserving}
The shift map is an ergodic measure-preserving transformation (see Definition \ref{def:ergodic_measure_preserving_transformation}) if the sequence $(X_n : n \in \N)$ are i.i.d.
\end{theorem}
\begin{proof}[\bf Proof]
Let $m$ be a probability measure on $\R$. In subsection \ref{subsec:rademacher_function}, we constructed a probability space $(\Omega,\sF,\pro)$ on which there exists a sequence of independent random variables $(X_n:n\in \N)$, all having distribution $m$.

$\forall A\in \sA$ of the form $A = \bigotimes^n_{k=1}A_k \times \bigotimes_{k>n}\R$ for $n$ sufficiently large, then
\be
\theta^{-1} (A) = \R\times \bigotimes^{n}_{k=1}A_k \times \bigotimes_{k> n}\R \in \sA \subseteq \sE.
\ee

Thus, by Theorem \ref{thm:pi_system_measurable}, $\theta$ is measurable. Also, $\mu = \pro \circ X^{-1}$, for $A = \bigotimes_{n\in\N} A_n \in \sA$,
\be
\mu(A) = \prod_{n\in \N} m(A_n)
\ee
since $(X_n : n \in \N)$ are independent (by Theorem \ref{thm:characteristic_function_independence}.(ii)). Thus,
\be
\mu(A) = \prod^n_{k=1} m(A_n),\quad \mu(\theta^{-1}(A)) = \prod^n_{k=1} m(A_n) \ \ra \ \mu(A)= \mu(\theta^{-1}(A)) = (\mu \circ \theta^{-1})(A)
\ee

Note that $\mu \circ \theta^{-1}$ is also a probability measure. Thus, by uniqueness of extension (Theorem \ref{thm:uniqueness_of_extension_measure}),
\be
\mu(A)= \mu(\theta^{-1}(A)),\ \forall A\in \sE \ \ra \ \theta \text{ is measure-preserving transformation.}
\ee

%The details of showing that $\theta$ is measurable and measure-preserving are left as an exercise.


To see that $\theta$ is ergodic, we recall the definition of the tail $\sigma$-algebras (Definition \ref{def:tail_sigma_algebra}),
\be
\sT_n = \sigma(X_m : m \geq n + 1), \quad\quad \sT_\infty = \bigcap_n \sT_n.
\ee

For $A =\bigotimes_{n\in\N} A_n \in \sA$ we have
\be
\theta^{-n}(A) = \{X_{n+k} \in A_k \text{ for all }k\} \in \sT_n.
\ee

Since $\sT_n$ is a $\sigma$-algebra, it follows that $\theta^{-n}(A) \in \sT_n$ for all $A \in \sE$, so by Proposition \ref{pro:invariant_measure_preserving},
\be
\bra{A\in \sE:\theta^{-1}(A) = A} = \sE_\theta \subseteq \sT_\infty.
\ee

Hence $\theta$ is ergodic by Kolmogorov's 0-1 law (Theorem \ref{thm:kolmogorov_0_1}) since the sequence $X_n$ are independent.
\end{proof}

\subsection{Stationary sequence of random variables}

\begin{definition}[stationary sequence of random variables\index{stationary!sequence of random variables}]\label{def:stationary_sequence_of_random_variables}
A sequence of random variables $(X_n : n \in \N)$ on a probability space $(\Omega, \sF, \pro)$ is stationary if for each $n, k \in \N$ the random vectors $(X_1, \dots,X_n)$ and $(X_{k+1}, \dots,X_{k+n})$ have the same distribution: for $A_1, \dots,A_n \in \sB(\R)$,
\be
\pro(X_1 \in A_1, \dots,X_n \in A_n) = \pro(X_{k+1} \in A_1, \dots,X_{k+n} \in A_n).
\ee
\end{definition}

\begin{remark}
Note that $(X_n : n \in \N)$ are not independent. %Compare this with canonical probability space (Definition \ref{def:canonical_probability_space_random_variable_sequence}).
\end{remark}

\begin{proposition}
If $(X_n : n \in \N)$ is a stationary sequence and $X_1 \in \sL^p(\Omega,\sF,\pro)$, for some $p \in [1,\infty)$, then
\be
\frac 1n \sum^n_{i=1} X_i \to Y\quad \text{a.s. and in } \sL^p(\Omega,\sF,\pro),
\ee
for some random variable $Y \in \sL^p(\Omega,\sF,\pro)$ with $\E(Y) = \E(X_1)$.
\end{proposition}

\begin{proof}[\bf Proof]%Let $E=\R^\mathbb{N}$ with the $\sigma$-algebra $\Sigma$ generated by the coordinate functions (see the proof that the shift map is ergodic in your lecture notes). Define random variable $Y$ by %Let $\mu = \pro \circ Y^{-1}$. Define $\mathcal{A}$ as the $\pi$-system
%\[
% \mathcal{A}= \left\{ \prod_{n \in \mathbb{N}} A_n: A_n \in \mathcal{B} \text{ for all } n, \, A_n \in \R \text{ for } n \text{ sufficiently large} \right\}
%\]
%Note that $\sigma(\mathcal{A})=\Sigma$ (see lecture notes). Now we claim that the shift map $\theta$, is measure-preserving. Indeed,
Let $(E,\sE,\mu)$ be the canonical probability space of $(X_n : n \in \N)$ and $\theta$ be the shift map (see Definition \ref{def:shift_map}). $\forall A\in \sA$ of the form $A = \bigotimes^n_{k=1}A_k \times \bigotimes_{k>n}\R$ for $n$ sufficiently large, then %For $A \in \mathcal{A}$ with $A=A_1 \times \cdots \times A_n \times \R \times \cdots$, we have
\beast
\mu(\theta^{-1}(A)) & = & \pro \circ X^{-1} \brb{\theta^{-1}(A)} = \pro\brb{\theta(X) \in A}  = \pro \brb{X\in \theta^{-1}(A)} = \pro (X_2 \in A_1, \ldots, X_{n+1} \in A_n) \\%= \pro \circ Y^{-1}(\R \times A_1 \times \cdots \times A_n \times \R \times \cdots) \\
& = & \pro(X_1 \in A_1, \ldots, X_n \in A_n) \qquad \text{$(X_n : n \in \N)$ is stationary}\\
&= & \pro\brb{X\in A} = \mu(A).%\pro \circ Y^{-1}(A_1 \times \cdots \times A_n \times \R \times \cdots)
\eeast

So probability measures $\mu \circ \theta^{-1}$ and $\mu$ agree on $\sA$, so they must agree on $\sE$. Thus, $\theta$ is measure-preserving.

Now let $f:(E,\sE,\mu) \to (\R, \sB\brb{\R})$ be the first coordinate map. Then
\be
f\circ X=X_1 \ (f(x) = x_1), \quad f\circ \theta \circ X = X_2 \ (f\circ \theta (x) = x_2),\quad \dots
\ee

Thus,
\be
\mu\brb{\abs{f}^p} = \E \brb{\abs{X_1}^p} < \infty \qquad \text{since $X_1\in \sL^p(\Omega,\sF,\pro)$} \ \ra \ f\in \sL^p(E,\sE,\mu).
\ee

%, and $\abs{f}^p \circ X =\abs{X_1}^p$. Note that $\mu(\abs{f}^p)=\E(\abs{X_1}^p)<\infty$ as $X_1 \in L^p$ (see the proposition on image measures in your lecture notes).

Then by Birkhoff's ergodic theorem (Theorem \ref{thm:birkhoff_ergodic} since $f$ is integrable by Proposition \ref{pro:lp-norm_monotonicity}) and von Neumann's ergodic theorem (Theorem \ref{thm:von_neumann_lp_ergodic}), we have that
\be
\frac{1}{n}(f + f \circ \theta + \cdots + f \circ f^{n-1}) = S_n(f) / n\to \bar{f} \quad \text{a.e. and in $\sL^p(E,\sE,\mu)$, }
\ee
for some $\ol{f} \in \sL^p(E,\sE,\mu)$. Set $Y = \ol{f} \circ X$, so $Y:(\Omega, \sF, \pro) \to (\R, \sB(\R))$. Then
\be
\E \brb{\abs{Y}^p} = \E\brb{\abs{\ol{f}}^p \circ X} = \mu \brb{\abs{\ol{f}}^p} < \infty \ \ra \ Y \in \sL^p(\Omega,\sF,\pro).
\ee

Then we have that,
\beast
1 = \mu\left( S_n(f)/n \to \bar{f}\right) & = & \pro \circ X^{-1} \brb{\bra{x \in E: \frac{1}{n}S_n(f)(x)\to \bar{f}(x)}} = \pro \brb{ \bra{ \omega: \frac{1}{n}S_n(f)(X(\omega))\to \ol{f}(X(\omega))}} \\
& = & \pro \brb{ \bra{ \omega: \frac{1}{n}\brb{f\circ X(\omega) + f\circ \theta \circ X (\omega) + \dots + f\circ \theta^{n-1} \circ X (\omega)})\to \ol{f}(X(\omega))}} \\
& = & \pro \left( \frac{1}{n} \sum_{i=1} ^n X_i(\omega) \to Y (\omega) \right).
\eeast
\be
 \E\left( \abs{\frac{1}{n} \sum_{i=1} ^n X_i - Y}^p \right) = \E(\abs{S_n(f)/n - \ol{f}}^p \circ X) = \mu\brb{\abs{S_n(f)/n - \ol{f}}^p } \to 0.
\ee

Hence,
\be
\frac 1n \sum^n_{i=1} X_i \to Y\quad \text{a.s. and in } \sL^p(\Omega,\sF,\pro).
\ee

Finally, by Proposition \ref{pro:convergence_slp_monotone_probability},
\be
\frac{1}{n}\sum_{i=1} ^n X_i \to Y \ \text{ in $\sL^1(\Omega,\sF,\pro)$} \ \ra \ \E(Y) = \lim_{n \to \infty} \E \left( \frac{1}{n}\sum_{i=1} ^n X_i \right) = \E(X_1)
\ee
by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}).
\end{proof}


\subsection{Strong law of large numbers}

\begin{theorem}\label{thm:mean_probability}
Let $m$ be a probability measure on $\R$, with
\be
\int_\R |x_n|m(dx_n) < \infty, \qquad \int_\R x_n m(dx_n) = \nu,\qquad n\in \N
\ee
Let $(E, \sE, \mu)$ be the canonical model for a sequence of independent random variables with law $m$. Then
\be
\mu(\{x \in \R^\N: (x_1 + \dots + x_n)/n \to \nu \text{ as }n \to \infty\}) = 1.
\ee
\end{theorem}
\begin{proof}[\bf Proof]
By Theorem \ref{thm:measure_preserving}, the shift map $\theta$ on $E$ is measure-preserving and ergodic. The coordinate function $f(x) = x_1,x\in \R^\N$ is integrable (since $\int \abs{f} d\mu = \int_\R |x_1|m(dx_1) < \infty$) and
\be
S_n(f)(x) = f(x) +f \circ \theta(x) +\dots +f \circ \theta^{n-1}(x) = x_1 + \dots + x_n.
\ee

Also by Proposition \ref{pro:integrable_measure_preserving},
\be
\mu(f\circ \theta^{n-1}(x)) = \mu(f(x)) = m(x_1) = \nu \ \ra \ \mu \brb{S_n(f)(x)/n} = n\mu(f(x))/n = \nu.\quad (*)
\ee

So $S_n(f)/n  \to \ol{f}$ a.e. and in $\sL^1(E,\sE,\mu)$, for some invariant function $\ol{f}$, by Birkhoff's theorem (Theorem \ref{thm:birkhoff_ergodic}) and von Neumann's $\sL^p$ ergodic theorem (Theorem \ref{thm:von_neumann_lp_ergodic}). Since $\theta$ is ergodic, $\ol{f} = c$ a.e., for some constant $c$ by Proposition \ref{pro:ergodic_invariant_constant}. Also, $\mu \brb{S_n(f)/n} \to \mu \brb{\ol{f}}$ by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}).

Then by Theorem \ref{thm:lebesgue_integrable_function_property},% and $\int_\R xm(dx) = \nu$,
\be
c = \mu(\ol{f}) = \lim_n \mu(S_n(f)/n) = \lim_n \nu = \nu.\quad (\text{by }(*))
\ee
\end{proof}

\begin{theorem}[strong law of large numbers\index{strong law of large numbers!ergodic theorem approach}]\label{thm:slln}
Let $(Y_n : n \in\N)$ be a sequence of independent, identically distributed, integrable random variables with mean $\nu$. Set $S_n = Y_1 + \dots + Y_n$. Then
\be
S_n/n \to \nu  \text{ a.s. as }n \to \infty.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
In the notation of Theorem \ref{thm:mean_probability}, take $m$ to be the law of the random variables $Y_n$. Then $\mu = P \circ Y^{-1}$, where $Y : \Omega \to E$ is given by $Y (\omega) = (Y_n(\omega) : n \in \N)$. Hence
\beast
\pro(S_n/n \to \nu\text{ as }n \to \infty) & = & \pro\brb{(Y_1(\omega) + \dots + Y_n(\omega))/n \to \nu\text{ as }n \to \infty} \\
& = & \mu(\{x : (x_1 + \dots + x_n)/n \to \nu\text{ as }n \to \infty\}) = 1.
\eeast
\end{proof}



\subsection{Central limit theorem}

\begin{theorem}[central limit theorem\index{central limit theorem}]\label{thm:central_limit}
Let $(X_n : n \in \N)$ be a sequence of independent, identically distributed (i.i.d.\index{i.i.d.!random variables}), random variables with mean 0 and variance 1. Set $S_n = X_1 + \dots + X_n$. Then, for all $a < b$, as $n \to \infty$,
\be
\pro\brb{\frac{S_n}{\sqrt{n}} \in [a, b]} \to \int^b_a \frac 1{\sqrt{2\pi}} e^{-y^2/2}dy, \quad \text{i.e.,}\quad \frac{S_n}{\sqrt{n}} \to \sN(0, 1).
\ee
\end{theorem}

\begin{remark}
The alternative form\footnote{details needed} is: $(X_n)_{n\in \N}$ are i.i.d. with mean $\mu$ and variance $\sigma^2$ and $S_n = X_1 + \dots + X_n$. Then
\be
\frac{S_n - n\mu}{\sigma \sqrt{n}}\stackrel{d}{\to } \sN(0,1).
\ee
\end{remark}

\begin{proof}[\bf Proof]
Set $\phi(u) = \E(e^{iuX_1})$ where $\phi(u)$ is the characteristic function (Definition \ref{def:characteristic_function_n}). Since $e^{iuX_1}$ is bounded and $\E(X^2_1) = 1 < \infty$, we can differentiate $\E(e^{iuX_1})$ twice under the expectation by Theorem \ref{thm:differentiation_under_integral_sign}. It is shown that
\be
\phi(0) = \E\brb{e^0} = 1,\quad \phi'(0) = \E\brb{iX_1 e^0} = 0,\quad \phi''(0) = \E\brb{-X_1^2 e^0} = -1.
\ee

Hence, by Taylor's theorem (Theorem \ref{thm:taylor_lagrange}), as $u \to 0$,
\be
\phi(u) = 1 - u^2/2 + o(u^2).
\ee

So, for the characteristic function $\phi_n$ of $S_n/\sqrt{n}$, by Theorem \ref{thm:characteristic_function_independence}.(iii),
\be
\phi_n(u) = \E\brb{e^{iu(X_1+ \dots +X_n)/\sqrt{n}}} = \brb{\E\brb{e^{i(u/\sqrt{n})X_1}}}^n = (1 - u^2/2n + o(u^2/n))^n.
\ee

The complex logarithm satisfies, as $z\to 0$, $\log(1 + z) = z + o(|z|)$. So for each $u \in \R$, as $n \to \infty$,
\be
\log \phi_n(u) = n \log\brb{1 - u^2/2n + o(u^2/n)} = -u^2/2 + o(1).
\ee
Hence $\phi_n(u) \to e^{-u^2/2}$ for all $u$. But $e^{-u^2/2}$ is the characteristic function of the $\sN(0, 1)$ distribution, so $S_n/\sqrt{n} \to \sN(0, 1)$ in distribution by \levy's continuity theorem (Theorem \ref{thm:levy_continuity}), as required.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{proof}[\bf Alternative Proof.]%Here is an alternative argument, which does not rely on L\'evy's continuity theorem.
Take a random variable $Y \sim \sN(0, 1)$, independent of the sequence $(X_n)_{n \in \N}$. Fix $a < b$ and $\delta > 0$ and consider the function $f$ which interpolates linearly the points $(-\infty, 0)$, $(a - \delta, 0)$, $(a, 1)$, $(b, 1)$, $(b + \delta, 0)$, $(\infty, 0)$.

\begin{center}
\psset{yunit=3cm,xunit=3cm}
\begin{pspicture}(-2,-0.1)(2,0.6)
\psline[linestyle=dashed](0.5,0)(0.5,0.5)
\psline[linestyle=dashed](-0.5,0)(-0.5,0.5)
\psline[linestyle=dashed](-1,0.5)(1,0.5)

\psline(-1.5,0)(1.5,0)
\psline[linewidth=1.5pt](-1.5,0)(-1,0)(-0.5,0.5)(0.5,0.5)(1,0)(1.5,0)

\rput[lb](-1.1,-0.16){$a-\delta$}
\rput[lb](-0.6,-0.16){$a$}
\rput[lb](0.45,-0.16){$b$}
\rput[lb](0.8,-0.16){$b+\delta$}
\rput[lb](-1.1,0.4){1}
\rput[lb](1,0.2){$f$}
\rput[lb](1.6,-0.1){$\R$}

\end{pspicture}
\end{center}

Note that $|f(x + y) - f(x)| \leq |y|/\delta$ for all $x$, $y$. So, given $\ve > 0$, for $t = (\pi/2)(\ve\delta/3)^2$ and any random variable $Z$, by Theorem \ref{thm:lebesgue_integrable_function_property}.(iv) and Proposition \ref{pro:abs_gaussian},
\be
\abs{\E(f(Z + \sqrt{t}Y )) - \E(f(Z))} \leq \E(\sqrt{t}|Y|)/\delta = \ve/3.
\ee

Recall from the proof of the Fourier inversion formula ((*) in the proof of Theorem \ref{thm:inversion_density}) that
\be
\E\brb{f\brb{\frac{S_n}{\sqrt{n}} + \sqrt{t}Y}} = \int_\R \brb{\frac 1{2\pi} \int_\R \phi_n(u)e^{-u^2t/2}e^{-iuy}du} f(y)dy.
\ee

Consider a second sequence of independent random variables $(\ol{X}_n : n \in \N)$, also independent of $Y$, and with $\ol{X}_n \sim \sN(0, 1)$ for all $n$. Note that $\ol{S}_n/\sqrt{n} \sim \sN(0, 1)$ for all $n$. So
\be
\E\brb{f\brb{ \frac{\ol{S}_n}{\sqrt{n}} + \sqrt{t}Y}} = \int_\R \brb{\frac 1{2\pi} \int_\R e^{-u^2/2}e^{-u^2t/2}e^{-iuy}du} f(y)dy.
\ee

Now $e^{-u^2t/2}f(y) \in \sL^1(\R^2, \sB(\R^2),du \otimes dy)$ and $\phi_n$ is bounded, with $\phi_n(u) \to e^{-u^2/2}$ for all $u$ as $n \to \infty$, so, by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability}) for $n$ sufficiently large,
\be
\abs{\E\brb{f\brb{ \frac{S_n}{\sqrt{n}} + \sqrt{t}Y}}  - \E\brb{f\brb{ \frac{\ol{S}_n}{\sqrt{n}} + \sqrt{t}Y}}} \leq \ve/3.
\ee%Hence, by taking $Z = S_n/\sqrt{n}$ and then $Z = \ol{S}_n/\sqrt{n}$, we obtain
\beast
\abs{\E\brb{f\brb{S_n/\sqrt{n}}} - \E\brb{f\brb{\ol{S}_n/\sqrt{n}}}} & \leq & \abs{\E\brb{f\brb{S_n/\sqrt{n}}} - \E\brb{f\brb{S_n/\sqrt{n} + \sqrt{t}Y}}} \\
& & + \abs{\E\brb{f\brb{\ol{S}_n/\sqrt{n} + \sqrt{t}Y}} - \E\brb{f\brb{S_n/\sqrt{n} + \sqrt{t}Y}}}\\
& & + \abs{\E\brb{f\brb{\ol{S}_n/\sqrt{n}}} - \E\brb{f\brb{\ol{S}_n/\sqrt{n} + \sqrt{t}Y}}} \\
& \leq & \frac {\ve}3 + \frac {\ve}3 + \frac {\ve}3 = \ve.
\eeast

But $\ol{S}_n/\sqrt{n} \sim Y$ for all $n$ and $\ve > 0$ is arbitrary, so we have shown that
\be
\E(f(S_n/\sqrt{n})) \to \E(f(Y)) \quad \text{as } n \to \infty.
\ee

The same argument applies to the function $g$, defined like $f$, but with $a$, $b$ replaced by $a + \delta$, $b - \delta$ respectively. Now $g \leq \ind_{[a,b]} \leq f$, so by Theorem \ref{thm:lebesgue_integrable_function_property}.(ii),
\be
\E\brb{g\brb{\frac{S_n}{\sqrt{n}}}} \leq \pro\brb{\frac{S_n}{\sqrt{n}} \in [a,b]} \leq \E\brb{f\brb{\frac{S_n}{\sqrt{n}}}}.
\ee

On the other hand, as $\delta \da 0$, by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}),
\be
\E(g(Y)) \ua \int^b_a \frac 1{\sqrt{2\pi}} e^{-y^2/2}dy,\quad\quad \E(f(Y)) \da \int^b_a \frac 1{\sqrt{2\pi}} e^{-y^2/2} dy
\ee
so we must have, as $n \to \infty$,
\be
\pro\brb{\frac{S_n}{\sqrt{n}} \in [a, b]} \to \int^b_a \frac 1{\sqrt{2\pi}} e^{-y^2/2}dy.
\ee
\end{proof}

\begin{example}[normal approximation to the binomial distribution]
If the random variable $Y \sim \bd(n, p)$, we may think of the distribution of $Y$ as being the same as that of the sum of $n$ i.i.d. random variables each of which has the Bernoulli distribution, thus the random variable $(Y -np)/\sqrt{np(1 - p)}$ has approximately the $\sN(0, 1)$ distribution for large $n$.

Note that here $p$ is being held fixed and $n \to \infty$, unlike the situation we described in the Poisson approximation to the binomial where $n \to \infty$ and $p \to 0$ in such a way that the product $np \to \lm > 0$.
\end{example}

\begin{example}[normal approximation to the Poisson distribution]
When the random variable $Y \sim \pd(n)$, where $n \geq 1$ is an integer, we may think of $Y$ as having the same distribution as that of the sum of $n$ i.i.d. random variables each with the $\pd(1)$ distribution. Thus $(Y - n)/\sqrt{n}$ has approximately the $\sN(0, 1)$ distribution for large $n$. The same conclusion is true for $Y \sim \pd(\lm)$ for non-integer $\lm$, that is, $(Y - \lm)/\sqrt{\lm}$ is approximately $\sN(0, 1)$ for $\lm$ large.
\end{example}

\begin{example}[opinion polls]
Suppose that the proportion of voters in the population who vote Labour is $p$, where $p$ is unknown. A random sample of $n$ voters is taken and it is found that $S$ voters in the sample vote Labour and we estimate $p$ by $S/n$. We want to ensure that $\abs{S/n - p} < \ve$, for some small given $\ve$, with high probability, $\geq 0.95$, say. How large must $n$ be? Note that $S \sim \bd(n, p)$, so that $\E S = np$ and $\var (S) = np(1 - p)$. Then by central limit theorem (Theorem \ref{thm:central_limit}), we require
\beast
\pro\brb{\abs{\frac Sn - p} < \ve} & = & \pro\brb{-\ve\sqrt{\frac{n}{p(1 - p)}} < \frac{S - np}{\sqrt{np(1 - p)}} < \ve\sqrt{\frac{n}{p(1 - p)}}}\\
& \approx & \Phi \brb{\ve\sqrt{\frac n{p(1 - p)}}} - \Phi \brb{-\ve \sqrt{\frac n{p(1 - p)}}} \\
& = & 2\Phi \brb{\ve \sqrt{\frac n{p(1 - p)}}} - 1 \geq 0.95, \quad\quad \text{since }\Phi(x) = 1 - \Phi(-x),
\eeast
so that we need $\ve\sqrt{n/p(1 - p)} > 1.96$, that is
\be
n \geq (1.96)^2p(1 - p)/\ve^2.
\ee
We do not know the value of $p$, but it is always the case that $p(1-p) \leq \frac 14$, with equality occurring when $p = \frac 12$, so to ensure that we have the required bound we need $n \geq (1.96/2\ve)^2$. For example, if we take $\ve = 0.02$, so that the estimate of the percentage of Labour voters is
accurate to within 2 percentage points with 95\% probability we would need to take a sample with $n \geq 2401$. The typical sample size in opinion polls is $n \approx 1000$ which corresponds to an error $\ve\approx 0.03$.
\end{example}



\section{Geometric Probability}

\subsection{Buffon's needle}

Consider a needle of length $r$ which is thrown at random on to a plane surface on which there are parallel straight lines at distance $d > r$ apart. What is the probability that the needle intersects one of the lines?

Think of the parallel lines running West-East and let $X$ be the distance from the point representing the Southern end of the needle to the nearest line North of that point, if the needle is parallel to the lines, take the right-hand end point. Let $\Theta$ be the angle that the needle makes with the West-East lines. Then we will assume that $X$ is uniformly distributed on $[0, d)$ and $\Theta$ is uniformly distributed on $[0, \pi]$, and that $X$ and $\Theta$ are independent.


\begin{center}
\psset{yunit=2.5cm,xunit=2.5cm}
\begin{pspicture}(-2,-0.2)(2,2.2)
\psaxes[labels=none,ticks=none]{->}(0,0)(-0.2,-0.2)(2.2,2.2)%Dy=0.25,dy=0.25
\psset{algebraic}%,linewidth=1.5pt}
\pscustom[fillstyle=solid,fillcolor=red!30]{%linestyle=dashed,
\psplot{0}{2}{1.6*sin(x*3.1416/2)}
\psline(2,0)(0,0)}%
\psline[linestyle=dashed](2,0)(2,2)(0,2)
\psline[linestyle=dashed](1,1.6)(0,1.6)
\rput[lb](-0.1,1.55){$r$}
\rput[lb](-0.1,1.95){$d$}
\rput[lb](0.1,2.15){$x$}
\rput[lb](2,-0.15){$\pi$}
\rput[lb](-0.1,-0.15){0}
\rput[lb](2.2,0.05){$\theta$}
\rput[lb](1.5,1.2){$r\sin\theta$}

\psline(-3,1.8)(-1,1.8)
\psline(-3,1)(-1,1)
\psline(-3,0.2)(-1,0.2)
\psline[linewidth=2.5pt](-2,0.5)(-1.2,1.3)
\pcline{|<->|}(-2.5,1.8)(-2.5,1)\Aput{$d$}

\psline[linestyle=dashed](-2,1)(-2,0.5)(-1,0.5)
\rput[lb](-2.15,0.7){$X$}
\rput[lb](-1.5,1.2){$r$}%\rput[lb](-1.7,0.6){$\Theta$}

\pstGeonode[PointSymbol=none,PointName=none](-2,0.5){A}(-1,0.5){B}(-1.2,1.3){C}%(0,4){B}
\pstMarkAngle[LabelSep=0.3]{B}{A}{C}{$\Theta$}%linecolor=red, arrows=->
\end{pspicture}
\end{center}


The joint probability density of $(X, \Theta)$ is
\be
f(x, \theta) = \left\{ \ba{ll}
\frac 1{\pi d}, \quad\quad & 0 \leq x < d, \ 0 \leq \theta \leq \pi,\\
0 & \text{otherwise}.
\ea\right.
\ee



Then if $A$ is the shaded area in the $x-\theta$ plane illustrated, the probability that the needle intersects a line is
\be
\pro (X \leq r \sin \Theta) = {\int\int}_{A} f(x, \theta)dxd\theta = \int^\pi_0 \int^{r \sin \theta}_0 \frac 1{\pi d} dxd\theta = \frac r{\pi d} \int^\pi_0
\sin \theta d\theta = \frac{2r}{\pi d}.
\ee
This probability was derived in 1777 by the French mathematician and naturalist Georges Louis Leclerc, Comte de Buffon, who suggested a method of approximating the value of $\pi$ by repeatedly dropping a needle and estimating the probability that a line is intersect (and hence the value of $\pi$) by recording the proportion of times that the needle crosses a line. First note that if $X \sim \sN(\mu, \sigma^2)$, where $\sigma^2$ is small, and $g : \R \to \R$ then
\be
g(X) = g(\mu) + (X - \mu)g'(\mu) + \dots \sim \sN\brb{g(\mu), (g'(\mu))^2 \sigma^2},
\ee
where the symbol $\simeq$ may be read as ``approximately distributed as''. Now, if $S_n = X_1 + \dots + X_n$ denotes the total number of times that the needle intersects a line in $n$ drops of the needle, where $X_i$ is the indicator of the event that a line is intersected on the $i$th drop, then $S_n \sim \bd(n, p)$ where $p = 2r/(\pi d)$. By the Central Limit Theorem we have that $S_n/n \sim \sN(p, p(1 - p)/n)$. Let $g(x) = 2r/(xd)$, so that $g(p) = \pi$ and $g'(p) = -\pi^2d/(2r)$. We see that an estimate of $\pi$ is given by $\wh{\pi} = g(S_n/n)$, where
\be
\wh{\pi} \sim \sN\brb{\pi, \frac{\pi^2}{2rn} (\pi d - 2r)}.
\ee
One small difficulty that arises when one tries to replicate Buffon's procedure for estimating $\pi$ on a computer is how to do the simulation without using the value of $\pi$ to take random samples of $\Theta$ from the uniform distribution on $(0, \pi]$. One way around this is to generate a sample $(X, Y)$ which has the uniform distribution over a quadrant of the circle centre the origin and of unit radius as follows:

Step 1. generate independent $X$ and $Y$ each with the uniform distribution on $[0, 1]$,

Step 2. if $X^2 + Y^2 > 1$ repeat Step 1, otherwise take $(X, Y )$.

Now set $\Theta = 2\arctan(Y/X)$, which will be uniform on $(0, \pi)$.

\subsection{Bertrand's Paradox}

This results from the following question posed by Bertrand in 1889: What is the probability that a chord chosen at random joining two points of a circle of radius $r$ has length $\leq r$?

The difficulty with the question is that there is not a unique interpretation of what it means for a chord to be chosen `at random', there are different ways to do this and they lead to different probabilities for the length, $C$, of the chord being less than $r$. We will consider two approaches.

Approach 1. Let $X$ be a random variable having the uniform distribution on $(0, r)$ and let $\Theta$ be a random variable, independent of $X$, with the uniform distribution on $(0, 2\pi)$.


\begin{center}
\psset{yunit=1cm,xunit=1cm}
\begin{pspicture}(-7,-2.5)(2,2.5)
%\pnodes(-2,0.8){A}%(10.5,5){B}
\pstGeonode[PointSymbol=none,PointName=none](0,0){O}(2,0){A}(-1,1){B}
%\pscircle[](O){2.5}%fillstyle=solid,fillcolor=blue!60,opacity=0.5\pstCircleOA{O}{A}
\pscircle[](0,0){2}
\psline(0,2)(-2,0)
\psline(0,0)(-1.414,1.414)
\psline[linestyle=dashed](0,0)(2.3,0)
\pstMarkAngle[LabelSep=0.7]{A}{O}{B}{$\Theta$}
%\rput[lb](-1,0){}
\pcline{|<->|}(-0.25,-0.25)(-1.25,0.75)
\mput*{$X$}

\rput[lb](-7,1){The length of the chord is}
\rput[lb](-7,0){$C = 2\sqrt{r^2 - X^2}$}
\end{pspicture}
\end{center}

Construct the chord by taking a reference line (the $x$-axis, say) and drawing the radius at angle $\Theta$ with the line. Then take the chord at right angles to this radius at distance $X$ from the centre of the circle. We then have
\be
\pro(C \leq r) = \pro\brb{4(r^2 - X^2) \leq r^2} = \pro\brb{\sqrt{3} r/2 \leq X} = 1 - \sqrt{3}/2 \approx 0.134.
\ee

Approach 2. Let $\Theta_1$ and $\Theta_2$ be independent random variables each with the uniform distribution on $(0, 2\pi)$. Take the end points of the chord as the points $(r \cos\Theta_i, r \sin\Theta_i)$, $i = 1, 2$, on the circumference of the circle, where the angles are measured from some reference line. The length of the chord is $C = 2r \sin\brb{\frac{\abs{\Theta_1-\Theta_2}}2}$.


\begin{center}
\psset{yunit=1.25cm,xunit=1.25cm}
\begin{pspicture}(-7,-0.5)(4.5,4.5)
\psaxes[labels=none,ticks=none]{->}(0,0)(-0.5,-0.5)(4.5,4.5)%Dy=0.25,dy=0.25
\pstGeonode[PointSymbol=none,PointName=none](-5,2){O}(-3,2){A}(-3,4){B}(-7,4){C}
%\pscircle[](O){2.5}%fillstyle=solid,fillcolor=blue!60,opacity=0.5\pstCircleOA{O}{A}
%\pscircle[](-5,2){2}
\pstCircleOA{O}{A}
\psline(-5,2)(-6.414,3.414)(-3.586,3.414)(-5,2)
\psline[linestyle=dashed](-5,2)(-2.3,2)
\pstMarkAngle[LabelSep=1,MarkAngleRadius=0.7]{A}{O}{B}{$\Theta_1$}
\pstMarkAngle[LabelSep=0.6,LabelAngleOffset=20]{A}{O}{C}{$\Theta_2$}

\rput[lb](4.2,0.2){$\Theta_1$}
\rput[lb](0.2,4.2){$\Theta_2$}

\rput[lb](3.9,-0.3){$2\pi$}
\rput[rb](-0.1,4){$2\pi$}

\rput[rb](-0.1,0.5){$\frac{\pi}3$}
\rput[rb](-0.1,3.2){$\frac{5\pi}3$}

\rput[lb](0.5,-0.4){$\frac{\pi}3$}
\rput[lb](3.2,-0.4){$\frac{5\pi}3$}


\psline[linestyle=dashed](4,0)(4,4)(0,4)

\pscustom[fillstyle=solid,fillcolor=red!30]{%linestyle=dashed,
\psline(0,4)(0.667,4)(0,3.333)}

\pscustom[fillstyle=solid,fillcolor=red!30]{%linestyle=dashed,
\psline(4,0)(4,0.667)(3.333,0)}

\pscustom[fillstyle=solid,fillcolor=red!30]{%linestyle=dashed,
\psline(0,0)(0.667,0)(4,3.333)(4,4)(3.333,4)(0,0.667)(0,0)}
\end{pspicture}
\end{center}

The probability is then
\be
\pro (C \leq r) = \pro\brb{\sin\brb{\frac{\abs{\Theta_1 - \Theta_2}}2} \leq \frac 12} = \pro\brb{\abs{\Theta_1 - \Theta_2} \leq \frac {\pi}3 \text{ or } \abs{\Theta_1 - \Theta_2}\geq \frac{5\pi}3},
\ee
this probability is the area of the shaded region in the square divided by $(2\pi)^2$ which gives the probability to be $\frac 13 \approx 0.3333$. This probability is the same as when you take one end of the chord as fixed (say on the reference line) and take the other end at a point at angle $\Theta$ uniformly distributed on $(0, 2\pi)$.

It should be noted that both probabilities may arise as the outcomes of physical experiments which are choosing the chord 'at random'. For example, the probability in Approach 1 would be found if a circular disc of radius $r$ is thrown randomly onto a table on which parallel lines at distance $2r$ are drawn, the chord would be determined by the unique line intersecting the circle and the distribution of the distance of center of the circle to the nearest line would be uniform on $(0, r)$. By contrast, the probability in Approach 2 is obtained if the disc is pivoted on a point on its circumference, which is on a given straight line and the disc is spun around that point, then the chord would be determined by the intersection of the given line and the circumference of the disc.


\section{Summary}

\subsection{Convergence of random variables}

$X_n \to X$ a.s. $\ \ra \ $ $X_n \stackrel{p}{\to} X$ $\ \ra\ $ $X_n \stackrel{d}{\to} X$.

For $1\leq p\leq q <\infty$, we have $X_n \stackrel{\sL^q}{\to} X$ $\ \ra\ $ $X_n \stackrel{\sL^p}{\to} X$ $\ \ra\ $ $X_n \stackrel{p}{\to} X$ $\ \ra\ $ $X_n \stackrel{d}{\to} X$.

$X_n \stackrel{d}{\to}c \ \lra\ X_n \stackrel{p}{\to}c$ for constant $c$ (Proposition \ref{pro:convergence_in_distribution_constant_implies_convergence_in_probability})



\section{Problems}

\subsection{Classical probability}

\begin{problem}
From a table of random digits, $k$ are chosen. What are the probabilities that for $0\leq r\leq 9$, (i) no digit exceed $r$? (ii) $r$ is the greatest digit drawn?
\end{problem}

\begin{solution}[\bf Solution.]
Let $\Omega=\left\{\{i_1,\dots,i_k\}, 0\leq i_j\leq 9, j=1,\dots,k\right\}$.
\ben
\item [(i)] $\pro(\text{no digit exceed $r$}) = \pro(\text{all $i_j\leq r$})=\left(\frac{r+1}{10}\right)^k$.
\item [(ii)] $\pro(\text{$r$ is the greatest digit drawn} = \pro(\text{all $i_j\leq r$, but not all $\leq r-1$}) = \pro(\text{all $i_j\leq r$}) - \pro(\text{all $i_j\leq r-1$}) = \left(\frac{r+1}{10}\right)^k -\left(\frac{r}{10}\right)^k$.
\een
\end{solution}

%-----------------

\begin{problem}
Five mices are chosen (without replacement) from a litter, three of which are tagged $A$, $B$ and $C$. The probability that all three tagged mice are chosen is twice the probability that $A$ is the only tagged mouse chosen. How many mice are three in the litter?
\end{problem}

\begin{solution}[\bf Solution.]
The probability all three tagged mice are chosen is $\left.\binom{n-3}{2}\right/\binom{n}{5}$, while the probability only A is chosen is $\left.\binom{n-3}{4}\right/\binom{n}{5}$. This leads to the equation
\be
\frac{(n-3)(n-4)}{2!} = 2\frac{(n-3)(n-4)(n-5)(n-6)}{4!}
\ee
which gives $6=n^2-11n+30$, or $n^2-11n+24=0$, so that $n=8$ or $n=3$; the latter cannot happen so we conlude that $n=8$.
\end{solution}

\begin{problem}
Two cards are taken at random from an ordinary pack of 52 cards. Find the probabilities that
\ben
\item [(i)] both cards are aces (event $A$).
\item [(ii)] the pair of cards includes an ace (event $B$).
\item [(iii)] the pair of cards includes the ace of hearts (event $C$).
\een

Show that $\mathbb{P}(A|B)\neq \mathbb{P}(A|C)$.
\end{problem}

\begin{solution}[\bf Solution.]
(i)-(iii)
\be
\mathbb{P}(A)=\frac{\binom{4}{2}}{\binom{52}{2}} = \frac{4\times 3}{52\times 51} = \frac{1}{221},\quad \mathbb{P}(B)=1-\frac{\binom{48}{2}}{\binom{52}{2}} = \frac{48\times 47}{52\times 51} = \frac{33}{221},\quad \mathbb{P}(C)=1-\frac{\binom{51}{1}}{\binom{52}{2}} = \frac{2\times 51}{52\times 51} = \frac{1}{26}.
\ee

Also, we have \be \mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(A)}{\mathbb{P}(B)} = \frac{1}{33},\quad \pro(A|C)=\frac{\mathbb{P}(A\cap C)}{\mathbb{P}(C)} = \left.\frac{3}{\binom{52}{2}}
\right/ \frac{1}{26} = \frac{1}{17}. \ee

Thus, $\mathbb{P}(A|B)\neq \mathbb{P}(A|C)$.
\end{solution}

\begin{problem}
Sarah collects figures from cornflakes packets. Each packet contains one figure, and $n$ distinct figures make a complete set. Show that the expected number of packets Sarah needs
to buy to collect a complete set is $n\sum^n_{i=1}i^{-1}$. How much easier is it to collet half the set than the complete set?
\end{problem}

\begin{solution}[\bf Solution.]
The total number is $N=1+\sum^{n-1}_{i=1}X_i$, where $\mathbb{P}(X_i=k)=(i/n)^{k-1}(1-i/n)$, for $k=1,2,\dots$. We have
\be
\E X_i=\sum^\infty_{k=1}k\brb{\frac in}^{k-1}\brb{1-\frac in}=\frac n{n-i}.
\ee

It follows that $\E N=n\sum^n_{i=1}\frac 1i$.

The expected number of packets Sarah needs to buy to collet half is
\be
\sum^n_{i=n/2}\frac{n}{i}=n\sum^n_{i=n/2}\frac{1}{n}\frac{1}{i/n}\approx n\int^1_{1/2}\frac{1}{x}dx=n\ln 2.
\ee
\end{solution}


\begin{problem}
Mary tosses two coins and John tosses one coin. What is the probability that Mary gets strictly more heads than John? Answer the same question if Mary tosses three coins and John tosses two. Make a conjecture for the same probability when Mary tosses $n+1$ coins and John tosses $n$. Can you prove your conjecture?
\end{problem}

\begin{solution}[\bf Solution.]
If $M_H$ and $J_H$ denote the number of heads for Mary and John, respectively, then in the first case \be \pro(M_H>J_H)= \pro(M_H=2) + \pro\brb{M_H=1,J_H=0} = 1/4+1/4=1/2. \ee

Similarly you may see that the probability for 3 and 2 coins is 1/2, so the conjecture is that for the general case it is the same. With the obvious notation, $M_H+M_T=n+1$ and $J_H+J_T=n$ so that the event
\be
\bra{M_H>J_H}^c=\bra{n+1-M_T\leq n-J_T}= \bra{1+J_T\leq M_T}=\bra{J_T<M_T}
\ee

That is, exactly one of the events 'Mary get more heads' and 'Mary gets more tails' must occur, but these must have equal probabilities.
\end{solution}


\begin{problem}
Assume there are $m$ ordered bins and $n$ balls marked by 1 to $n$. Then what's the probability that there are least k bins containing at least 2 balls?
\end{problem}

\begin{solution}[\bf Solution.]
First, pick $k$ bins among $m$ and $2k$ balls among $n$. Then allocate these balls into the selected bins which gives the possibilities
\be
C_m^k C_n^{2k} C_{2k}^{2}C_{2k-2}^{2} \dots C_{4}^2 C^2_2 = \frac{m!}{k!(m-k)!} \frac{n!}{(n-2k!)2k!} \frac{(2k)!}{2!(2k-2)!} \frac{(2k-2)!}{2!(2k-4)!} \dots \frac{4!}{2!2!} = \frac{m!n!}{k!(m-k)!(n-2k)!2^k}.
\ee

Then we can pick all possible cases for the rest balls with $m^{n-2k}$. Then the probability is
\be
\frac{C_m^k C_n^{2k} C_{2k}^{2}C_{2k-2}^{2} \dots C_{4}^2 C^2_2 m^{n-2k}}{m^n} = \frac{m!n!}{k!(m-k)!(n-2k)!2^k m^{2k}}.
\ee
\end{solution}


\subsection{Theoretical problems}

\begin{problem}
Let $X$ be a positive random variable taking only finitely many values in $\R^+$. Show that
\be
\mathbb{E}\left(\frac 1X\right)\geq \frac{1}{\mathbb{E}X}
\ee
and that the inequality is strict unless $\mathbb{P}(X=\mathbb{E}X)=1$.
\end{problem}

\begin{solution}[\bf Solution.]
Since $f(x) = 1/x$ is convex on $(0,\infty)$, we can use Jensen's inequality (Theorem \ref{thm:jensen_inequality_expectation}) to get the $\mathbb{E}\left(\frac 1X\right)\geq \frac{1}{\mathbb{E}X}$.

Let $x_i,i=1,\dots,n$ be $\Omega$ of $X$, each with probability $p_i$. Thus, $\mathbb{E}\left(\frac 1X\right)= \frac{1}{\mathbb{E}X}$ gives us
\begin{equation}
\sum^n_{i=1}\frac{p_i}{x_i} = \frac{1}{\sum^n_{j=1}p_jx_j}\ \ra \ \sum^n_{i=1}\frac{p_i\prod_{k\ne i}x_k}{\prod^n_{k=1}x_k} = \frac{1}{\sum^n_{j=1}p_jx_j}\ \Rightarrow \ \prod^n_{k=1}x_k = \sum^n_{j=1}p_jx_j\left(\sum^n_{i=1}p_i\prod_{k\ne i}x_k\right)
\end{equation}
After the expansion, we have
\begin{eqnarray}
\prod^n_{k=1}x_k & = & \sum^n_{j=1}p_j^2 \prod^n_{i=1}x_i + \sum^n_{j=1}\sum_{i\ne j}p_ip_j \frac{x_j}{x_i}\prod^n_{k=1}x_k \nonumber\\
\left(\sum^n_{j=1}p_j\right)^2 & = & \sum^n_{j=1}p_j^2 + \sum^n_{j=1}\sum_{i> j}p_ip_j \left(\frac{x_j}{x_i}+ \frac{x_i}{x_j}\right) \nonumber\\
0 & = & \sum^n_{j=1}\sum_{i> j}p_ip_j \left(\frac{x_j}{x_i}+ \frac{x_i}{x_j}-2\right) = \sum^n_{j=1}\sum_{i> j}p_ip_j\frac{(x_i-x_j)^2}{x_ix_j}
\end{eqnarray}
Since $x_i>0$, we have $x_i=x_j,\forall i,j$ which implies that $\mathbb{P}(X=\mathbb{E}X)=1$.
\end{solution}


\begin{problem}
How large a random sample should be taken from a distribution in order for the probability to be at least 0.99 that the sample mean will be within two standard deviations of the mean of the distribution? Use Chebyshev's inequality to determine a sample size that will be sufficient, whatever the distribution.
\end{problem}

\begin{solution}[\bf Solution.]
Chebyshev's inequality (Theorem \ref{thm:chebyshev_inequality_probability}) says
\begin{equation}
\pro\left(|X-\mu|\geq k\sigma \right)\leq \frac{1}{k^2}
\end{equation}
Now $\hat{\mu} = \frac{1}{N}\sum^N_{i=1}X_i\sim \mathcal{D}(\mu,\frac{\sigma^2}{N})$ if we let $X_i$ be i.i.d. Then we need
\begin{equation}
\pro\left(|\hat{\mu}-\mu|\geq 2\sigma \right) = \mathbb{P}\left(|\hat{\mu}-\mu|\geq 2\sqrt{N}\frac{\sigma}{\sqrt{N}} \right)\leq \frac{1}{4N}=1-0.99=0.01
\end{equation}
Thus a sufficient sample space has size 25.
\end{solution}

\begin{problem}
Let $X$ and $Y$ be integrable random variables and suppose that
\be
\E\brb{X|Y} = Y\quad \text{ and }\quad \E\brb{Y |X} = X\ \text{a.s.}
\ee

Show that $X = Y$ a.s.
\end{problem}%Hint: Consider quantities like $\E\brb{(X - Y )\ind(X > c, Y \leq c)} + \E\brb{(X - Y )\ind(X \leq c, Y \leq c)}$.

\begin{solution}[\bf Solution.]
Fix $c \in \R$. As $\bra{Y \leq  c} \in \sigma(Y)$ and $\E\brb{X|Y} = Y$ a.s., $\E\brb{X\ind_{\bra{Y\leq c}}} = \E\brb{Y\ind_{\bra{Y\leq c}}}$, and hence
\be
0 = \E\brb{(X -Y)\ind_{\bra{Y\leq c}}} = \E\brb{(X -Y)\ind_{\bra{X>c,Y\leq c}}}+\E\brb{(X -Y)\ind_{\bra{X\leq c,Y\leq c}}}
\ee

By symmetry, we also have that
\be
0 = \E\brb{(Y -X)\ind_{\bra{X\leq c}}} = \E\brb{(Y -X)\ind_{\bra{X\leq c,Y>c}}}+\E\brb{(Y -X)\ind_{\bra{X\leq c,Y\leq c}}}
\ee

By adding these equations, we then see that
\be
0 = \E\brb{(X -Y)\ind_{\bra{X>c,Y\leq c}}} + \E\brb{(Y -X)\ind_{\bra{X\leq c,Y>c}}}
\ee

Each of these terms is itself nonnegative and therefore, from the above, equal to 0. It follows that $\pro\brb{X > c,Y \leq  c} = 0$ and $\pro\brb{X \leq  c,Y > c} = 0$. As $c \in \R$ was arbitrary, we conclude from these equations that $X =Y$ a.s..
\end{solution}

\begin{problem}[coprime numbers]
Given two randomly chosen integers $a$ and $b$, it is reasonable to ask how likely it is that $a$ and $b$ are coprime.
\end{problem}

\begin{solution}[\bf Solution.]
In this determination, it is convenient to use the characterization that $a$ and $b$ are coprime if and only if no prime number divides both of them (see Fundamental theorem of arithmetic\footnote{need link}).

Informally, the probability that any number is divisible by a prime (or in fact any integer) $p$ is $1/p$; for example, every 7th integer is divisible by 7. Hence the probability that two numbers are both divisible by this prime is $1/p^2$, and the probability that at least one of them is not is $1-1/p^2$. For distinct primes, these divisibility events are mutually independent. For example, in the case of two events, a number is divisible by $p$ and $q$ if and only if it is divisible by $pq$; the latter event has probability $1/pq$. (Independence does not hold for numbers in general, but holds for prime numbers.) Thus the probability that two numbers are coprime is given by a product over all primes,
\be
\prod_p^{\infty} \left(1-\frac{1}{p^2}\right) = \left( \prod_p^{\infty} \frac{1}{1-p^{-2}} \right)^{-1} = \frac{1}{\zeta(2)} = \frac{6}{\pi^2} \approx 0.607927102 \approx 61\%.
\ee

Here $\zeta$ refers to the Riemann zeta function (see Theorem \ref{thm:euler_product_riemann_zeta_function}), the identity relating the product over primes to $\zeta(2)$ is an example of an Euler product, and the evaluation of $\zeta(2)$ as $\pi^2/6$ is the Basel problem, solved by Leonhard Euler in 1735. In general, the probability of $k$ randomly chosen integers being coprime is $1/\zeta(k)$.

The notion of a `randomly chosen integer' in the preceding paragraphs is not rigorous. One rigorous formalization is the notion of natural density: choose the integers $a$ and $b$ randomly between 1 and an integer $N$. Then, for each upper bound $N$, there is a probability $P_N$ that two randomly chosen numbers are coprime. This will never be exactly $6/\pi^2$, but in the limit as $N \to \infty$, the probability $P_N$ approaches $6/\pi^2$. This theorem was proved by Ernesto Ces��ro in 1881. For a more rigorous proof, see Hardy-Wright\cite{Hardy_Wright_2008}.
\end{solution}


\begin{problem}
Let $f$ be a bounded continuous function on $(0,\infty)$, having Laplace transform
\be
\hat{f}(\lm) = \int^\infty_0 e^{-\lm x}f(x)dx,\quad \lm \in (0,\infty).
\ee
Let $(X_n : n \in \N)$ be a sequence of independent exponential random variables, of parameter $\lm$. Show that $\hat{f}$ has derivatives of all orders on $(0,\infty)$ and that, for all $n \in \N$, for some $C(\lm, n)\neq 0$ independent of $f$, we have
\be
(d/d\lm)^{n-1} \hat{f}(\lm) = C(\lm, n)\E (f(S_n))
\ee
where $S_n = X_1 + \dots + X_n$. Deduce that if $\hat{f} \equiv 0$ then also $f \equiv 0$.
\end{problem}

\begin{solution}[\bf Solution.]
Let $\epsilon > 0$. Then
\ben
\item [(i)] $x \mapsto e^{-\lambda x} f(x)$ is integrable on for all $\lm \in (\epsilon, \infty)$.
\item [(ii)] $\lambda \mapsto e^{-\lambda x} f(x)$ is infinitely differentiable on $(\epsilon, \infty)$ for all $x$.
\item [(iii)]  $x \mapsto \frac{\partial}{\partial \lambda^n}(e^{-\lambda x} f(x)) = (-x)^n e^{-\lambda x} f(x)$ is integrable (on $\R_+$) for all $\lambda \in (\epsilon, \infty)$, as $\abs{ (-x)^n e^{-\lambda x} f(x)} \leq x^n e^{-\epsilon x}$, and $x^n e^{-\epsilon x}$ is integrable.
\een

These conditions allow for differentiation under the integral sign any number of times (by Theorem \ref{thm:differentiation_under_integral_sign}), for all $\lambda \in (\epsilon, \infty)$. But $\epsilon$ was arbitrary so the same is true for $\lambda \in (0, \infty)$. Thus
\[
 \frac{d^{n-1}\hat{f}(\lambda)}{d \lambda^{n-1}}  = \int_0 ^\infty (-x)^{n-1} e^{-\lambda x} f(x) dx
\]
Now $S_n \sim \Gamma(n, \lambda)$ which has pdf $e^{-\lambda x} \lambda^n x^{n-1} / (n-1)!$. So by Proposition \ref{pro:density_function_probability},
\be
 \frac{d^{n-1} \hat{f}(\lambda) }{d \lambda^{n-1}} = \frac{(n-1)!(-1)^{n-1}}{\lambda^{n}} \int_0 ^\infty \frac{\lm^nx^{n-1} }{(n-1)!}  e^{-\lambda x} f(x) dx = \frac{(n-1)!(-1)^{n-1}}{\lambda^{n}}\E (f(S_n)).
\ee

Moreover, since $S_n / n \sim \Gamma(n, n\lambda)$, we have
\[
 \frac{d\hat{f}}{d \lambda^{n-1}} \bigg|_{n\lambda} = \int_0 ^\infty (-x)^{n-1} e^{-n\lambda x} f(x) dx=\frac{(n-1)!(-1)^{n-1}}{(n\lambda)^{n}}\E (f(S_n / n))
\]
Thus if $\hat{f} \equiv 0$ then $\E(f(S_n / n)) =0$ for all $n$. But by the strong law of large numbers (Theorem \ref{thm:slln}), since $\E(X) = \frac 1{\lm}$,
\be
 1=\pro(S_n / n \to 1 / \lambda) = \pro(f(S_n / n) \to f(1 / \lambda)),
\ee
where in the last step we have used the fact that $f$ is continuous. As $f$ is bounded, by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability}) we have
\[
 0 = \lim_{n \to \infty} \E(f(S_n / n)) = \E \left( \lim_{n \to \infty} f(S_n / n) \right) = f(1 / \lambda).
\]
This is true for all $\lambda$, so $f \equiv 0$.
\end{solution}


\begin{problem}
For each $n \in \N$, there is a unique probability measure $\mu_n$ on the unit sphere $S^{n-1} = \{x \in \R^n : |x| = 1\}$ such that $\mu_n(A) = \mu_n(UA)$ for all Borel sets $A$ and all orthogonal $n\times n$ matrices $U$. Fix $k \in \N$ and, for $n \geq k$, let $\gamma_n$ denote the probability
measure on $\R^k$ which is the law of $\sqrt{n}(x_1, \dots, x_k)$ under $\mu_n$. Show
\ben
\item [(i)] if $X \sim \sN(0, I_n)$ then $X/|X| \sim \mu_n$,
\item [(ii)] if $(X_n : n \in \N)$ is a sequence of independent $\sN(0, 1)$ random variables and if $R_n = (X^2_1 + \dots + X^2_n)^{\frac 12}$ then $R_n/
\sqrt{n}\to 1$ a.s.,
\item [(iii)] for all bounded continuous functions $f$ on $\R^k$, $\gamma_n(f)\to\gamma(f)$, where $\gamma$ is the standard Gaussian distribution on $\R^k$.
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(i)] Since $U^{-1}X \sim \sN(0,I_n)$ ($U$ is orthogonal) and $\abs{X} = \abs{U^{-1}X}$,
  \[
\mu_{X/\abs{X}}(A)=\pro(X/\abs{X} \in A) = \pro(U^{-1}X/\abs{U^{-1}X} \in A) = \pro(U^{-1}X /\abs{X} \in A) = \pro(X/\abs{X} \in UA) = \mu_{X/\abs{X}}(UA) .
  \]
\item [(ii)] By of the strong law of large number,
\be
\frac{X_1^2+ \dots + X_n^2}{n} \to 1,\ \text{ a.s.}.
\ee

Also, with the fact that $\sqrt{\cdotp}$ is continuous, we have $R_n/\sqrt{n}\to 1$ a.s..

\item [(iii)] Let $g: x \mapsto \sqrt{n}(x_1, \ldots, x_k)$. Then
\be
\gamma_n(f) = \mu_n (f \circ g) = \mu_{X/\abs{X}} (f \circ g) = \E(f \circ g(X/\abs{X})) = \E \left(f\left(\frac{X_1}{\left(\frac{R_n}{\sqrt{n}}\right)}, \ldots, \frac{X_k}{\left( \frac{R_n}{\sqrt{n}}\right)} \right) \right).
\ee

By the continuity of $f$ and (ii),
\be
f\left(\frac{X_1}{\left(\frac{R_n}{\sqrt{n}}\right)}, \ldots, \frac{X_k}{\left( \frac{R_n}{\sqrt{n}}\right)} \right) \to f\left(\frac{X_1}{\left(\lim_{n\to \infty}\frac{R_n}{\sqrt{n}}\right)}, \ldots, \frac{X_k}{\left(\lim_{n\to \infty}\frac{R_n}{\sqrt{n}}\right)} \right) = f(X_1,\dots,X_k)\ \text{ a.s.}.
\ee

Then by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability}),
\be
\lim \gamma_n(f) = \lim_{n \to \infty} \E \left(f\left(\frac{X_1}{\left(\frac{R_n}{\sqrt{n}}\right)}, \ldots, \frac{X_k}{\left( \frac{R_n}{\sqrt{n}}\right)} \right) \right)  = \E(f(X_1), \ldots, f(X_k)) = \gamma(f).%& = & \E \left(\lim_{n \to \infty} f\left(\frac{X_1}{\left(\frac{R_n}{\sqrt{n}}\right)}, \ldots, \frac{X_k}{\left( \frac{R_n}{\sqrt{n}}\right)} \right) \right)
\ee
\een
\end{solution}

%\subsection{Conditional probability and conditional expectation}
%\subsection{Law of large number and central limit theorem}

\begin{problem}
Let $(X_k)$ be a sequence of independent, identically distributed random variables, each with mean $\mu$ and variance $\sigma^2$. Show that
\begin{equation}
\sum^n_{k=1}\left(X_k-\ol{X}\right)^2=\sum^n_{k=1}\left(X_k-\mu\right)^2 - n\left(\ol{X}-\mu\right)^2
\end{equation}
where $\ol{X}=n^{-1}\sum^n_{k=1}X_k$. Prove that, if $\mathbb{E}\left[(X_1-\mu)^4\right]<\infty$, then for every $\epsilon>0$
\begin{equation}
\mathbb{P}\left\{\left|\frac 1n\sum^n_{k=1}\left(X_k-\ol{X}\right)^2-\sigma^2\right|>\epsilon\right\}\to 0\ \text{as } n\to \infty.
\end{equation}
\end{problem}

\begin{solution}[\bf Solution.]
We have $\sum^n_{k=1}(X_k-\ol{X})^2=\sum^n_{k=1}X_k^2+n\ol{X}^2-2\ol{X}\sum^n_{k=1}X_k = \sum^n_{k=1}X_k^2 - n\ol{X}^2$, thus,
\beast
\sum^n_{k=1}\left(X_k-\mu\right)^2 - n\left(\ol{X}-\mu\right)^2 = \sum^n_{k=1}X_k^2+n\mu^2-2\mu\sum^n_{k=1}X_k - n\ol{X}^2 + 2n\mu\ol{X}-n\mu^2 = \sum^n_{k=1}X_k^2 - n\ol{X}^2 = \sum^n_{k=1}(X_k-\ol{X})^2
\eeast

Hence we have
\beast
\pro\brb{\left|\frac 1n\sum^n_{k=1}\left(X_k-\ol{X}\right)^2-\sigma^2\right|>\ve} & = & \pro\brb{\abs{\frac 1n\sum^n_{k=1}\left(X_k-\mu\right)^2-\sigma^2- (\ol{X}-\mu)^2} > \ve} \\
& \leq & \pro\brb{\abs{\frac 1n\sum^n_{k=1}\left(X_k-\mu\right)^2-\sigma^2}+ (\ol{X}-\mu)^2>\ve} \\
& = & \pro\brb{\left|\frac 1n\sum^n_{k=1}\left(X_k-\mu\right)^2-\sigma^2\right|+ (\ol{X}-\mu)^2>\ve, (\ol{X}-\mu)^2>\frac{\ve}{2}} \\
& & \qquad + \pro\brb{\left|\frac 1n\sum^n_{k=1}\left(X_k-\mu\right)^2-\sigma^2\right|+ (\ol{X}-\mu)^2>\ve, (\ol{X}-\mu)^2\leq\frac{\ve}{2}}\\ %\nonumber\\
& \leq & \pro\brb{(\ol{X}-\mu)^2>\frac{\ve}{2}} + \pro\brb{\left|\frac 1n\sum^n_{k=1}\left(X_k-\mu\right)^2-\sigma^2\right| > \frac{\ve}{2}}
\eeast

Applying Chebyshev's inequality (Theorem \ref{thm:chebyshev_inequality_probability}), we have
\begin{equation}
\mathbb{P}\left\{\left|\frac 1n\sum^n_{k=1}\left(X_k-\ol{X}\right)^2-\sigma^2\right|>\epsilon\right\}  \leq \frac{\mathbb{E}\left[(\ol{X}-\mu)^2\right]}{\epsilon/2} + \frac{2\mathbb{E}\left[(X_1-\mu)^4\right]}{n\epsilon} = \frac{2\var X_1}{n\epsilon} + \frac{2\mathbb{E}\left[(X_1-\mu)^4\right]}{n\epsilon} \to 0
\end{equation}
\end{solution}

\subsection{Random variables}

\begin{problem}
Find approximately the probability that the number of 6's in 12000 rolls of a fair die is between 1900 and 2150.
\end{problem}

\begin{solution}[\bf Solution.]
Let $N$ be the number of 6's, then
\be
N\sim \bd(12000,1/6)\approx \mathcal{N}(2000,10000/6)\quad (\sN(np,np(1-p)))
\ee
by central limit theorem (Theorem \ref{thm:central_limit}). So that if $Z\sim \mathcal{N}(0,1)$,
\beast
\mathbb{P}(1900 \leq N \leq 2150) = \mathbb{P}\left(-\frac{100}{\sqrt{10000/6}} \leq \frac{N-2000}{\sqrt{10000/6}} \leq \frac{150}{\sqrt{10000/6}}\right) = \mathbb{P}\left(-2.45 \leq Z \leq 3.67 \right) = \Phi(3.67) - \Phi(-2.45)
\eeast
which is $0.9999 - (1-0.9929) \approx 0.9928$.
\end{solution}

\begin{problem}
Find the number $c$ such that the probability is about $\frac 12$ that in 1000 tosses of a fair coin the number of heads lie between 490 and $c$.
\end{problem}

\begin{solution}[\bf Solution.]
The number $N\sim \bd(1000,1/2)\approx \sN(500,250)$, so that if $Z\sim \sN(0,1)$,
\be
\mathbb{P}(490 \leq N \leq c) = \mathbb{P}\left(-\frac{10}{\sqrt{250}} \leq \frac{N-500}{\sqrt{250}} \leq \frac{c-500}{\sqrt{250}}\right) \approx \mathbb{P}\left(-0.6324 \leq Z \leq \frac{c-500}{\sqrt{250}} \right),
\ee
which gives is $\Phi((c-500)/\sqrt{250})-0.2643 = 0.5$, so that $c=500+0.72\sqrt{250}\approx 511$.
\end{solution}

\begin{problem}
Let $X, Y$ be two independent Bernoulli random variables with parameter $p \in (0, 1)$. Let $Z = \ind_{\bra{X + Y = 0}}$. Compute $\E\brb{X|Z}$ and $\E\brb{Y |Z}$.
\end{problem}

\begin{solution}[\bf Solution.]
By symmetry $\E\brb{X|Z} = \E\brb{Y|Z}$ a.s., so it is enough to find just one. Note that, as $\bra{Z = 0}$ and $\bra{Z = 1}$ are non-null events that partition our sample space, it follows that (by Definition
\ref{def:conditional_expectation_elementary_sigma_algebra} and Definition \ref{def:conditional_expectation_elementary_set}) \beast
\E\brb{X|Z} & = & \E\brb{X|Z=0}\ind_{\bra{Z=0}}+ \E\brb{X|Z=1}\ind_{\bra{Z=1}}\\
& = & \frac{\E\brb{X\ind_{\bra{Z=0}}}}{\pro\brb{Z = 0}} \ind_{\bra{Z=0}} + \frac{\E\brb{X\ind_{\bra{Z=1}}}}{\pro\brb{Z = 1}} \ind_{\bra{Z=1}} \eeast

Recall that $Z = \ind_{\bra{X+Y=0}}$. It is immediate then that the second term above is equal to 0. For the first term, observe that \be \pro\brb{Z = 0} = 1 - \pro\brb{Z = 1} = 1 - \pro\brb{X = 0,Y = 0} = 1 - (1- p)^2 = p(2-
p) \ee and, additionally, that \be \E\brb{X\ind_{\bra{Z=0}}} = \pro\brb{Z = 0,X = 1} = \pro\brb{X = 1} = p. \ee

Putting the above into the earlier work, it follows that \be
\E\brb{X|Z} = \frac{\ind_{\bra{Z=0}}}{2- p}.%\text{ a.s.}
\ee
\end{solution}

\begin{problem}
If $X$, $Y$ and $Z$ are independent random variables each uniformly distributed on $(0,1)$, show that $(XY)^Z$ is also uniformly distributed on $(0,1)$.
\end{problem}

\begin{solution}[\bf Solution.]
It is sufficient to show that $-\log \left((XY)^Z\right)=Z(-\log X - \log Y)$ has the exponential distribution with parameter 1. Equivalently, show that if random variables $X\sim U(0,1)$ and $Y\sim \Gamma(2,1)$ are independent then $XY\sim\mathcal{E}(1)$. The joint pdf of $X$ and $Y$ is
\begin{equation}
f(x,y)=\left\{\begin{array}{ll}
ye^{-y} \quad\quad &  0<x<1, y>0 \\
0 & \text{otherwise}
\end{array}\right.
\end{equation}

Now put $U=XY,V=X/Y$, so that $X=\sqrt{UV}$ and $Y=\sqrt{U/V}$, and the Jacobian of the transformation is
\begin{equation}
\left|\begin{array}{cc}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}\\
 & \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array}\right|=
\left|\begin{array}{cc}
\frac 12\sqrt{\frac{v}{u}} & \frac 12\sqrt{\frac{u}{v}} \\
& \\
\frac 1{2\sqrt{uv}} & -\frac{\sqrt{u}}{2v^{3/2}}
\end{array}\right| = -\frac{1}{4v} - \frac{1}{4v} =  -\frac{1}{2v}
\end{equation}
and the joint density of $U$ and $V$ is
\begin{equation}
g(u,v)=\frac{\sqrt{u}}{2v^{3/2}}e^{-\sqrt{u/v}}, \quad 0<uv<1
\end{equation}

Integrating out $v$ we get that the marginal density of $U$ is
\begin{equation}
\int^{1/u}_0\frac{\sqrt{u}}{2v^{3/2}}e^{-\sqrt{u/v}}dv = \int^\infty_u e^{-z}dz = e^{-u},
\end{equation}
after making the substitution $z=\sqrt{u/v}$.
\end{solution}

\begin{problem}
$X_1,X_2,\dots$ from a sequence of independent random variables, each uniformly distributed on $(0,1)$. Let $N=\min\bra{n:X_1+X_2+\cdots+X_n\geq 1}$. Calculate $\pro(N\geq k)$ for each $k\geq 1$ and show that $\E N=e$.
\end{problem}

\begin{solution}[\bf Solution.]
For $x\leq 1$ let $a_n(x)=\mathbb{P}(X_1+\cdots+X_n<x)$. Show by induction on $n$ that $a_n(x)=x^n/n!$. It is true for $n=1$ and then
\beast
a_{n+1}(x) & = & \int^1_0\mathbb{P}(X_1+\cdots+X_{n+1}<x|X_{n+1}=y)dy = \int^x_0a_n(x-y)dy = \int^x_0((x-y)^n/n!)dy = x^{n+1}/(n+1)!
\eeast

We then have $\mathbb{P}(N\geq k) = a_{k-1}(1) = 1/(k-1)!$ and
\be
\mathbb{E}N=\sum^\infty_{k=1}\mathbb{P}(N\geq k) = \sum^\infty_{k=1}\frac{1}{(k-1)!}=e.
\ee
\end{solution}


\begin{problem}[Cambridge TRIPOS PART IA/2009/2/II/11F]
Let $X$ and $Y$ be two independent uniformly distributed random variables on $[0,1]$. Prove that $\E X^k = \frac 1{k + 1}$ and $\E (XY)^k = \frac 1{(k + 1)^2}$, and find $\E (1 - XY)^k$, where $k$ is a non-negative integer.

Let $(X_1, Y_1), \dots, (X_n, Y_n)$ be $n$ independent random points of the unit square $S = \{(x, y) : 0 \leq x, y \leq 1\}$. We say that $(X_i, Y_i)$ is a \emph{maximal external point} if, for each $j = 1,\dots, n$,
either $X_j \leq X_i$ or $Y_j \leq Y_i$. (For example, in the figure below there are three maximal external points.) Determine the expected number of maximal external points.
\begin{center}
\psset{yunit=2cm,xunit=2cm}
\begin{pspicture}(0,-0.1)(3,3)
\psline(0,0)(3,0)(3,3)(0,3)(0,0)
\pstGeonode[PointSymbol=*,PointName=none,dotscale=1](2.6,1.2){A1}(2,2.3){A2}(2,0.4){A3}(1.4,1.5){A4}(1.2 ,2){A5}(1.2,1){A6}(1.2,0.35){A7}(1,2.2){A8} (0.9,2.6){A9}(0.8,0.8){A10}(0.7,1.9){A11}(0.6,1.5){A12}(0.5,1.2){A_13} (0.4,1.85){A14}
\end{pspicture}
\end{center}
\end{problem}

%\centertexdraw{
%
%\def\bdot {\fcir f:0 r:0.03 }
%
%\drawdim in \arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
%
%\linewd 0.01 \setgray 0
%
%\move (2.6 1.2) \bdot \move (2 2.3) \bdot \move (2 0.4) \bdot \move (1.4 1.5) \bdot \move (1.2 2) \bdot \move (1.2 1) \bdot \move (1.2 0.35) \bdot \move (1 2.2) \bdot \move (0.9 2.6) \bdot \move (0.8 0.8) \bdot \move (0.7
%1.9) \bdot \move (0.6 1.5) \bdot \move (0.5 1.2) \bdot \move (0.4 1.85) \bdot
%
%\move (0 0) \lvec(3 0) \lvec(3 3) \lvec(0 3) \lvec(0 0) \move (0 3.1) }


\begin{solution}[\bf Solution.]
First we have that
\be \E X^k = \int^1_0 u^k du = \frac 1{k+1}. \ee

Further, \be \E(XY)^k = \E X^k \E Y^k = \frac 1{(k+1)^2} \quad \text{by independence}. \ee

Next, by following the hint, $\forall k=0,1,\dots$: \be \E\brb{1-XY}^k = \sum_{0\leq i\leq k} \binom{k}{i}(-1)^i \E\brb{XY}^i = \sum_{0\leq i\leq k}\binom{k}{i} \frac{(-1)^i}{(i+1)^2}. \ee

%Additional marks may be given for calculations leading to the answer
\beast \sum_{0\leq i\leq k}\binom{k}{i} \frac{(-1)^i}{(i+1)^2} & = & \frac 1{k+1} \int^1_0 \sum_{0\leq i\leq k}(-1)^i \binom{k+1}{i+1} x^i dx = \frac 1{k+1} \int^1_0 \frac 1x \sum_{1\leq i\leq k+1}(-1)^{i-1} \binom{k+1}{i} x^i dx\\
& = & \frac 1{k+1} \int^1_0 \frac {1-(1-x)^{k+1}}x dx  =  \frac 1{k+1} \int^1_0 \frac{1-x^{k+1}}{1-x} dx\\
& = & \frac 1{k+1} \int^1_0 (1+x+\dots + x^k) dx = \frac 1{k+1} \sum_{1\leq i\leq k+1} \frac 1i. \eeast

Let $A$ denote the number of maximal external points. By linearity and symmetry: \be A = \sum_{1\leq i\leq n}\ind_{\bra{\text{Point $i$ is a maximal external point}}},\qquad \E A = n\pro(\text{Point 1 is a maximal external
point}). \ee

Next, by total law of probability,
\beast
n\pro\brb{\text{Point 1 is a maximal external point}} & = & n\E \brb{\E \brb{\ind(\text{Point 1 is a maximal external point})}|X_1,Y_1}\\
& = & n\E \brb{\pro\brb{\text{no point with $X_i > X_1$ and $Y_i > Y_1$}}|X_1,Y_1}\\
& = & n\E \brb{ 1- (1-X_1)(1-Y_1)}^{n-1} = n\E \brb{1-X_1Y_1}^{n-1}. \eeast

Now, by using the above result, with $k = n-1$, the expected number of maximal external is

\be  n\E \brb{1-X_1Y_1}^{n-1} = n\sum_{0\leq i\leq n-1} \binom{n-1}{i} \frac{(-1)^i}{(i+1)^2} = \sum_{1\leq i \leq n} \frac 1i. \ee
\end{solution}

\begin{problem}
Derive the distribution of the sum of $n$ independent random variables each having the Poisson distribution with parameter 1. Use the central limit theorem to prove that
\be
e^{-n}\left(1+\frac{n}{1!}+\frac{n^2}{2!}+\cdots+ \frac{n^n}{n!}\right)\to\frac 12 \quad \text{as }n\to \infty
\ee
\end{problem}

\begin{solution}[\bf Solution.]
For random variables $\{N_i\}$ which are i.i.d. with the Poisson distribution mean 1, let $M_n=N_1+N_2+\cdots+N_n$, then $M_n$ has the Poisson distribution with mean $n$ and by the central limit theorem (Theorem \ref{thm:central_limit}), for any $x$,
\begin{equation}
\lim_{n\to\infty}\mathbb{P}\left(\frac{M_n- n}{\sqrt{n}}\leq x\right) = \Phi(x),
\end{equation}
where $\Phi(x)$ is the standard normal distribution function. Take $x=0$, then $\Phi(x)=\frac 12$, and the left-hand side is
\begin{equation}
\lim_{n\to\infty}\mathbb{P}\left( M_n \leq n \right) = \lim_{n\to\infty}e^{-n}\left(1+\frac{n}{1!}+\frac{n^2}{2!}+\cdots+ \frac{n^n}{n!}\right).
\end{equation}
\end{solution}


\begin{problem}
How large a random sample should be taken from a normal distribution in order for the probability to be at least 0.99 that the sample mean will be within one standard deviation of the mean of the distribution? [Hint: $\Phi(2.58)=0.995$.]
\end{problem}

\begin{solution}[\bf Solution.]
$X\sim\mathcal{N}(\mu,\sigma^2)\ \Rightarrow \ \ol{X}\sim \mathcal{N}(\mu,\sigma^2/n)$. Thus
\begin{equation}
\mathbb{P}\brb{\abs{\ol{X}-\mu}<\sigma} \geq 0.99 \ \ra \ \mathbb{P}\left(-\sqrt{n}<\frac{\ol{X}-\mu}{\sigma/\sqrt{n}}<\sqrt{n}\right) \geq 0.99 = \Phi(2.58) - \Phi(-2.58)
\end{equation}
So we get $\sqrt{n}\geq 2.58\ \Rightarrow \ n\geq 7$.
\end{solution}


%\subsection{Distribution function and inverse distribution function}




\subsection{Practical problems}


\begin{problem}[The P\'olya urn model for contagion]
We start with an urn which contains one white ball and one black ball. At each second we choose a ball at random from the urn and replace it together with one more ball of the same colour. Is there
a tendency to have a large fraction of balls of the same colour in the long run? [Computer simulations are interesting.] Calculate the probability that when $n$ balls are in the urn, $i$ of them are
white\footnote{Also see Problem \ref{exe:polya_urn}}.
\end{problem}

\begin{solution}[\bf Solution.]
Let $M_{n,k}$ is the event of having $k$ white balls among $n$. Obviously, we have $\mathbb{P}(M_{2,1})=1$, $\mathbb{P}(M_{n,0})=0$ and $\mathbb{P}(M_{n,n})=0$. Taking $n\geq 3$ and $0<k<n$, we have
\beast
\pro(M_{n,k}) & = & \mathbb{P}(\text{having chosen a white ball from }M_{n-1,k-1}) + \mathbb{P}(\text{having chosen a black ball from }M_{n-1,k})\nonumber\\
& = & \frac{k-1}{n-1}\mathbb{P}(M_{n-1,k-1}) + \frac{n-1-k}{n-1}\mathbb{P}(M_{n-1,k}). \eeast

Thus, it is not difficult to derive that $\mathbb{P}(M_{n,k})=\frac{1}{n-1}$.
\end{solution}

\begin{problem}[fixed point problem (see Grinstead-Snell\cite{Grinstead_1997})]
We want to put $n$ number into $n$ positions. If number $i$ is put in the $i$th position, we call it a fixed point. What the probability that there is no fixed point.
\end{problem}

\begin{solution}[\bf Solution.]
If we can define $M_n$ the number of no-fixed point situations if we want to put $n$ number into $n$ positions. To avoid fixed point, we can simply assume that the number in the position 1 is $i\neq 1$.
If number 1 is not in the position $i$, then there are $(n-1)M_{n-1}$ possibilities. If number 1 is in the position $i$, then there are $(n-1)M_{n-2}$ possibilities. Thus,
\be
M_n = (n-1)\brb{M_{n-1} + M_{n-2}}.
\ee

We know that there $n!$ possibilities for putting the numbers. Thus, we have define $p_n$ the probability that there is no fixed point. Then
\be
p_n = \frac{M_n}{n!} = \frac {n-1}{n!}\brb{M_{n-1} + M_{n-2}} = \frac{n-1}n \cdot p_{n-1} + \frac 1n \cdot p_{n-2}.\quad (*)
\ee

$p_n$ can also be given by (for position 1, if it takes 1, then there is a fixed point)
\be
p_n = \frac 1n \cdot 0 + \frac{n-1}n \cdot \brb{p_{n-1} + \frac 1{n-1}\cdot p_{n-2}}.
\ee

From ($*$), we have
\beast
p_n - p_{n-1} & = & \frac {-1}n \brb{p_{n-1} - p_{n-2}}\\
& \vdots & \\
p_3 - p_2 & = & \frac {-1}3 \brb{p_2 - p_1}
\eeast

Thus, with $p_1 = 0$ and $p_2 = \frac 12$,
\be
p_n - p_{n-1} = \frac{2\cdot (-1)^{n-2}}{n!} \cdot \brb{p_2 - p_1} = \frac{2\cdot (-1)^{n-2}}{n!} \cdot \frac 12 = \frac{(-1)^{n}}{n!}
\ee

Thus,
\be
p_n = \sum^{n}_{i=2} \frac{(-1)^{i}}{i!} \to e^{-1}\quad \text{as $n\to \infty$}.
\ee
Thus, we have
\be
p_5 = \frac 12 - \frac 16 + \frac 1{24} - \frac 1{120} = \frac {11}{30}.
\ee
\end{solution}

\begin{problem}
John chooses a sequence, such as $HHH$, and then Mary chooses a sequence, perhaps $THH$. A fair coin is tossed until on or other sequence occurs, when the coin is awarded to the person whose sequence has been observed. Advise Mary on which sequence she should choose for each (or at least some) of John's eight possible choices.
\end{problem}

\begin{solution}[\bf Solution.]
Let $p$ be the probability that Mary's choice happens before John's choice when the first toss is $H$ and $q$ be the probability that Mary's choice happens before John's choice when the first toss is $T$. Thus, in the case that John chooses $HHH$ and Mary chooses $THH$, we have
\begin{equation}
\left\{\begin{array}{ccl}
p & = & \underbrace{\frac 14 0}_{HHH} + \underbrace{\frac 14 q}_{HHT} + \underbrace{\frac 18 1}_{HTHH} + \underbrace{\frac 18 q}_{HTHT} + \underbrace{\frac 14 q}_{HTT} = \frac 18 + \frac 58 q \\
q & = & \underbrace{\frac 14 1}_{THH} + \underbrace{\frac 14 q}_{THT} + \underbrace{\frac 18 1}_{TTHH} + \underbrace{\frac 18 q}_{TTHT} + \underbrace{\frac 14 q}_{TTT} = \frac 38 + \frac 58 q
\end{array}\right.\quad\Rightarrow\quad
\left\{\begin{array}{ccl}
p & = & \frac 34 \\
q & = & 1
\end{array}\right.
\end{equation}
Thus, the probability that John wins the game is $\frac 12(p+q)=\frac 78$. Similarly, we have the probabilities of winning for Mary in the following table,

\begin{tabular}{c|cccccccc}
\backslashbox{John}{Mary} & \quad $HHH$ \quad &\quad $HHT$ \quad & \quad $HTH$ \quad & \quad $HTT$ \quad & \quad $THH$ \quad &\quad $THT$ \quad &\quad $TTH$ \quad & \quad $TTT$ \quad \\ \hline
$HHH$ & - & 1/2 & 3/5 & 3/5 & {\bf 7/8} & 7/12 & 7/10 & 1/2 \\
$HHT$ & 1/2 & - & 1/3 & 1/3 & {\bf 3/4} & 3/8 & 1/2 & 3/10 \\
$HTH$ & 2/5 & {\bf 2/3}  & - & 1/2 & 1/2 & 1/2 & 5/8 & 5/12 \\
$HTT$ & 2/5 & {\bf 2/3}  & 1/2  & - & 1/2 & 1/2 & 1/4 & 1/8  \\
$THH$ & 1/8 & 1/4  & 1/2  & 1/2 & - & 1/2 & {\bf 2/3} & 2/5 \\
$THT$ & 5/12 & 5/8 & 1/2 & 1/2  & 1/2  & - & {\bf 2/3} & 2/5 \\
$TTH$ & 3/10  & 1/2 & 3/8 & {\bf 3/4} & 1/3 & 1/3  & - & 1/2 \\
$TTT$ & 1/2 & 7/10  & 7/12 & {\bf 7/8} & 3/5 & 3/5 & 1/2  & -
\end{tabular}

where the bold probabilities maximize the value for given sequence chosen by John.
\end{solution}



\begin{problem}
Alice and Bob agree to meet in the Copper Kettle after their Saturday lectures. They arrive at times that are indepedent and uniformly distributed between 12.00 and 1.00pm. Each is prepared to wait 10 minutes before leaving. Find the probability they meet.
\end{problem}

\begin{solution}[\bf Solution.]
If the times they arrive are $X$ and $Y$ uniformly distributed on $[0,1]$, then they meet if

\begin{center}
\psset{yunit=4cm,xunit=4cm}
\begin{pspicture}(-0.5,-0.2)(1.2,1.2)%[showgrid](-3,-1.5)(3,4)
\psaxes[dx =1,dy=1,labels=none,ticks=none]{->}(0,0)(-0.2,-0.2)(1.2,1.2)%Dx=0.25,Dy=0.25
\psset{algebraic}
\psplot{0}{0.5}{x+0.167}
\psplot{0.5}{1}{x-0.167}
%\rput[lb](-1.5,0.75){$\max\{X,Y\}-\min\{X,Y\}<\frac 12$}
\rput[lb](-0.7,0.5){$|X-Y|\leq \frac 16$}
\rput[lb](1.1,0.05){$x$}
\rput[lb](0.05,1.1){$y$}
\rput[lb](-0.1,0.167){$\frac 16$}
\rput[lb](0.167,-0.15){$\frac 16$}
\rput[lb](-0.1,1){1}
\rput[lb](1,-0.1){1}
\rput[lb](1.1,0.7){$x-y=\frac 16$}
\rput[lb](0.6,1.1){$x-y=-\frac 16$}

\pstGeonode[PointSymbol=none,PointName=none](0,0.167){A}(1,0.833){AA}(0.167,0){B}(0.833,1){BB}(0,1){C}(1,1){D}(1,0){E}

\pstLineAB[linestyle=dashed]{A}{AA}
\pstLineAB[linestyle=dashed]{B}{BB}

\pstLineAB[linestyle=dashed]{C}{D}
\pstLineAB[linestyle=dashed]{D}{E}

\pscustom[fillstyle=solid,fillcolor=red!30]{%linestyle=dashed,
\psline(0,0.167)(0.833,1)(1,1)(1,0.833)(0.167,0)(0,0)(0,0.167)%\psGauss[sigma=1,mue=0,linecolor=black,linewidth=1pt]{-4}{-1.96}
}%
\end{pspicture}
\end{center}

so the probability is the area of the shaded region which is $1 - \brb{\frac {5}{6}}^2 = \frac{11}{36}$.
\end{solution}


%\centertexdraw{
%    \drawdim in
%
%    \arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
%
%    \linewd 0.01 \setgray 0
%
%    \move (-0.2 0) \avec(1.8 0)
%    \move (0 -0.2) \avec(0 1.8)
%
%    \move (0 1.5) \lvec(1.5 1.5) \lvec(1.5 0)
%
%    \move (1.25 1.5)
%    \lvec(1.5 1.5) \lvec(1.5 1.25) \lvec(0.25 0) \lvec(0 0) \lvec(0 0.25) \lvec(1.25 1.5)
%    \lfill f:0.8
%
%    \htext (-1 0.8){$|X-Y|\leq \frac 16$}
%    \htext (1.8 0.1){$x$}
%    \htext (0.1 1.7){$y$}
%    \htext (-0.1 0.2){$\frac 16$}
%    \htext (0.2 -0.2){$\frac 16$}
%
%    \htext (-0.1 1.5){1}
%    \htext (1.5 -0.15){1}
%
%    \htext (1.6 0.5){$x-y=\frac 16$}
%    \htext (0.5 1.6){$x-y=-\frac 16$}
%
%    \move (1.55 0.55) \avec(0.8 0.55)
%    \move (0.75 1.55) \avec(0.75 1)
%}
%


\begin{problem}
A unit stick is broken in two places:
\ben
\item [(i)] Two breaks are idependently and uniformly distributed along its length. What is the probability that the three pieces will make a triangle?
\item [(ii)] The first break is at a $\ud{0}{1}$ point, and the second break is made by breaking the longer part at uniformly chosen position (the two breaks are otherwise independent). What is the probability that the three pieces will make a triangle?
\een
\end{problem}

\begin{solution}[\bf Solution.]
\ben
\item [(i)] Let $X$ and $Y$ be independent $\ud{0}{1}$ representing the positions of the breaks along the stick. The pieces are of length $A=\min\{X,Y\}$, $B=\max\{X,Y\}-\min\{X,Y\}$ and $C=1-\max\{X,Y\}$ and they form a triangle if $A+B>C$, $B+C>A$ and $C+A>B$. These translate to

\begin{center}
\psset{yunit=4cm,xunit=4cm}
\begin{pspicture}(-1,-0.2)(1.2,1.2)%[showgrid](-3,-1.5)(3,4)
\psaxes[dx =1,dy=1,labels=none,ticks=none]{->}(0,0)(-0.2,-0.2)(1.2,1.2)%Dx=0.25,Dy=0.25
\psset{algebraic}
\psplot{0}{0.5}{x+0.5}
\psplot{0.5}{1}{x-0.5}
\rput[lb](-1.5,0.75){$\max\{X,Y\}-\min\{X,Y\}<\frac 12$}
\rput[lb](-1.5,0.5){$\max\{X,Y\}>\frac 12>\min\{X,Y\}$}
\rput[lb](1.1,0.05){$x$}
\rput[lb](0.05,1.1){$y$}
\rput[lb](-0.1,0.5){$\frac 12$}
\rput[lb](0.5,-0.15){$\frac 12$}
\rput[lb](-0.1,1){1}
\rput[lb](1,-0.1){1}
\rput[lb](1.1,0.3){$x-y=\frac 12$}
\rput[lb](0.4,1.1){$x-y=-\frac 12$}

\pstGeonode[PointSymbol=none,PointName=none](0,0.5){A}(1,0.5){AA}(0.5,0){B}(0.5,1){BB}(0,1){C}(1,1){D}(1,0){E}

\pstLineAB[linestyle=dashed]{A}{AA}
\pstLineAB[linestyle=dashed]{B}{BB}

\pstLineAB[linestyle=dashed]{C}{D}
\pstLineAB[linestyle=dashed]{D}{E}


%\rput[lb](-7,2){$\theta(x) = \left\{\ba{ll}x+a & x\in [0,1-a)\\ x+a-1 \quad\quad & x\in [1-a ,1)\ea\right.$}

\pscustom[fillstyle=solid,fillcolor=red!30]{%linestyle=dashed,
\psline(0,0.5)(0.5,1)(0.5,0)(1,0.5)(0,0.5)%\psGauss[sigma=1,mue=0,linecolor=black,linewidth=1pt]{-4}{-1.96}
}%
\end{pspicture}
\end{center}

%\centertexdraw{
%    \drawdim in
%
%    \arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
%
%    \linewd 0.01 \setgray 0
%
%    \move (-0.2 0) \avec(1.8 0)
%    \move (0 -0.2) \avec(0 1.8)
%
%    \move (0 1.5) \lvec(1.5 1.5) \lvec(1.5 0)
%
%    \move (0.75 0)
%    \lvec(1.5 0.75) \lvec(0 0.75) \lvec(0.75 1.5) \lvec(0.75 0)
%    \lfill f:0.8
%
%    \htext (-2 1){$\max\{X,Y\}-\min\{X,Y\}<\frac 12$}
%    \htext (-2 0.5){$\max\{X,Y\}>\frac 12>\min\{X,Y\}$}
%    \htext (1.8 0.1){$x$}
%    \htext (0.1 1.7){$y$}
%    \htext (-0.1 0.7){$\frac 12$}
%    \htext (0.7 -0.2){$\frac 12$}
%
%    \htext (-0.1 1.5){1}
%    \htext (1.5 -0.15){1}
%
%    \htext (1.6 0.5){$x-y=\frac 12$}
%    \htext (0.4 1.6){$x-y=-\frac 12$}
%
%    \move (1.55 0.55) \avec(1.3 0.55)
%    \move (0.6 1.55) \avec(0.6 1.35)
%}

and the probability is the area of the shaded region which is $\frac 14$.

\item [(ii)] If we choose the first break and then choose the second break is made by breaking the longer part, $X$ and $Y$ are no longer independent. we can only consider the case that $X$ is in
    $[\frac 12,1]$ due the symmetric property. Thus, $Y$ is choosen in $[0,X]$ uniformly with density function $f(Y|X)=\frac 1X$. Also, to satisfy the triangle conditions, we have

\be
\left\{\begin{array}{rcl}
y + (x-y) & > & 1-x \\
y + (1-x) & > & x-y \\
(1-x) + (x-y) & > & y
\end{array}\right.
\ \Rightarrow \
\left\{\begin{array}{rcl}
x & > & \frac 12\\
y & > & x-\frac 12 \\
y & < & \frac 12
\end{array}\right.
\ee

Thus, we have the probability
\begin{equation}
2\int^1_{\frac 12}\int^{\frac 12}_{x-\frac 12}\frac 1x dy dx = 2\int^1_{\frac 12}\frac{1-x}{x} dx = 2\left(\left.\ln x -x\right|^1_{\frac 12}\right) = 2\ln 2-1
\end{equation}
\een
\end{solution}

\begin{problem}[candies in a jar (Zhou\cite{Zhou_2008}.$P_{79}$)]
You are taking out candies one by one from  a jar that has 10 red candies, 20 blue candies, and 30 green candies in it. What is the probability that there are at least 1 blue candy and 1 green candy
left in the jar when you have taken out all the red candies?
\end{problem}

\begin{solution}[\bf Solution.]
Let $T_r$, $T_b$ and $T_g$ be the number that the last red, blue, and green candies are taken out respectively. To have at least 1 blue candy and 1 green candy left when all the red candies are
taken out, we need to have $T_r < T_b$, $T_r < T_g$. In other words, we want to derive $\pro(T_r< T_b, T_r< T_g)$. Thus, \be \pro(T_r< T_b, T_r< T_g) = \pro(T_r<T_b< T_g) + \pro(T_r< T_g < T_b). \ee

$T_r<T_b< T_g$ means that the last candy is green ($T = 60$). Since each of the 60 candies are equally likely to be the last candy among them 30 are green ones, we have $\pro(T_g = 60) = \frac{30}{60}$. Conditioned on $T_g = 60$, we need $\pro(T_r < T_b|T_g = 60)$. Among the 30 red and blue candies, each candy is again equally likely to be the last candy and there are 20 blue candies, so
\be
\pro(T_r<T_b|T_g = 60) = \frac{20}{30} \ \ra \ \pro(T_r< T_b< T_g) =\pro(T_r<T_b|T_g = 60)\pro(T_g = 60) =  \frac{20}{30}\frac{30}{60}.
\ee

Similarly, we have
\be
\pro(T_r< T_g< T_b) =  \frac{30}{40}\frac{20}{60}.
\ee

Therefore,
\be
\pro(T_r< T_b, T_r< T_g) = \frac{20}{30}\frac{30}{60} + \frac{30}{40}\frac{20}{60} = \frac{7}{12}.
\ee
\end{solution}



\begin{problem}[Zhou\cite{Zhou_2008}.$P_{102}$]
$N$ ants are randomly put on a 1-foot string (independent uniform distribution for each ant between 0 and 1). Each ant randomly moves toward one end of the string (equal probability to the left or right) at constant speed 1 foot/minute until it falls off at one end of the string. Also assume that the size of the ant is infinitely small. When two ants collide head-on, they both immediately change directions and keep on moving at 1 foot/min. What is the expected time for all ants to fall off the string?
\end{problem}

\begin{solution}[\bf Solution.]%The ants are randomly located; each ant can go either direction; an ant needs to change direction when it meets another ant.
When two ants collide head-on, both immediately change directions. What does it mean? The following diagram illustrates the key point:
\be
\text{before collision: }\overset{A}{\rightarrow}\overset{B}{\leftarrow},\quad \text{after collision: }\overset{A}{\leftarrow}\overset{B}{\rightarrow},\quad \text{switch label: }\overset{B}{\leftarrow}\overset{A}{\rightarrow}.
\ee

When an ant $A$ collides with another ant $B$, both switch direction. But if we exchange the ants' labels, it's like that the collision never happens, $A$ continuous to move to the right and $B$ moves to the left. Since the labels are random assigned anyway, collisions make no difference to the result. So we can assume that when two ants meet, each just keeps on going in its original direction. What about the random direction that each ant chooses? Once the collision is removed, we can use symmetry to argue that it makes no difference which direction that an ant goes either.

That means if an ant is put at the $x$-th foot, the expected value for it to fall off is just $x$ min. If it goes in the other direction, simply set $x$ to $1-x$. So the orginial problem is equivalent to the expected value of the maximum of $N$ i.i.d. random variables with uniform distribution between 0 and 1. By Proposition \ref{pro:uniform_max_min}, the answer is $\frac{N}{N+1}$ min.
\end{solution}

\begin{problem}[Zhou\cite{Zhou_2008}.$P_{79}$]
Suppose 3 points are independently uniformly distributed on a circle. What's the probability that there exists a diameter such that all these 3 points are on the same side of that diameter? What about more general case for $n$ points?
\end{problem}

\begin{solution}[\bf Solution.]
Suppose that these $n$ points have the angles $\theta_1,\dots,\theta_n$ with $\theta_i \sim \sU(0,2\pi)$ and let $\wh{\theta}_n$ be size of the smallest interval containing all the points. Thus, for any $x\in [0,\pi]$,
\be
\pro\brb{\wh{\theta}_n \leq x} = \pro\brb{\wh{\theta}_{n-1} \leq x, 0\leq \theta_n \leq \wh{\theta}_{n-1}} + 2\pro\brb{\theta_n \leq x,\theta_n \geq \wh{\theta}_{n-1}}
\ee
since $\theta_n$ could be at either side of the interval $[0,\wh{\theta}_{n-1}]$. Thus, ($f_{\wh{\theta}_{n-1}}$ is the density function of $\wh{\theta}_{n-1}$)
\be
\pro\brb{\wh{\theta}_n \leq x} = \int^x_0 f_{\wh{\theta}_{n-1}}(y)\int^{y}_0 f_{\theta_n}(z)dz dy + 2 \int^x_0 f_{\theta_n}(z) \int^z_0 f_{\wh{\theta}_{n-1}}(y)dy dz.
\ee

We know that $\wh{\theta}_{n-1}$ is independent of $\theta_n$ and thus $f_{\theta_n} = \frac 1{2\pi}$. Thus,
\be
\pro\brb{\wh{\theta}_n \leq x} = \frac 1{2\pi}\int^x_0 f_{\wh{\theta}_{n-1}}(y)y dy + \frac 1{\pi} \int^x_0 \pro\brb{\wh{\theta}_{n-1} \leq z} dz
\ee

If $n=3$, then $f_{\wh{\theta}_2} = \frac 1{\pi}$. Thus,
\be
\pro\brb{\wh{\theta}_2 \leq x} = \frac x{\pi},\qquad \pro\brb{\wh{\theta}_3 \leq x} = \frac 1{2\pi}\int^x_0 \frac 1{\pi} y dy + \frac 1{\pi} \int^x_0 \frac z{\pi} dz = \frac {x^2}{4\pi^2} + \frac{x^2}{2\pi^2} = \frac{3x^2}{4\pi^2}.
\ee

Thus, by induction we assume $\pro\brb{\wh{\theta}_n \leq x} = n\brb{\frac{x}{2\pi}}^{n-1}$ with $f_{\wh{\theta}_n}(x) = n(n-1)\frac{x^{n-2}}{(2\pi)^{n-1}}$. Then
\beast
\pro\brb{\wh{\theta}_{n+1} \leq x} & = & \frac 1{2\pi}\int^x_0 f_{\wh{\theta}_{n}}(y)y dy + \frac 1{\pi} \int^x_0 \pro\brb{\wh{\theta}_{n} \leq z} dz =  \frac 1{2\pi}\int^x_0 \frac{n(n-1){y}^{n-1}}{(2\pi)^{n-1}} dy + \frac 1{\pi} \int^x_0 n\brb{\frac{z}{2\pi}}^{n-1} dz\\
& = & \frac 1{(2\pi)^{n}}(n-1)x^n + \frac 2{(2\pi)^n} x^n = (n+1)\frac{x^n}{(2\pi)^n}.
\eeast

Thus, let $x = \pi$, we have the probability is $\frac{n}{2^{n-1}}$.
\end{solution}

\begin{solution}[\bf Alternative Solution.]
Let's start at one point and clockwise label the point as $1,2,\dots,n$. The probability that all the remaining $N-1$ points from 2 to $n$ are in the clockwise semicircle starting at point 1 (That is, if point 1 is at 12:00, point 2 to $n$) are all between 12:00 and 6:00) is $1/2^{n-1}$. Similarly the probability that a clockwise semicircle starting at any point $i$, where $i\in \bra{2,\dots,n}$ contains all the other $n-1$ points is also $1/2^{n-1}$. Thus, the total probability is $n/2^{n-1}$.
\end{solution}
%\qcutline

%{\large \textcolor{red}{unsorted below}}

\begin{problem}
Let a particle in the plane $\R^2$ executes random jumps at discrete times $t=1,2,3,\dots$. At each step, the particle jumps with distance of length 1. The angle of any new jump is uniformly distributed in $[0,2\pi]$. Then what's the probability that the particle gets back to the unit disk of the plane after three steps? More generally, what's the probability that the particle gets back to the unit disk of the plane after $n$ steps?\footnote{More general case needed.}
\end{problem}


%2 steps with two angles $\theta_1,\theta_2$.
%\be
%\tan \theta = \frac{\sin \theta_1 + \sin \theta_2}{\cos \theta_1 + \cos \theta_2}
%\ee

\begin{solution}[\bf Solution.]
Consider two steps first. Since these two steps can have any direction, we have $\theta_1,\theta_2\in [0,2\pi]$. Then by law of cosines (Theorem \ref{thm:law_of_cosines}), $r$, the distance from origin to the particle after two steps can be expressed by
\be
r^2 = 1^2 + 1^2 - 2\cdot 1 \cdot 1 \cos(\pi - (\theta_2 - \theta_1)) = 2 + 2\cos(\theta_1 - \theta_2) = 4\cos^2 \brb{\frac{\theta_1 - \theta_2}2}
\ee

\be
r = 2\abs{\cos\brb{\frac{\theta_1 - \theta_2}2}}, \qquad \theta = \frac 12(\theta_1 + \theta_2) + k \pi,\quad k\in \Z.
\ee

\begin{center}\psset{yunit=1.2cm,xunit=1.2cm}  %%%%%%% this is wrong as t is theta
\begin{pspicture}[algebraic](-0.5,-0.5)(5,4.5)
\psaxes[ticks=none,labels=none]{->}(0,0)(-0.5,-0.5)(5,4.5)
%\pscustom[fillstyle=solid,fillcolor=blue!20]{%fillstyle=crosshatch
%\psplot{1}{5}{5-sqrt(16-(x-5)^2)}
%\psplot{5}{3}{sqrt(4-(x-3)^2)+3}}

%\psplot[linecolor=red,linewidth=2pt]{1}{5}{5-sqrt(16-(x-5)^2)}
%\psplot[linecolor=green,linewidth=2pt]{5}{3}{sqrt(4-(x-3)^2)+3}

\psline[linecolor=red,linewidth=2pt,arrowscale=1.5]{->}(0,0)(3,1)
\psline[linecolor=orange,linewidth=2pt,arrowscale=1.5]{->}(3,1)(4,4)
\psline[linecolor=blue,linewidth=2pt,arrowscale=1.5]{->}(0,0)(4,4)
%\psaxes[axesstyle=polar,xAxisLabel=some,subticks=2,tickcolor=red,tickwidth=1pt,subtickcolor=green]{->}(0,0)(-2.5,2.5)(2.5,2.5)%axesstyle=frame,dx=2,dy=2
%\rput[cb](3,3){\large $\sD$}%{\textcolor{blue}
%\rput[cb](3,1){\large \textcolor{red}{$C_1$}}%{\textcolor{blue}
%\rput[cb](4.5,5){\large \textcolor{green}{$C_3$}}%{\textcolor{blue}
%\rput[cb](5.4,2){\large \textcolor{orange}{$C_2$}}%{\textcolor{blue}
%\rput[cb](2.5,5.3){\large \textcolor{cyan}{$C_4$}}%{\textcolor{blue}

\psline[linecolor=black,linestyle=dashed](3,1)(5,1)

\psline[linecolor=orange,linestyle=dashed](0,0)(1,3)
\psline[linecolor=red,linestyle=dashed](1,3)(4,4)

\pstGeonode[PointSymbol=none,PointName=none](5,1){A}(3,1){B}(4,4){C}(0,0){O}(5,0){E}(1,3){F}

\pstMarkAngle[MarkAngleRadius=1,LabelSep=1.1]{E}{O}{B}{$\theta_1$}
\pstMarkAngle[LabelSep=0.6]{A}{B}{C}{$\theta_2$}

\pstMarkAngle[MarkAngleRadius=0.8,LabelSep=1.1,LabelAngleOffset=-10]{B}{O}{F}{$\theta_2-\theta_1$}

%\psline[linecolor=black,linestyle=dashed](3,1.5)(5,1.5)

%\rput[cb](5,5.3){$C_5$}%{\textcolor{blue}
\end{pspicture}
\end{center}

Let $\theta_1 - \theta_2 \in [-\pi,\pi]$. So we can have
\be
r = 2 \cos\brb{\frac{\theta_1 - \theta_2}2}, \qquad \theta = \frac 12(\theta_1 + \theta_2)+ k \pi,\quad k\in \Z.
\ee

Thus,
\be
\theta_1 - \theta_2 = \pm 2 \arccos\frac r2, \qquad \theta_1 + \theta_2 = 2\brb{\theta + k \pi},\quad k\in \Z.
\ee

Without loss of generality, we have
\be
\begin{cases}
\theta_1 = -\arccos\frac r2 + \theta + k\pi \\
\theta_2 = \arccos\frac r2 + \theta + k\pi
\end{cases}
\ee

Therefore,
\be
\fp{\theta_1}{r} = \frac 1{\sqrt{4-r^2}},\quad \fp{\theta_2}{r} = -\frac 1{\sqrt{4-r^2}},\quad \fp{\theta_1}{\theta} = 1,\quad \fp{\theta_2}{\theta} = 1.
\ee

Thus, the Jacobian matrix is
\be
\bevm
\fp{\theta_1}{r} & \fp{\theta_1}{\theta} \\
\fp{\theta_2}{r} & \fp{\theta_2}{\theta}
\eevm = \frac 1{\sqrt{4-r^2}} - \brb{-\frac 1{\sqrt{4-r^2}}} = \frac 2{\sqrt{4-r^2}}.
\ee

Then the joint density function is
\be
\frac 1{(2\pi)^2} d\theta_1 d\theta_2 = \frac 1{(2\pi)^2}\frac 2{\sqrt{4-r^2}} dr d\theta
\ee
so the density function of $r$ is
\be
\frac 1{2\pi}\frac 2{\sqrt{4-r^2}} = \frac 1{\pi \sqrt{4-r^2}}.
\ee

Adding the case $\theta_1 - \theta_2 \in [-2\pi,-\pi]\cup [\pi,2\pi]$, we have the total density function is doubled. Thus,
\be
f(r) = \frac{2}{\pi\sqrt{4 - r^2}},\qquad r\in [0,2]
\ee

Then
\beast
\pro\brb{r\leq 2} & = & \int^2_0 \frac 2{\pi \sqrt{4-r^2}}dr = \int^1_0 \frac 4{2\pi \sqrt{1-r^2}}dr = \frac 2{\pi} \int^1_0 \frac 1{\sqrt{1-r^2}}dr =  \left.\frac 2{\pi} \arcsin r \right|^1_0 = \frac 2{\pi} \frac {\pi}2 = 1
\eeast
and
\beast
\pro\brb{r\leq 1} & = & \int^1_0 \frac 2{\pi \sqrt{4-r^2}}dr = \int^{1/2}_0 \frac 4{2\pi \sqrt{1-r^2}}dr = \frac 2{\pi} \int^{1/2}_0 \frac 1{\sqrt{1-r^2}}dr =  \left.\frac 2{\pi} \arcsin r \right|^{1/2}_0 = \frac 2{\pi} \frac {\pi}6 = \frac 13.
\eeast

Alternatively, we may assume the initial position is $(0,\rho)$. Then we want to get the density function of the distance from new point to origin. Thus,
\be
x = \cos \theta,\qquad y = \rho + \sin\theta,\qquad r^2 = x^2 + y^2 = 1 + \rho^2 + 2\rho\sin\theta
\ee

Then
\be
\frac{r^2 - \rho^2 - 1}{2\rho} = \sin\theta  \ \ra\  \theta = \arcsin\brb{\frac{r^2 - \rho^2 - 1}{2\rho}},\quad \theta\in [-\pi/2,\pi/2]
\ee

Thus,
\be
\frac{d\theta_3}{dr} = \frac 1{\sqrt{1 -\brb{\frac{r^2 - \rho^2 - 1}{2\rho}}^2}} \frac{r}{\rho} = \frac{2r}{\sqrt{4\rho^2 - \brb{r^2 - \rho^2 - 1}^2}} = \frac{2r}{\sqrt{\brb{r^2 - (\rho-1)^2}\brb{(\rho+1)^2-r^2}}}
\ee

Then given $\rho$, the density function of $r$ is
\be
f(r) = \frac {2r}{2\pi \sqrt{\brb{r^2 - (\rho-1)^2}\brb{(\rho+1)^2-r^2}}} = \frac {r}{\pi \sqrt{\brb{r^2 - (\rho-1)^2}\brb{(\rho+1)^2-r^2}}}  .
\ee

Also, by the symmetry of $\theta$, we have that
\be
f(r) = \frac {2r}{\pi \sqrt{\brb{r^2 - (\rho-1)^2}\brb{(\rho+1)^2-r^2}}}.
\ee

In particular, if $\rho=1$ (after 1 step), we have that
\be
f(r) = \frac {2r}{\pi \sqrt{r^2 \brb{2^2-r^2}}} = \frac{2}{\pi\sqrt{4 - r^2}}
\ee
which is consistent with the previous result.

Now consider the case after 3 steps. Assume the position is $(0,\rho)$ after two steps. Then length of arc such that the new position is within unit disc is $2\arccos\frac{\rho}2$ and thus its probability is length of blue arc divided by $2\pi$. Therefore, the probability is $\frac 1{\pi}\arccos\frac{\rho}2$.

%\begin{figure}[t]
\begin{center}\psset{yunit=1cm,xunit=1cm}
\begin{pspicture}(-3,-2)(3,5)
\pstGeonode[PosAngle=-135,PointSymbol=none,PointName=none](0,0){O}(0,1){A}(0,-1){B}(1.9365,0.5){C}(-1.9365,0.5){D}(0,0.5){E}
\psset{PointSymbol=none,PointName=none}
\psaxes[labels=none]{->}(0,0)(-3,-2)(3,5)

\pstGeonode(1.871,0.707){A1}(-1.871,0.707){B1}(0,-2.828){AB1}(1.871,-0.707){A2}(-1.871,-0.707){B2}(0,2.828){AB2}(-2.828,0){CC}(2.828,0){DD}(0,2){N}(1.414,1.414){NE}

\pstCircleOA{A}{B}
\pstArcOAB[linecolor=blue,linewidth=2pt]{A}{D}{C}%\pstArcOAB{AB2}{CC}{DD}
\pstArcOAB[linecolor=red,linewidth=2pt]{A}{C}{D}

\pstCircleOA[linecolor=black,linestyle=dashed,fillstyle=solid,fillcolor=green,opacity=0.3]{O}{C}
\psline[linecolor=black](O)(C)
%\psline[linecolor=black,linestyle=dashed](A)(D)
\psline[linecolor=black,linestyle=dashed,arrowscale=2]{->}(A)(C)
\psline[linecolor=black,linestyle=dashed](E)(C)


\rput[cb](-0.5,1.1){$(0,\rho)$}%{\textcolor{blue}
\end{pspicture}
\begin{pspicture}(-3,-2)(3,5)
\pstGeonode[PosAngle=-135,PointSymbol=none,PointName=none](0,0){O}(0,2.5){A}(0,0.5){B}(1.56125,1.25){C}(-1.56125,1.25){D}(0,1.25){E}
\psset{PointSymbol=none,PointName=none}
\psaxes[labels=none]{->}(0,0)(-3,-2)(3,5)

\pstGeonode(1.871,0.707){A1}(-1.871,0.707){B1}(0,-2.828){AB1}(1.871,-0.707){A2}(-1.871,-0.707){B2}(0,2.828){AB2}(-2.828,0){CC}(2.828,0){DD}(0,2){N}(1.414,1.414){NE}

\pstCircleOA{A}{B}
\pstArcOAB[linecolor=blue,linewidth=2pt]{A}{D}{C}%\pstArcOAB{AB2}{CC}{DD}
\pstArcOAB[linecolor=red,linewidth=2pt]{A}{C}{D}

\pstCircleOA[linecolor=black,linestyle=dashed,fillstyle=solid,fillcolor=green,opacity=0.3]{O}{C}
\psline[linecolor=black](O)(C)
%\psline[linecolor=black,linestyle=dashed](A)(D)
\psline[linecolor=black,linestyle=dashed,arrowscale=2]{->}(A)(C)
\psline[linecolor=black,linestyle=dashed](E)(C)


\rput[cb](-0.5,2.5){$(0,\rho)$}%{\textcolor{blue}
\end{pspicture}
\end{center}%\end{figure}

Then combining the density function after two steps, we have
\beast
\pro\brb{r\leq 1} & = & \int^2_0 \frac {\arccos\frac{\rho}2}{\pi} \frac{2}{\pi \sqrt{4-\rho^2}}d\rho = \frac{2}{\pi^2}\int^1_0 \arccos \rho \frac{1}{\sqrt{1-\rho^2}}d\rho = \frac{2}{\pi^2}\int^1_0 \arccos \rho d\arcsin \rho \\
& = & \frac{2}{\pi^2}\int^1_0 \brb{\frac{\pi}2 - \arcsin \rho } d\arcsin \rho = \left.\frac 2{\pi^2}\brb{\frac{\pi}2 \arcsin\rho- \frac 12\arcsin^2\rho}\right|^1_0 \\
& = & \frac 2{\pi^2}\brb{\frac{\pi}2 \frac{\pi}2- \frac 12\brb{\frac{\pi}2}^2} = 2\brb{\frac 14 - \frac 18} = \frac 14.
\eeast
\end{solution}
