\chapter{Markov Chains}

\section{Discrete-time Markov Chains}

\subsection{Definition and basic facts}




\section{Continuous-time Markov Chains}

\section{Problems}

\subsection{Discrete-time Markov chains}

%\begin{problem}
%A flower is passed on among 4 persons randomly. How many ways are there for the flower to return to the same person after 6 steps?
%\end{problem}

%\begin{solution}[\bf Solution.]
%We can have passing-on matrix


%Therefore, there are 183 ways.\footnote{need to do the general case.}
%\end{solution}


\begin{problem}
Assume you have 50 noodles in a bowl and each noodle has two ends. You can put any two ends (of the same noodle or not) together with equal probability until there does not exist any available end.
What's the expected number of circles in the bowl then?
\end{problem}

\begin{solution}[\bf Solution.] Assume it is $X_n$ for $n$ noodles. Then
\be
X_n = \frac 1{2n-1} \bb{1+ X_{n-1}} + \frac {2n-2}{2n-1} X_{n-1} = \frac 1{2n-1} + X_{n-1} \ \ra \ X_n = \sum^{n}_{k=1} \frac 1{2k-1}.
\ee

In this case, $n= 50$, $X_n = 2.9378$.
\end{solution}

\begin{problem}
You are in a cave and there are 2 paths leading out of it. One of the paths leads to an exit, the other leads back into the cave. Each path take 1 hour to traverse. You choose one of the paths randomly every time (you have no memory of your previous choices). What is the expected amount of time before you exit the cave?

Now there are 3 paths instead of 2. One path leads to an exit, one leads back to the cave, the third path is very slippery and also leads back to the cave. Whenever you take the third path, you fall down and injure yourself, and as a result your traversal time on each path is doubled. What is the expected amount of time before you exit?

If the traversal time is not doubled when you take the third path, what's the change of traversal time at most will guarantee your escape?
\end{problem}

\begin{solution}[\bf Solution.]
\footnote{Solution needed.}
\end{solution}

\begin{problem}
The following procedure may be considered as an old-fashioned way of choosing a bride (or a bridegroom). You have a possibility to see $m$ individuals, one upon the time, appearing in a random
order, and you want to select the best individual. You have to decide if an individual is the best one or not when you see him/her, by comparing his/her qualities with individual you have already
seen; if you reject, there is no way to go back. Clearly, you must decide at those times when you see an individual whose qualities exceed those of anyone seen before. In fact, it turns out that the
optimal way to proceed is to reject $k$ initial individuals and then select the first one whose qualities exceed those of all seen before. [Such an individual may not appear, but this probability is
small.]

Sep up a Markov chain $X_1, \dots, X_m$, with states $1,2,\dots,m$, describing the times you see the best individual seen so far. You want to maximise the chance to choose the best individual among
all $m$ candidates. Find the value of $k$ as a function of $m$ and check that for large $m$, $k\approx m/e$.
\end{problem}

\begin{solution}[\bf Solution.]
The transition matrix of the Markov chain $X_1, \dots, X_m$ is
\be
\bepm
\frac 12 & \frac 12 & 0 & \dots & 0 & 0\\
0 & \frac 23 & \frac 13 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \dots & \frac{m-1}{m} & \frac 1m\\
0 & 0 & 0 & \dots & 0 & 1
\eepm
\ee

Let $h_k = \pro(\text{choose the best of $m$}|\text{reject $k$ initial individuals})$. Thus, we have $h_m = 0$ and
\be
h_k = \frac 1{k+1}\pro(\text{best of $m$}|\text{best of first $k+1$}) + \frac k{k+1}h_{k+1}, \quad k = 0,1,\dots,m-1.
\ee

We know that
\be
\pro(\text{best of $m$}|\text{best of first $k+1$}) = \frac {\pro(\text{best of $m$})}{\pro(\text{best of first $k+1$})} = \frac {1/m}{1/(k+1)} = \frac{k+1}{m}
\ee

Thus, we have
\be
h_k = \frac 1m + \frac k{k+1}h_{k+1}, \quad k = 0,1,\dots,m-1 \ \ra \ \frac 1{k} h_k = \frac 1{mk} + \frac 1{k+1}h_{k+1}
\ee

Let $u_k = \frac 1k h_k$, we have
\be
u_k = \frac 1m\sum^{m-1}_{i=k}\frac 1i + u_m = \frac 1m\sum^{m-1}_{i=k}\frac 1i \ \ra \ h_k = \frac km\sum^{m-1}_{i=k}\frac 1i.
\ee

To maximise the change to choose the best of $m$ we choose $k$ s.t.
\be
\left\{\ba{l}
h_k \geq h_{k+1}\\
\\
h_k \geq h_{k-1} \ea\right.\ \ra \ \left\{\ba{l}
\frac km\sum^{m-1}_{i=k}\frac 1i \geq \frac {k+1}m\sum^{m-1}_{i=k+1}\frac 1i\\
\\
\frac km\sum^{m-1}_{i=k}\frac 1i \geq \frac {k-1}m\sum^{m-1}_{i=k-1}\frac 1i \ea\right. \ \ra \ \left\{\ba{l}
\sum^{m-1}_{i=k+1}\frac 1i \leq 1\\
\\
\sum^{m-1}_{i=k}\frac 1i \geq 1 \ea\right.
\ee

Thus, $k$ should be the greatest integer satisfying
\be
\sum^{m-1}_{i=k}\frac 1i \geq 1.
\ee

Hence, for large $m$ and $k$, the left above is about $\ln (m/k)$\footnote{theorem needed.}, so
\be
\ln (m/k) \approx 1 \ \ra \ k \approx m/e.
\ee
\end{solution}



\begin{problem}[Cambridge TRIPOS PART IB/2008/1/II/19H] The village green is ringed by a fence with $N$ fence posts, labelled $0,1,\dots,N-1$. The village idiot is given a pot of paint and a brush, and started at post 0 with instructions, 1 paint all the posts. He paints post 0, and then chooses one of the two nearest neighbours, 1 or $N-1$, with equal probability, moving to the chosen post and painting it. After painting a post, he chooses with equal probability one of the two nearest neighbours, moves there and paints it (regardless of whether it is already painted). Find the distribution of the last post unpainted. What is the expected time to paint all the fence?
\end{problem}

\begin{solution}[\bf Solution.] If $k$ is the last painted fence, there are 2 ways to reach it, from $k-1$ and $k+1$. We can find these 2 probabilities separately.


\begin{center}
\psset{yunit=1cm,xunit=1cm}
\begin{pspicture}(0,-2)(0,2)
\newpsstyle{Cblue}{fillstyle=solid,fillcolor=blue!30}
\newpsstyle{CC}{linecolor=black!10!cyan!20}
\newpsstyle{Cred}{fillstyle=solid,fillcolor=red!30}%,shadow=true}
\psframebox*[fillcolor=black!10!cyan!20,fillstyle=solid]{%
\rule[-2cm]{0pt}{3.9cm}
$\psmatrix[mnode=Circle,radius=6mm,colsep=0.1cm,rowsep=0.8cm,
	arrowscale=1.5]
[name=1,style=Cblue] k & [name=8,style=CC] & [name=2,style=Cblue] k-1 & [name=8,style=CC]   & [name=3,style=Cblue] k-2 & [name=8,style=CC] \dots & [name=4,style=Cblue] 0 & [name=8,style=CC] \dots &  [name=5,style=Cblue] -(N-k-2) & [name=8,style=CC] & [name=6,style=Cblue] -(N-k-1) & [name=8,style=CC] & [name=7,style=Cblue] -(N-k)
%\ncline[nodesep=1pt]{->}{1}{2}_{S_o\lambda^{-1}}
%\ncline[nodesep=1pt]{->}{2}{3}_{S_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{1}{2}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{2}{1}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{2}{3}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{3}{2}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{5}{6}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{6}{5}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{6}{7}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{7}{6}%^{F_a\lambda^{-1}}
%\nccurve[angleA=-105,angleB=-55,ncurv=4,nodesep=1pt]{->}{3}{3}
%\nbput[nrot=:U]{S_a\lambda^{-1}}
\endpsmatrix$ }
\end{pspicture}
\end{center}


%
%\centertexdraw{
%
%\drawdim in
%
%\def\bdot {\fcir f:0 r:0.025 }
%\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
%\linewd 0.01 \setgray 0
%
%\move (0 0)\bdot
%\move (0.5 0)\bdot
%\move (1.0 0)\bdot
%\htext(1.4 0){$\dots$}
%\move (2 0)\bdot
%\htext(2.4 0){$\dots$}
%\move (3 0)\bdot
%\move (3.5 0)\bdot
%\move (4.0 0)\bdot
%
%\move (0.25 -0.2) \larc r:0.3 sd:45 ed:135
%\move (0.25 0.1) \avec(0.24 0.1)
%\move (0.25 0.2) \larc r:0.3 sd:225 ed:315
%\move (0.25 -0.1) \avec(0.26 -0.1)
%
%\move (0.75 -0.2) \larc r:0.3 sd:45 ed:135
%\move (0.75 0.1) \avec(0.74 0.1)
%\move (0.75 0.2) \larc r:0.3 sd:225 ed:315
%\move (0.75 -0.1) \avec(0.76 -0.1)
%
%\move (3.25 -0.2) \larc r:0.3 sd:45 ed:135
%\move (3.25 0.1) \avec(3.24 0.1)
%\move (3.25 0.2) \larc r:0.3 sd:225 ed:315
%\move (3.25 -0.1) \avec(3.26 -0.1)
%
%\move (3.75 -0.2) \larc r:0.3 sd:45 ed:135
%\move (3.75 0.1) \avec(3.74 0.1)
%\move (3.75 0.2) \larc r:0.3 sd:225 ed:315
%\move (3.75 -0.1) \avec(3.76 -0.1)
%
%\htext (-0.03 -0.3){$k$}
%\htext (0.37 -0.3){$k-1$}
%\htext (1.97 -0.3){$0$}
%\htext (2.97 -0.3){$-(N-k-1)$}
%\htext (3.97 -0.3){$-(N-k)$}
%}



First, from $k-1$, the probability is
\be
\pro_0(\text{hit $-(N-k-1)$ before $k$})\ \pro_{k-1}(\text{hit $k$ before $-(N-k)$})
\ee

We know
\be
\pro_i(\text{hit $-(N-k-1)$ before $k$}) = \frac{k-i}{N-1},\quad \pro_i(\text{hit $k$ before $-(N-k)$}) = \frac{i+N-k}{N}
\ee

Thus,
\be
\pro(\text{painter reach $k$ from $k-1$}) = \frac{k}{N-1} \frac{1}{N} = \frac k{N(N-1)}.
\ee

Similarly,
\be
\pro(\text{painter reach $k$ from $k+1$}) = \frac{N-k}{N-1} \frac{1}{N} = \frac {N-k}{N(N-1)}.
\ee

Hence
\beast
\pro(\text{$k$ is the last painted fence}) & = & \pro(\text{painter reach $k$ from $k-1$}) + \pro(\text{painter reach $k$ from $k+1$}) \\
& = & \frac k{N(N-1)} + \frac {N-k}{N(N-1)} = \frac 1{N-1}.
\eeast

We define $h_i$ the probability that the painter hits $-(N-k-1)$ before $k$ from $i$ and
\beast
\tilde{p}_{ij} & = & \pro(X_1=j|X_0=i,\text{hit $-(N-k-1)$ before $k$}) \\
& = & \frac {\pro(X_1=j,\text{hit $-(N-k-1)$ before $k$}|X_0=i)}{\pro(\text{hit $-(N-k-1)$ before $k$}|X_0=i)} \\
& = & \frac {\pro(\text{hit $-(N-k-1)$ before $k$}|X_1=j,X_0=i)\pro(X_1=j|X_0=i)}{h_i} \\
& = & \frac {\pro(\text{hit $-(N-k-1)$ before $k$}|X_1=j)\pro(X_1=j|X_0=i)}{h_i} \\
& = & \frac{h_j}{h_i}p_{ij}
\eeast

Then we calculate
\be\left\{
\ba{l}
\tilde{p}_{i,i+1} = \frac {k-i-1}{k-i}p_{i,i+1} = \frac{k-i-1}{2(k-i)} \\
\\
\tilde{p}_{i,i-1} = \frac {k-i+1}{k-i}p_{i,i+1} = \frac{k-i+1}{2(k-i)}
\ea\right.
\ee

The expected time starting at $i$, $T_i$ is given by
\be
\left\{
\ba{l}
T_{k-1} = 1+ T_{k-2}\\
\vdots\\
T_i = 1+ \frac{k-i+1}{2(k-i)} T_{i-1} + \frac{k-i-1}{2(k-i)} T_{i+1}\\
\vdots\\
T_{-(N-k-2)} = 1 + \frac{N-3}{2(N-2)} T_{-(N-k-3)}
\ea\right.\ \ra \
\left\{\ba{l}
2T_{k-1} = 2+ 2T_{k-2}\\
\vdots\\
2(k-i) T_i = 2(k-i) + (k-i+1)T_{i-1} + (k-i-1) T_{i+1}\\
\vdots\\
2(N-2) T_{-(N-k-2)} = 2(N-2) + (N-3) T_{-(N-k-3)}
\ea\right.
\ee

Let $U_i = (k-i)T_i$
\be
\left\{\ba{l}
2U_{k-1} = 2+ U_{k-2}\\
\vdots\\
2U_i = 2(k-i) + U_{i-1} + U_{i+1}\\
\vdots\\
2U_{-(N-k-2)} = 2(N-2) + U_{-(N-k-3)}
\ea\right.
\ \ra \ U_{k-1} + U_{-(N-k-2)} = 2\sum^{N-2}_{i=1} i = (N-1)(N-2)
\ee

Also, we have
\be
\left\{\ba{l}
2U_{k-1} = 2+ U_{k-2}\\
3U_{k-2} = 10 + 2U_{k-3}\\
4U_{k-3} = 28 + 3U_{k-4}\\
\vdots\\
(k-i+1)U_i = 2\sum^{k-i}_{j=1}j^2 + (k-i)U_{i-1} \\
\vdots\\
(N-3)U_{-(N-k-4)} = 2\sum^{N-4}_{j=1}j^2 + (N-4)U_{-(N-k-3)}\\
\\
(N-2)U_{-(N-k-3)} = 2\sum^{N-3}_{j=1}j^2 + (N-3)U_{-(N-k-2)}
\ea\right.
\ee

Thus,
\be
\ \ra \ \left\{\ba{l}
(N-2)U_{-(N-k-3)} = \frac {(N-3)(N-2)(2N-5)}{3} + (N-3)U_{-(N-k-2)}\\
2U_{-(N-k-2)} = 2(N-2) + U_{-(N-k-3)}
\ea\right. \ \ra \
\left\{\ba{l}
U_{-(N-k-2)} = \frac {(N-2)(2N-3)}{3}\\
U_{-(N-k-3)} = \frac {4(N-2)(N-3)}{3}
\ea\right.
\ee

and $U_{k-1} = \frac {N(N-2)}{3}$. Thus, we have
\be
U_i = \frac{k-i}{3}\left[(N-1)^2 - (k-i)^2\right] \ \ra \ T_i = \frac 13\left[(N-1)^2 - (k-i)^2\right] \ \ra \ T_0 = \frac 13\left[(N-1)^2 - k^2\right]
\ee

Similarly, expected time from $-(N-k-1)$ to $k$ is
\be
T_{-(N-k-1)}' = \frac {N^2-1}{3}
\ee

Thus, the painter approach $k$ from $k-1$ with probability $\frac{k}{N}$ and expected time
\be
\frac 13\left[(N-1)^2 - k^2\right] + \frac {1}{3}(N^2-1)= \frac 13\lob 2N^2 -2N- k^2\rob
\ee

Similarly, the painter approach $k$ from $k+1$ with probability $\frac{N-k}{N}$ and expected time
\be
\frac 13\lob 2N^2 -2N- (N-k)^2\rob
\ee

Thus, the expected time to reach the last fence $k$ is
\bea
& & \frac 1{3N}\lob 2kN^2 -2kN- k^3 + 2(N-k)N^2 -2(N-k)N- (N-k)^3\rob \nonumber\\
& = & \frac 1{3N}\lob 2N^3 -2N^2- k^3 - (N-k)^3\rob = \frac 13\lob 2N^2 - 2N - (N^2-3kN +3k^2) \rob\nonumber\\
& = & \frac 13\lob N^2 - 2N + 3kN - 3k^2 \rob
\eea

Since the distribution is uniform, we have the total expected time
\be
\frac 13\lob N^2 - 2N\rob + \frac{1}{N-1}\lob N\sum^{N-1}_{k=1}k - \sum^{N-1}_{k=1}k^2 \rob = \frac 13\lob N^2 - 2N\rob + \frac12 N^2 - \frac 16 N(2N-1) = \frac 12 N(N-1).
\ee

{\bf Alternative approach.} Assume the village idiot have already painted $k$. To get a new fence painted, he need to take expected time $k$ (think about gambler's ruin problem). Thus, the total
expected time will be
\be
\sum^{N-1}_{k=1} k = \frac 12 N(N-1).
\ee
\end{solution}


\begin{problem}[Zhou\cite{Zhou_2008}.$P_{113}$]
A box contains $n$ balls of $n$ different colors. Each time, you randomly select a pair of balls, repaint the first to match the second, and put the pair back into the box. What is the expected
number of steps until all balls in the box are of the same color?
\end{problem}

\begin{solution}[\bf Solution.]
Let $N_n$ be the number of steps needed to make all balls the same color, and let $F_i$, $i=1,2,\dots,n$, be the event that all balls have color $i$ in the end. Applying the law of total expectation
(Proposition \ref{pro:conditional_expectation_elementary_event}), we have
\be
\E N_n = \E\bb{N_n|F_1}\pro(F_1) + \dots + \E\bb{N_n|F_n}\pro(F_n).
\ee

Since all the colors are symmetric (i.e., they should have equivalent properties), we have
\be
\pro(F_1) = \dots = \pro(F_n) = 1/n,\qquad \E N_n = \E\bb{N_n|F_1} = \dots = \E\bb{N_n|F_n}.
\ee

That means we can assume that all the balls have color 1 in the end and use $\E\bb{N_n|F_1}$ to represent $\E N_n$.

Since we only consider event $F_1$, color 1 is different from other color and colors $2, \dots,n$ become equivalent. In other words, any pair of balls that have no color 1 ball involved are
equivalent and any pairs with a color 1 and a ball of another color are equivalent if the order is the same as well. So we only need to use the number of balls that have color 1 as the states (see
the graph).


\begin{center}
\psset{yunit=1cm,xunit=1cm}
\begin{pspicture}(0,-2)(0,2)
\newpsstyle{Cblue}{fillstyle=solid,fillcolor=blue!30}
\newpsstyle{CC}{linecolor=black!10!cyan!20}
\newpsstyle{Cred}{fillstyle=solid,fillcolor=red!30}%,shadow=true}
\psframebox*[fillcolor=black!10!cyan!20,fillstyle=solid]{%
\rule[-2cm]{0pt}{3.9cm}
$\psmatrix[mnode=Circle,radius=6mm,colsep=0.1cm,rowsep=0.8cm,
	arrowscale=1.5]
[name=1,style=Cblue] 1 & [name=8,style=CC] & [name=2,style=Cblue] 2 & [name=8,style=CC]   & [name=3,style=Cblue] 3 & [name=8,style=CC] \dots & [name=4,style=Cblue] i & [name=8,style=CC] \dots &  [name=5,style=Cblue] n-2 & [name=8,style=CC] & [name=6,style=Cblue] n-1 & [name=8,style=CC] & [name=7,style=Cblue] n
%\ncline[nodesep=1pt]{->}{1}{2}_{S_o\lambda^{-1}}
%\ncline[nodesep=1pt]{->}{2}{3}_{S_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{1}{2}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{2}{1}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{2}{3}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{3}{2}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{5}{6}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{6}{5}%^{F_a\lambda^{-1}}
%\ncarc[nodesep=1pt,arcangle=30]{<-}{6}{7}%^{F_a\lambda^{-1}}
\ncarc[nodesep=1pt,arcangle=30]{<-}{7}{6}%^{F_a\lambda^{-1}}
\nccurve[angleA=60,angleB=120,ncurv=3,nodesep=1pt]{->}{1}{1}
\nccurve[angleA=60,angleB=120,ncurv=3,nodesep=1pt]{->}{2}{2}
\nccurve[angleA=60,angleB=120,ncurv=3,nodesep=1pt]{->}{3}{3}
\nccurve[angleA=60,angleB=120,ncurv=3,nodesep=1pt]{->}{4}{4}
\nccurve[angleA=60,angleB=120,ncurv=3,nodesep=1pt]{->}{5}{5}
\nccurve[angleA=60,angleB=120,ncurv=3,nodesep=1pt]{->}{6}{6}
\nccurve[angleA=60,angleB=120,ncurv=3,nodesep=1pt]{->}{7}{7}
%\nbput[nrot=:U]{S_a\lambda^{-1}}
\endpsmatrix$ }
\end{pspicture}
\end{center}


%\centertexdraw{
%
%\drawdim in
%
%\def\bdot {\fcir f:0 r:0.025 }
%\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04 \linewd 0.01 \setgray 0
%
%\move (0 0)\bdot \move (0.5 0)\bdot \move (1.0 0)\bdot \htext(1.4 0){$\dots$} \move (2 0)\bdot \htext(2.4 0){$\dots$} \move (3 0)\bdot \move (3.5 0)\bdot \move (4.0 0)\bdot
%
%\move (0.25 -0.2) \larc r:0.3 sd:45 ed:135 \move (0.25 0.1) \avec(0.24 0.1) \move (0.25 0.2) \larc r:0.3 sd:225 ed:315 \move (0.25 -0.1) \avec(0.26 -0.1)
%
%\move (0.75 -0.2) \larc r:0.3 sd:45 ed:135 \move (0.75 0.1) \avec(0.74 0.1) \move (0.75 0.2) \larc r:0.3 sd:225 ed:315 \move (0.75 -0.1) \avec(0.76 -0.1)
%
%\move (3.25 -0.2) \larc r:0.3 sd:45 ed:135 \move (3.25 0.1) \avec(3.24 0.1) \move (3.25 0.2) \larc r:0.3 sd:225 ed:315 \move (3.25 -0.1) \avec(3.26 -0.1)
%
%%\move (3.75 -0.2) \larc r:0.3 sd:45 ed:135 \move (3.75 0.1) \avec(3.74 0.1)
%\move (3.75 0.2) \larc r:0.3 sd:225 ed:315 \move (3.75 -0.1) \avec(3.76 -0.1)
%
%\move (0 0) \clvec (-0.5 -0.5)(-0.5 0.5)(0 0) \move (-0.37 0) \avec(-0.37 0.04)
%
%\move (0.5 0) \clvec (0 0.5)(1 0.5)(0.5 0) \move (0.5 0.37) \avec(0.54 0.37) %\larc r:0.3 sd:45 ed:135
%
%\move (1 0) \clvec (0.5 0.5)(1.5 0.5)(1 0) \move (1 0.37) \avec(1.04 0.37)
%
%\move (2 0) \clvec (1.5 0.5)(2.5 0.5)(2 0) \move (2 0.37) \avec(2.04 0.37)
%
%\move (3 0) \clvec (2.5 0.5)(3.5 0.5)(3 0) \move (3 0.37) \avec(3.04 0.37)
%
%\move (3.5 0) \clvec (3 0.5)(4 0.5)(3.5 0) \move (3.5 0.37) \avec(3.54 0.37)
%
%\move (4 0) \clvec (4.5 0.5)(4.5 -0.5)(4 0) \move (4.37 0) \avec(4.37 -0.04)
%
%\htext (-0.03 -0.3){$1$} \htext (0.45 -0.3){$2$} \htext (1.97 -0.3){$i$} \htext (3.27 -0.3){$n-1$} \htext (4.0 -0.3){$n$}
%
%\move (0 0.5) }

State $n$ is the only absorbing state. Note that there is no state 0, otherwise it will never reach $F_1$. In fact, all the transition probability is conditioned on $F_1$ as well, which makes the
transition probability\footnote{We use $j$ to represent the state of Markov chain.} $p_{j,j+1}|F_1$ higher than the unconditional probability $p_{j,j+1}$ and $p_{j,j-1}|F_1$ lower than $p_{j,j-1}$.
For example, $p_{1,0}|F_1 = 0$ and $p_{1,0} = 1/n$ (Without conditioning, each ball is likely to be the second ball, so color 1 has $1/n$ probability of being the second ball.). Using the conditional transition probability and letting $M_j$ be the number of steps needed to make $n$ color 1 balls by holding $i$ color 1 balls, the problem essentially becomes expected time to absorption
with system equations:
\be
\E\bb{M_j|F_1}  = 1 + \E\bb{M_{j-1}|F_1} p_{j,j-1}|F_1 + \E\bb{M_{j}|F_1} p_{j,j}|F_1 + \E\bb{M_{j+1}|F_1} p_{j,j+1}|F_1.
\ee

To calculate $p_{j,j-1}|F_1$, we use $X$ to represent the Markov chain state and rewrite this probability as $\pro\bb{X_{k+1} = j-1|X_k = j,F_1}$ for any $k=0,1,\dots$. Therefore,

\beast
\pro\bb{X_{k+1} = j-1|X_k = j,F_1} & = & \frac{\pro\bb{X_{k+1}= j-1, X_k = j,F_1}}{\pro\bb{X_k = j,F_1}} \\
& = & \frac{\pro\bb{F_1|X_{k+1}= j-1, X_k = j}\pro\bb{X_{k+1}= j-1| X_k = j}\pro\bb{X_k = j} }{\pro\bb{F_1|X_k = j}\pro\bb{X_k = j}} \\
& = & \frac{\pro\bb{F_1|X_{k+1}= j-1}\pro\bb{X_{k+1}= j-1| X_k = j}}{\pro\bb{F_1|X_k = j}} \\
& = & \frac{\frac{j-1}{n} \times \frac{n-j}{n}\frac{j}{n-1}}{\frac jn} = \frac{(n-j)(j-1)}{n(n-1)}.
\eeast

The first equation is simply the definition of conditional probability; the second equation is the application of Bayes' theorem (Theorem \ref{thm:bayes_theorem}); the third equation applies the
Markov property; the fourth equation we use classic probability (pick another color from $n-j$ of $n$ balls and then pick color 1 from $j$ of $n-1$ balls.) To derive $\pro\bb{F_1|X_k = j} = j/n$, we
again need to use symmetry. We have shown that if all the balls have different colors, then we have $\pro(F_1) = \dots = \pro(F_n) = 1/n$. Thus, the probability of ending in a given color, labelled
as $c$, is simply the number of color $c$ divided by $n$, in our case, $j/n$.% Or we can

Similarly we can have
\beast
\pro\bb{X_{k+1} = j+1|X_k = j,F_1} & = & \frac{\pro\bb{X_{k+1}= j+1, X_k = j,F_1}}{\pro\bb{X_k = j,F_1}} \\
& = & \frac{\pro\bb{F_1|X_{k+1}= j+1, X_k = j}\pro\bb{X_{k+1}= j+1| X_k = j}\pro\bb{X_k = j} }{\pro\bb{F_1|X_k = j}\pro\bb{X_k = j}} \\
& = & \frac{\pro\bb{F_1|X_{k+1}= j+1}\pro\bb{X_{k+1}= j+1| X_k = j}}{\pro\bb{F_1|X_k = j}} \\
& = & \frac{\frac{j+1}{n} \times \frac{j}{n}\frac{n-j}{n-1}}{\frac jn} = \frac{(n-j)(j+1)}{n(n-1)}.
\eeast

\beast
\pro\bb{X_{k+1} = j|X_k = j,F_1} & = & \frac{\pro\bb{X_{k+1}= j, X_k = j,F_1}}{\pro\bb{X_k = j,F_1}} \\
& = & \frac{\pro\bb{F_1|X_{k+1}= j, X_k = j}\pro\bb{X_{k+1}= j| X_k = j}\pro\bb{X_k = j} }{\pro\bb{F_1|X_k = j}\pro\bb{X_k = j}} \\
& = & \frac{\pro\bb{F_1|X_{k+1}= j}\pro\bb{X_{k+1}= j| X_k = j}}{\pro\bb{F_1|X_k = j}} \\
& = & \frac{\frac{j}{n} \times\bb{\frac{j}{n}\frac{j-1}{n-1} + \frac{n-j}{n}\frac{n-j-1}{n-1}}}{\frac jn} = \frac{n(n-1)+ 2j(j-n)}{n(n-1)} .
\eeast
as we know that there could be two cases (two color 1 balls or two other color balls) for $X_{k+1}= j| X_k = j$. Thus, we plug the above conditional probability into formula of $\E(M_j|F_1):= K_j$ with $K_n = 0$ % and $K_2 = 1$,
\be
K_j = 1 + \frac 1{n(n-1)} \bb{(n-j)(j-1)K_{j-1} + \bb{n(n-1)+ 2j(j-n)}K_j + (n-j)(j+1)K_{j+1}}
\ee
which becomes
\be
2j(n-j)K_j = n(n-1) + (n-j)(j-1)K_{j-1} + (n-j)(j+1)K_{j+1},\quad j = 1,\dots,n-1.
\ee

Letting $W_j = (j-1)K_{j-1} - jK_j$ and rearranging the equation, we have
\be
W_{j+1} + (n-j-1)W_{j+1} = n(n-1) + (n-j)W_{j},\quad j = 1,\dots,n-1.
\ee

Then by summing these equations up we have
\be
K_1 - n K_n + 0\cdot W_n  = n(n-1)^2 + (n-1) W_1 =  n(n-1)^2 + (n-1)\bb{0K_0 - K_1}.
\ee

This implies that\footnote{Note that $K_0$ is actually $\infty$ and we use the convention $0\cdot \infty = 0$.}
\be
nK_1 = n(n-1)^2 \ \ra \  K_1 = (n-1)^2.
\ee

Hence, this implies that $\E(N_n|F_1) = \E(M_1|F_1) = K_1 = (n-1)^2$ is the wanted answer.
\end{solution}


\subsection{Continuous-time Markov chains}

\begin{problem}
You want to sell the ticket of a game. A system generates a uniformly distributed random variable in [0,1] for each round. If the random variable of the current round is bigger than the one of the
previous round, the game continues. Otherwise, the game is stopped and the payoff the random variable of previous round (which is maximal of all random variables). What's the ticket value of this
game?
\end{problem}

\begin{solution}[\bf Solution.] {\bf Approach 1.} First we have that
\beast
\pro\bb{X_n\leq x,X_1\leq \dots\leq X_n,X_{n+1}\leq X_n} & = & \int^x_0 \int^{x_n}_0\int^{x_n}_0\int^{x_{n-1}}_0 \dots \int^{x_3}_0\int^{x_2}_0 dx_1 dx_2\dots dx_{n-2}dx_{n-1} dx_{n+1} dx_n\\
& = & \int^x_0 x_n \frac{x_n^{n-1}}{(n-1)!} dx_n = \int^x_0 \frac{x_n^n dx_n}{(n-1)!} = \frac{nx^{n+1}}{(n+1)!}.
\eeast

Thus, taking differentiation, we have
\be
f_{X_n\ind_{X_1\leq \dots\leq X_n,X_{n+1}\leq X_n}}(x) = \frac{x^n}{(n-1)!}.
\ee

The expected value is that
\beast
\sum^\infty_{n=1}\E\bb{X_n \ind_{X_1\leq \dots\leq X_n,X_{n+1}\leq X_n}} & = & \sum^\infty_{n=1} \int^1_0 \frac{x^{n+1}}{(n-1)!}dx = \sum^\infty_{n=1}  \frac{n(n+1)}{(n+2)!} \\
& = & \sum^\infty_{n=1}  \frac{(n+2)(n+1) - 2(n+2) + 2}{(n+2)!} = \sum^\infty_{n=1}  \frac{1}{n!} - \frac{2}{(n+1)!} + \frac{2}{(n+2)!} \\
& = & \sum^\infty_{n=1} \bb{e-1} - 2\bb{e-1-1} + 2\bb{e-1-1-\frac 12} = e-2.
\eeast

{\bf Approach 2.} Define the expected value $f(x)$ with current maximal value $x$, then
\be
f(x) = \E\bb{x\ind_{u\leq x}} + \E\bb{f(u)\ind_{u>x}} =  x\int^x_0 du + \int^1_x f(u)du \ \ra \ f'(x) = 2x - f(x)
\ee
with boundary condition $f(1) = 1$. Thus, the solution is
\be
f(x) = Ae^{-x} + 2(x-1) \ \ra \ A = e \ \ra \ f(0) = e - 2.
\ee
\end{solution}
