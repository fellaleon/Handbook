\chapter{Discrete-time Stochastic Processes}

\section{Stochastic processes and filtrations}

Let $(\Omega,\sF,\pro)$ be a probability space and $(E,\sE)$ be a measurable space. (We will mostly consider $E =\R,\R^d,\C$). %Let $I \subseteq \R$ (i.e. $I = \Z$ or $I = [0,\infty)$) be the set of times on which we will define a process.

\begin{definition}[stochastic process, discrete]
A (discrete) stochastic process\index{stochastic process!discrete} in $E$, $X = (X_n)_{n\geq 0}$ is a sequence of random variables in $E$.
\end{definition}

\begin{remark}
stochastic process is also called random process\index{random process!discrete}.
\end{remark}

\begin{definition}\label{def:integrable_stochastic_process_discrete}
If $X_n$ is $\R$-valued, $X$ is integrable\index{integrable!stochastic process} if $X_n \in \sL^1(\Omega,\sF,\pro)$ for every $n\geq 0$, i.e. $\E\abs{X_n} <\infty$.
\end{definition}

\begin{definition}[filtration, discrete]\label{def:filtration_discrete}
A filtration\index{filtration!discrete} is an increasing family of sub-$\sigma$-algebras of $\sF$, i.e., $\sF_{n}\subseteq \sF_{n+1}\subseteq \sF$, for all $n$. % indexed by $I$, $(\sF_t)_{t \in I}$, such that if $s \leq t$ then $\sF_s \subseteq \sF_t$.
\end{definition}

\begin{remark}
We think of $\sF_n$ as `the information available at and before time $n$.'
\end{remark}

\begin{definition}\label{def:sigma_algebra_infinite_discrete}
We define
\be
\sF_\infty = \bigvee_{n\geq 0} \sF_n = \sigma\bb{\bigcup_{n\geq 0} \sF_n}. %= \sigma (\sF_n:n\geq 0)
\ee
\end{definition}

\begin{remark}
$\sF_\infty\subseteq \sF$\footnote{proof needed.}.%\footnote{We need to check that if $\sF_\infty\subseteq \sF$.}Usually, we set
\end{remark}

\begin{definition}[filtered probability space, discrete]
If $(\sF_n)_{n\geq 0}$ is a filtration, we say that $(\Omega,\sF, (\sF_n)_{n\geq 0},\pro)$ is a filtered probability space\index{filtered probability space!discrete} (or f.p.s.).
\end{definition}

\begin{definition}[adapted process, discrete]\label{def:adapted_process_discrete}
Let $(\Omega,\sF, (\sF_n)_{n\geq 0},\pro)$ be a filtered probability space. A stochastic process $X = (X_n)_{n\geq 0}$ is adapted to the filtration $(\sF_n)_{n \geq 0}$\index{adapted process} if $X_n$ is $\sF_n$-measurable for all $n\geq 0$. $(X_n)_{n\geq 0}$ is called adapted process (or non-anticipating process).
\end{definition}

\begin{remark}
An adapted process is one that cannot `see into the future'.
\end{remark}

\begin{definition}[natural filtration, discrete]
Let $(X_n)_{n\geq 0}$ be a stochastic process, and let $\sF^X_n = \sigma\bb{X_k, k \leq n}$. Then $\sF^X_n$ is the natural filtration\index{natural filtration!discrete} of $X$. It is the smallest filtration with respect to which $X$ is adapted.
\end{definition}

\section{Stopping times}

\begin{definition}[Doob's stopping time, discrete]
Let $(\Omega,\sF, (\sF_n)_{n\geq 0},\pro)$ be a filtered probability space. A stopping time\index{stopping time!discrete} with respect to the filtration $(\sF_n)_{n\geq 0}$ is a random variable $T:\Omega \to \Z^+\cup \bra{+\infty}$ such that $\bra{T \leq n} \in \sF_n$ for all $n$.
\end{definition}
%Let $T : \Omega \to I \cup \{+\infty\}$ be a random variable.


\begin{example}\label{exa:stopping_time_discrete}
\ben
\item [(i)] Constant times are trivial stopping times.
\item [(ii)] Let $X = (X_n)_{n\geq 0}$ be an adapted process taking values in $\R$. Let $A \in \sB(\R)$. The first entrance time to $A$ is
\be
T_A = \inf\bra{n \geq 0 : X_n(\omega) \in A}
\ee
with the convention that $\inf(\emptyset) = \infty$, so that $T_A = \infty$, if $X$ never enters $A$. This is a stopping time, since
\be
\bra{T_A \leq n} = \bigcup_{k\leq n} \bra{X_k \in A} \in \sF_n.
\ee

\item [(iii)] The last exit time though, $L_A = \sup\bra{n \leq N: X_n\in A}$ for some $N\in \R$, is not a stopping time in general.
\een
\end{example}


%\begin{example}.
%Let $I = \Z^+$, $(X_n)_{n \geq 0}$ be a random process, and $\sF_n = \sF^X_n$. Let $A$ be a Borel subset of $\R$ and $T_A(\omega) = \inf \{n \geq 0 : X_n(\omega) \in A\}$ (with convention $\inf \bra{\emptyset} = \infty$). Then $T_A$ is a stopping time since
%\be
%\{T_A \leq n\} = \bigcup_{m\leq n} \{X_m \in A\}
%\ee
%and $\bra{X_m \in A}$ are $\sF_n$-measurable. $T_A$ is known as the first entrance time into $A$ and is an important example of a stopping time. However, $L_A(\omega ) = \sup\{N \geq n \geq 0: X_n(\omega) \in A\}$ is not a stopping time in general.
%\end{example}

\begin{proposition}\label{pro:stopping_time_discrete_equal}
$T$ is a stopping time if and only if $\bra{T=n} \in \sF_n$ for all $n$.
\end{proposition}

\begin{proof}[\bf Proof]
Indeed, if $\bra{T \leq n}\in \sF_n$ for all $n$, $\bra{T=n} = \bra{T\leq n}\bs \bra{T\leq n-1} \in \sF_n$.

Conversely, if $\bra{T = n} \in \sF_n$ for all $n$. $\bra{T\leq n} = \bigcup_{k\leq n} \bra{T=k} \in \sF_n$.%Note that constant random variables $T \equiv t$ for some $t \in I$ are stopping times.
\end{proof}

\begin{proposition}\label{pro:stopping_time_discrete_strictly_smaller}
$T$ is stopping time if and only if $\bra{T<n}\in \sF_{n-1}$ for all $n$.
\end{proposition}

\begin{proof}[\bf Proof]
Obviously, if $T$ is a stopping time, then $\bra{T<n} = \bra{T\leq n-1} \in \sF_{n-1}$.

Conversely, $\bra{T\leq n} = \bra{T<n+1} \in \sF_{n}$ for all $n$ implie that $T$ is a stopping time.
\end{proof}



\begin{proposition}\label{pro:stopping_time_property_discrete}
Let $(\Omega,\sF, (\sF_n)_{n\geq 0},\pro)$ be a filtered probability space. Let $S$, $T$, and $(T_n)_{n \geq 0}$ all be stopping times. Then the following are stopping times\footnote{Note that in discrete time everything follows straight from the definitions. But when one considers continuous time processes, then right continuity of the filtration is needed to ensure that the limits are indeed stopping times. See Proposition \ref{pro:stopping_time_strictly_smaller_than_t_measurable}}
\beast
\text{(i)}\ S \land T,\quad\quad \text{(ii)}\ S\vee T,\quad\quad \text{(iii)}\ S+T,\quad\quad \text{(iv)}\ \sup_{n\geq 0} T_n,\quad\quad \text{(v)}\ \inf_{n\geq 0} T_n, \quad\quad \text{(vi)}\ \liminf_n T_n, \quad\quad\text{(vii)}\ \limsup_n T_n.
\eeast
\end{proposition}

\begin{remark}
$S-T$ is not.
\end{remark}

\begin{proof}[\bf Proof]
\ben
\item [(i)] $\bra{S\land T \leq n} = \underbrace{\bra{S\leq n}}_{\in \sF_n}\cup \underbrace{\bra{T\leq n}}_{\in \sF_n} \in \sF_n$ for all $n$.
\item [(ii)] $\bra{S\vee T \leq n} = \underbrace{\bra{S\leq n}}_{\in \sF_n}\cap \underbrace{\bra{T\leq n}}_{\in \sF_n} \in \sF_n$ for all $n$.

\item [(iii)] For $n\in [0,\infty)$, by Proposition \ref{pro:stopping_time_discrete_equal} and \ref{pro:stopping_time_discrete_strictly_smaller},
\beast
& & \bra{S+T > n} \\
& = & \bra{S=0,S+T>n} + \bra{0<S<n,S+T>n} + \bra{S\geq n,S+T>n} \\
& = & \bra{S=0}\cap \bra{T>n} + \bigcup_{k\in [0,n),k\in \Z} \underbrace{\bra{k<S<n}\cap\bra{T>n-k}}_{\in \sF_n} + \bra{S\geq n,T=0,S+T>n} + \bra{S\geq n,T>0,S+T>n} \\
& = & \bra{\underbrace{\bra{S=0}\cap \bra{T>n}}_{\in \sF_n} + \underbrace{\bigcup_{k\in [0,n),k\in \Z} \bra{k<S<n}\cap\bra{T>n-k}}_{\in \sF_n} + \underbrace{\bra{S> n}\cap \bra{T=0}}_{\in \sF_n} + \underbrace{\bra{S\geq n}\cap \bra{T>0}}_{\in \sF_n} } \in \sF_n .
\eeast

\item [(iv)] If $\sup_n T_n \in [0,\infty]$
\be
\bra{\sup_n T_n \leq k} = \bigcap_n\underbrace{\bra{T_n \leq k}}_{\in \sF_k} \in \sF_k.
\ee

\item [(v)] If $\inf_n T_n \in [0,\infty)$
\be
\bra{\inf_n T_n < k} = \bra{\inf_n T_n \geq k}^c = \bb{\bigcap_n \bra{T_n \geq k}}^c = \bigcup_n \bb{ \bb{\bra{T_n < k}}^c}^c =\bigcup_n \underbrace{\bra{T_n < k}}_{\in \sF_k} \in \sF_k.
\ee

\item [(vi)] By (iv) and (v),
\be
\bra{\limsup_n T_n \leq k}= \bra{\inf_{n}\bb{\sup_{m\geq n}T_m} \leq k} \in \sF_k.
\ee

\item [(viii)]
\be
\bra{\liminf_n T_n \leq k}= \bra{\sup_{n}\bb{\inf_{m\geq n}T_m} \leq k} \in \sF_k.
\ee%where $\sup_{m\geq n}T_m$ and $\inf_{m\geq n}T_m$ are stopping time sequences.
\een%\cite{Klenke_2008}.$P_{193}$
\end{proof}

\begin{definition}\label{def:sigma_algebra_stopping_time_discrete}
Let $(\Omega,\sF, (\sF_n)_{n\geq 0},\pro)$ be a filtered probability space. Let $T$ be stopping time with respect to $(\sF_n)_{n\geq 0}$, and
\be
\sF_T = \bra{A \in\sF : A\cap \{T \leq n\} \in \sF_n \text{ for all }n}.
\ee

This defines a $\sigma$-algebra $\sF_T$, called the $\sigma$-algebra of measurable events before $T$\index{sigma-algebra of measurable events before stopping time@$\sigma$-algebra of measurable events before stopping time!discrete}.
\end{definition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Since $\bra{\emptyset \cap \bra{T\leq n}} = \bra{\emptyset} \in \sF_n$, $\emptyset \in \sF_T$.
\item [(ii)] If $A\in \sF_T$, $A \cap \bra{T\leq n} \in \sF_n$. Since $T$ is stopping time, $\bra{T\leq n}\in \sF_n$. Thus, since $\sF_n$ is $\sigma$-algebra
\be
A^c \cap \bra{T\leq n} = \bra{T\leq n} \bs \bb{A \cap \bra{T\leq n}} \in \sF_n.
\ee
\item [(iii)] For a sequence $A_m \in \sF_T$, we have $A_m \cap \bra{T\leq n}\in \sF_n$ for all $n$. Thus, since $\sF_n$ is $\sigma$-algebra
\be
\bb{\bigcup_m A_m} \cap \bra{T\leq n} = \bigcup_m \bb{A_m \cap \bra{T\leq n}} \in \sF_n  \ \ra \ \bigcup_m A_m \in \sF_T.
\ee
\een

Thus, $\sF_T$ is a $\sigma$-algebra.
\end{proof}

\begin{remark}
Intuitively $\sF_T$ is the information available at time $T$.%It is easy to check that if $T = n$, then $T$ is a stopping time and $\sF_T = \sF_n$.
\end{remark}


\begin{proposition}
Let $T$ be stopping time with respect to $(\sF_n)_{n\geq 0}$. Then $T$ is $\sF_T$-measurable.
\end{proposition}

\begin{proof}[\bf Proof]
It suffices to show that $\bra{T\leq n}\in \sF_T$ for each $n\in [0,\infty)$. First we have
\be
\bra{T\leq n}\cap \bra{T\leq m} = \left\{
\ba{ll}
\bra{T\leq n} (\in \sF_n\subseteq \sF_m) \quad\quad & n\leq m \\
\bra{T\leq m}(\in \sF_m) & n> m
\ea
\right. \ \ra\ \bra{T\leq n}\cap \bra{T\leq m} \in \sF_m.
\ee

Then by definition of $\sigma$-algebra of stopping time, we have that $\bra{T\leq n}\in \sF_T$.
\end{proof}


\begin{proposition}
Suppose the stopping time $T(\omega)=n$ for all $\omega \in \Omega$ with fixed $n\in [0,\infty)$. Then $\sF_T = \sF_n$.
\end{proposition}

\begin{proof}[\bf Proof]
Suppose that $A\in \sF_n$ and then $A\in \sF$. Thus, for $m\in [0,\infty)$,
\be
A\cap \bra{T\leq m} = \left\{
\ba{ll}
A\ (\in \sF_n\subseteq \sF_m) \quad\quad & n\leq m \\
\emptyset & n>m
\ea\right. \ \ra\ A\cap \bra{T\leq m} \in \sF_m \ \ra\ A\in \sF_T \ \ra\ \sF_n\subseteq \sF_T.
\ee

Conversely, suppose that $A\in \sF_T$. Then
\be
A = A \cap \bra{T\leq n} \in \sF_n.
\ee

Thus, $\sF_T\subseteq \sF_n$ and therefore $\sF_n= \sF_T$.
\end{proof}




\begin{proposition}
Let $T$ be stopping time and $A\in \sF_T$. Then for $n\in [0,\infty)$,
\ben
\item [(i)] $A\cap \bra{T<n} \in \sF_n$.
\item [(ii)] $A\cap \bra{T=n}\in \sF_n$.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] By definition $A\cap \bra{T\leq n} \in \sF_n$. Also, by Proposition \ref{pro:stopping_time_discrete_strictly_smaller}, $\bra{T<n}\in \sF_{n-1}$ and $\bra{T<n}\subseteq \bra{T\leq n}$. Hence
\be
A\cap \bra{T<n} = \bb{A\cap \bra{T\leq n}}\cap \bra{T<n} \in \sF_n.
\ee

\item [(ii)] Similarly, $\bra{T=n} \subseteq \bra{T\leq n} \in \sF_n$. Hence
\be
A\cap \bra{T=n} = \bb{A\cap \bra{T\leq n}} \cap \bra{T=n} = \bb{A\cap \bra{T\leq n}} \cap \bb{\underbrace{\bra{T\leq n}}_{\in \sF_n} \bs\underbrace{\bra{T<n}}_{\in \sF_n}} \in \sF_n.
\ee
\een
\end{proof}



\begin{proposition}\label{pro:sigma_algebra_stopping_time_increasing}
Let $(\Omega,\sF, (\sF_n)_{n\geq 0},\pro)$ be a filtered probability space. If $S$ and $T$ are stopping times such that $S \leq T$ then $\sF_S \subseteq \sF_T$.
\end{proposition}

\begin{proof}[\bf Proof]
Since $S$ and $T$ are stopping times, then $\bra{S \leq n}\in \sF_n$, $\bra{T \leq n}\in \sF_n$ with $\bra{T\leq n} \subseteq \bra{S\leq n}$ for all $n$. $\forall A \in \sF_S$, we have for all $n$, $A \cap \bra{S \leq n} \in \sF_n$. Then
\be
A \cap \bra{T \leq n} = A \cap \bb{\bra{S \leq n} \cap \bra{T \leq n}} = \underbrace{\bb{A \cap \bra{S \leq n}}}_{\in \sF_n} \cap \underbrace{\bra{T \leq n}}_{\in \sF_n} \in \sF_n.
\ee

Thus, $A\in \sF_T$.
%\footnote{need proof}
\end{proof}



\begin{proposition}
Let $S,T$ be two stopping times. Then each of the following events is in $\sF_S$ and $\sF_T$:
\be
\text{(i)}\ \bra{S<T},\quad\text{(ii)}\ \bra{S=T},\quad \text{(iii)}\ \bra{S>T}, \quad \text{(iv)}\  \bra{S\leq T},\quad\text{(v)}\ \bra{S\geq T}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] First, $\bra{S<T}\in \sF$ since it can be expressed by
\be
\bra{S<T} = \bigcup_{k\in \N} \bb{\underbrace{\bra{S\leq k}}_{\in \sF}\cap \underbrace{\bra{T>k}}_{\in \sF}}.
\ee

Then for $n\in [0,\infty)$,
\be
\bra{S<T} \cap \bra{T\leq n} = \bra{S<T\leq n} = \bigcup_{k\in \N,k< n} \bra{S\leq k}\cap \bra{k<T\leq n} \in \sF_n \ \ra \ \bra{S<T}  \in \sF_T.
\ee
\be
\bra{S<T} \cap \bra{S\leq n} = \bra{S<T\leq n}  \cup \bra{S\leq n<T} = \bb{\bigcup_{k\in \N,k< n} \bra{S\leq k}\cap \bra{k<T\leq n}} \cup \bb{\bra{S\leq n}\cap \bra{T\leq n}^c} \in \sF_n \nonumber
\ee
which implies $\bra{S<T}  \in \sF_S$.

\item [(ii)] Similarly, $\bra{S=T}\in \sF$ and
\be
\bra{S=T}\cap \bra{T\leq n} = \bra{S=T\leq n} = \bigcup_{k\in \N,k<n} \bra{S= k}\cap \bra{k=T\leq n} \in \sF_n \ \ra \ \bra{S=T}  \in \sF_T.
\ee

Then switching $S$ and $T$ we can have $\bra{S=T}  \in \sF_S$.

\item [(iii)] Switch the argument of $S$ and $T$ in (i).

\item [(iv)] $\bra{S\leq T} = \bra{S<T}\cup\bra{S=T}$ in $\sF_S$ and $\sF_T$.
\item [(v)] Switch argument of $S$ and $T$ in (iv).
\een
\end{proof}


\subsection{Stopped process}

\begin{definition}[stopped process]\label{def:stopped_process_discrete}
For a process $X$, we set $X_T (\omega) = X_{T(\omega)}(\omega)$, whenever $T(\omega) < \infty$.

We also define the stopped process\index{stopped process!discrete} $X^T$ by $X^T_n = X_{T\land n}$.
\end{definition}



\begin{proposition}
Let $(\Omega,\sF, (\sF_n)_{n\geq 0},\pro)$ be a filtered probability space. Let $S$ be stopping time and let $X = (X_n)_{n\geq 0}$ be an adapted process. ($X: \Omega \to E$)
\ben
\item [(i)] $X_T\ind_{\bra{T <\infty}}$ is an $\sF_T$-measurable random variable.
\item [(ii)] $X^T$ is adapted.
\item [(iii)] If $X$ is integrable, then $X^T$ is integrable.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] $\forall A \in \sE$. Then since $X$ is adapted ($\bra{X_n \in A} \in \sF_n$) and $\bra{T = m} = \bra{T \leq m} \bs \bigcup_{k\leq m} \bra{T \leq k} \in \sF_m$.
\be
\bra{X_T\ind_{\bra{T < \infty}} \in A} \cap \bra{T \leq n} = \bigcup^n_{m=1} \bra{X_m(\omega) \in A} \cap \bra{T(\omega)=m} \in \sF_n
\ee

\item [(ii)] For every $n$ we have that $X_{T\land n}$ is $\sF_{T\land n}$-measurable since $X$ is adapted. Hence, $X_{T\land n}$ is $\sF_n$-measurable since $T \land n \leq n$ by Proposition \ref{pro:stopping_time_property_discrete} and Proposition \ref{pro:sigma_algebra_stopping_time_increasing} (constant $n$ is also a stopping time).
\item [(iii)] We have $X_n^T = X_n \ind_{\bra{T>n}} + \sum_{m\leq n}X_m \ind_{\bra{T=m}}$. Thus, since $X$ is integrable, for every $n$,
\be
\E\abs{X_n^T} = \E\abs{X_n^T} \leq \E\abs{X_n \ind_{\bra{T>n}}} + \sum_{m\leq n}\abs{X_m} \ind_{\bra{T=m}} \leq \E\abs{X_n} + \sum_{m\leq n}\abs{X_m} < \infty.
\ee

%\be
%\E \abs{X_{T\land n}} =  \E\bb{\sum^{n-1}_{m=0} \abs{X_m} \ind_{\bra{T = m}} } + \E\bb{\sum^\infty_{m=n} \abs{X_n} \ind_{\bra{T = m}}} \leq \sum^n_{m=0} \E\abs{X_n} < \infty.
%\ee

\een
\end{proof}

\section{Branching process}

As an example of conditional expectations and of generating functions we will consider a model of population growth and extinction known as the Bienaym\'e-Galton-Watson process (branching process).

\begin{definition}[branching process\index{branching process}]\label{def:branching_process}
Consider a sequence of random variables $X_0,X_1,\dots$, where $X_n$ represents the number of individuals in the $n$th generation. We will assume that the population is initiated by one individual, take $X_0 \equiv 1$, and
when he dies he is replaced by $k$ individuals with probability $p_k$, $k = 0, 1, 2, \dots$. These individuals behave independently and identically to the parent individual, as do those in subsequent generations.

The number in the $(n+1)$st generation, $X_{n+1}$, depends on the number in the $n$th generation and is given by
\be X_{n+1} = \left\{\ba{ll}
Y^n_1 + Y^n_2 + \dots + Y^n_{X_n} \quad\quad & X_n \geq 1,\\
0 & X_n = 0. \ea\right.
\ee

Here $\{Y^n_j : n \geq 1, j \geq 1\}$ are independent, identically distributed random variables with $\pro(Y^n_j = k) = p_k$, for $k \geq 0$ and $Y^n_j$ represents the number of offspring of the $j$th individual in the
$n$th generation, $j \leq X_n$.

Additionally, we have two assumptions:

\ben
\item [(i)] $p_0 > 0$. It means that the population can die out (extinction) since in each generation there is positive probability that all individuals have no offspring.

\item [(ii)] $p_0 + p_1 < 1$. It means that the population may grow, there is positive probability that the next generation has more individuals than the present one.
\een
\end{definition}

\begin{definition}[probability generating function of $X_n$]\label{def:probability_generating_function_branching}
Now let
\be
G(z) = \sum^\infty_{k=0} p_kz^k = \E \bb{z^{X_1}}\,\qquad G_n(z) = \E \bb{z^{X_n}},\qquad \text{for }n \geq 1,
\ee
so that $G_1 = G$.
\end{definition}

\begin{theorem}
Let $(X_n)_{n\geq 0}$ be a branching process. For all $n \geq 1$, \be G_{n+1}(z) = G_n (G(z)) = G(\dots (G(z)) \dots) = G(G_n(z)) \ee.
\end{theorem}

\begin{proof}[\bf Proof]
Note that $Y^n_1, Y^n_2 , \dots$ are independent of $X_n$, so that by total law of probability (Proposition \ref{pro:conditional_expectation_elementary_event}),
\beast G_{n+1}(z) & = & \E\bb{z^{X_{n+1}}} = \sum^\infty_{k=0} \E \bb{z^{X_{n+1}} |X_n = k} \pro (X_n = k) = \sum^\infty_{k=0} \E\bb{z^{Y^n_1 + \dots+Y^n_k}|X_n = k}\pro (X_n = k) \\
& = & \sum^\infty_{k=0} \E\bb{z^{Y^n_1 + \dots+Y^n_k}}\pro (X_n = k) = \sum^\infty_{k=0} (G(z))^k p_k  =  \E\bb{(G(z))^{X_n}} = G_n (G(z)). \eeast

Similary, we have $G_n (G(z)) = G (G_n(z))$.
\end{proof}

\begin{corollary}
Let $(X_n)_{n\geq 0}$ be a branching process. For $m = \E X_1 = \sum^\infty_{k=1} k p_k$ and $\sigma^2 = \var X_1 = \sum^\infty_{k=0} (k - m)^2 p_k$, then for $n \geq 1$, we have

\be \E X_n = m^n,\quad\quad \var X_n = \left\{\ba{ll}
\frac{\sigma^2m^{n-1} (m^n - 1)}{m- 1} \quad\quad & m \neq 1,\\
n\sigma^2 & m=1. \ea\right. \ee
\end{corollary}

\begin{proof}[\bf Proof]
Differentiating $G_n(z) = G_{n-1}(G(z))$ to obtain $G_n'(z) = G'_{n-1}(G(z))G'(z)$ and letting $z \ua 1$ (so $G(z) \ua 1$ accordingly), by Theorem \ref{thm:pgf_moment} it follows that

\be \E (X_n) = m\E (X_{n-1}) = \dots = m^n\E (X_0) = m^n, \ee
since $X_0 = 1$.

Differentiating $G_n(z)$ a second time gives

\be G''_n(z) = G''_{n-1} (G(z)) (G'(z))^2 + G'_{n-1} (G(z))G''(z), \ee

and letting $z \ua 1$ again we have

\be \E (X_n (X_n - 1)) = m^2 \E (X_{n-1} (X_{n-1} - 1)) + \bb{\sigma^2 + m^2 - m} \E (X_{n-1}). \ee

We then have, using the fact that $\E X_n = m^n$, \beast
\var (X_n) & = & \E (X_n(X_n - 1)) + \E (X_n) - (\E X_n)^2\\
& = & m^2\E (X_{n-1}(X_{n-1} - 1)) + \bb{\sigma^2 + m^2 - m}\E (X_{n-1}) + m^n - m^{2n} \\
& = & m^2\bb{\var (X_{n-1}) - \E (X_{n-1}) + (\E X_{n-1})^2} + \bb{\sigma^2 + m^2} m^{n-1} - m^{2n}\\
& = & m^2\var (X_{n-1}) + \sigma^2 m^{n-1}. \eeast Iterating this, we see that \beast
\var (X_n) & = & m^2\var (X_{n-1}) + \sigma^2 m^{n-1} = m^4\var (X_{n-2}) + \sigma^2 \bb{m^{n-1} + m^n} = \dots \\
& = & m^{2n}\var (X_0) + \sigma^2 \bb{m^{n-1} + \dots + m^{2n-2}}\\
& = & \sigma^2 \bb{m^{n-1} + \dots + m^{2n-2}},\quad\quad \text{since $\var (X_0) = 0$ because $X_0 = 1$}, \eeast and then the result may be obtained immediately.
\end{proof}

\begin{definition}[extinction of branching process]
Let $(X_n)_{n\geq 0}$ be a branching process. If $X_n = 0$ for some $n$, then we say that the branching process get extincted.
\end{definition}

\begin{theorem}
For branching process $(X_n)_{n\geq 0}$, its extinction probability $q$ is the smallest positive root of the equation $G(z) = z$. When $m$, the mean number of offspring per individual, satisfies $m \leq 1$ then $q = 1$,
when $m
> 1$ then $q < 1$.
\end{theorem}

\begin{proof}[\bf Proof]
Notice that $X_n = 0$ implies that $X_{n+1} = 0$ so that if we let $A_n = \bra{X_n = 0}$, the event that the population is extinct at or before generation $n$, we have $A_n \subseteq A_{n+1}$ and $A = \bigcup^\infty_{n=1}
A_n$ represents the event that extinction ever occurs. Notice that $\pro(A_n) = G_n(0)$ and by the continuity property of probabilities on increasing events (fundamental property of measure, Lemma
\ref{lem:measure_increasing_sequence}) we see that the extinction probability is \be q = \pro(A) = \lim_{n\to\infty} \pro (A_n) = \lim_{n\to\infty} G_n(0) = \lim_{n\to\infty} \pro \bb{X_n = 0}. \ee

The fact that the extinction probability $q$ is well defined follows from the above and since $G$ is continuous and $q = \lim_{n\to \infty}G_n(0)$ we have

\be G\bb{\lim_{n\to\infty} G_n(0)} = \lim_{n\to\infty}G_{n+1}(0) \ee,

so that $G(q) = q$, that is $q$ is a root of $G(z) = z$, note that 1 is always a root since $G(1) = \sum^\infty_{r=0} p_r = 1$.

Let $\alpha > 0$ be any positive root of $G(z) = z$, so that because $G$ is increasing, $\alpha = G(\alpha) > G(0)$, and repeating $n$ times we have $\alpha > G_n(0)$, whence $\alpha \geq \lim_{n\to\infty} G_n(0) = q$, so
that we must have $\alpha\geq q$, that is, $q$ is the smallest positive root of $G(z) = z$.

Now let $H(z) = G(z)-z$ (which is continuous and differentiable on $(0,1)$), then

\be H'' = \sum^\infty_{r=0} r(r-1)p_rz^{r-2} > 0,\qquad 0 < z < 1 \ee

provided $p_0 + p_1 < 1$, so the derivative of $H$ is strictly increasing in the range $0 < z < 1$, hence $H$ can have at most one root different from 1 in $[0, 1]$ with the following two cases.% (Rolle's Theorem (Theorem \ref{thm:rolle_analysis})).

%Note. The following two figures illustrate the two situations $m \leq 1$ and $m > 1$, the dotted lines illustrate the iteration $G_{n+1}(0) = G(G_n(0))$ tending to the smallest positive root, $q$.

%\centertexdraw{
%
%\drawdim in
%
%\def\bdot {\fcir f:0 r:0.02 }
%\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04 \linewd 0.01 \setgray 0
%
%\move (-0.2 0) \avec(2 0) \move (0 -0.2) \avec(0 1.8)
%
%\move (0 0) \lvec (1.8 1.8) \move (0 0.6) \clvec (0.9 1)(1.3 1.3)(1.6 1.6)
%
%\htext (-0.1 -0.15){0} \htext (1.6 -0.15){1} \htext (0.55 -0.2){$m\leq 1$, $q=1$} \htext (1.9 -0.15){$z$} \htext (-0.4 0.45){$G(0)$}
%
%\lpatt (0.05 0.05)
%
%\move (1.6 1.6) \lvec (1.6 0)
%
%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\lpatt (1 0)
%
%\move (2.8 0) \avec(5 0) \move (3 -0.2) \avec(3 1.8)
%
%\move (3 0) \lvec (4.8 1.8) \move (3 0.6) \clvec (4.2 0.9)(4.5 1.4)(4.6 1.6)
%
%\htext (2.9 -0.15){0} \htext (4.6 -0.15){1} \htext (3.55 -0.2){$m>1$, $q<1$} \htext (4.9 -0.15){$z$} \htext (2.6 0.45){$G(0)$}
%
%\lpatt (0.05 0.05) \move (4 1) \lvec (4 0) \move (4.6 1.6) \lvec (4.6 0)
%
%\move(0 2) }



\begin{center}
\psset{yunit=4cm,xunit=4cm}
\begin{pspicture}(-0.5,-0.2)(1.2,1.2)%[showgrid](-3,-1.5)(3,4)
\psaxes[dx=1,dy=1,labels=none,ticks=none]{->}(0,0)(-0.2,-0.2)(1.2,1.2)%Dx=0.25,Dy=0.25
\psset{algebraic}
\psplot{0}{1.2}{x}
\psplot{0}{1}{(x+1)^2/4}
\rput[lb](1.2,-0.1){$z$}
\rput[lb](-0.2,0.2){$G(0)$}
\rput[cb](0.5,-0.15){$m\leq 1,q=1$}
\rput[lb](-0.1,1){1}
\rput[lb](1,-0.1){1}
\pstGeonode[PointSymbol=none,PointName=none](0,0.5){A}(1,0.5){AA}(0.5,0){B}(0.5,1){BB}(0,1){C}(1,1){D}(1,0){E}
%\pstLineAB[linestyle=dashed]{A}{AA}
%\pstLineAB[linestyle=dashed]{B}{BB}
\pstLineAB[linestyle=dashed]{C}{D}
\pstLineAB[linestyle=dashed]{D}{E}
\end{pspicture}
\begin{pspicture}(-0.5,-0.2)(1.2,1.2)%[showgrid](-3,-1.5)(3,4)
\psaxes[dx=1,dy=1,labels=none,ticks=none]{->}(0,0)(-0.2,-0.2)(1.2,1.2)%Dx=0.25,Dy=0.25
\psset{algebraic}
\psplot{0}{1.2}{x}
\psplot{0}{1}{4^x/4}
\rput[lb](1.2,-0.1){$z$}
\rput[lb](-0.2,0.2){$G(0)$}
\rput[cb](0.5,-0.15){$m> 1,q<1$}
\rput[lb](-0.1,1){1}
\rput[lb](1,-0.1){1}
\pstGeonode[PointSymbol=none,PointName=none](0,0.5){A}(1,0.5){AA}(0.5,0){B}(0.5,1){BB} (0,1){C}(1,1){D}(1,0){E}(0.5,0.5){F}(0.5,0){G}
%\pstLineAB[linestyle=dashed]{A}{AA}
%\pstLineAB[linestyle=dashed]{B}{BB}
\pstLineAB[linestyle=dashed]{C}{D}
\pstLineAB[linestyle=dashed]{D}{E}
\pstLineAB[linestyle=dashed]{F}{G}
\end{pspicture}
\end{center}

Firstly, suppose that $H$ has no root in $[0, 1)$ and $q=1$ in this case. Since $H(0) = p_0 > 0$ we must have $H(z) > 0$ for all $0 < z < 1$, so $H(1) - H(z) < H(1) = 0$ and so

\be H'(1^-) = \lim_{z\ua 1} \frac{H(1) - H(z)}{1 - z} \leq 0 \ \ra \ G'(1^-) - 1 \leq 0\ \ra\  m = G'(1^-) \leq 1. \ee

Next, suppose that $H$ has a unique root $r$ in $[0, 1)$ and $q<1$ in this case. Then $H'$ must have a root in $[r, 1)$ (by Rolle's theorem, Theorem \ref{thm:rolle_analysis} as $H$ is continuous and differentiable), that is
$H'(z) = G'(z)-1 = 0$ for some $z$, $r \leq z < 1$. The function $G'$ is strictly increasing (since $p_0 + p_1 < 1$) so that $m = G'(1^-) > G'(z) = 1$. Thus we see that $m \leq 1$, if and only if, $q = 1$.
\end{proof}


\section{Random walks}

\begin{definition}[random walk\index{random walk!one-dimensional}]\label{def:random_walk_one_dimensional}
Let $X_1,X_2,\dots$ be i.i.d. random variables and set

\be S_n = S_0 + \sum^n_{k=1}X_k, \quad n \geq 1 \ee where $S_0$ is a constant then $\bb{S_n}_{n \geq 0}$ is known as a (one-dimensional) random walk.

When each $X_i$ just takes the two values +1 and -1 with probabilities $p$ and $q = 1 - p$, respectively, it is a simple random walk\index{random walk!simple}\footnote{It goes also by the title of the Bernoulli walk, Rademacher walk, etc.} and further when $p = q = \frac 12$ it is a simple symmetric
random walk\index{random walk!simple symmetric} (see Example \ref{exa:random_walk_simple_symmetric_single_boundary}, \ref{exa:random_walk_simple_symmetric_double_boundaries}).
\end{definition}

In the following context, we will consider simple random walks.

\subsection{Symmetric random walks}

At any step of simple random walk construction, a lattice path can go up or down. Therefore, there are $2^n$ paths of length $n$. Consequently, with probability $2^{-n}$, the first $n$ steps of our simple random walk $(S_k)_{k\geq 0}$ are equal to a given path of length $n$ that starts from $(0,0)$, where $S_0 = 0$. That is, all paths that start from $(0,0)$ and have length $n$are equally likely. Therefore, if $\Pi$ is a property of paths of length $n$, then
\be
\pro\bb{(S_k)^n_{k=0}\in \Pi} = \frac{\text{\# of paths of length $n$ that start from (0,0) and are in $\Pi$}}{2^n}.
\ee

Thus, any probabilistic problem for the simple random walk has a combinatorial variant, and vice versa.

\begin{definition}\label{def:paths_count_simple_symmetric_random_walk}
For the simple symmetric random walk $(S_n)_{n\geq 0}$. Let $N_{n,x}$ denote the number of paths that go from $(0,0)$ to $(n,x)$.

An elementary computation shows that
\be
N_{n,x} = \left\{\ba{ll}
\binom{n}{(n+x)/2},\quad\quad & n+x\text{ is even},\\
0 & \text{otherwise}.
\ea\right.
\ee
\end{definition}

Another important fact is the `reflection principle' of D\'esir\'e Andr\'e (1887) (see \footnote{Andr\'e D\'esir\'e, Solution directe du probl\`em r\'esolu par M. Bertrand, Comptes Rendus Acad Sci Paris, 105, 436-437, 1887})

\begin{theorem}[reflection principle\index{reflection principle!simple random walk}]\label{thm:reflection_principle_simple_symmetric_random_walk}
Let $(S_n)_{n\geq 0}$ be a simple symmetric random walk. Suppose $n,x,y>0$ and $k\geq 0$ are integers. Let $M$ denote the number of paths that go from $(k,x)$ to $(k+n,y)$ and hit zero at some point. Then $M$ is equal to the number of paths that go from $(0,-x)$ to $(n,y)$. That is, $M=N_{n,x+y}$.
\end{theorem}

\begin{proof}[\bf Proof]
Let $T$ denote the first instant when a given path from $(k,x)$ to $(k+n,y)$ crosses zero. Reflect the pre-$T$ portion of this path to obtain a path that goes from $(k,-x)$ to $(k+n,y)$.








This map is an invertible operation, therefore $M$ is equal to the number of paths that go from $(k,-x)$ to $(k+n,y)$. It is easy to see that $M$ does not depend on $k$.
\end{proof}

\begin{corollary}
Suppose $\bb{S_n}_{n\geq 0}$ is a simple symmetric random walk and $a\in \Z^+$. Then
\be
\pro\bb{\max_{0\leq k\leq n}S_k \geq a} = 2\pro\bb{S_n \geq a} = \pro\bb{\abs{S_n}\geq a}.
\ee
\end{corollary}

\begin{proof}[\bf Proof]
\footnote{proof needed. see CAM Part IB past paper.}
\end{proof}



\begin{theorem}[ballot problem\index{ballot problem}]\label{thm:ballot_problem}
For the simple symmetric random walk $(S_n)_{n\geq 0}$ and $n,x>0$ be integers. Then the number of paths that go from $(0,0)$ to $(n,x)$ and $S_1,\dots,S_n>0$ (i.e., never goes back to zero) is $(x/n) N_{n,x}$.
\end{theorem}

\begin{remark}
Define $T_0$ to be the first time the simple walk crosses $y=0$, i.e.,
\be
T_0 := \inf\bra{k\geq 1:S_k=0}\qquad (\inf \emptyset :=\infty).
\ee

Then, the ballot theorem is saying the following
\be
\pro\bb{T_0 >n|S_n=x} = \frac xn,\qquad \forall x=1,\dots,n.
\ee
\end{remark}

\begin{proof}[\bf Proof]
Let $M$ denote the number of paths that go from $(0,0)$ to $(n,x)$ and $S_1,\dots,S_n$. All paths question have the property that they go from $(1,1)$ to $(n,x)$. Therefore, we might as well assume that $x\leq n$ (otherwise $x$ cannot be archived by $n$ steps), whence
\beast
M & = & \#\text{paths from $(1,1)$ to $(n,x)$} - \#\text{paths from $(1,1)$ to $(n,x)$ and cross $y=0$ at some intervening time}\\
& = & N_{n-1,x-1} - N_{n-1,x+1}.
\eeast

We have applied the reflection principle (Theorem \ref{thm:reflection_principle_simple_symmetric_random_walk})in the very last step. If $n+x$ is odd then
\be
N_{n-1,x-1} = N_{n-1,x+1} = N_{n,x} = 0,
\ee
and the result follows (as $(x/n)N_{n,x} = 0$). On the other hand, if $n+x$ is even, then by Definition \ref{def:paths_count_simple_symmetric_random_walk},
\beast
N_{n-1,x-1} = N_{n-1,x+1} & = & \binom{n-1}{\frac{n+x}2-1} - \binom{n-1}{\frac{n+x}2} = \frac{(n-1)!}{(\frac{n+x}2-1)!(\frac{n-x}2)!} - \frac{(n-1)!}{(\frac{n+x}2)!(\frac{n-x}2-1)!} \\
& = & \frac {N_{n,x}}n \bb{\frac{n+x}2 - \frac{n-x}2} = \frac xn N_{n,x}.
\eeast
\end{proof}

\begin{proposition}\label{pro:random_walk_number_paths_greater_than_zero}
For the simple symmetric random walk $(S_n)_{n\geq 0}$. If $S_k > 0$ for all $k\leq 2n-1$ and $S_{2n} = 0$. Then for any $n\in \Z^+$, the number of paths of length $2n$ is $\frac 1{n}\binom{2n-2}{n-1}$.
\end{proposition}

\begin{proof}[\bf Proof]
Let $M$ denote the number of all paths of length $2n$ which have $S_k>0$ for all $k\leq 2n-1$ and $S_{2n}=0$. Evidently, $M$ is the number of all paths that go from $(0,0)$ to $(2n-1,1)$ and do not cross $y=0$. By the ballot theorem (Theorem \ref{thm:ballot_problem}, put $x=1$, $n=2n-1$)
\be
M = \frac 1{2n-1}N_{2n-1,1} = \frac 1{2n-1}\binom{2n-1}{(2n-1+1)/2} = \frac 1{2n-1}\binom{2n-1}{n} = \frac{(2n-2)!}{n!(n-1)!} = \frac 1n\binom{2n-2}{n-1}.
\ee
\end{proof}

\begin{proposition}\label{pro:random_walk_number_paths_greater_equal_than_zero}
For the simple symmetric random walk $(S_n)_{n\geq 0}$, if $S_k \geq 0$ for all $k\leq 2n-1$ and $S_{2n} = 0$. Then for any $n\in \Z^+$, the number of paths of length $2n$ is $\frac 1{n+1}\binom{2n}{n}$.
\end{proposition}

\begin{proof}[\bf Proof]
Consier the random walk $S_{k+1}' = S_k+1$ with $S_0' = 0$. If $M$ denotes the number of $(S_k)_{k\geq 0}$ paths of interest, then $M$ is the number of paths for $(S_k')_{k\geq 0}$ that go from $(0,0)$ to $(2n+1,1)$ such that $S_k'>0$ for all $k\leq 2n+1$. That is, we simply shift the axes to the -1 and a piece that goes from $(0,0)$ to $1,1$ in new coordinate system.


% (see Problem \ref{exe:ticket_line}).

\centertexdraw{

\drawdim in

\def\bdot {\fcir f:0 r:0.03 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0

\move (-0.2 0) \avec(4.9 0)
\move (0 -0.6) \avec(0 1)

%\move (4.5 -0.5) \bdot


\move (0 0) \lvec(0.5 0.5)
\move (0.5 0.5)\lvec(1 0)
\move (1 0)\lvec(1.25 0.25)
\move (1.25 0.25)\lvec(1.5 0)\lvec(2 0.5)\lvec(2.5 0)
\move (2.5 0) \lvec(3.25 0.75)
\move (3.25 0.75) \lvec (4 0)
\move (4 0) \lvec(4.25 0.25)
\move (4.25 0.25) \lvec (4.5 0)

\lpatt (0.05 0.05)

%\move ( 1.5) \lvec(5 1.5)
\move (-0.25 -0.25) \avec(4.9 -0.25)
\move (4.5 0)\lvec(4.75 -0.25)
\move (-0.25 -0.25)\lvec(0 0)
\move (-0.25 -0.6) \avec(-0.25 1)

\htext (4.5 0.1){$2n$}

%\move (1.75 -0.25)\lvec(2 0)\lvec(3.25 -1.25) \lvec (4 -0.5) \lvec(4.25 -0.75) \lvec (4.5 -0.5)

\move (0 1)
\move (0 -0.8)

}

Therefore, what we want becomes the number of paths of length of $2n+2$ which have $S_k >0$ forall $k\leq 2n+1$ and $S_{2n+2}=0$. Then we can apply Proposition \ref{pro:random_walk_number_paths_greater_than_zero} and replace $n$ with $n+1$ in the conclusion and get $\frac 1{n+1}\binom{2n}{n}$.
\end{proof}


\begin{proposition}
For the simple symmetric random walk $(S_n)_{n\geq 0}$, we define
\be
T_0 :=\inf\bra{k\geq 1:S_k=0}.
\ee
%and
%\be
%u_{2n} := \frac{\binom{2n}{n}}{2^{2n}},\qquad f_{2n} := \frac{u_{}}{2n}
%\ee

Then we have
\ben
\item [(i)] $\pro\bb{S_{2n}=0} =  \frac{\binom{2n}{n}}{2^{2n}}$.
\item [(ii)] $\pro\bb{T_0 = 2n} = \frac{\binom{2n-2}{n-1}}{2^{2n+1}n}$.
\item [(iii)] $\pro\bb{S_k\geq 0,\forall k\leq 2n-3, S_{2n-2} =0, S_{2n-1}=-1} = \frac{\binom{2n-2}{n-1}}{2^{2n+1}n}$.
\item [(iv)] $\pro(T_0>2n) = \frac{\binom{2n}{n}}{2^{2n}}$.
\item [(v)] $\pro\bb{S_k\geq 0, \forall k\leq 2n} = \frac{\binom{2n}{n}}{2^{2n}}$.
\een
\end{proposition}

\begin{proof}[\bf Proof]%\be
%\pro(T_0=2n) = u_{2n} = \frac{\binom{2n}{n}}{2^{2n}},\qquad f_{2n} = \frac{u_{2n-2}}{2n} = \frac{\binom{2n-2}{n-1}}{2^{2n-1}n}
%\ee
\ben
\item [(i)] Because there $N_{2n,0} = 2^{2n}\pro\bb{S_{2n} = 0}$ paths that end up at zero at time $2n$, we have by Definition \ref{def:paths_count_simple_symmetric_random_walk}
\be
\pro\bb{S_{2n} = 0} = \frac{\binom{2n}{n}}{2^{2n}}.
\ee

\item [(ii)] To prove this, note that the number of paths that hit zero, for the first time, at time $2n$ is exactly twice the number of paths which satisfying $S_k>0$ for all $k\leq 2n-1$ and $S_{2n}=0$. Then Proposition \ref{pro:random_walk_number_paths_greater_than_zero} implies that
\be
\pro\bb{T_0 = 2n} = \frac{2\binom{2n-2}{n-1}}{n} \frac 1{2^{2n}} = \frac{\binom{2n-2}{n-1}}{2^{2n-1}n}.
 \ee


\item [(iii)] By Proposition \ref{pro:random_walk_number_paths_greater_equal_than_zero},
\beast
\pro\bb{S_k\geq 0,\forall k\leq 2n-3,S_{2n-2}=0,S_{2n-1}=-1} & = & \frac 12 \pro\bb{S_k\geq 0,\forall k\leq 2n-3,S_{2n-2}=0} \\
& = & \frac 12 \frac {\binom{2n-2}{n-1}}{2^{2n-2}n} = \frac {\binom{2n-2}{n-1}}{2^{2n-1}n}.
\eeast


\item [(iv)] By (ii), we have
\be
\pro\bb{T_0 > 2n} = 1 - \sum^{n}_{k=1} \pro\bb{T_0 = 2k} = 1 - \sum^n_{k=1} \frac{\binom{2k-2}{k-1}}{2^{2k-1}k}
\ee

It is easy to check that
\be
\frac{\binom{2k-2}{k-1}}{2^{2k-1}k} = \frac{\binom{2k-2}{k-1}}{2^{2k-2}} - \frac{\binom{2k}{k}}{2^{2k}}.
\ee

Therefore, we have
\be
\pro\bb{T_0 > 2n} = 1 - \sum^n_{k=1} \bb{\frac{\binom{2k-2}{k-1}}{2^{2k-2}} - \frac{\binom{2k}{k}}{2^{2k}}}  = \frac{1}{2^{2n}}\binom{2n}{n}.
\ee


\item [(v)] $S_{2n}$ can take value $0,2,\dots,2n$. Then by ballot problem (Theorem \ref{thm:ballot_problem})
\beast
\pro\bb{S_k \geq 0, \forall k \leq 2n} & = & 2\sum^n_{k=0} \pro\bb{S_{-1} = -1, S_0 = 0,S_m \geq 0, m = 1,\dots,2n-1,S_{2n}=2k}\\
 & = & 2\sum^n_{k=0} \pro\bb{S_{0} = 0, S_m > 0, m = 1,\dots,2n,S_{2n+1}=2k+1}\\
 & = & 2\sum^{n}_{k=0} \frac {2k+1}{2n+1}N_{2n+1,2k+1} \frac 1{2^{2n+1}}= \frac 1{2^{2n}}\sum^{n}_{k=0} \frac {2k+1}{2n+1}\binom{2n+1}{n+k+1} \\ %1 - \pro\bb{\exists k\leq 2n, S_k = -1}
 & = & \frac 1{2^{2n}}\sum^{n}_{k=0} \bb{\binom{2n}{n+k} - \binom{2n}{n+k+1}} = \frac 1{2^{2n}}\binom{2n}{n}.
\eeast

Alternatively, we can have that
\be
\#\bra{S_k \geq 0, \forall k \leq 2n} = \#\bra{S_{-1}=-1, S_k \geq 0, k =0,\dots, 2n} = \#\bra{S_{0}=0, S_k \geq 1, k =1,\dots, 2n+1}
\ee

Thus,
\beast
\pro\bb{S_k \geq 0, \forall k \leq 2n} & = & \frac 1{2^{2n}} \#\bra{S_k \geq 0, \forall k \leq 2n} = \frac{2}{2^{2n+1}}\#\bra{S_{0}=0, S_k \geq 1, k =1,\dots, 2n+1}\\
& = & \frac{1}{2^{2n+1}}\bb{\#\bra{S_{0}=0, S_k \geq 1, k =1,\dots, 2n+1} + \#\bra{S_{0}=0, S_k \leq -1, k =1,\dots, 2n+1}} \\
& = & \frac{1}{2^{2n+1}}\#\bra{S_{0}=0, S_k \neq 0, k =1,\dots, 2n+1} = \pro\bb{T_0 > 2n} = \frac 1{2^{2n}}\binom{2n}{n}
\eeast
by (iv).
\een
\end{proof}

\begin{proposition}
Suppose $\bb{S_n}_{n\geq 0}$ is a simple symmetric random. Then if $n$ is odd, $k = 0,1\dots \floor{\frac n2}$,
\beast
& &\pro\bb{\max_{0\leq i\leq n}S_i - S_n = n-2k} = \left.\binom{n}{k}\right/ 2^n,\\
& &\pro\bb{\max_{0\leq i\leq n}S_i - S_n = n-2k-1} = \left.\binom{n}{k}\right/ 2^n = \left.\binom{n}{n-k}\right/ 2^n.
\eeast

If $n$ is even, $k = 0,1\dots \frac n2-1$,
\beast
& & \pro\bb{\max_{0\leq i\leq n}S_i - S_n = 0} = \left.\binom{n}{n/2}\right/ 2^n,\\
& &\pro\bb{\max_{0\leq i\leq n}S_i - S_n = n-2k} = \left.\binom{n}{k}\right/ 2^n,\\
& &\pro\bb{\max_{0\leq i\leq n}S_i - S_n = n-2k-1} = \left.\binom{n}{k}\right/ 2^n = \left.\binom{n}{n-k}\right/ 2^n.
\eeast

In particular, the probability that the process reaches the maximum point at time $n$ is
\be
\pro\bb{S_n = \max_{0\leq i\leq n}S_i} = \left\{\ba{ll}\left.\binom{n}{n/2}\right/2^n\quad \quad & n\text{ is even} \\
\left.\binom{n+1}{(n+1)/2}\right/2^{n+1}\quad \quad & n\text{ is odd}
\ea \right..
\ee
\end{proposition}

\begin{proof}[\bf Proof]
We give the proof by using induction\footnote{other method needed.}. For $n=1$, we have
\be
\pro\bb{\max_{0\leq i\leq 1} S_i - S_1 =0} = \pro\bb{\max_{0\leq i\leq 1} S_i - S_1 = 1} = \frac 12 = \left.\binom{1}{1}\right/2^1= \left.\binom{1}{0}\right/2^1.
\ee

For $n=2$, we have
\beast
& & \pro\bb{\max_{0\leq i\leq 2} S_i - S_2 =0} = \frac 12 = \left.\binom{2}{1}\right/2^2,\\
& & \pro\bb{\max_{0\leq i\leq 2} S_i - S_2 =1} = \frac 14 = \left.\binom{2}{0}\right/2^2,\\
& & \pro\bb{\max_{0\leq i\leq 2} S_i - S_2 =2} = \frac 14 = \left.\binom{2}{0}\right/2^2.
\eeast

We can see that the required results hold for these two cases. Also, we let $h_{k,n} = \pro\bb{\max_{0\leq i\leq n} S_i - S_n =k}$ and for $k=0,1,\dots,n-2$
\beast
h_{0,n} & = & \frac 12 h_{-1,n-1} + \frac 12 h_{1,n-1}\\
\vdots & = & \vdots \\
h_{k,n} & = & \frac 12 h_{k-1,n-1} + \frac 12 h_{k+1,n-1}\\
\vdots & = & \vdots \\
h_{n-1,n} & =& h_{n,n} = 2^{-n}
\eeast
where $h_{0,n} = h_{-1,n}$.

The have $\max_{0\leq i\leq n} S_i - S_n =n-1$, the process must have +1 at the first step and -1 for the rest and therefore the probability $h_{n-1,n}$ is $2^{-n}$.

Now assume the required result holds for $n$. Thus, %for $m=0,1,\dots, \frac n-1$
\be
h_{n-m,n+1} = \frac 12 h_{n-m-1,n} + \frac 12 h_{n-m+1,n}
\ee
and
\be
h_{n,n+1} = h_{n+1,n+1} = 2^{-(n+1)}.
\ee

If $n$ is even, by the required result for $n$ ($m=2k$ or $m=2k+1$)
\be
h_{n-m,n+1} = \frac 12 \frac{\binom{n}{k}}{2^n} + \frac 12 \frac{\binom{n}{k-1}}{2^n} = 2^{-(n+1)}\bb{\binom{n}{k} +\binom{n}{k-1}} = 2^{-(n+1)}\binom{n+1}{k}
\ee
satisfying the required result. Similarly, we have the required result for odd $n = 2k+1$. In particular, if $m = n$ we have
\beast
h_{0,n+1} & = & \frac 12 h_{-1,n} + \frac 12 h_{1,n} = \frac 12 h_{0,n} + \frac 12 h_{1,n} = \frac 12 \frac{\binom{n}{k}}{2^n} + \frac 12 \frac{\binom{n}{k}}{2^n} = 2^{-(n+1)}2\binom{2k+1}{k} \\
& = & 2^{-(n+1)} \frac{(2k+1)!2(k+1)}{k!(k+1)!(k+1)} = 2^{-(n+1)}\binom{2k+2}{k+1} = 2^{-(n+1)}\binom{n+1}{(n+1)/2},
\eeast
as required.

For $n = 2k+1$ is odd, we have that
\be
\pro\bb{S_n = \max_{0\leq i\leq n}S_i} = 2^{-n}\binom{n}{k} = 2^{-(n+1)} 2\binom{2k+1}{k} = 2^{-(n+1)} \binom{2k+2}{k+1} = 2^{-(n+1)} \binom{n+1}{(n+1)/2}.
\ee
\end{proof}


\subsection{Asymmetric random walks}

\begin{example}[gambler's ruin]\label{exa:randam_walk_simple}
For the simple random walk, $\bb{S_n}_{n\geq 0}$ may represent the fortune of a gambler after $n$ plays of a game where on each play he either wins \pounds 1, with probability $p$, or loses \pounds 1 with probability $q =
1-p$, his initial fortune is \pounds $S_0$ and a classical problem is to calculate the probability that his fortune achieves the level $a$, $a > S_0$ , before the time of ruin, that is the time that he goes bankrupt (his
fortune hits the level 0). If $T_a$ denotes the first time that the random walk hits the level $a$ and $T_0$ the time the random walk first hits the level 0, we would wish to calculate $\pro (T_a < T_0)$, given that his
fortune starts at $S_0 = r$, $0 < r < a$.

\centertexdraw{

\drawdim in

\def\bdot {\fcir f:0 r:0.03 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04 \linewd 0.01 \setgray 0

\move (-0.2 0) \avec(5 0) \move (0 -0.2) \avec(0 1.8)

\move (0 0.6) \bdot \move (0.2 0.9) \bdot \move (0.4 0.6) \bdot \move (0.6 0.9) \bdot \move (0.8 1.2) \bdot \move (1 0.9) \bdot \move (1.2 1.2) \bdot \move (1.4 1.5) \bdot \move (1.6 1.2) \bdot \move (1.8 0.9) \bdot \move
(2 1.2) \bdot \move (2.2 1.5) \bdot \move (2.4 1.2) \bdot \move (2.6 0.9) \bdot \move (2.8 1.2) \bdot \move (3 0.9) \bdot \move (3.2 0.6) \bdot \move (3.4 0.9) \bdot \move (3.6 0.6) \bdot \move (3.8 0.3) \bdot \move (4 0)
\bdot \move (4.2 -0.3) \bdot \move (4.4 0) \bdot \move (4.6 0.3) \bdot \move (4.8 0.6) \bdot



\htext (1.4 -0.15){$T_a$} \htext (3.8 -0.15){$T_0$} \htext (-0.15 1.45){$a$} \htext (-0.2 0.5){$S_0$} \htext (4.9 0.5){$S_n$} \htext (4.8 -0.15){$n$}

\move (0 0.6) \lvec (0.2 0.9) \lvec (0.4 0.6) \lvec (0.6 0.9) \lvec (0.8 1.2) \lvec (1 0.9) \lvec (1.2 1.2) \lvec (1.4 1.5) \lvec (1.6 1.2) \lvec (1.8 0.9) \lvec (2 1.2) \lvec (2.2 1.5) \lvec (2.4 1.2) \lvec (2.6 0.9) \lvec
(2.8 1.2) \lvec (3 0.9) \lvec (3.2 0.6) \lvec (3.4 0.9) \lvec (3.6 0.6) \lvec (3.8 0.3) \lvec (4 0) \lvec (4.2 -0.3) \lvec (4.4 0) \lvec (4.6 0.3) \lvec (4.8 0.6)


\lpatt (0.05 0.05)

\move (0 1.5) \lvec(5 1.5) \move (1.4 1.4) \lvec (1.4 0)


\move (0 2)

}

The figure illustrates a path of the random walk-although, in the case of the game, it finishes at the instant $T_0$, the time of bankruptcy! Let $p_r = \pro (T_a < T_0)$ when $S_0 = r$, for $0 \leq r \leq a$, so that we
have the boundary conditions $p_a = 1$ and $p_0 = 0$.

A general rule in problems of this type in probability may be summed up as `condition on the first thing that happens', which here would be a shorthand for using the law of total probability (Theorem
\ref{thm:law_total_probability}) to express the probability conditional on the outcome of the first play of the game, that is, whether $X_1 = 1$ or $X_1 = -1$, or equivalently, $S_1 = r+1$ or $S_1 = r-1$. Thus, for $0 < r <
a$, \be p_r = \pro (T_a < T_0 | S_1 = r + 1) \pro(X_1 = 1) + \pro(T_a < T_0 | S_1 = r - 1) \pro (X_1 = -1) = p \cdot p_{r+1} + q \cdot p_{r-1}. \ee

The auxiliary equation\footnote{details needed in ODE.} for this relation is $px^2 - x +q = 0$, and since $p+q = 1$, this may be factored as $(x - 1)(px - q) = 0$ to give roots $x = 1$ and $x = q/p$.

Case $p \neq q$: the roots are distinct and the general solution is of the form $p_r = A+B (q/p)^r$ for some constants $A$ and $B$, the boundary conditions at $r = a$ and $r = 0$, fix $A$ and $B$ and we conclude that \be
p_r = \pro (T_a < T_0) = \frac{1 - (q/p)^r}{1 - (q/p)^a},\quad 0 \leq r \leq a. \ee

Case $p = q = \frac 12$: here $x = 1$ is a repeated root of the auxiliary equation so that the general solution of the recurrence relation is $p_r = A+Br$, which, after using the boundary conditions, leads to the solution
$p_r = r/a$, $0 \leq r \leq a$.

We do not know necessarily that at least one of $T_0$ and $T_a$ must be finite, but if we interchange $p$ and $q$ and replace $r$ by $a - r$, (or just calculate directly as above) we may obtain, for $S_0 = r$, $0 \leq r
\leq a$, that \be \pro (T_0 < T_a) = \left\{\ba{ll}
\frac{(q/p)^r - (q/p)^a}{1 - (q/p)^a} \quad \quad & p \neq q,\\
1 - r/a & p = q = \frac 12. \ea\right. \ee

It follows, in both cases, that $\pro(T_a < T_0) + \pro (T_0 < T_a) = 1$, so that at least one of the the two barriers, 0 or $a$, must be reached with certainty.
\end{example}








\section{Summary}

\section{Problems}

\subsection{Random walks}

\begin{problem}[ticket line, see \cite{Zhou_2008}.$P_{117}$]\label{exe:ticket_line}
At a theater ticket office, $2n$ people are waiting to buy tickets, $n$ of them have only \$5 bills and the other $n$ people have only \$10 bills. The ticket seller has no change to start with. If
each person buys one \$5 ticket, what is the probability that all people will be able to buy their tickets without having to change positions?
\end{problem}

\begin{solution}[\bf Solution.]
Assign +1 to the $n$ people with \$5 bills and -1 to the $n$ people with \$10 bills. Consider the process as a random walk. Let $(a,b)$ represent that after $a$ steps, the walk ends at $b$. So we
start at $(0,0)$ and reaches $(2n,0)$ after $2n$ steps.

For these $2n$ steps, we need to choose $n$ steps as +1, so there are $\binom{2n}{n} = \frac{(2n)!}{n!n!}$ possible paths. We are interested in the paths that have the property $b\geq 0$, $\forall
0<a<2n$ steps. It is easier to calculate the number of complement paths that reach $b=-1$, $\exists 0<a<2n$.

\centertexdraw{

\drawdim in

\def\bdot {\fcir f:0 r:0.03 }
\arrowheadtype t:F \arrowheadsize l:0.08 w:0.04
\linewd 0.01 \setgray 0

\move (-0.2 0) \avec(4.9 0)
\move (0 -1.2) \avec(0 1)

\move (4.5 -0.5) \bdot
%\move (4.2 -0.3) \bdot
%\move (4.4 0.1) \bdot
%\move (4.6 0.8) \bdot
%\move (4.8 1.6) \bdot

%\htext (0.4 -0.15){$S_1$}
%\htext (1.4 -0.15){$T_1$}
%\htext (1.8 -0.15){$S_2$}
%\htext (2.4 -0.15){$T_2$}
%\htext (3.8 -0.15){$S_3$}
%\htext (4.8 -0.15){$T_3$}

\htext (4.4 -0.15){$2n$}
%\htext (-0.15 0.5){$a$}
%\htext (-0.2 0.5){$S_0$}
%\htext (4.9 0.5){$S_k$}
%\htext (4.8 -0.15){$k$}

\move (0 0) \lvec(0.5 0.5)
\move (0.5 0.5)\lvec(1 0)
\move (1 0)\lvec(1.25 0.25)
\move (1.25 0.25)\lvec(2 -0.5)
\move (2 -0.5) \lvec(3.25 0.75)
\move (3.25 0.75) \lvec (4 0)
\move (4 0) \lvec(4.25 0.25)
\move (4.25 0.25) \lvec (4.5 0)

\lpatt (0.05 0.05)

%\move ( 1.5) \lvec(5 1.5)
\move (0 -0.25) \lvec(4.9 -0.25)

%\move (0.4 0.4) \lvec (0.4 0)
%\move (1.4 1.6) \lvec (1.4 0)
%\move (1.8 0.2) \lvec (1.8 0)
%\move (2.4 1.6) \lvec (2.4 0)
%\move (3.8 0.3) \lvec (3.8 0)
%\move (4.8 1.6) \lvec (4.8 0)

\move (1.75 -0.25)\lvec(2 0)\lvec(3.25 -1.25) \lvec (4 -0.5) \lvec(4.25 -0.75) \lvec (4.5 -0.5)

%\move (0 0.6) \lvec (0.2 0.9) \lvec (0.4 0.4) \lvec (0.6 0.9) \lvec (0.8 1.2) \lvec (1 0.6) \lvec (1.2 1.2) \lvec (1.4 1.6) \lvec (1.6 0.9) \lvec (1.8 0.2) \lvec (2 0.9) \lvec (2.2 1.4) \lvec (2.4 1.6) \lvec (2.6 1.4) \lvec (2.8 1.2) \lvec (3 0.9) \lvec (3.2 0.6) \lvec (3.4 0.9) \lvec (3.6 0.6) \lvec (3.8 0.3) \lvec (4 0) \lvec (4.2 -0.3) \lvec (4.4 0.1) \lvec (4.6 0.8) \lvec (4.8 1.6)

\move (0 1)

}

As shown in the figure, if we reflect the path across the line $y=-1$ after a path first reach -1., for every path that reaches $(2n,0)$ at step $2n$, we have one corresponding reflected path that
reaches $(2n,-2)$ at step $(2n)$. For a path to reach $(2n,2)$, there are $(n-1)$ steps of +1 and $(n+1)$ steps of -1. So there $\binom{2n}{n-1} = \frac{(2n)!}{(n-1)!(n+1)!}$ such paths. The number
of paths that have the property $b=-1$, $\exists 0<a<2n$, given that the path reaches $(2n,0)$ is also $\binom{2n}{n-1}$ and the number of paths that have the property $b\geq 0$, $\forall 0<a<2n$ is
\be
\binom{2n}{n} - \binom{2n}{n-1} = \binom{2n}{n} - \frac n{n+1}\binom{2n}{n} = \frac 1{n+1}\binom{2n}{n}.
\ee

Hence, the probability that all people will be able to buy their tickets without have to change positions is $1/(n+1)$.

Alternatively, we can apply Proposition \ref{pro:random_walk_number_paths_greater_equal_than_zero}.
\end{solution}
