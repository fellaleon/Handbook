\chapter{Brownian Motions}

This chapter is devoted to the construction and some properties of one of probability theory's most fundamental objects, Brownian motion (also known as Wiener Process).


\section{History and Definition}


Brownian motion earned its name after R. Brown, who observed around 1827 that tiny particles of pollen in water have an extremely erratic motion. It was observed by Physicists that this was due to an important number of random shocks undertaken by the particles from the (much smaller) water molecules in motion in the liquid. A. Einstein established in 1905 the first mathematical basis for Brownian motion, by showing that it must be an isotropic Gaussian process. The first rigorous mathematical construction of Brownian motion is due to N. Wiener in 1923, using Fourier theory. \levy\ studied the sample path properties of Brownian motion, and Kakutani and Doob made the link with potential theory. It\^o's calculus was developed in 1950.

%Our treatment follows later ideas of \levy\ and Kolmogorov.


\subsection{Definitions}

We now start to define and study Brownian motions on Euclidean space.

\begin{definition}[standard Brownian motion\index{Brownian motion!standard}]\label{def:standard_brownian_motion_d}
An $\R^d$-valued stochastic process $(B_t)_{t \geq 0}$ is called a standard Brownian motion if it is a (sample) continuous adapted process (has continuous sample path a.s.), that satisfies the following conditions:
\ben
\item [(i)] $B_0 = 0$ a.s.,
\item [(ii)] for every $0 = t_0 \leq t_1 \leq t_2 \leq \dots \leq t_k$, the increments $(B_{t_1} - B_{t_0} ,B_{t_2} - B_{t_1} ,\dots, B_{t_k} - B_{t_{k-1}})$ are independent,
\item [(iii)] for every $t, s \geq 0$, the law of $B_{t+s} - B_t$ is Gaussian with mean 0 and covariance $sI_d$.
\een
\end{definition}

\begin{remark}
The term ``standard" refers to the choice $B_0 = 0$ a.s..%the fact that $B_1$ is normalized to have variance $I_d$, and
\end{remark}

\begin{definition}[Brownian motion\index{Brownian motion}]\label{def:brownian_motion_d}
An $\R^d$-valued stochastic process $(B_t)_{t \geq 0}$ is called a Brownian motion if it is a (sample) continuous adapted process (has continuous sample path a.s.), that satisfies the following conditions:
\ben
\item [(i)] for every $0 = t_0 \leq t_1 \leq t_2 \leq \dots \leq t_k$, the increments $(B_{t_1} - B_{t_0} ,B_{t_2} - B_{t_1} ,\dots, B_{t_k} - B_{t_{k-1}})$ are independent,
\item [(ii)] for every $t, s \geq 0$, the law of $B_{t+s} - B_t$ is Gaussian with mean 0 and covariance $sI_d$.%the law of $B_{t+s} - B_t$ is Gaussian with mean 0 and covariance $cI_d$ for some $c$ proportional to $s$.
\een
\end{definition}

\begin{definition}[transition density of standard Brownian motion\index{transition density!Brownian motion}]\label{def:transition density_standard_brownian_motion}
Let $\dabs{\cdot}$ be the Euclidean norm\footnote{need definition} on $\R^d$ and for $t > 0$ and $x \in \R^d$, define
\be
p_t(x) = \frac 1{(2\pi t)^{d/2}} \exp\brb{-\frac{\dabs{x}^2}{2t}},
\ee
which is the density of the Gaussian distribution $\sN(0, tI_d)$ with mean 0 and covariance matrix $tI_d$ (see Definition \ref{def:standard_gaussian_density}). By convention, the Gaussian law $\sN(m, 0)$ is the Dirac mass at $m$.
\end{definition}

\begin{remark}
See Definition \ref{def:transition density_d_dimensional_brownian_motion} for more general definition. Also see the heat kernel in Definition \ref{def:heat_kernel}.
\end{remark}


%As a first approximation, consider $S_n = X_1 + \dots+ X_n$, where the $X_j$ are $\R^d$-valued i.i.d. r.v.'s and $X_1 = \pm e_i$ with probability $\frac 1{2d}$, where $e_i = (0, \dots, 0, 1, 0,\dots, 0)$. By the CLT,
%\be
%\frac{S_n}{\sqrt{n}} \stackrel{(d)}{\to} \sN (0, I_d ) \stackrel{(d)}{=} N,
%\ee
%where $N_i$ are i.i.d. $\sN (0, 1)$. Consider $B^{(n)}_t = \frac{S_{\floor{tn}}}{\sqrt{n}}$ for $t \geq 0$, so in particular $\sL(B^{(n)}_1) \stackrel{(w)}{\to}  N (0, I_d)$. Brownian motion is a continuous process $B$ such that '$B^{(n)} \to B$ in distribution.'

%The finite dimensional marginal distributions $\sL(B_{t_1} , \dots, B_{t_k})$ for every $k$ should be given by
%\be
%\lim_{n\to \infty} \sL\brb{B^{(n)}_{t_1} ,\dots, B^{(n)}_{t_k} }.
%\ee



%\begin{proof}[\bf Proof]
%(For $d = 1$.) Using characteristic functions. Let $\xi \in \R^k$.
%\beast
%\E\bsb{\exp\brb{i\sum^k_{j=1} \xi_j(B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}} )}} & = & \prod^k_{j=1} \E\bsb{\exp\brb{i\xi_j(B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}})}} = \prod^k_{j=1} \E\bsb{\exp\brb{i\xi_j\sum^{\floor{nt_j}-\floor{nt_{j-1}}}_{r=1} X_r}} \\
%& \stackrel{CLT}{\to } & \prod^k_{j=1} \E\bsb{\exp\brb{i\xi_jN_j)}} = \E \bsb{\exp\brb{i\sum^k_{j=1}\xi_jN_j}}
%\eeast
%since $B^{(n)}_{t_i} -B^{(n)}_{t_{i-1}} = \frac 1{\sqrt{n}} \sum^{\floor{nt_j}}_{r=\floor{nt_{j-1}}+1} X_r$, so the $\{B^{(n)}_{t_i} -B^{(n)}_{t_{i-1}}\}$ are $k$ independent r.v.'s.

%And further
%\be
%B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}} = \frac 1{\sqrt{n}} \sum^{\floor{nt_j} -\floor{nt_{j-1}}}_{r=1}X_r.
%\ee

%Conclude with L\'evy's Theorem.
%\end{proof}

%
%\begin{definition}
%Let $(B_t )_{t\geq 0}$ be a stochastic process in $\R^d$. We say that $(B_t)$ is a standard Brownian motion if
%\ben
%\item [(i)] $B_0 = 0$;
%\item [(ii)] For $0 = t_0 < t_1 < \dots < t_k$, $(B_{t_i} - B_{t_{i-1}}, 1 \leq i \leq k)$ are independent;
%\item [(iii)] $B_t - B_s$ has distribution $\sN (0, (t -s)I_d )$ for all $s < t$.
%\item [(iv)] For all $\omega \in \Omega$, $t \to B_t (\omega)$ is continuous.
%\een
%\end{definition}

%Here 'standard' refers to the fact that $B_0 = 0$ and $\cov B_1 = I_d$. The finite dimensional marginal distribution for $t_1 < \dots < t_k$ is the law of $(B_{t_1},\dots, B_{t_k})$. Let
%\be
%p_t (x) = \frac 1{(2\pi t)^{\frac d2}} e^{-\frac{\abs{x}^2}{2t}}
%\ee
%be the probability density function of $\sN (0, t I_d )$. Then

\subsection{Microscopical approach to Brownian motion}

%In order to motivate the introduction of this object, we first begin by a ``microscopical" depiction of Brownian motion.

Suppose $(X_n)_{n \geq 0}$ is a sequence of $\R^d$ valued random variables with mean 0 and covariance matrix $\sigma^2I_d$, which is the identity matrix in $d$ dimensions, for some $\sigma^2 > 0$. Namely, if $X_1 = (X_1^1, \dots,X^d_1)$,
\be
\E\brb{X^i_1} = 0,\quad \E\brb{X^i_1X^j_1} = \sigma^2\delta_{ij},\quad 1 \leq i, j \leq d.
\ee

We interpret $X_n$ as the spatial displacement resulting from the shocks due to water molecules during the $n$-th time interval, and the fact that the covariance matrix is scalar stands for an isotropy assumption (no direction of space is privileged).

From this, we let $S_n = X_1 + \dots + X_n$ and we embed this discrete-time process into continuous time by letting
\be
B^{(n)}_t = \frac 1{\sqrt{n}}S_{\floor{nt}},\quad t \geq 0.
\ee


%\qcutline

\begin{proposition}\label{pro:marginal_distribution_brownian_motion}
Let $0 = t_1 < t_2 < \dots < t_k$. Then the finite marginal distributions of $B^{(n)}$ with respect to times $t_1,\dots, t_k$ converge weakly as $n \to\infty$. More precisely, if $f$ is a bounded continuous function, and letting $x_0 = 0$, $t_0 = 0$,
\be
\E\bsb{f\brb{B^{(n)}_{t_1} ,\dots,B^{(n)}_{t_k} }} \to \int_{(\R^d)^k} f(x_1,\dots, x_k) \prod_{1\leq i\leq k} p_{\sigma^2(t_i-t_{i-1})}(x_i - x_{i-1})dx_i\quad \text{as }n \to \infty.
\ee
where $p$ is transition density of standard Brownian motion (see Definition \ref{def:transition density_standard_brownian_motion}).

Otherwise said, $(B^{(n)}_{t_1} ,\dots,B^{(n)}_{t_k})$ converges in distribution to $(G_1,G_2, \dots,G_k)$, which is a random vector whose law is characterized by the fact that $(G_1,G_2-G_1, \dots,G_k-G_{k-1}) = (N_1,\dots, N_k)$ are independent centered Gaussian random variables with respective covariance matrices $\sigma^2(t_i - t_{i-1})I_d$. ($(B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}}, 1 \leq i \leq k)$ converges in distribution to $(N_i)_{1 \leq i \leq k}$, where the $N_i$'s are independent and $N_i \sim \sN (0, \sigma^2(t_i - t_{i-1})I_d)$. In particular, the law of $B^{(n)}_t$, $\sL(B^{(n)}_t )\to \sN (0, \sigma^2tI_d)$.)
\end{proposition}

\begin{remark}
This suggests that $B^{(n)}$ should converge to a process $B$ whose increments are independent and Gaussian with covariances dictated by the above formula. This will be set in a rigorous way later with Donsker's invariance theorem (Theorem \ref{thm:donsker_invariance_principle}).%in the course,
\end{remark}

\begin{proof}[\bf Proof]
With the notations of the theorem, we first check that $\brb{B^{(n)}_{t_1} ,B^{(n)}_{t_2}-B^{(n)}_{t_1} ,\dots,B^{(n)}_{t_k} - B^{(n)}_{t_{k-1}}}$ is a sequence of independent random variables. Indeed, one has for $1 \leq i \leq k$
\be
B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}} = \frac 1{\sqrt{n}} \sum^{\floor{nt_i}}_{j=\floor{nt_{i-1}}+1} X_j ,
\ee
and the independence follows by the fact that $(X_j , j \geq 0)$ is an i.i.d. family. Even better, we have the identity in distribution for the $i$-th increment
\be
B^{(n)}_{t_i} - B^{(n)}_{t_{i-1}} \stackrel{d}{=} \frac{\sqrt{\floor{nt_i} - \floor{nt_{i-1}}}}{\sqrt{n}} \frac{ 1}{\sqrt{\floor{nt_i} - \floor{nt_{i-1}}}} \sum^{\floor{nt_i}-\floor{nt_{i-1}}}_{j=1} X_j
\ee
and the central limit theorem (Theorem \ref{thm:central_limit}) shows that this converges in distribution to $G_j - G_{j-1}$ with a Gaussian law $\sN(0, \sigma^2(t_i - t_{i-1})I_d)$.

Summing up our study, and introducing characteristic functions, we have shown that for every $\xi = (\xi_j)_{1 \leq j \leq k}$, since $B^{(n)}_{t_j} - B^{(n)}_{t_{j-1}}$ are independent,
\beast
\E\brb{\exp\brb{i\sum^{k}_{j=1} \xi_j\brb{B^{(n)}_{t_j} - B^{(n)}_{t_{j-1}}}}}  & = & \prod^k_{j=1} \E\brb{\exp\brb{i\xi_j\brb{B^{(n)}_{t_j} - B^{(n)}_{t_{j-1}}}}} \quad \text{(by Theorem \ref{thm:characteristic_function})}\\
& \to & \prod^k_{j=1} \E\brb{\exp (i\xi_j(G_j - G_{j-1}))} \quad \text{(by Theorem \ref{thm:equivalent_modes_of_convergence})}\\
& = & \E\brb{\exp\brb{i\sum^k_{j=1} \xi_j(G_j - G_{j-1})}}\quad \text{(by Theorem \ref{thm:characteristic_function})}.
\eeast
as we assume $G_j - G_{j-1}$ are independent. %where $G_1,\dots,G_k$ is distributed as in the statement of the proposition.
By \levy's continuity theorem (Theorem \ref{thm:levy_continuity}) we deduce that increments of $B^{(n)}$ between times $t_i$ converge to increments of the sequence $G_i$ weakly. %, which is easily equivalent to the statement.{thm:levy_convergence}
That is, by Corollary \ref{cor:convergence_in_distribution_iff_expectation}, for any bounded continuous function $f$,
\be
\E\brb{f(B_{t_1} ,\dots, B_{t_k} )} \to \int_{(\R^d)^k} f(x_1, \dots, x_k) \prod^k_{i=1} p_{\sigma^2(t_i-t_{i-1})} (x_i - x_{i-1})d x_1 \dots d x_k
\ee
as $n\to \infty$.  %With a appropriate change of variables, we may write %\be %\E[G(B_{t_i} - B_{t_{i-1}} , 1 \leq i \leq k)] = \int_{(\R^d)^k} G(x_1, \dots, x_k) \prod^k_{i=1} p_{t_i-t_{i-1}} (x_i)d x_1 \dots d x_k %\ee
\end{proof}



\subsection{Existence and uniqueness of Brownian motion}

The characteristic properties (i), (ii), (iii) exactly amount to say that the finite dimensional marginals of a Brownian motion are given by the formula of Proposition \ref{pro:marginal_distribution_brownian_motion}. Therefore the law of the Brownian motion is uniquely determined by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}. We now show Wiener's theorem that Brownian motion exists!

\begin{theorem}[Wiener theorem]\label{thm:wiener_brownian_motion_existence}
There exists a standard Brownian motion on some probability space.
\end{theorem}

\begin{proof}[\bf Proof]
We will first prove the theorem in dimension $d = 1$ and construct a process $(B_t)_{t\in [0,1]}$ satisfying the properties of a Brownian motion. This proof is essentially due to P. \levy\ in 1948. The proof given by Ciesielski\cite{Ciesielski_1961} is the ultimate refinement of Wiener's original idea of representing Brownian motion as a random Fourier series.

%Before we start, we will need the following lemma, which is left as an exercise.
%\begin{lemma}\label{lem:gaussian_boundary}
%Let $\sN$ be a standard Gaussian random variable. Then
%\be
%\frac{x^{-1} - x^{-3}}{\sqrt{2\pi }} e^{-x^2/2} \leq \pro(\sN > x) \leq \frac{x^{-1}}{\sqrt{2\pi }} e^{-x^2/2}.
%\ee
%\end{lemma}

Let $\sD_0 = \{0, 1\}$, $\sD_n = \{k2^{-n}, 0 \leq k \leq 2^n\}$ for $n \geq 1$, and $\sD = \bigcup_{n\geq 0} \sD_n$ be the set of dyadic rational numbers in $[0, 1]$. On some probability space $(\Omega,\sF,\pro)$, let $(Z_d)_{d \in D}$ be a collection of i.i.d. random variables all having a Gaussian distribution $\sN(0, 1)$ with mean 0 and variance 1.

It is a well-known and important fact that if the random variables $X_1,X_2,\dots$ are linear combinations of independent centered Gaussian random variables, then $X_1,X_2,\dots$ are independent if and only if they are pairwise uncorrelated, namely $\cov(X_i,X_j) = \E\brb{X_iX_j} = 0$ for every $i \neq j$\footnote{need theorem}.


\begin{center}
\psset{yunit=3cm,xunit=3cm}
\def\myLine#1(#2)(#3)#4{{%
  \pnode(#2){myA}\pnode(#3){myB}%
  \pcline[linestyle=dashed,tbarsize=15pt]{#1}(myA)(myB)%
  \ncput*{#4}}}
\begin{pspicture}(-0.2,-0.2)(2,1.2)
% \psgrid[griddots=10,gridlabels=0pt, subgriddiv=0]
\psaxes[labels=none,ticks=none]{->}(0,0)(-0.2,0)(2.2,1.2)%Dy=0.25,dy=0.25

\pstGeonode[PointSymbol=*,PointName=none,dotscale=1](0,0){A}(1,1.1){B}(2,0.6){C}
\rput[lb](0,-0.15){$d^-$}
\rput[lb](2,-0.15){$d^+$}
\rput[lb](1,-0.15){$d$}
\rput[lb](-0.75,0.2){$X^n_{d^-}(=X^{n-1}_{d^-})$}
\rput[lb](1.8,0.7){$X^n_{d^+}(=X^{n-1}_{d^+})$}
\rput[lb](0.9,1.15){$X^n_{d}$}
%\rput[lb](0.82,0.3){$\left. \ba{l} \\ \\ \\ \\ \ea\right\}\frac{Z_d}{2^{(n+1)/2}}$}

\myLine{|<->|}(0.8,0.3)(0.8,1.1){$\frac{Z_d}{2^{(n+1)/2}}$}

\psline[linestyle=dashed](0,0)(2,0.6)
\psline[linestyle=dashed](1,1.1)(1,0)
\psline[linestyle=dashed](2,0.6)(2,0)
\end{pspicture}
\end{center}


We set $X^0_0 = 0$ and $X^0_1 = Z_1$. Inductively, given $(X^{n-1}_d)_{d \in \sD_{n-1}}$, we build $(X^n_d)_{d \in \sD_n}$ in such a way that $(X^n_d)_{d \in \sD_n}$ satisfies (i), (ii), (iii) in the definition of the Brownian motion (where the instants under consideration are taken in $\sD_n$). To this end, take $d \in \sD_n \bs \sD_{n-1}$, and let $d^- = d - 2^{-n}$ and $d^+ = d + 2^{-n}$ so that $d^-$, $d^+$ are consecutive dyadic numbers in $\sD_{n-1}$. Then define:
\be
X^n_d = \frac{X^{n-1}_{d^-} + X^{n-1}_{d^+}}2 + \frac{Z_d}{2^{(n+1)/2}} .
\ee
and put $X^{n}_{d^-} = X^{n-1}_{d^-}$ and $X^{n}_{d^+} = X^{n-1}_{d^+}$. Note that with these definitions,
\be
X^{n}_{d} - X^{n}_{d^-} = N_d + N'_d,\quad \quad X^{n}_{d^+} - X^{n}_{d} = N_d - N'_d,%\quad \quad (*)
\ee
where $N_d := (X^{n-1}_{d^+} - X^{n-1}_{d^-} )/2$, $N'_d  = Z_d/2^{(n+1)/2}$ are by the induction hypothesis two independent centered Gaussian random variables with variance $2^{-n-1}$. From this, one deduces
\be
\cov(N_d + N'_d ,N_d - N'_d) = \var(N_d) - \var(N'_d) = 0,
\ee
so that the increments $X^n_d - X^n_{d^-}$ and $X^n_{d^+} - X^n_{d}$ are independent with variance $2^{-n}$, as should be.

Moreover, these increments are independent of the increments $X^n_{d'+2^{-n}} - X^n_{d'}$ for $d' \in \sD_{n-1}$ with $d' \neq d^-$ and of $Z_{d'}$ with $d'\in\sD_n\bs \sD_{n-1}$, $d' \neq d$ so they are independent of the increments $X_{d''+2^{-n}} - X_{d''}$ for $d'' \in \sD_n$ with $d'' \notin \{d^-, d\}$. This allows the induction argument to proceed one step further.

We have thus defined a process $(X^n_d)_{d \in \sD_n}$ which satisfies properties (i), (ii) and (iii) for all dydadic times $t_1, t_2,\dots, t_k \in \sD_n$. Observe that if $\sD \in \sD_n$, $X^m_d = X^n_d$ for all $m \geq n$. Hence for all $d \in \sD$,
\be
X_d = \lim_{n\to \infty} X^n_d
\ee
is well-defined and the process $(X_d)_{d \in \sD}$ obviously satisfies (i), (ii) and (iii).


To extend this to a process defined on the entire interval $[0, 1]$, we proceed as follows. Define, for each $n \geq 0$, a process $Y^n_t$, $0 \leq t \leq 1$ to be the linear interpolation of the values $(X_d)_{d \in\sD_n}$ the dyadic times at level $n$ (with straight line connecting $X_d$ for $d\in \sD_n$).


\begin{center}
\psset{yunit=2cm,xunit=2cm}
\begin{pspicture}(-0.4,-0.2)(2.2,1.6)
% \psgrid[griddots=10,gridlabels=0pt, subgriddiv=0]
\psaxes[labels=none,ticks=none]{->}(0,0)(-0.2,-0.2)(2,1.6)%Dy=0.25,dy=0.25
\psline(0,0)(2,0.8)
\rput[lb](1,-0.2){$Y^0$}
\end{pspicture}
\begin{pspicture}(-0.4,-0.2)(2.2,1.6)
% \psgrid[griddots=10,gridlabels=0pt, subgriddiv=0]
\psaxes[labels=none,ticks=none]{->}(0,0)(-0.2,-0.2)(2,1.6)%Dy=0.25,dy=0.25
\psline[linestyle=dashed](0,0)(2,0.8)
\psline(0,0)(1,1.5)(2,0.8)
\rput[lb](1,-0.2){$Y^1$}
\end{pspicture}
\begin{pspicture}(-0.4,-0.2)(2.2,1.6)
% \psgrid[griddots=10,gridlabels=0pt, subgriddiv=0]
\psaxes[labels=none,ticks=none]{->}(0,0)(-0.2,-0.2)(2,1.6)%Dy=0.25,dy=0.25
\psline[linestyle=dashed](0,0)(1,1.5)(2,0.8)
\psline(0,0)(0.5,1.1)(1,1.5)(1.5,0.9)(2,0.8)
\rput[lb](1,-0.2){$Y^2$}
\end{pspicture}
\end{center}

Note that if $d \in \sD$, say $d \in \sD_m$ with $m \geq 0$, then for any $n \geq m$, $Y^n_d = Y^m_d = B_d$. Furthermore, define an event $A_n$ by
\be
A_n = \bra{\sup_{0\leq t\leq 1} \abs{Y^n_t - Y^{n-1}_t} > 2^{-n/4}}.
\ee

We then have, let $Z$ be a standard gaussian random variable.
\beast
\pro(A_n) & = & \pro\brb{\bigcup^{2^{n-1}-1}_{j=0} \bra{\sup_{t\in[(2j)2^{-n},(2j+2)2^{-n}]} \abs{Y^n_t - Y^{n-1}_t} > 2^{-n/4}}}  \\
& \leq & \sum^{2^{n-1}-1}_{j=0} \pro\brb{\sup_{t\in[(2j)2^{-n},(2j+2)2^{-n}]} \abs{Y^n_t - Y^{n-1}_t} > 2^{-n/4}} \leq \sum^{2^{n-1}-1}_{j=0} \pro\brb{\frac{\abs{Z_{(2j+1)2^{-n}}}}{2^{(n+1)/2}} > 2^{-n/4}}\\
& \leq & \sum^{2^{n-1}-1}_{j=0} \pro\brb{\abs{Z} > 2^{(n+2)/4}}  \leq \sum^{2^{n-1}-1}_{j=0} \pi^{-1/2} 2^{-n/4-1} \exp\brb{-2^{n/2}} \leq \pi^{-1/2} 2^{3n/4} \exp\brb{-2^{n/2}}
\eeast
by Proposition \ref{pro:bound_of_gaussian_law}. We conclude that $\sum\limits^\infty_{n=0} \pro(A_n) < \infty$ and by Borel-Cantelli theorem (Theorem \ref{lem:borel_cantelli_1_probability}), the events $A_n$ occur only finitely often. We deduce immediately that the sequence of functions $Y^n$ is almost surely Cauchy in $C[0, 1]$ equipped with the topology of uniform convergence, and hence $Y^n$ converges toward a (sample) continuous limit function $(B_t)_{0 \leq t \leq 1}$ uniformly, almost surely. If $\omega$ is in the null set, we simply define $B\equiv 0$.

Since $Y^n_t$ is constantly equal to $X_t$ for $t \in \sD$ and for $n$ large enough, it must be that $X_t = B_t$ for all $t \in \sD$. Thus $B$ is a continuous extension of $X$. (The extension of $(X_d, d \in D)$ could have been obtained by appealing to the existence of a continuous modification (version), whose existence is provided by Kolmogorov's criterion below\footnote{need theorem, see unsorted part}.%This can also obtained by the following argument.

For $s, t \in \sD$, $s < t$, $\E\brb{\abs{X_s-X_t }^p} = \abs{t-s}^{\frac p2} \E\brb{N^p}$ for $p > 2$, where $N \sim \sN (0, 1)$, since $X_s - X_t \sim \sN (0, t -s) \sim \sqrt{t -s}\sN (0, 1)$. But this latter is $C_p\abs{t -s}^{1+(\frac p2 -1)}$ for some $C_p > 0$. By the Kolmogorov Criterion (Theorem \ref{thm:kolmogorov_continuity_criterion}), there exists a.s. a continuous function $(B_t)_{t\in [0,1]}$ that extends $(X_d)_{d\in \sD}$.) %Let $\Omega_0 = \{\omega| (B_d (\omega))_{d\in D}\}$ does not have a continuous extension to $[0, 1]$. On $\Omega_0$, let $B_t (\omega) = 0$ for all $t \in [0, 1]$. (Notice that $\pro(\Omega_0) = 0$.)

%and we still denote this extension by $X$.

We now deduce properties (i), (ii) and (iii) for $B$ by continuity and the fact that $Y^n$ satisfies these properties. Indeed, let $k \geq 1$ and let $0 < t_1 < t_2 \dots < t_k < \infty$. Fix $\alpha_1, \alpha_2,\dots, \alpha_k \geq 0$. For every $1 \geq i \geq k$, fix a sequence $(d^{(n)}_i)_t$ such that $\lim_{n\to \infty} d^{(n)}_i = t_i$, and assume (since $\sD$ is dense in $[0, 1]$) that $d^{(n)}_i\in \sD$ and $t_{i-1} < d^{(n)}_i \leq t_i$. Thus, $X_{d^{(n)}_i} \to B_{t_i}$ a.s.. Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}),
\beast
& & \E\brb{\exp \brb{i\alpha_1 B_{t_1} + i\alpha_2(B_{t_2} - B_{t_1}) +\dots+ i\alpha_k(B_{t_k} - B_{t_{k-1}})}}\\
& = & \lim_{n\to \infty} \E\brb{\exp\brb{i\alpha_1X_{d^{(n)}_1} + i\alpha_2\brb{X_{d^{(n)}_2} - X_{d^{(n)}_1}}  +\dots+ i\alpha_k\brb{X_{d^{(n)}_k}- X_{d^{(n)}_{k-1}}}}}\\
& = & \lim_{n\to \infty} \exp\brb{-\frac{\alpha^2_1}2 d^{(n)}_1 -\dots- \frac{\alpha^2_k}2 \brb{d^{(n)}_k - d^{(n)}_{k-1}}} = \exp\brb{-\frac{\alpha^2_1}2 t_1 -\dots - -\frac{\alpha^2_k}2(t_k - t_{k-1})}.
\eeast

Thus, we can say $B$ is a standard Brownian motion on $[0,1]$ by \levy\ convergence theorem (Theorem \ref{thm:levy_convergence}).

It is now easy to construct a Brownian motion indexed by $\R^+$. Simply take independent standard Brownian motions $(B^i_t)_{t\in [0 ,1],i \geq 0}$ as we just constructed, and let
\be
B_t = \sum^{\floor{t}-1}_{i=0} B^i_1 + B^{\floor{t}}_{t-\floor{t}},\quad t \geq 0.
\ee

It is easy to check that this has the wanted properties\footnote{need details}.

Finally, it is straightforward to build a Brownian motion in $\R^d$, by taking $d$ independent copies $B^1,\dots,B^d$ of $B$ and checking that $((B^1_t ,\dots, ,B^d_t ), t \geq 0)$ is a Brownian motion in $\R^d$\footnote{need details}.
\end{proof}

%\begin{proof}[\bf Proof]
%Let $D_n = \{k2^{-n}| 0 \leq k < 2^n\}$ for $n \geq 1$, and $D_0 = \{0, 1\}$. Let us construct $(B_d)_{d\in D}$, where $D = \bigcup_n D_n$ are the dyadic rationals, that satisfy (i), (ii), and (iii) of the definition. (We are doing the case $d = 1$ on the time interval $[0, 1]$.) Let $(Z_d)_{d\in D}$ be a family of independent r.v.'s with distribution $\sN (0, 1)$. We'll construct $B_d$ such that for all $d$, $B_d \in \text{Vect}(Z_{d'} , d' \in D) =: G$. For $X_1,\dots, X_k \in G$, the $(X_i)$'s are independent if and only if they are pairwise orthogonal, i.e. $\E[X_iX_j] = 0$ for all $i \neq j$.

%The construction is by induction. For $n = 0$ take $B_0 = 0$ and $B_1 = Z_0$. Suppose now that $(B_d )_{d\in D_{n-1}}$ satisfies properties (i), (ii), and (iii) of the definition, and that $(B_d )_{d\in D_{n-1}}$ is independent of $(Z_d)_{d\in D\bs D_{n-1}}$. Let $d \in D_n \bs D_{n-1}$, and defined $d^- = d -2^{-n}$ and $d^+ = d +2^{-n}$, so $d^-, d^+ \in D_{n-1}$. Let $B_d = \frac 12 (B_{d^+} +B_{d^-})+ \frac 1{2^{\frac{n+1}2}} Z_d$. Now $B_d -B_{d^-}$, $B_d -B_{d^+}$ are independent $\sN (0, \frac 1{2^n})$ (since (check that) if $N_1$, $N_2$ are $\sN (0,\sigma^2)$ and independent then $N_1+N_2$ and $N_1-N_2$ are independent $\sN (0, 2\sigma^2)$. Indeed, $\cov(N_1 + N_2),N_1 - N_2) = \cov(N_1,N_1)-\cov(N_2,N_2) = 0$.)

%Further, the same method (check this) proves that these increments are independent of all other $B_{d'} - B_{d'^-}$ for $d'\in D\bs D_{n-1}$, so the induction step is proved. By induction we get $(B_d )_{d\in D}$ satisfying (i), (ii), (iii) of the definition.

%Let us check (i), (ii), (iii) for $(B_t)_{t\in [0,1]}$. (i) is trivial. Let $0 = t_0 < t_1 < \dots < t_k$, and consider $0 \leq t^n_1 < \dots < t^n_k$ such that $t^n_i \in D_n$ for all $n$ and $t^n_i \to t_i$ as $n\to \infty$. We know that $(B_{t^n_i} - B_{t^n_{i-1}}, 1 \leq i \leq k)$ are independent $\sN (0, t^n_i - t^n_{i-1})$ for all $n$.

%Claim. If $(N^n_1 , \dots,N^n_k)$ are independent Gaussian $N_i \sim \sN (0, (\sigma^n_i)^2)$, then if $\sigma^n_i \to \sigma_i$ then $(N^n_1 , \dots,N^n_k ) \to (N_1, \dots,N_k)$ in distribution, where $N_i$ are independent $N_i \sim \sN (0,\sigma^2_i)$.

%Indeed, use L\'evy's convergence theorem.

%By continuity of $B$, $B_{t^n_i} - B_{t^n_{i-1}} \to  B_{t_i} - B_{t_{i-1}}$ for every $\omega$, so by the lemma we do have $(B_{t_i} - B_{t_{i-1}}, 1 \leq i \leq k)$ are independent $\sN (0, t_i - t_{i-1})$, so $(B_t )_{t\in[0,1]}$ is a Brownian motion.

%To get a Brownian motion $(B_t)_{t\geq 0}$, take Brownian motions $(B^n_t)_{t\in [0,1]}$ independent on $[0, 1]$ for $n \geq 1$ and define $B_t = \sum^{\floor{t}-1}_{n=0} B^n_1 + B^{\floor{t}}_{t-\floor{t}}$ for $t \geq 0$. In higher dimensions $d > 1$ take $B^{(1)}, \dots, B^{(d)}$ independent Brownian motion in $\R$ and set $B_t = (B^{(1)}_t , \dots, B^{(d)}_t ), t \geq 0$, which is a Brownian motion in $\R^d$ (check).
%\end{proof}

%\begin{proof}[\bf Alternative proof]
%\end{proof}

\begin{theorem}\label{thm:uniqueness_brownian_motion_in_distribution}
The uniqueness of Brownian motion is in the distributional sense.
\end{theorem}

\begin{proof}[\bf Proof]
Direct result from Definition of Brownian motion (Definition \ref{def:standard_brownian_motion_d}) and Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}.%\footnote{add Wiener measure}
\end{proof}

\begin{remark}
Note that a process having the same finite dimensional marginal distribution as Brownian motion might not be a Brownian motion.
\end{remark}

\begin{example}
Suppose that $(B_t)_{t\geq 0}$ is a standard Brownian motion and $U$ is an independent random variable uniformly distributed on $[0,1]$. Then the process $(\wt{B}_t)_{t\geq 0}$ defined by
\be
\wt{B}_t = \left\{ \ba{ll}
B_t \quad\quad & t \neq U\\
0 & t = U
\ea\right.
\ee
has the same finite dimensional marginal distribution as Brownian motion, but is discontinuous if $B(U)\neq 0$, which happens with probability one, and hence it is not a Brownian motion.
\end{example}


\subsection{Wiener measure and Wiener space}


\begin{definition}[Wiener measure, Wiener space]\label{def:wiener_measure}
Let $W = C(\R^{++},\R^d) = C([0,\infty),\R^d)$ (space of continuous functions, that is, $f:[0,\infty) \to \R^d$) be the space, endowed with the $\sigma$-algebra $\sW :=\sB\brb{C([0,\infty))}$. %product $\sigma$-algebra $\sW$ (or the Borel $\sigma$-algebra associated with the compact-open topology (wrt time)).

Let $(B_t)_{t \geq 0}$ be a Brownian motion, and let $\W$ be the law of $B$. that is, for any $A \in \sW$,
\be
\W(A) := \pro\brb{B \in A} = \pro\brb{\omega:(B_t(\omega))_{t \geq 0}\in A}.
\ee

It is also true that $\sW_t = \sigma(B_s: s \leq t)$ and $\sW = \sigma(B_t: t\geq 0)$ where $B_t: W\to\R, B(\omega) \mapsto B_t(\omega)$.

Then $\W$ is called Wiener measure\index{Wiener measure} and $\brb{W,\sW,\W}$ is called Wiener space\index{Wiener space}.
More precisely, given a Brownian motion $B$, we can view it as a random variable taking values in $C([0,\infty),\R^d)$, the space of real-valued continuous functions on $[0,\infty)$. The law of $B$ is the measure $\W$ on $C([0,\infty))$ defined by $\W(A) = \pro\brb{B \in A}$ for all Borel subsets $A$ of $C([0,\infty))$.
\end{definition}

\begin{remark}
We must check that this definition makes sense (there is a bijection between Brownian motion and Wiener measure), i.e., that $\W$ does not depend on the construction of $B$. To see this, note that the finite-dimensional distributions (i.e., the joint law of $(B_{t_1} ,\dots,B_{t_n})$ are entirely specified by the definition of a Brownian motion.

Since the $\sigma$-field $\sW$ is generated by cylinder events of the form $\{B_{t_1} \in A_1,\dots,B_{t_n} \in A_n\}$ for $A_i \in \sB(\R^d)$, the right-hand side in the above display is indeed uniquely specified by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}.
\end{remark}


%Recalling Definition \ref{def:canonical_process}, we have

\begin{proposition}\label{pro:canoncial_process_brownian_motion_wiener_space}
Let $B$ be a Brownian motion wrt $(\sF_t)_{t\geq 0}$. Then the canonical process $Y$ of $B$ is a Brownian motion under Wiener's space.
\end{proposition}

\begin{proof}[\bf Proof]
Recalling ($*$) in Definition \ref{def:canonical_process} ($Y_t: B(\omega) \to B_t(\omega)$), we let $(\phi(\omega))(t) = B_t(\omega)$ for $\phi:\Omega \to (\R^d)^{\R^{++}},\ \omega \mapsto \phi(\omega)$\footnote{$(\R^d)^{\R^{++}}$ is a function space which maps $\R^{++}$ to $\R^d$} and $\phi(\omega) = B(\omega)$. Thus, for $A_i\in \sB(\R^d)$,
\beast
\W \brb{\phi(\omega) \in W:Y_{t_1}(\phi(\omega))\in A_1,\dots, Y_{t_n}(\phi(\omega))\in A_n}
& = & \W\brb{\phi(\omega) \in W:(\phi(\omega))(t_1) \in A_1,\dots, (\phi(\omega))(t_n)\in A_n}\\
& = & \W\brb{B(\omega)\in W: B_{t_1}(\omega)\in A_1,\dots B_{t_n}(\omega)\in A_n} \\
& = & \pro\brb{\omega \in \Omega: B_{t_1}(\omega)\in A_1,\dots B_{t_n}(\omega)\in A_n} \\
& = & \pro\brb{B_{t_1}\in A_1,\dots B_{t_n}\in A_n} . %\pro\circ \phi^{-1} \brb{Y_{t_1}\in A_1,\dots, Y_{t_n}\in A_n}
\eeast

Thus, two sides have the same distribution since they have the same finite dimensional marginal distribution (by Theorem \ref{thm:uniqueness_brownian_motion_in_distribution}).%(by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}).

Hence, we can see that the canonical process $Y$ is actually a Brownian motion under Wiener's measure.
\end{proof}

%since $B$ and $Y$ have the same finite dimensional marginal distributions,
%the law of process $B$ and $Y$ are the same by

\begin{remark}
This is the canonical construction of Brownian motion.
%\end{remark}



%\begin{definition}
%We now think of $Omega$ as our probability space. For $\omega\in \Omega$ define.
%\be
%X_t(\omega) = \omega(t), \quad t \geq 0
%\ee

%We call $(X_t(\omega), t \geq 0)$ the canonical process. Then $(X_t, t \geq 0)$, under the probability measure $\W$, is a Brownian motion.
%\end{definition}

%\begin{remark}

It is rarely the case in probability theory that we put some emphasis on the probability space on which a certain random process is constructed. (In all practical cases, we usually assume that such a random process is given to us). However the full advantage of specifying the probability space and measure will come to light when we deal with Girsanov's change of measure theorem.
\end{remark}

\begin{definition}
For $x \in \R^d$ we also let $\W_x(dw)$ be the image measure of $\W$ by $(w_t)_{t \geq 0} \mapsto (x+w_t)_{t \geq 0}$.

A (continuous) process with law $\W_x(dw)$ is called a Brownian motion started at $x$.

We let $(\sF^B_t)_{t \geq 0}$ be the natural filtration of $(B_t)_{t \geq 0}$, completed by zero-probability events.
\end{definition}


\begin{definition}
We say that $B$ is a Brownian motion (started at $X$) if $(B_t - X)_{t \geq 0}$ is a standard Brownian motion which is independent of $X$.
\end{definition}

\begin{remark}
Otherwise said, it is the same as the definition as a standard Brownian motion, except that we do not require that $B_0 = 0$. If we want to express this on the Wiener space with the Wiener measure, we have for every measurable functional $F:\Omega\to \R^+$,
\be
\E\brb{F(B_t)_{t \geq 0}} = \int_{\R^d} \brb{\int_\Omega F(\brb{x + w(t)}_{t \geq 0}) \W(dw) }\pro(X \in dx) = \int_{\R^d} \W_x(F) \pro(X \in dx).
\ee

It will be handy to use the notation $W_X(F)$ for the random variable $\omega \mapsto W_{X(\omega)}(F)$, so that the right-hand side can be shortened as $\E(W_X(F))$.
\end{remark}

\section{Basic Properties of Brownian motions}

\subsection{Basic properties}

\begin{proposition}\label{pro:brownian_motion_basic_properties}
Let $B$ be a standard Brownian motion in $\R^d$.
\ben
\item [(i)] rotational invariance. If $U \in O(d)$ is an orthogonal matrix, then $UB = (UB_t)_{t \geq 0}$ is again a Brownian motion. In particular, $-B$ is a Brownian motion.
\item [(ii)] scaling invariance. If $\lm > 0$ then $(\lm^{-1/2} B_{\lm t})_{t \geq 0}$ is a standard Brownian motion.
\item [(iii)] time-inversion. $\wt{B}_t := (tB_{1/t})_{t \geq 0}$ is also a Brownian motion (at $t = 0$ the process is defined by its value 0 a.s., i.e., $\wt{B}_0 = 0$ a.s.).
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Since $U$ is orthogonal matrix, we have $U^TU = UU^T = I$. Then by Theorem \ref{thm:multivariate_gaussian_rv_property}.(i), $UB$ has covariance matrix $UVU^T$ where $V$ is the covariance matrix of $B$, say $\sigma^2 I_d$. Thus the covariance matrix of $UB$ is
\be
U \sigma^2 I_d U^T = \sigma^2 U I_d U^T = \sigma^2 UU^T = \sigma^2 I_d \ \ra \ \text{$UB$ is a Brownian motion.}
\ee
%Use the fact that $N \sim \sN (0,\sigma^2 I_d)$ then $UN \sim \sN (0,\sigma^2 I_d )$.
\item [(ii)] Obviously, $B_0 = 0$ a.s.. Since
\be
\frac 1{\sqrt{\lm}} B_{\lm t} \sim \frac 1{\sqrt{\lm}} \sN (0,\lm tI_d) \sim \sN (0, tI_d)
\ee
we have the increments of $\brb{\lm^{-1/2} B_{\lm t}}_{ t \geq 0}$ are independent Gaussian. Thus, $(\lm^{-1/2} B_{\lm t})_{t \geq 0}$ is a standard Brownian motion.

\item [(iii)] At any time $t>0$, we have
\be
tB_{1/t} \sim t\sN\brb{0,\frac 1t} \sim \sN\brb{0\cdot t, t^2 \frac 1t} = \sN(0,t) \sim B_t.
\ee

Thus, the only thing left to prove is that $B_0 = 0$ a.s.. Consider the event $\wt{A} = \bra{\omega :\wt{B}_t(\omega) \to 0\text{ as } t\to 0}$ ($\forall n >0$, $\exists m > 0$ such that all $q\in \Q\cap \left(0,1/m\right]$) is %< \delta$, then $\wt{B}_t(\omega) < \ve$)
\be
\wt{A} = \bigcap_n \bigcup_m \bigcap_{q\in \Q\cap (0,1/m]} \bra{\dabs{\wt{B}_q} \leq \frac 1n}
\ee
since $\wt{B}$ is certainly continuous on $(0,\infty)$. %That is, $\forall n$, $\exists m$ such that all $q\in \Q\cap \left(0,1/m\right]$ satisfies
But the processes $(B_t)_{t\geq 0}$ and $(\wt{B}_t)_{t\geq 0}$ have the same distribution (they are Gaussian processes with the same covariance), so
\be
\pro\brb{\wt{A}} = \pro\brb{{A}} = 1
\ee
where $A$ is the event $\bigcap_n \bigcup_m \bigcap_{q\in \Q\cap (0,1/m]} \bra{\dabs{B_q} \leq \frac 1n}$ that $B\to 0$ at 0, a.s., which, by definition of $B$ ($B_0 = 0$ a.s.).%are continuous
\een
\end{proof}

\begin{corollary}
Let $B$ be a standard Brownian motion, then $\lim_{t\to \infty} \frac{B_t}{t} = 0$ a.s.
\end{corollary}

\begin{remark}
Of course one can show the above result directly using the strong law of large numbers, i.e., $\lim_{n\to \infty} B_n/n = 0$. The one needs to show that $B$ does not oscillate too much between $n$ and $n+1$.\footnote{see example sheet, need details}
\end{remark}

\begin{proof}[\bf Proof]
Recalling Proposition \ref{pro:brownian_motion_basic_properties}.(iii), suppose that $B_t = t\wt{B}_{1/t}$ where $\wt{B}$ is another standard Brownian motion. Thus,
\be
\lim_{t\to\infty} \frac {B_t}t = \lim_{t\to \infty} \wt{B}_{1/t} = \wt{B}_0 = 0 \text{ a.s.}.
\ee
\end{proof}

\begin{lemma}\label{lem:standard_brownian_motion_covariance}
Let $B$ be a standard Brownian motion and $s, t \geq 0$. Then $\cov(B_s,B_t) = s \land t$.
\end{lemma}

\begin{proof}[\bf Proof]
Wlog, we assume $t\geq s$, by independent Gaussian increment of Brownian motion, we have
\be
\cov \brb{B_s,B_t} = \E\brb{B_sB_t} - \E B_s \E B_t = \E\brb{B_s(B_t - B_s + B_s)} - 0 = \E\brb{B_s(B_t-B_s)} + \E\brb B_s^2 = 0 + s = s.
\ee

Thus, we have $\cov(B_s,B_t) = \min\bra{s,t} = s \land t$.
\end{proof}

Recall Theorem \ref{thm:local_martingale_indistinguishable_0}, we have
\begin{proposition}\label{pro:brownian_motion_no_finite_variation}
Brownian motion is not of finite variation.
\end{proposition}




\subsection{Markov property}

We now start to discuss ideas revolving around the Markov property of Brownian motion and its applications to path properties. We begin with the simple Markov property, which takes a particularly nice form in this context.


\begin{theorem}[simple Markov property\index{Markov property!simple, Brownian motion}]\label{thm:simple_markov_property_brownian_motion}
Let $(B_t)_{t \geq 0}$ be a Brownian motion, and let $s > 0$. Then
\be
\brb{\wt{B}_t := B_{t+s} - B_s}_{t \geq 0}
\ee
is a Brownian motion, independent of the $\sigma$-field $\sF^B_{s^+} = \bigcap_{t>s} \sF^B_t$.
\end{theorem}

\begin{remark}
In $\sF_s^+$, we allow an additional infinitesimal glance into the future.
\end{remark}

\begin{proof}[\bf Proof]
Since $\wt{B}$ is continuous and $\wt{B}_0 = 0$, to show that $\wt{B}$ is a Brownian motion it suffices to check that the increments have the correct distribution. However if $t \geq u$, $\wt{B}_t - \wt{B}_u = B_{s+t} - B_{s+u}$ so this follows directly from the fact that $B$ itself is a Brownian motion. It remains to show that $\wt{B}$ is independent from $\sF_{s^+}$.

We start by checking independence with respect to $\sF_s$, for which we can assume $d = 1$.
%Now, to prove independence of $\wt{B}$ with repsect to $\sF_s$,
It suffices to check that the finite dimensional marginals are independent. i.e., if $s_1 \leq\dots s_m \leq s$ and $t_1 \leq \dots t_n$, we want to show that
\be
(B_{s_1} ,\dots,B_{s_m}) \text{ and }(\wt{B}_{t_1} ,\dots, \wt{B}_{t_n})
\ee
are independent. However, the $m+n$-coordinate vector $(B_{s_1} ,\dots,B_{s_m}, \wt{B}_{t_1} ,\dots, \wt{B}_{t_n})$ is a Gaussian vector (since it is the image by a linear combination of Gaussian random variables), and it suffices to check that the covariance of two distinct terms is 0. Since each term has zero expectation, we have (by Lemma \ref{lem:standard_brownian_motion_covariance})
\be
\cov(\wt{B}_{t_i} ,B_{s_j}) = \E(\wt{B}_{t_i}B_{s_j}) = \E(B_{s+t_i}B_{s_j}) - \E(B_sB_{s_j}) = s_j \land (s + t_i) - (s_j \land s) = s_j - s_j = 0
\ee
which proves the independence with respect to $\sF_s$.

To show that $\wt{B}$ is independent from $\sF_{s^+}$, we wish to show that for every function $f:(\R^d)^k \to \R$ continuous and bounded and $A \in \sF_{s^+}$,
\be
\E\brb{\ind_A f(\wt{B}_{t_1} ,\dots, \wt{B}_{t_k})} = \pro(A)\E\brb{f(\wt{B}_{t_1} ,\dots, \wt{B}_{t_k})}.%\footnote{need explain}.
\ee

Now, for any $\ve > 0$, $A \in \sF_{s^+} \subseteq \sF_{s+\ve}$, thus, using the property just proved ($B_{t_i+s+\ve}-B_{s+\ve}$ are independent of $\sF_{s+\ve}$)
\be
\E(\ind_Af(B_{t_1+s+\ve}-B_{s+\ve},\dots,B_{t_k+s+\ve}-B_{s+\ve})) =\pro(A)\E(f(B_{t_1+s+\ve}-B_{s+\ve},\dots,B_{t_k+s+\ve}-B_{s+\ve}))
\ee

Letting $\ve \to 0$ in the above identity, since $B$ is continuous and $f$ is bounded and continuous, then by Proposition \ref{pro:sigma_algebra_random_variable_independence},
\be
f(B_{t_1+s+\ve}-B_{s+\ve},\dots,B_{t_k+s+\ve}-B_{s+\ve}) \to f(B_{t_1+s}-B_{s},\dots,B_{t_k+s}-B_{s})
\ee

we have (by bounded convergence theorem, Theorem \ref{thm:bounded_convergence_probability}),
\be
\E\brb{\ind_Af(\wt{B}_{t_1} ,\dots, \wt{B}_{t_k})} = \pro(A)\E\brb{f(\wt{B}_{t_1} ,\dots, \wt{B}_{t_k})}.
\ee

Hence, we can say that $\bra{\wt{B}_{t_1},\dots,\wt{B}_{t_k}}$ is independent of the $\sigma$-field $\sF^B_{s^+}$ by Proposition \ref{pro:sigma_algebra_random_variable_independence}. Thus, $\wt{B}$ is independent of $\sF^B_{s^+}$ by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}.%Theorem \ref{thm:uniqueness_of_extension_measure} and
\end{proof}

\begin{theorem}[Blumenthal's 0-1 law\index{Blumenthal's 0-1 law}]\label{thm:blumenthal_zero_one_law}
Let $B$ be a standard Brownian motion. The $\sigma$-algebra $\sF^B_{0^+} = \bigcap_{\ve>0} \sF^B_\ve$ is trivial, for all
$A \in \sF^B_{0^+}$, $\pro(A) \in \bra{0, 1}$. % i.e. constituted of events of probability 0 or 1.
\end{theorem}

%\begin{proof}[\bf Proof]
%By the previous result, $(B_t, t \geq 0)$ is independent from $\sF_{0^+}$. However $\sF^B_\infty$ contains $\sF^B_{0^+}$, so this implies that the $\sigma$-field $\sF_{0^+}$ is independent of itself, and $\pro(A) = \pro(A \cap A) = \pro(A)^2$ by independence. Thus $\pro(A)$ is solution to the equation $x = x^2$ whose roots are precisely 0 and 1.
%\end{proof}

%\begin{proposition}[Blumenthal's 0-1 law]
%Let $(B_t , t \geq 0)$ be a standard Brownian motion and let $(\sF^B_t)$ be its natural filtration, and $\sF^B_{0^+} = \bigcap_{t>0} \sF^B_t$. Then for all $A \in \sF^B_{0^+}$, $\pro(A) \in \{0, 1\}$.
%\end{proposition}

\begin{proof}[\bf Proof]
Let $A \in \sF^B_{0^+}$, then $A$ is independent of $\sigma(B_{t_i} , 1 \leq i \leq k)$, for all $t_1, \dots , t_k > 0$ %.so for all $\ve > 0$, $A \in \sF^B_\ve$. Let $f$ be a continuous bounded function $(\R^d)^k \to \R$ and $t_1, \dots, t_k > 0$. Then
by simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion}).
%\beast
%\E\brb{\ind_A f(B_{t_1} , \dots, B_{t_k})} & = & \lim_{\ve\to 0}\E\brb{\ind_Af(B^{(\ve)}_{t_1-\ve}, \dots,B^{(\ve)}_{t_k-\ve})} \quad\text{by DCT}\\
%& = & \lim_{\ve\to 0} \pro(A)\E\brb{f(B^{(\ve)}_{t_1-\ve}, \dots,B^{(\ve)}_{t_k-\ve})} \quad\text{SM}\\
%& = & \pro(A)\E\brb{f(B_{t_1},\dots, B_{t_k})}
%\eeast
%\be
%\E\brb{\ind_A f(B_{t_1} , \dots, B_{t_k})} = \pro(A)\E\brb{f(B_{t_1},\dots, B_{t_k})}
%\ee
Therefore $A$ is independent of $\sigma(B_s, s \geq 0) = \sF^B_\infty \supseteq \sF^B_{0^+}$, so $\sF^B_{0^+}$ is independent of itself. Thus for any $A \in \sF^B_{0^+}$, $\pro(A) = \pro(A\cap A) = \pro(A)^2$, so $\pro(A) \in \{0, 1\}$.
\end{proof}

\begin{proposition}
Suppose that $(B_t)_{t\geq 0}$ is a standard Brownian motion in 1 dimension. Definie
\be
\tau = \inf\bra{t>0:B_t >0},\quad \quad \sigma = \inf\bra{t>0: B_t = 0}.
\ee

Then $\pro\brb{\tau = 0} = \pro\brb{\sigma = 0} = 1$.
\end{proposition}

\begin{proof}[\bf Proof]
For all $n$ we have
\be
\bra{\tau = 0} = \bigcap_{k\geq n} \bra{\exists 0< \ve < 1/k: B_\ve >0}
\ee
and thus $\bra{\tau = 0} \in \sF^B_{1/n}$ for all $n$ and hence $\bra{\sF^+_0}$. Therefore, $\pro\brb{\tau = 0} \in \bra{0,1}$ by Blumental 0-1 law (Theorem \ref{thm:blumenthal_zero_one_law}). It remains to show that it has positive probability. Clearly, for all $t>0$ we have
\be
\pro\brb{\tau > t} \geq \pro\brb{\sup_{0\leq s\leq t}B_s \leq 0} \leq \pro\brb{B_t \leq 0} \ \ra \ \pro\brb{\tau \leq t} \geq \pro\brb{B_t >0} = \frac 12.
\ee

Hence by letting $t\da 0$ we get that $\pro\brb{\tau = 0} \geq 1/2$ and this finishes the proof.

In exactly the same way we get that (by symmetry)
\be
\inf\bra{t>0: B_t < 0} = 0 \text{ a.s.}.
\ee

Since $B$ is a (sample) continuous function, by the intermediate value theorem (Theorem \ref{thm:intermediate_value}), we have that
\be
\pro\brb{\sigma = 0} = 1.
\ee
\end{proof}


\begin{proposition}\label{pro:brownian_motion_limit_value}
Let $(B_t)_{t \geq 0}$ be standard Brownian motion in dimension $d = 1$. Let $S_t = \sup_{0\leq s \leq t} B_s$ and $I_t = \inf_{0\leq s\leq t} B_s$. Then
\ben
\item [(i)] a.s. for all $t > 0$, $S_t > 0$ and $I_t < 0$;
\item [(ii)] a.s. $S_\infty = \sup_{t\geq 0} B_t = +\infty$ and $I_\infty = \inf_{t\geq 0} B_t = -\infty$;
\item [(iii)] In dimensions $d \geq 2$, let $C$ be an open cone with origin at 0 and non-empty interior. Let $H_C = \inf\{t > 0: B_t \in C\}$. Then a.s. $H_C = 0$.
\een
\end{proposition}

\begin{proof}[\bf Proof]
\ben
\item [(i)] Let $t_k$ be a positive decreasing sequence with $t_k \to 0$ as $k \to\infty$. Let $A_k = \{B_{t_k} > 0\}$, so $\limsup_{k} A_k = \{B_{t_k} > 0 \text{ i.o.}\}\in \sF^B_t$ for any $t > 0$. Therefore it is in $\sF^B_{0^+}$. By Fatou's lemma (Lemma \ref{lem:fatou_set}),
\be
\pro\brb{\limsup_k A_k} \geq \limsup_k \pro(A_k) = \limsup_k \pro(B_{t_k} > 0) = \frac 12.
\ee
%On the other hand,

By Blumenthal's 0-1 law (Theorem \ref{thm:blumenthal_zero_one_law}), $\pro(\limsup_k A_k) \in \{0, 1\}$, so it must be one. But
\be
\limsup_k A_k = \limsup_k \bra{B_{t_k} > 0} = \bigcap_n \bigcup_{m\geq n}\bra{B_{t_m} >0} \subseteq \bigcap_n \bra{S_{t_n} >0} % \bigcup_n \bra{B_{t_n} >0} \subseteq \bigcup_n \bra{\sup_{0\leq s\leq t_n}B_s >0}
\ee

Thus, $\pro\brb{B_t > 0} = 1$ for any $t >0$. %$\limsup_k A_k \subseteq \{S_t > 0 | \forall t > 0\}$ we have the result.
$(-B_t)_{t \geq 0}$ is a Brownian motion, so the result for It follows.

\item [(ii)] By scaling invariance of Brownian motion (Proposition \ref{pro:brownian_motion_basic_properties}.(ii)) we get that for any $\lm > 0$,
\be
S_\infty = \sup_{t\geq 0} B_t = \sup_{t\geq 0} B_{\lm t} \stackrel{d}{=} \sup_{t\geq 0} \sqrt{\lm }B_t = \sqrt{\lm} S_\infty.
\ee

%Therefore $S_\infty \stackrel{(d)}{=} \sqrt{\lm} \sup_{t\geq 0} B_t = \sqrt{\lm} S_\infty$ for all $\lm > 0$, so $\pro(S_\infty \in (\ve, \ve^{-1})) = \pro(\lm S_\infty \in (\ve, \ve^{-1})) \to  0$ as $\lm \to\infty$. Therefore $S_\infty \in \{0,\infty\}$. But $S_\infty \geq S_1 > 0$ a.s. by (i). Same for $I_t$.

Thus for all $x>0$ the probability $\pro(S_\infty \geq x)$ is a constant $c$, and hence $\pro\brb{S_\infty \geq 0} =c$. But we have already showed that $\pro\brb{S_\infty \geq 0} = 1$ as $B_0 = 0$ a.s.. Therefore, for all $x >0$ we have
\be
\pro\brb{S_\infty \geq x} = 1 \ \ra \ \pro\brb{S_\infty = \infty} =1.
\ee

The same argument for $I_\infty$ gives the required result.


\item [(iii)] Since the cone $C$ is invariant under multiplication by a positive scalar, by the scaling invariance property of Brownian motion we get that for any $t$, $\pro\brb{B_t\in C} = \pro(B_1\in C)$. Since $C$ has non-empty interior, we can have that $\pro\brb{B_1\in C} >0$ and thus $\pro(B_t \in C) >0$ for any $t$.

%Copy the proof of (i),

Let $t_k$ be a positive decreasing sequence with $t_k \to 0$ as $k \to\infty$. Let $A_k = \{B_{t_k} \in C\}$, so $\limsup_{k} A_k = \{B_{t_k} \in C \text{ i.o.}\}\in \sF^B_t$ for any $t > 0$. Therefore it is in $\sF^B_{0^+}$. By Fatou's lemma (Lemma \ref{lem:fatou_set}),
\be
\pro\brb{\limsup_k A_k} \geq \limsup_k \pro(A_k) = \limsup_k \pro(B_{t_k} \in C) > 0.%= \frac 12.
\ee
%On the other hand,

By Blumenthal's 0-1 law (Theorem \ref{thm:blumenthal_zero_one_law}), $\pro(\limsup_k A_k) \in \{0, 1\}$, so it must be one. Thus,
\be
\pro\brb{B_{t_k} \in C \text{ i.o.}} = 1 \ \ra \ \pro\brb{H_C = 0} = 1.
\ee
%But
%\be
%\limsup_k A_k = \limsup_k \bra{B_{t_k} \in C} = \bigcap_n \bigcup_{m\geq n}\bra{B_{t_m} \in C} \subseteq \bigcap_n \bra{S_{t_n} >0} % \bigcup_n \bra{B_{t_n} >0} \subseteq \bigcup_n \bra{\sup_{0\leq s\leq t_n}B_s >0}
%\ee
%Thus, $\pro\brb{B_t > 0} = 1$ for any $t >0$. %$\limsup_k A_k \subseteq \{S_t > 0 | \forall t > 0\}$ we have the result. $(-B_t)_{t \geq 0}$ is a Brownian motion, so the result for It follows.
%and note that since $\lm C = C$ for all $\lm > 0$, $\pro(A_k) = \pro(\sN (0, I_d )\in C) > 0$ since $C$ has non-empty interior (and the density is everywhere positive?).
\een
\end{proof}

We now want to prove an important analog of the simple Markov property, where deterministic times are replaced by stopping times. To begin with, we extend a little the definition of Brownian motion, by allowing it to start from a random location, and by working with filtrations that are (slightly) larger than the natural filtration of a standard Brownian motion.

\begin{definition}
Let $(\sF_t)_{t \geq 0}$ be a filtration. We say that a Brownian motion $B$ is an $(\sF_t)$-Brownian motion if $B$ is adapted to $(\sF_t)_{t \geq 0}$, and if $B^{(t)} = (B_{t+s}-B_t)_{s \geq 0}$ is independent of $\sF_t$ for every $t \geq 0$.
\end{definition}

\begin{remark}
For instance, if $(\sF_t)$ is the natural filtration of a 2-dimensional Brownian motion $(B^1_t ,B^2_t)_{t \geq 0}$, then $(B^1_t)_{t \geq 0}$ is an $(\sF_t)$-Brownian motion. If $B'$ is a standard Brownian motion and $X$ is a random variable independent of $B'$, then $B = (X + B'_t)_{t \geq 0}$ is a Brownian motion (started at $B'_0 = X$), and is an $(\sF^B_t) = (\sigma(X) \lor \sF^{B'}_t)$-Brownian motion. A Brownian motion is always an $\sF^B_t)$-Brownian motion. If $B$ is a standard Brownian motion, then the completed filtration $\sF_t = \sF^B_t \lor \sN$ ($\sN$ being the set of events of probability 0) can be shown to be right-continuous, i.e. $\sF_{t^+} = \sF_t$ for every $t \geq 0$, and $B$ is an $(\sF_t)$-Brownian motion.
\end{remark}


\begin{theorem}[strong Markov property\index{Markov property!strong, Brownian motion}]\label{thm:strong_markov_property_brownian_motion}
Let $(B_t)_{t \geq 0}$ be an $(\sF_t)$-Brownian motion in $\R^d$ and $T$ be an $(\sF_t)$-stopping time. We let
\be
B^{(T)} = \left\{\ba{ll}
(B_{T+t} - B_T)_{t \geq 0}\quad\quad & T < \infty \\
0& \text{otherwise}
\ea\right.
\ee
%$B^{(T)}_t = B_{T+t} - B_T$ for every $t \geq 0$ on the event $\{T < \infty\}$, and 0 otherwise.

Conditionally on $\{T < \infty\}$, the process $B^{(T)}$ is a standard Brownian motion, which is independent of $\sF_{T^+}$. Otherwise said, conditionally given $\sF_T$ and $\{T < \infty\}$, the process $(B_{T+t})_{t \geq 0}$ is an $(\sF_{T+t})$-Brownian motion started at $B_T$.
\end{theorem}

%\begin{theorem}[Strong Markov Property]
%Let ($B_t$) be an ($\sF_t$)-Brownian motion and let $T$ be an ($\sF_t$)-stopping time. Then, conditionally on $\{T < \infty\}$, the process
%is a standard Brownian motion which is independent of $\sF_T$.
%Equivalently, given $\sF_T$ , the process $(B_{t+T})_{t \geq 0}$ is an $(\sF_{T+t})_{t \geq 0}$-Brownian motion.
%\end{theorem}


\begin{proof}[\bf Proof]
Suppose first that $T < \infty$ a.s. We will first prove for the stopping times $T_n = 2^{-n}\ceil{2^nT}$ that discretely approximate $T$ from above. We write $B^{(k)}_t = B_{t+k2^{-n}} - B_{k2^{-n}}$ which is a Brownian motion and $B^*$ for the process defined by
\be
B^*_t = B_{t+T_n} - B_{T_n}.
\ee

We will first show that $B^*$ is a Brownian motion independent of $\sF_{T_n^+}$. Let $A \in \sF_{T_n^+}$ and $0\leq t_1 \leq \dots \leq t_m$. Then for any bounded continuous function $f$, %For every event $\bra{B^* \in D}$, we have
\beast
\E\brb{\ind_A f\brb{B^*_{t_1},\dots,B^*_{t_m}}} & = & \E\brb{\sum^\infty_{k=0}\ind_A f\brb{B^*_{t_1},\dots,B^*_{t_m}}\ind_{\bra{T_n = k2^{-n}}}} = \E\brb{\sum^\infty_{k=0}\ind_A f\brb{B^{(k)}_{t_1},\dots,B^{(k)}_{t_m}}\ind_{\bra{T_n = k2^{-n}}}}\\
& = & \sum^\infty_{k=0} \E\brb{\ind_{A \cap \bra{T_n = k2^{-n}}} f\brb{B^{(k)}_{t_1},\dots,B^{(k)}_{t_m}}}\quad\quad (\text{by Fubini theorem (Theorem \ref{thm:fubini})})\\
& = & \sum^\infty_{k=0} \pro\brb{A \cap \bra{T_n = k2^{-n}}} \E\brb{f\brb{B^{(k)}_{t_1},\dots,B^{(k)}_{t_m}}}\quad\quad (\text{by Proposition \ref{pro:sigma_algebra_random_variable_independence}})\\
& = & \pro\brb{A} \E\brb{f\brb{B^{(k)}_{t_1},\dots,B^{(k)}_{t_m}}}.
\eeast

By simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion}), $B^{(k)}$ is Brownian motion. Thus, for standard Brownian motion $B$, $B^{(k)}\stackrel{d}{=}B$. Let $A$ be the whole space $\Omega$. Then we have
\be
\E\brb{f\brb{B^*_{t_1},\dots,B^*_{t_m}}} = \E\brb{f\brb{B^{(k)}_{t_1},\dots,B^{(k)}_{t_m}}}.
\ee

Thus, letting $f(x)$ be $\ind_{x\in D}$, we have that $B^*$ and $B$ has the same finite dimensional marginal distributions. Thus, we have
\be
\E\brb{\ind_A f\brb{B^*_{t_1},\dots,B^*_{t_m}}} = \pro\brb{A} \E\brb{f\brb{B^{*}_{t_1},\dots,B^{*}_{t_m}}}.
\ee

Then by Proposition \ref{pro:sigma_algebra_random_variable_independence}, $\bra{B^*_{t_1},\dots,B^*_{t_m}}$ is independent of $\sF_{T_n^+}$. (This can also be proved by checking $\pro\brb{\bra{B^* \in D}\cap A} = \pro(A)\pro\brb{B^* \in D}$ where $D\in \sW$.)

By the continuity of Brownian motion we get that
\be
B_{t+s+T} - B_{s+T} = \lim_{n\to \infty} \brb{B_{s+t+T_n} - B_{s+T_n}}.
\ee

The increments $(B_{t+s+T_n} - B_{s+T_n})$ are normally distributed with 0 mean variance equal to $t$. Thus for any $s \geq 0$ the increments $B_{t+s+T} - B_{s+T}$ are also normally distributed with 0 mean and variance $t$. As the process $(B_{t+T} - B_T)_{t \geq 0}$ is a.s. continuous, it is a Brownian motion. It only remains to show that it is independent of $\sF_{T^+}$.

Let $A \in \sF_{T^+}$ and $0\leq t_1\leq \dots \leq t_m$. Also, since $T_n > T$, it follows that $A \in \sF_{T_n^+}$. Thus, for any bounded continuous function $f : (\R^d)^m \to \R$,
%\beast
%\E\brb{\ind_A f\brb{B^*_{t_1},\dots, B^*_{t_m}}} & = & \E\brb{\ind_A f\brb{B_{t_1+T_n}-B_{T_n},\dots, B_{t_m+T_n}-B_{T_n}}}= \E\brb{\ind_A f\brb{B^{(T)}_{t_1},\dots, B^{(T)}_{t_m}}} \\
%& = & \pro(A)\E\brb{f\brb{B^{(T)}_{t_1},\dots, B^{(T)}_{t_m}}}  \\
%& = & \pro\brb{A}\E\brb{f\brb{B_{t_1+T}-B_T,\dots, B_{t_m+T}-B_T}}
%\eeast
\be
\E\brb{\ind_A f\brb{B^*_{t_1},\dots, B^*_{t_m}}} = \pro\brb{A} \E\brb{f\brb{B^{*}_{t_1},\dots,B^{*}_{t_m}}}.
\ee

But we know that since $f$ and $B$ are continuous, as $n\to \infty$, $T_n \to T$,
\beast
f\brb{B^{(T)}_{t_1},\dots, B^{(T)}_{t_m}} & = & f\brb{B_{t_1+T}-B_T,\dots, B_{t_m+T}-B_T} \\
& = & \lim_{n\to \infty} f\brb{B_{t_1+T_n}-B_{T_n},\dots, B_{t_m+T_n}-B_{T_n}} = \lim_{n\to \infty} f\brb{B^*_{t_1},\dots, B^*_{t_m}}
\eeast
and $\ind_A f\brb{B^{(T)}_{t_1},\dots, B^{(T)}_{t_m}} = \lim_{n\to \infty} \ind_A f\brb{B^*_{t_1},\dots, B^*_{t_m}}$. Then by bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability}),
\beast
\E\brb{\ind_A f\brb{B^{(T)}_{t_1},\dots, B^{(T)}_{t_m}}} & = & \lim_{n\to \infty}\E\brb{\ind_A f\brb{B^*_{t_1},\dots, B^*_{t_m}}} \\
& = & \pro\brb{A} \lim_{n\to \infty}  \E\brb{f\brb{B^{*}_{t_1},\dots,B^{*}_{t_m}}} = \pro\brb{A} \E\brb{f\brb{B^{(T)}_{t_1},\dots,B^{(T)}_{t_m}}}.
\eeast

Thus, we have that $B$ is independent of $\sF_{T^+}$ by Proposition \ref{pro:process_law_is_uniquely_determined_by_its_finite_dimensional_marginal_distribution}.%Theorem \ref{thm:uniqueness_of_extension_measure} and

Finally, if $\pro(T = \infty) > 0$, check that the above is true when replacing $A$ by $A \cap \bra{T < \infty}$, and divide by $\pro\brb{T < \infty}$.
\end{proof}

%We will show that for any function $f : (\R^d)^m \to \R$ continuous and bounded we have
%\be
%E[1(A)F((Bt1+T - BT ; : : : ;Btk+T - BT ))] = P(A)E[F((Bt1+T - BT ; : : : ;Btk+T - BT ))]:
%\ee
%Using the continuity again and the dominated convergence theorem, we get that
%\be
%E[1(A)F((Bt1+T-BT ; : : : ;Btk+T-BT ))] = lim_{n\to \infty}E[1(A)F((Bt1+Tn-BTn; : : : ;Btk+Tn-BTn))]:
%\ee
%But we already showed that the process $(B_{t+T_n} - B_{T_n})_{t\geq 0}$ is independent of $\sF_{T_n^+}$, hence using the continuity and dominated convergence one more time gives the claimed independence.

%\be
%\pro\brb{B^*_{t_1},\dots,B^*_{t_m}}
%\ee

%\be
%\pro\brb{\bra{B^* \in D} \cap A} =  \pro\brb{\sum^\infty_{k=0} \bra{B^* \in D} \cap A \cap \bra{T_n = k2^{-n}}} = \sum^\infty_{k=0}  \pro\brb{\bra{B^* \in D} \cap A \cap \bra{T_n = k2^{-n}}}
%\ee
%= 1 k=0 P(B(k) 2 A)P(E \ fTn = k2-ng); 57

%\qcutline

%Let $A \in \sF_T$, and consider times $t_1 < t_2 < \dots < t_p$. We want to show that for every bounded continuous function $f$ on $(\R^d)^p$,
%\be
%\E\brb{\ind_A f(B^{(T)}_{t_1} ,\dots,B^{(T)}_{t_p})} = \pro(A)\E\brb{f(B_{t_1} ,\dots,B_{t_p})}.\quad\quad (*)
%\ee

%Indeed, taking $A = \Omega$ entails that $B^{(T)}$ is a Brownian motion, while letting A vary in $\sF_T$ entails the %independence of $(B^{(T)}_{t_1} ,\dots,B^{(T)}_{t_k})$ and $\sF_T$ for every $t_1,\dots,t_k$, hence of $B^{(T)}$ and $\sF_T$.
%\beast
%\E\brb{\ind_Af(B^{(T)}_{t_1} ,\dots,B^{(T)}_{t_p})} & = & \lim_{n\to \infty} \sum^\infty_{k=1} \E\brb{\ind_{A\cap \{(k-1)2^{-n}<T\leq k2^{-n}\}}]f(B^{(k2^{-n})}_{t_1} ,\dots,B^{(k2^{-n})}_{t_p})} \\
%& = & \lim_{n\to \infty} \sum^\infty_{k=1} \pro\brb{A \cap \{(k - 1)2^{-n} < T \leq k2^{-n}\}}\E\brb{f(B_{t_1} ,\dots,B_{t_p})}\\
%& = & \pro(A)\E\brb{f(B_{t_1} ,\dots,B_{t_p})}.
%\eeast
%where we used the simple Markov property and the fact that $A\cap \{(k-1)2^{-n} < T \leq k2^{-n}s \} \in \sF_{k2^{-n}}$ by definition.




%\begin{proof}[\bf Proof]
%Let $T$ be such that $\pro(T < \infty) = 1$. Let $A \in \sF_T$ and $0 \leq t_1 \leq \dots \leq t_k$ be fixed times. Let $F : (\R^d )^k \to \R$ be a bounded continuous function. Assume first that $T$ takes values in $D$, the dyadic rationals. Then
%\beast
%\E[\ind_AF(B^{(T)}_{t_1},\dots, B^{(T)}_{t_k})] & = & \sum_{d\in D} \E[\ind_{A\cap\{T=d\}}F(B^{(T)}_{t_1}, \dots, B^{(T)}_{t_k})]\\
%& = & \sum_{d\in D} \pro(A, T = d)\E[F(B_{t_1},\dots, B_{t_k})] \quad\quad \text{SM}\\
%& = & \pro(A)\E[F(B_{t_1} ,\dots, B_{t_k})]
%\eeast

%In general, let $T_n = 2^{-n}\ceil{2^nT}$, so that $T_n \da T$. $\sF_T \subseteq \sF_{T_n}$ since $T \leq T_n$, so $A \in \sF_{T_n}$, and by the previous argument,
%\be
%\E[\ind_AF(B^{(Tn)}_{t_1}, \dots, B^{(T_n)}_{t_k})] = \pro(A)\E[F(B_{t_1},\dots, B_{t_k})].
%\ee

%Since $F$ is continuous and bounded and Brownian motion is continuous, by the DCT the left hand side goes to $\E[\ind_AF(B^{(T)}_{t_1}, \dots, B^{(T)}_{t_k})]$ as $n\to \infty$. For $A = \Omega$, we obtain that $B^{(T)}$ is a standard Brownian motion. By letting $A$ vary in $\sF_T$ , we obtain that $\sF_T$ is independent of $\sigma(B^{(T)}_{t_1}, \dots, B^{(T)}_{t_k} )$, and is therefore independent of $B^{(T)}$.

%Finally, if $\pro(T <\infty) < 1$ then use the same argument with $A\cap \{T < \infty\}$ instead of $A$, and divide both sides of the above equation by $\pro(T <\infty)$.
%\end{proof}

\begin{example}
Let $\tau= \inf\bra{t \geq 0 : B_t = \max_{0\leq s\leq 1} B_s}$. It is intuitively clear that $\tau$ is not a stopping time. To prove that, first show that $\tau < 1$ a.s. The increment $B_{t+\tau} -B_\tau$ is negative in a small neighbourhood of 0, which contradicts the strong Markov property. See Problem \ref{exe:max_brownian_motion_no_stopping_time} for details.
\end{example}


An important example of application of the strong Markov property is the so-called reflection principle. %Recall that $S_t = \sup_{0\leq s\leq t} B_s$.

\subsection{Reflection principle}

\begin{theorem}[reflection principle\index{reflection principle}]\label{thm:reflection_principle_brownian_motion}
Let $T$ be an a.s. finite stopping time and $(B_t)_{t \geq 0}$ a standard Brownian motion. Then the process $(\wt{B}_t)_{t \geq 0}$ defined by
\be
\wt{B}_t = B_t\ind_{\bra{t \leq T}} + (2B_T - B_t)\ind_{\bra{t > T}}
\ee
is also a standard Brownian motion and we call it Brownian motion reflected at $T$.
\end{theorem}

\begin{proof}[\bf Proof]
By the strong Markov property (Theorem \ref{thm:strong_markov_property_brownian_motion}), the process $B^{(T)} = (B_{T+t} - B_T)_{t \geq 0}$ is a standard Brownian motion independent of $(B_t)_{0 \leq t \leq T}$. Also the process $-B^{(T)} = (B_T - B_{t+T})_{t \geq 0}$ is a standard Brownian motion independent of $(B_t)_{0 \leq t \leq T}$. Therefore, the pair $((B_t)_{0 \leq t \leq T},B^{(T)})$ has the same law as $((B_t)_{0 \leq t \leq T},-B^{(T)})$.

We now define the concatenation operation\index{concatenation operation} at time $T$ between two continuous paths $X$ and $Y$ by
\be
\Psi(X, Y)(t) = X_t\ind_{\bra{t \leq T}} + (X_T + Y_{t-T})\ind_{\bra{t > T}}.
\ee

Applying $\Psi_T$ to $B$ and $B^{(T)}$ gives us the Brownian motion $B$, while applying it to $B$ and $-B^{(T)}$ gives us the process $\wt{B}$.

Let $\sA$ be the product $\sigma$-algebra on $C[0,\infty)$, the space of continuous functions on $[0,\infty)$. It is easy
to see that $\Psi_T$ is a measurable mapping from $(C[0,\infty) \times C[0,\infty), \sA \otimes \sA)$ to $(C[0,\infty),\sA)$ (by approximating $T$ by discrete stopping times).

Hence, $B$ and $\wt{B}$ have the same law.
\end{proof}


\begin{theorem}[Reflection principle]\label{thm:reflection_principle_brownian_motion_upper_bound}
Let $(B_t)_{t \geq 0}$ be a Brownian motion and $a > 0$ and $b \leq a$, then for every $t \geq 0$,
\be
\pro(S_t \geq a,B_t \leq b) = \pro(B_t \geq 2a - b)\quad\quad \text{where $S_t = \sup_{0\leq s\leq t} B_s$.}
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Let $T_a = \inf\{t \geq 0: B_t \geq a\}$ be the entry time\footnote{need definition} of $B_t$ in $[a,\infty)$ for $a > 0$. Then $T_a$ is an $(\sF^B_t)$-stopping time for every $a$ and $T_a < \infty$ a.s. since $S_\infty = \sup_{t\geq 0}B_t = \infty$ a.s. where $S_\infty = \lim_{t\to \infty} S_t$ (Note that $\bra{S_t \geq a} = \bra{T_a \leq t}$).

Now by continuity of $B$, $B_{T_a} = a$ for every $a$. We thus have $B_t - B_{T_a} = B^{(T_a)}_{t-T_a}$ and thus
\beast
\pro(S_t \geq a,B_t \leq b) & = & \pro(T_a \leq t,B_t \leq b) = \pro(T_a \leq t,B_t - B_{T_a} \leq b - a)\\
& = & \pro\brb{T_a \leq t,B^{(T_a)}_{t-T_a} \leq b - a} = \pro\brb{T_a \leq t,-B^{(T_a)}_{t-T_a} \geq a - b}.
\eeast


%= \pro\brb{T_a \leq t, 2a - B_t \geq 2a -b} = \pro\brb{T_a \leq t, 2B_{T_a} - B_t \geq 2a -b}.

By the strong Markov property (Theorem \ref{thm:strong_markov_property_brownian_motion}) at time $T_a$, $B^{T_a}$ is a Brownian motion independent of $\sF_{T_a}$ and thus of $T_a$. In particular, we deduce that the joint law of $(T_a,B^{(T_a)})$ is identical to the joint law of $(T_a,-B^{(T_a)})$, by symmetry of Brownian motion. It follows that
\beast
\pro(S_t \geq a,B_t \leq b) & = & \pro\brb{T_a \leq t,-B^{(T_a)}_{t-T_a} \geq a - b} = \pro\brb{T_a \leq t,B^{(T_a)}_{t-T_a} \geq a - b} \\
& = & \pro\brb{T_a \leq t,B_{T_a} + B^{(T_a)}_{t-T_a} \geq 2a - b} = \pro(T_a \leq t,B_t \geq 2a - b) = \pro(B_t \geq 2a - b)
\eeast
since $\bra{B_t \geq 2a-b}$ implies that $\bra{S_t \geq a} =\bra{T_a\leq t}$.
\end{proof}

\begin{remark}
We can prove the above by Theorem \ref{thm:reflection_principle_brownian_motion}.
\end{remark}

\begin{corollary}\label{cor:abs_sup_equal_brownian_motion}
Let $B$ be a standard Brownian motion and $S_t = \sup_{0\leq s\leq t}B_s$. We have the following identities in distribution. for all $t \geq 0$,
\be
S_t \sim \abs{B_t} \sim \abs{\sN (0, t)}.%\stackrel{d}{=}
\ee
\end{corollary}

\begin{proof}[\bf Proof]
We write, for all $t \geq 0$ and all $a \geq 0$,
\beast
\pro(S_t \geq a) & = & \pro(S_t \geq a,B_t \leq a) + \pro(S_t \geq a,B_t \geq a) \\
& = & \pro(B_t \geq 2a - a) + \pro(B_t \geq a) = 2\pro(B_t \geq a) = \pro(\abs{B_t} \geq a)
\eeast
since when $B_t \geq a$, $S_t \geq a$ automatically as well.
\end{proof}

\begin{proposition}\label{pro:stopping_time_brownian_motion_touch_special_point}
For every $x > 0$, the random time $T_x := \inf\bra{t\geq 0:B_t = x}$ has same law as $(x/B_1)^2$.

Also, $T_x$ has independent and stationary increments.%We leave the computation of the distribution of $T_x$ as an exercise (cf. Problem 1.1).
\end{proposition}

\begin{proof}[\bf Proof]
Let $x>0$. We know that $T_x< \infty$ a.s. because $B$ is continuous and $\sup_{t\geq 0}B_t = \infty$ a.s. Then for $x>0$, the reflection principle and Brownian scaling property (Proposition \ref{pro:brownian_motion_basic_properties}.(ii)) give that
\be
\pro\brb{T_x\leq t} = \pro\brb{S_t \geq x} = \pro\brb{\abs{B_t} \geq x} = \pro\brb{\sqrt{t}\abs{B_1} \geq x} = \pro\brb{\brb{\frac x{B_1}}^2\leq t}.
\ee

So $T_x$ has same law as $(x/B_1)^2$.

For $0<x<y$, by the strong Markov property at $T_x$, we have that $B^{(T_x)} = (B_{T_x+t} -B_{T_x})_{t\geq 0} = (B_{T_x+t} -x)_{t\geq 0}$ is a Brownian motion independent of $\sF_{T_x^+}$. Therefore,
\be
T_y - T_x := \inf\bra{t\geq 0: B_{T_x + t} = y}  = \inf\bra{t\geq 0: B_{T_x + t} - x = y-x} = \inf\bra{t\geq 0: B^{(T_x)}_t = y-x}
\ee
has the same law as $T_{y-x}$. Also, for any $t>0$ and $A\in \sF_{T_x^+}$, since $B^{(T_x)}$ is a Brownian motion independent of $\sF_{T_x^+}$,
\be
\pro\brb{\bra{T_y-T_x > t}\cap A} = \pro\brb{\bra{B^{(T_x)}_t < y-x}\cap A} = \pro\brb{B^{(T_x)}_t < y-x}\pro(A) = \pro\brb{T_{y}-T_x > t} \pro(A).
\ee

Therefore, $T_y - T_x$ is independent of $\sF_{T_x^+}$ (and thus independent of $\sF_{T_x}$). Hence we can see that $(T_x)_{x \geq 0}$ has independent and stationary increments (see Definition \ref{def:independent_stationary_increments_stochastic_process}).
\end{proof}


%\begin{proposition}
%Let $B$ be a standard Brownian motion in $\R$ and $T_x = \inf\bra{t\geq 0:B_t = x}$ for $x>0$. Then its probability density is
%\be
%f_{T_x}(t) = \frac{x e^{-\frac{x^2}{2t}}}{\sqrt{2\pi t^3}}.
%\ee
%\end{proposition}
%
%
%
%\begin{proof}[\bf Proof]
%By Corollary \ref{cor:abs_sup_equal_brownian_motion}, we have%\footnote{see Rogers' book}
%\be
%\pro\brb{T_x\leq t} = \pro\brb{S_t \geq x} = \pro\brb{\abs{B_t} \geq x} = 2\int^\infty_x \frac{1}{\sqrt{2\pi t}}e^{-\frac{x^2}{2t}}dx = 2\int^\infty_{x/\sqrt{t}} \frac{1}{\sqrt{2\pi }}e^{-\frac{x^2}{2}}dx.
%\ee
%
%Then we take the differentiation,
%\be
%f_{T_x}(t) = -\frac{2}{\sqrt{2\pi}}e^{-\frac{x^2}{2t}}\brb{ -\frac 12 \frac {x}{\sqrt{t^3}}} = \frac{x e^{-\frac{x^2}{2t}}}{\sqrt{2\pi t^3}}
%\ee
%which is inverse gamma distribution $\Gamma^{-1}\brb{\frac 12, \frac{x^2}2}$ where $k=\frac 12$ and $\lm = \frac {x^2}2$ (see Definition \ref{def:inverse_gamma_rv}). Also, since $k= \frac 12$, we know that this hitting time $\E T = \infty$ a.s. as the expectation only exists for $k>1$.
%\end{proof}



\subsection{Martingales for Brownian Motion}

\begin{theorem}\label{thm:brownian_motion_martingale}
Let $(B_t)_{t \geq 0}$ be an $(\sF_t)$-Brownian motion in $d \geq 1$ dimensions.
\ben
\item [(i)] If $d = 1$ and $B_0 \in \sL^1(\Omega,\sF,\pro)$, the process $(B_t)_{t \geq 0}$ is a $(\sF_{t^+})$-martingale.
\item [(ii)] If $d \geq 1$, $B_0 \in \sL^2(\Omega,\sF,\pro)$ and $\dabs{B_t}^2 = B_t^TB_t$, the process $(\dabs{B_t}^2 - dt)_{ t \geq 0}$ is a ($\sF_{t^+}$)-martingale.
\item [(iii)] Let $d \geq 1$ and $u = (u_1,\dots, u_d) \in \C^d$\footnote{If $x, y \in \C^d$, we note $\inner{x}{y} = \sum^d_{i=1} x_i\overline{y}_i$ their complex scalar product.}. Assume that $\E\abs{\exp(\inner{u}{B_0})} < \infty$, the process defined by
\be
M_t = \exp\brb{\bsa{u,B_t} - t\abs{u}^2/2}
\ee
is also a ($\sF_{t^+}$)-martingale for every $u \in \C^d$, where $\abs{u}^2$ is a notation for $\sum^d_{i=1} u_i\ol{u_i}$.
\een
\end{theorem}

\begin{remark}
All the above martingales are also $(\sF_t)$-martingales (as $B^{(s)}_u = B_{u+s} -B_s$ has mean 0 and is independent of $\sF_{s^+}$ thus it is independent of $\sF_s$). % (as $B_t = B_{t^+}$ a.s. and $\sF_{t}\subseteq \sF_{t^+}$).

Notice that in (iii), we are dealing with $\C$-valued processes. Also, the hypothesis on $B_0$ in (iii) is automatically satisfied whenever $u = iv$ is purely imaginary, i.e., $v \in \R$.

Note that the alternative proof by using It\^o formula is given later (see Theorem \ref{thm:brownian_motion_martingale_ito}).%\footnote{need link}.
\end{remark}

\begin{proof}[\bf Proof]
The adaptedness of these three process is obvious, Thus, we only check the integrability and conditional expectation.
\ben
\item [(i)] If $s \leq t$, then since $B$ is continuous a.s.,
\be
\E\brb{B_t -B_{s^+}|\sF_{s^+}} = \E\brb{B_t -B_{s}|\sF_{s^+}} = \E\brb{B^{(s)}_{t-s}|\sF_{s^+}}
\ee
where $B^{(s)}_u = B_{u+s} -B_s$ has mean 0 and is independent of $\sF_{s^+}$, by the simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion}). Thus, $\E\brb{B_t -B_s|\sF_{s^+}} = \E\brb{\left.B^{(s)}_{t-s}\right|\sF_{s^+}} = \E\brb{B^{(s)}_{t-s}} = 0$ a.s. by Proposition \ref{pro:conditional_expectation_tower_independence}.(v).

The integrability of the process is obvious by assumption on $B_0 \in \sL^1(\Omega,\sF,\pro)$ as
\be
\E\abs{B_t} \leq \E\abs{B_t- B_0} + \E\abs{B_0} \leq \dabs{B_t- B_0}_2 + \E\abs{B_0} < \infty.
\ee

\item [(ii)] Since $B_0 \in \sL^2(\Omega,\sF,\pro)$ and $B_t - B_0$ is a normal random variable, we have for any $t\geq 0$
\be
\E\abs{\dabs{B_t}^2 -dt} \leq \E\dabs{B_t - B_0 + B_0}^2 + dt = \dabs{B_t - B_0 + B_0}^2_2 + dt \leq \brb{\dabs{B_t - B_0}_2 + \dabs{B_0}_2}^2 + dt < \infty.
\ee
by Minkowski inequality (Theorem \ref{thm:minkowski_inequality_expectation}). %by the triangle inequality that $B_t \in \sL^2(\Omega,\sF,\pro)$.
Thus, for $s \leq t$, as $B_{u+s} -B_s$ has mean 0 and is independent of $\sF_{s^+}$ (by the simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion})), ($B_s = B_{s^+}$ a.s., $B_{s^+},B_t-B_s \in \sL^2 (\Omega,\sF,\pro)$)
\beast
\E\brb{\left.\dabs{B_t}^2\right|\sF_{s^+}} & = & \E\brb{\left.\dabs{B_t-B_{s}}^2 + 2B_{s^+}\cdot (B_t-B_s) + \dabs{B_{s^+}}^2\right|\sF_{s^+}} \\
& = & \E\brb{\left.\dabs{B_t-B_{s}}^2\right|\sF_{s^+}} + 2\E\brb{ B_{s^+}\cdot (B_t-B_s)|\sF_{s^+}} + \E\brb{\left.\dabs{B_{s^+}}^2\right|\sF_{s^+}}\\
& \stackrel{\text{a.s.}}{=} & \E\brb{\left.\dabs{B_t-B_{s}}^2\right|\sF_{s^+}} + 2B_{s^+}  \cdot \E\brb{B_t-B_s|\sF_{s^+}} + \E\brb{\left.\dabs{B_{s^+}}^2\right|\sF_{s^+}}\quad (\text{Proposition \ref{pro:conditional_expectation_tower_independence}.(iii)})\\
& \stackrel{\text{a.s.}}{=} & \E\dabs{B_t-B_{s}}^2 + 2B_{s^+} \cdot\E\brb{B_t-B_s} + \dabs{B_{s^+}}^2 \quad (\text{Propositions \ref{pro:conditional_expectation_tower_independence}.(v), \ref{pro:conditional_expectation_basic_property}.(ii)})\\
& \stackrel{\text{a.s.}}{=}  & d(t-s) + 2B_{s^+} \cdot 0 + \dabs{B_{s^+}}^2 = d(t-s) + \dabs{B_{s^+}}^2 = d(t-s) + \dabs{B_{s}}^2
\eeast
since $B$ is continuous a.s. %Taking conditional expectation given $\sF_s$ and using the simple Markov property gives that $\E[B^2_t] = (t - s) + B^2_s$, hence the result.
Thus, the process $(\dabs{B_t}^2 - dt)_{ t \geq 0}$ is a ($\sF_{t^+}$)-martingale.

%Note that the alternative proof by using It\^o formula is given later\footnote{need link}.

%\qcutline

\item [(iii)] To check integrability, note that by Proposition \ref{pro:mgf_gaussian},
\be
\E\brb{\exp(\lm B_t)} = \exp\brb{t\lm^2/2}
\ee
whenever $B$ is a standard Brownian motion, and since $\abs{e^z} = e^{\Re z}$, then we have
\beast
\E\abs{\exp\inner{u}{B_t}} & = & \E\abs{\exp\inner{u}{(B_t - B_0 + B_0)}} = \E\abs{\exp \inner{u}{B_t - B_0} \exp \inner{u}{B_0}}\\
& = & \E\abs{\exp\inner{u}{B_t - B_0}}\E\abs{\exp\inner{u}{B_0}}\quad (\text{Proposition \ref{pro:expectation_of_independent_product}})\\
& = & \exp\brb{t\sum^d_{i=1} (\Re u_i)^2/2}\E\abs{\exp\inner{u}{B_0}} < \infty.
\eeast

Then, for $s\leq t$,
\beast
\E\brb{M_t|\sF_{s^+}} & = & e^{-\abs{u}^2t/2}\E\brb{\left.\exp\inner{u}{B_t}\right|\sF_{s^+}} = e^{-\abs{u}^2t/2}\E\brb{\left.\exp\inner{u}{B_t-B_{s^+}}\exp\inner{u}{B_{s^+}}\right|\sF_{s^+}} \\
& \stackrel{\text{a.s.}}{=} &  e^{-\abs{u}^2t/2}\exp\inner{u}{B_{s^+}} \E\brb{\exp\inner{u}{B_t-B_{s^+}}|\sF_{s^+}} \quad (\text{Proposition \ref{pro:conditional_expectation_tower_independence}.(iv)})\\
& \stackrel{\text{a.s.}}{=} &  e^{-\abs{u}^2t/2}\exp\inner{u}{B_{s^+}} \E\brb{\exp\inner{u}{B_t-B_{s^+}}} \quad (\text{Proposition \ref{pro:conditional_expectation_tower_independence}.(v)})\\
& \stackrel{\text{a.s.}}{=} &  e^{-\abs{u}^2t/2}\exp\inner{u}{B_{s^+}} \exp\brb{(t-s^+)\sum^d_{i=1} u_i^2/2} \\
& = &  e^{-\abs{u}^2t/2}  e^{\abs{u}^2(t-s^+)/2} \exp\inner{u}{B_{s^+}} \stackrel{\text{a.s.}}{=} e^{-\abs{u}^2s^+/2} \exp\inner{u}{B_{s}} = M_{s}.
\eeast

Therefore, $M$ is a ($\sF_{t^+}$)-martingale.
\een
\end{proof}

\begin{proposition}\label{pro:third_fourth_moments_of_brownian_motion_martingale}
Let $(B_t)_{t \geq 0}$ be an $(\sF_t)$-Brownian motion in $d \geq 1$ dimensions with $\dabs{B_t}^2 = B_t^TB_t$ and $B_0 \in \sL^4(\Omega,\sF,\pro)$. Then the processes
\be
\brb{B_t\dabs{B_t}^2 - (d+2)tB_t}_{t \geq 0},\qquad \brb{\dabs{B_t}^4 - 2(d+2)t\dabs{B_t}^2 + d(d+2)t^2}_{t\geq 0}
\ee
are $(\sF_{t^+})$-martingales.
\end{proposition}

\begin{remark}
Thus, we have that for the case $B_0 = 0$ a.s.,
\be
\E\dabs{B_t}^4 = \E\brb{2(d+2)t\dabs{B_t}^2} - d(d+2)t^2 = d(d+2)t^2.
\ee
\end{remark}

\begin{example}
It is easy to see that for $d =1$,
\be
B_t^3 - 3tB_t,\qquad B_T^4 - 6tB_t^2 + 3t^2
\ee
are actually martingales. Also, for $M_t = \exp\brb{uB_t - \frac {u^2t}2}$,
\beast
B_t & = & \left.\frac{\partial M_t}{\partial u}\right|_{u=0} ,\\
B_t^2 - t & = & \left.\frac{\partial^2 M_t}{\partial u^2}\right|_{u=0}, \\
B_t^3 - 3tB_t & = & \left.\frac{\partial^3 M_t}{\partial u^3}\right|_{u=0} ,\\
B_t^4 - 6tB_t^2 + 3t^2 & = & \left.\frac{\partial^4 M_t}{\partial u^4}\right|_{u=0} .
\eeast
\end{example}

\begin{proof}[\bf Proof]
By the same arguments in proof of Theorem \ref{thm:brownian_motion_martingale}, we have that these two processes is $\sF_{t^+}$-measurable at time $t$. The integrability is also implied by the finite moment of multidimensional Gaussian random variable.

In particular,
\beast
\E\brb{B_t \dabs{B_t}^2|\sF_{s^+}} & = & \E\brb{(B_t-B_s + B_s)\brb{\left.\dabs{B_t-B_s}^2 + 2 (B_t-B_s)^T B_{s^+} + \dabs{B_{s^+}}^2}\right|\sF_{s^+}}\\
& \stackrel{\text{a.s.}}{=} & \E\brb{(B_t-B_s)\dabs{B_t-B_s}^2}  + B_s\E\brb{\dabs{B_t-B_s}^2}  + 2 \E\brb{(B_t-B_s)(B_t-B_s)^T} B_{s^+} \\
& & \qquad + 2B_{s^+}B_{s^+}^T \E(B_t-B_s)  + \dabs{B_{s^+}}^2\E(B_t-B_s) + \dabs{B_{s^+}}^2B_{s^+} \\
& = & B_s\E\brb{\dabs{B_t-B_s}^2}  + 2 \E\brb{(B_t-B_s)(B_t-B_s)^T} B_{s^+}  + \dabs{B_{s^+}}^2B_{s^+} \\
& = & d(t-s) B_{s^+} + 2(t-s)B_{s^+}  + B_{s^+}\dabs{B_{s^+}}^2= (d+2)(t-s) B_{s^+}  + B_{s^+}\dabs{B_{s^+}}^2.
\eeast

\beast
\E\brb{tB_t|\sF_{s^+}} = \E\brb{t(B_t-B_s + B_s|\sF_{s^+}} \stackrel{\text{a.s.}}{=}  tB_{s^+}.
\eeast

Therefore,
\beast
\E\brb{\left. B_t \dabs{B_t}^2 - (d+2)tB_t\right|\sF_{s^+}} & \stackrel{\text{a.s.}}{=} &  (d+2)(t-s) B_{s^+}  + B_{s^+}\dabs{B_{s^+}}^2 - (d+2)tB_{s^+} \\
& = & B_{s^+}\dabs{B_{s^+}}^2 - (d+2)s^+B_{s^+}
\eeast

For the fourth moment, we let $B_t^i$ be the $i$th element of vector $B_t$ and have
\beast
\E\brb{\left.\dabs{B_t}^4\right|\sF_{s^+}} & = & \E\brb{\left.\brb{\dabs{B_t}^2}^2\right|\sF_{s^+}} = \E\brb{\left.\brb{B_t^TB_t}^2\right|\sF_{s^+}} = \E\brb{\left.\brb{\sum^d_{i=1}\brb{B_t^i}^2 }^2\right|\sF_{s^+}} \\ %+ 2\sum_{1\leq i< j\leq d}B_t^iB_t^j
%& = & \E\brb{\left.\brb{\sum^d_{i=1}\brb{B_t^i}^2}^2 + 4\brb{\sum^d_{i=1}\brb{B_t^i}^2 \sum_{1\leq i< j\leq d}B_t^iB_t^j}+ 4 \brb{\sum_{1\leq i< j\leq d}B_t^iB_t^j}^2\right|\sF_{s^+}} \\
& = & \E\brb{\left.\sum^d_{i=1}\brb{B_t^i}^4 + 2\sum_{1\leq i<j\leq d}\brb{B_t^i}^2 \brb{B_t^j}^2\right|\sF_{s^+}} %+ 4\brb{\sum^d_{k=1}\brb{B_t^k}^2 \sum_{1\leq i< j\leq d}B_t^iB_t^j}+ 4 \brb{\sum_{1\leq i< j\leq d}B_t^iB_t^j}^2
\eeast

The first term is
\beast
& & \E\brb{\left.\sum^d_{i=1}\brb{B_t^i}^4\right|\sF_{s^+} } = \E\brb{\left. \sum^d_{i=1}\brb{B^i_t}^4\right|\sF_{s^+} }  = \E\brb{\left. \sum^d_{i=1}\brb{B^i_t-B_s^i + B_s^i}^4\right|\sF_{s^+} }  \\
& = & \E\brb{\left.\sum^d_{i=1}\brb{B^i_t-B^i_s}^4 + 4\brb{B^i_t-B^i_s}^3B_s^i + 6\brb{B^i_t-B^i_s}^2\brb{B_s^i}^2 + 4\brb{B^i_t-B^i_s}\brb{B_s^i}^3 + \brb{B_s^i}^4\right|\sF_{s^+}} \\
& \stackrel{\text{a.s.}}{=}  &  3d(t-s^+)^2 + 6(t-s^+)\sum^d_{i=1}\brb{B_{s^+}^i}^2 + \sum^d_{i=1}\brb{B_{s^+}^i}^4 \\
& = &  3d(t-s^+)^2 + 6(t-s^+)\dabs{B_{s^+}}^2 + \sum^d_{i=1}\brb{B_{s^+}^i}^4 .
\eeast

The second term is
\beast
& & 2\E\brb{\left.\sum_{1\leq i<j\leq d}\brb{B_t^i}^2 \brb{B_t^j}^2\right|\sF_{s^+}} = 2\E\brb{\left.\sum_{1\leq i<j\leq d}\brb{B_t^i-B_s^i+ B_s^i}^2 \brb{B_t^j-B_s^j+ B_s^j}^2\right|\sF_{s^+}} \\
& = & 2\E\brb{\left.\sum_{1\leq i<j\leq d}\brb{\brb{B_t^i-B_s^i}^2 + 2\brb{B_t^i-B_s^i}B_s^i+  \brb{B_s^i}^2} \brb{\brb{B_t^j-B_s^j}^2 + 2\brb{B_t^j-B_s^j}B_s^j+  \brb{B_s^j}^2}\right|\sF_{s^+}} \\
& \stackrel{\text{a.s.}}{=} & 2\brb{\sum_{1\leq i<j\leq d}(t-s)^2 + (t-s)\brb{ \brb{B_s^i}^2 + \brb{B_s^j}^2}+ \brb{B_s^i}^2\brb{B_s^j}^2 } \\
& = & d(d-1)(t-s^+)^2 + 2(t-s^+) \sum_{1\leq i<j\leq d}\brb{\brb{B_{s^+}^i}^2 +  \brb{B_s^j}^2} + 2\sum_{1\leq i<j\leq d}\brb{B_{s^+}^i}^2\brb{B_{s^+}^j}^2 \\
& = & d(d-1)(t-s^+)^2 + 2(t-s^+) (d-1)\dabs{B_{s^+}}^2 + 2\sum_{1\leq i<j\leq d}\brb{B_{s^+}^i}^2\brb{B_{s^+}^j}^2 .
\eeast

%The third term is
%\beast
%& & 4\E\brb{\left.\sum^d_{k=1}\brb{B_t^k}^2 \sum_{1\leq i< j\leq d}B_t^iB_t^j\right|\sF_{s^+}} \\
%& = & 4\E\brb{\left.\sum^d_{k=1}\brb{B_t^k-B_s^k + B_s^k}^2 \sum_{1\leq i< j\leq d}(B_t^i-B_s^i + B_s^i)(B_t^j-B_s^j + B_s^j)\right|\sF_{s^+}} \\
%& \stackrel{\text{a.s.}}{=} & 4\E\brb{\left.\sum^d_{k=1}\brb{B_s^k}^2 \sum_{1\leq i< j\leq d}(B_t^i-B_s^i + B_s^i)(B_t^j-B_s^j + B_s^j)\right|\sF_{s^+}}  \stackrel{\text{a.s.}}{=}  4\sum^d_{k=1}\brb{B_{s^+}^k}^2 \sum_{1\leq i< j\leq d}B_{s^+}^i B_{s^+}^j.
%\eeast
%
%The fourth term is
%\beast
%& & 4 \E\brb{\left.\brb{\sum_{1\leq i< j\leq d}B_t^iB_t^j}^2\right|\sF_{s^+}}  = 4 \E\brb{\left.\sum_{1\leq i< j\leq d}B_t^iB_t^j\sum_{1\leq m< n\leq d}B_t^mB_t^n\right|\sF_{s^+}} \\
%& = & 4 \E\brb{\left.\sum_{1\leq i< j\leq d}(B_t^i - B_s^i + B_s^i)(B_t^j - B_s^j + B_s^j)\sum_{1\leq m< n\leq d}(B_t^m - B_s^m + B_s^m)(B_t^n - B_s^n + B_s^n)\right|\sF_{s^+}} \\
%& \stackrel{\text{a.s.}}{=} & 4 \E\brb{\left.\sum_{1\leq i< j\leq d}(B_t^i - B_s^i)^2(B_t^j - B_s^j)^2 + \sum_{1\leq i,j<n\leq d}B_t^i B_s^j(B_t^n - B_s^n)^2 + \sum_{1\leq m< i,j\leq d}B_t^i B_s^j(B_t^m - B_s^m)^2 \right|\sF_{s^+}} \\
%& & \qquad + 4\E \brb{\left. \sum_{1\leq i< j\leq d}\sum_{1\leq m< n\leq d} B_s^i B_s^jB_s^m B_s^n\right|\sF_{s^+}} \\
%& \stackrel{\text{a.s.}}{=} & 2d(d-1)(t-s)^2  + 4(t-s)\sum_{1\leq i,j<n\leq d}B_t^i B_s^j + 4(t-s)\sum_{1\leq m< i,j\leq d}B_t^i B_s^j+ 4\sum_{1\leq i< j\leq d}\sum_{1\leq m< n\leq d} B_s^i B_s^jB_s^m B_s^n \\
%\eeast

Then by Theorem \ref{thm:brownian_motion_martingale},
\be
\E\brb{\left. t\dabs{B_t}^2\right|\sF_{s^+}} =  t \E\brb{\left. \dabs{B_t}^2\right|\sF_{s^+}} =  t\brb{ d(t-s^+) + \dabs{B_{s^+}}^2} = dt(t-s^+) + t\dabs{B_{s^+}}^2.
\ee

Thus,
\beast
& & \E\brb{\left. \dabs{B_t}^4 - 2(d+2)t\dabs{B_t}^2\right|\sF_{s^+}} \\
& \stackrel{\text{a.s.}}{=}  &  d(d+2)(t-s^+)^2 + 2(d+2)(t-s^+)\dabs{B_{s^+}}^2 + \dabs{B_{s^+}}^4 -2d(d+2)t(t-s^+) - 2(d+2)t\dabs{B_{s^+}}^2 \\
&= & \dabs{B_{s^+}}^4 - 2(d+2)s^+\dabs{B_{s^+}}^2 + d(d+2)(t-s^+)^2 -2d(d+2)t(t-s^+) \\
& = &  \dabs{B_{s^+}}^4 - 2(d+2)s^+\dabs{B_{s^+}}^2 - d(d+2)\brb{t^2-\brb{s^+}^2}.
\eeast

Hence, it is easy to see that
\be
\dabs{B_t}^4 - 2(d+2)t\dabs{B_t}^2 + d(d+2)t^2
\ee
is a martingale.
\end{proof}


%begin{proposition}
%et $(B_t)$ be a standard Brownian motion and $d = 1$, and for $x \in \R$, $T_x = \inf\{t \geq 0 | B_t = x\}$. Then for $x, y > 0$, $\pro(T_x < T_{-y}) = \frac y{x+y}$ and $\E[T_x \land T_{-y}] = x y$.
%\end{proposition}



\begin{theorem}\label{thm:brownian_motion_integral_martingale}
Let $f : \R^+ \times \R^d \to \R,\ (t,x) \mapsto f(t,x)$ be $C^{1,2}$ (i.e., continuously differentiable in the first coordinate (time) and twice continuously differentiable in the second coordinate (space)) and $(B_t)_{t\geq 0}$ is a standard Brownian motion with respect to $(\sF_t)_{t\geq 0}$. %, and be such that all partial derivatives are bounded.
If there exists a constant $K$ such that, for all $t\geq 0$, $x\in \R^d$,
\be
\abs{f(t,x)} + \abs{\fp{f(t,x)}{t}} + \sum^d_{i=1} \abs{\fp{f(t,x)}{x_i}} + \sum^d_{i=1}\sum^d_{i=1} \abs{\fp{f(t,x)}{x_i}} \leq K e^{K(t+\dabs{x})}\qquad (*),
\ee

then
\be
M_t := f (t, B_t )- f (0, B_0)- \int^t_0 \sL f(s,B_s)ds
\ee
is an $(\sF_{t^+})$-martingale, where (for the Laplaceian $\Delta$)
\be
\sL f(t,B_t) := \fp{f(t,x)}{t} + \frac 12 \Delta f(t,x) = \fp{f(t,x)}{t} + \frac12 \sum^d_{i=1} \frac{\partial^2 f (t, x)}{\partial x^2_i}.
\ee
\end{theorem}

\begin{remark}
We can extend this to the Brownian motions with start point $x$ a.s., i.e. $\wt{B}_t = x+ B_t$ where $B_t$ is standard Brownian motions.

If we assume that $f$ and its derivatives up to second order are bounded, they will satisfy $(*)$ automatically and the conclusion will hold for any Brownian motion (see Definition \ref{def:brownian_motion_d}).
\end{remark}

\begin{example}
\ben
\item [(i)] Let $f(t,x) = x$, then $M_t = B_t - B_0 - \int^t_0 0\ ds = B_t$ which is Theorem \ref{thm:brownian_motion_martingale}.(i).

\item [(ii)] Let $f(t,x) = x^2$, then $M_t = B_t^2 - B_0^2 - \frac 12 \int^t_0 2d\ ds = B_t^2 - dt$ which is Theorem \ref{thm:brownian_motion_martingale}.(ii).
\een
\end{example}

\begin{proof}[\bf Proof]
First we prove that $K e^{K(t+\dabs{B_t})}$ is integrable for any $t\geq 0$.
\be
\E\abs{K e^{K(t+\dabs{B_t})}} = Ke^{Kt} \E\brb{e^{K\dabs{B_t}}} \leq 2Ke^{Kt} \E\brb{e^{K (B_t-B_0} + KB_0} = 2Ke^{2Kt}\E\brb{e^{KB_0}} < \infty
\ee
as $B_0 = 0$ a.s.. Then for any $t\geq 0$, by ($*$),
%\be
%\E\abs{M_t} = \E\abs{f (t, B_t )- f (0, B_0)- \int^t_0 \sL f(s,B_s)ds} \leq \E\abs{K e^{K(t+\dabs{B_t})}} + 1 + \E\abs{\int^t_0 K e^{K(s+\dabs{B_s})}ds}
%\ee
\beast
\E\abs{M_t} & = & \E\abs{f (t, B_t )- f (0, B_0)- \int^t_0 \sL f(s,B_s)ds} \leq \E\abs{K e^{K(t+\dabs{B_t})}} + 1 + \E\abs{\int^t_0 K e^{K(s+\dabs{B_s})}ds} \\
& = & \E\abs{K e^{K(t+\dabs{B_t})}} + 1 + \E\abs{e^{K\dabs{B_t}} \int^t_0 K e^{Ks}ds} = \E\abs{K e^{K(t+\dabs{B_t})}} + 1 + \E\abs{e^{K\dabs{B_t}} \brb{e^{Kt}-1}}\qquad (\dag)\\
& \leq & \E\abs{(K+1) e^{K(t+\dabs{B_t})}} + 1 < \infty
\eeast
as $K e^{K(t+\dabs{B_t})}$ is integrable for any $t\geq 0$. Therefore, $M_t$ is integrable.
%\E\abs{K e^{K(t+\abs{B_t})}} + 1 + \E\abs{\int^t_0 K e^{K(s+ S_s)}ds} \leq \E\abs{K e^{K(t+\abs{B_t})}} + 1 + \E\abs{\int^t_0 K e^{K(s+ S_t)}ds} \\
%Since $\abs{B_t}$ has the same distribution as $S_t = \sup_{0\leq s\leq t}B_s$ (Corollary \ref{cor:abs_sup_equal_brownian_motion}),

From the assumption, $f(t,x)$ is continuous with respect to $x$. We know that
\be
\sL f(t,B_t) = \fp{f(t,x)}{t} + \frac12 \sum^d_{i=1} \frac{\partial^2 f (t, x)}{\partial x^2_i}
\ee
is dominated by $K e^{K(t+\dabs{B_t})}$ which is integrable. Furthermore, we have $\sL f(t,B_t+\ve) \to \sL f(t,B_t)$ as $\ve\to 0$ since $f(t,x)$ is twice continuously differentiable with respect to $x$. Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}),
\be
\int^t_0 \sL f(s,B_s+\ve)ds \to \int^t_0 \sL f(s,B_s)ds,\qquad \text{as }\ve \to 0.
\ee

Thus, $\int^t_0 \sL f(s,B_s)ds$ is continuous and therefore $M_t$ is $\sF_t$-measurable (and $\sF_{t^+}$-measurable) by Proposition \ref{pro:continuous_measurable}.

We will now show the martingale property. Let $t\geq 0$. Then
\beast
M_{t+s} - M_s & = & f(t+s,B_{t+s}) -f(s,B_s) - \int^{s+t}_s \brb{\fp{}{r} + \frac 12 \Delta}f(r,B_r)dr\\
& = & f(t+s,B_{t+s}) -f(s,B_s) - \int^{t}_0 \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr.
\eeast

Thus, by Proposition \ref{pro:conditional_expectation_basic_property}.(iii),
\be
\E\brb{\left.M_{t+s} - M_s\right|\sF_{s^+}} = \E\brb{\left.f(t+s,B_{t+s})\right|\sF_{s^+}} -f(s,B_s) - \E\brb{\left.\int^{t}_0 \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \right|\sF_{s^+}}
\ee

%Then since $B_{s} = B_{s^+}$ a.s. (sample continuity of standard Brownian motion),%

Since $B_{t+s} - B_s$ is independent of $\sF_{s^+}$ by Theorem \ref{thm:simple_markov_property_brownian_motion} and $B_s$ is $\sF_{s^+}$-measurable, by Proposition \ref{pro:density_function_probability} and Proposition \ref{pro:conditional_expectation_tower_independence}.(v),
\be
\E\brb{\left.f(t+s,B_{t+s})\right|\sF_{s^+}} = \E\brb{\left.f(t+s,B_{t+s} - B_s + B_s)\right|\sF_{s^+}} \stackrel{\text{a.s.}}{=}  \int_{\R^d} \brb{f(t+s,x + B_{s})} p_t(x)dx \qquad (\dag\dag)
\ee
where
\be
p_t(x) = \frac 1{(2\pi t)^{d/2}} \exp\brb{-\frac{\dabs{x}^2}{2t}}
\ee
is defined in Definition \ref{def:transition density_standard_brownian_motion} and satisfies (by Proposition\footnote{need proposition})
\be
\brb{\fp{}{t} - \frac 12\Delta} p_t(x) = 0\qquad (\dag\dag\dag).
\ee

Let
\be
Y = \E\brb{\left.\int^{t}_0 \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \right|\sF_{s^+}},\qquad Z = \E\brb{\left.\brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})\right|\sF_{s^+}}.
\ee

Then for any $A\in \sF_{s^+}$, by definition of conditional expectation and Fubini theorem (Theorem \ref{thm:fubini} as $\int^t_0\brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr$ is integrable\footnote{it dominated by $e^{K\dabs{B_t}} \brb{e^{Kt}-1}$, see ($\dag$) which is integrable.})
\beast
\E\brb{Y\ind_A} & = & \E\brb{\int^{t}_0 \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \ind_A} = \int^{t}_0 \E\brb{ \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})\ind_A} dr\\
& = & \int^t_0 \E(Z\ind_A)dr = \E\brb{\int^t_0 Z dr \ind_A} \ \ra \ Y = \int^t_0 Z dr \text{ a.s.}
\eeast

That is,
\be
\E\brb{\left.\int^{t}_0 \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \right|\sF_{s^+}} = \int^t_0 \E\brb{\left.\brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})\right|\sF_{s^+}} dr \text{ a.s.}
\ee

Also, by the same argument in $(\dag\dag)$, we have
\be
\E\brb{\left.\brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})\right|\sF_{s^+}} \stackrel{\text{a.s.}}{=}  \int_{\R^d} \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,x+ B_s) p_r(x) dx.
\ee

Then
\beast
\E\brb{\left.\int^{t}_0 \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \right|\sF_{s^+}} & \stackrel{\text{a.s.}}{=} & \int^t_0 \int_{\R^d} \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,x+ B_s) p_r(x) dx dr\\
& = & \lim_{\ve \to 0} \int^t_\ve \int_{\R^d} \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,x+ B_s) p_r(x) dx dr
\eeast
by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}). Furthermore, using integral by parts, we have
\beast
& & \int^t_\ve \int_{\R^d} \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,x+ B_s) p_r(x) dx dr \\
& = & \int_{\R^d} \int^t_\ve \fp{}{r} f(r+s,x+ B_s) p_r(x) dr dx + \int^t_\ve \int_{\R^d} \frac 12 \Delta f(r+s,x+ B_s) p_r(x) dx dr\\
& = & \int_{\R^d} \brb{f(t+s,x+B_s)p_t(x) - f(\ve+s,x+B_s)p_\ve(x) - \int^t_\ve f(r+s,x+ B_s) \fp{}{r} p_r(x) dr} dx \\
& & \qquad + \frac 12\int^t_\ve \brb{ \left.\sum^d_{i=1} \fp{f(r+s,x+B_s)}{x_i} p_r(x)\right|_{\R^d} - \left. f(r+s,x+B_s) \sum^d_{j=1}  \fp{p_r(x)}{x_j}\right|_{\R^d} + \int_{\R^d}  f(r+s,x+B_s) \Delta p_r(x)dx}  dr\\
& = & \int_{\R^d} \brb{f(t+s,x+B_s)p_t(x) - f(\ve+s,x+B_s)p_\ve(x)}dx - \int_{\R^d}  f(r+s,x+B_s) \brb{\fp{}{r} - \frac 12\Delta} p_r(x)dx dr
\eeast
by Fubini theorem (Theorem \ref{thm:fubini} by assumption ($*$)). Then by $(\dag\dag\dag)$, we have
\beast
\E\brb{\left.\int^{t}_0 \brb{\fp{}{r} + \frac 12 \Delta}f(r+s,B_{r+s})dr \right|\sF_{s^+}} \stackrel{\text{a.s.}}{=} \lim_{\ve \to 0} \int_{\R^d} \brb{f(t+s,x+B_s)p_t(x) - f(\ve+s,x+B_s)p_\ve(x)}dx\qquad (\dag\dag\dag\dag)
\eeast

Then combining $(\dag\dag)$ and $(\dag\dag\dag\dag)$, we have that
\beast
\E\brb{\left.M_{t+s} - M_s\right|\sF_{s^+}} & \stackrel{\text{a.s.}}{=} & \lim_{\ve \to 0}\int_{\R^d} f(\ve+s,x+B_s)p_\ve(x)dx - f(s,B_s)\\
& \stackrel{\text{a.s.}}{=} & \lim_{\ve \to 0} \E\brb{f(\ve+s,B_{s+\ve})|\sF_{s^+}} - f(s,B_s) = 0
\eeast
by the continuity of Brownian motion and of $f$ and by the conditional dominated convergence theorem (Theorem \ref{thm:dominated_convergence_conditional_expectation}).

Furthermore, we know that $M_t$ is continuous with respect to $t$ as $f$ is continuous and the integral is also continuous by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}) as the integrand is dominated by $Ke^{K(t+\dabs{B_t})}$ (which is integrable).

Therefore, $M_s = M_{s^+}$ and hence $(M_t)_{t\geq 0}$ is an $(\sF_{t^+})$-martingale.
\end{proof}

%We prove the case where $f (t, x) = f (x)$ does not depend on time. Let $p_t (x) = \frac 1{(2\pi t)^{\frac d2}} \exp\brb{- \frac 1{2t}\abs{x}^2}$ and let $g$ be bounded and continuous. We claim that if $(B_t)$ is a standard Brownian motion then
%\be
%\int^t_0 \int_{\R^d} g(x)\fp{}{s} p_s(x)d xds = \E[g(B_t )]- g(0).
%\ee

%Indeed,
%\beast
%\int^t_0 \int_{\R^d} g(x)\fp{}{s} p_s(x)d xds & = & \lim_{\ve\to 0} \int^t_\ve \int_{\R^d}g(x) \fp{}{s}p_s(x)d xds = \lim_{\ve\to 0} \int_{\R^d} g(x)(p_t (x)- p_\ve(x))d x\\
%& = & \lim_{\ve\to 0}\E[g(B_t )]-\E[g(B_\ve)] = \E[g(B_t )]- g(0)
%\eeast
%by DCT since $B_\ve \to 0$ and $g$ is bounded.

%For $s, t \geq 0$,
%\be
%\E\bsb{f (B_{s+t})- f (B_t )- \int^{t+s}_t \frac {\Delta}2 f (B_u)du | \sF_t} = \underbrace{\int p_s(x) f(x + B_t )dx - f (B_t )}_{(1)} - \underbrace{\E\bsb{\int^{t+s}_t \frac {\Delta}2 f (B_u - B_t + B_t )du |\sF_t}}_{(2)}.
%\ee
%But
%\beast
%(2) & = & \E\bsb{\int^s_0 \frac {\Delta}2 f (B_{t+u} - B_t + B_t )du |\sF_t} = \int_{W} W_0(dw) \int^s_0 \frac {\Delta}2 f (w_u + B_t )du\\
%& = & \int^s_0 du \int_{W} W_0(dw)\frac {\Delta}2 f (w_u + B_t ) = \int^s_0 du\int p_u(x)d x \frac {\Delta}2 f (x + B_t )
%\eeast

%By integrating by parts, this is equal to
%\be
%(2) = \int^s_0 du \int \frac {\Delta}2 p_u(x) f (x + B_t )d x.
%\ee

%It can be checked that $\brb{\fp{}{t} - \frac {\Delta}2}p_t (x) = 0$, i.e. the density of the Gaussian law is a solution to the heat equation. Therefore
%\be
%(2) = \int^s_0 du \int \partial_up_u(x) f (x + B_t )d x = \int_{\R^d} p_s(x) f (x + B_t )d x - f (B_t ) = (1)
%\ee
%by the claim.


%\subsection{Range of Brownian motions}%Transition densities}

\subsection{Maximum, minimum of Brownian motions}%Transition densities}

\begin{proposition}\label{pro:density_bm_maximum_minimum}
Let $B$ be a Brownian motion in $\R$ with $B_t\sim \sN(0,\sigma^2 t)$, $S_t:= \sup_{0\leq s\leq t}B_s$ and $I_t:= \inf_{0\leq s\leq t}B_s$. Then the transition densities of $S_t$ and $I_t$ (for $y,z$ respectively) are
\be
f_t(y) =  \sqrt{\frac{2}{\pi\sigma^2 t}} \exp\brb{-\frac {y^2}{2\sigma^2 t}} ,\quad f_t(z) =  \sqrt{\frac{2}{\pi\sigma^2 t}} \exp\brb{-\frac {z^2}{2\sigma^2 t}} \quad \text{where }y>0,z<0.
\ee

Also,
\be
\E S_t = \sigma\sqrt{\frac{t}{2\pi}},\qquad \E I_t = -\sigma\sqrt{\frac{t}{2\pi}}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
We know that by the reflection principle (Theorem \ref{thm:reflection_principle_brownian_motion_upper_bound})
\be
\pro\brb{S_t \leq y} = \pro\brb{\abs{B_t} \leq y} = \frac{2}{\sqrt{2\pi\sigma^2t}}\int^y_0 \exp\brb{-\frac {x^2}{2\sigma^2t}}dx =  \frac{2}{\sqrt{2\pi}}\int^{\frac{y}{\sqrt{\sigma^2t}}}_0 \exp\brb{-\frac {x^2}{2}}dx.
\ee

Taking the differentiation with respect to $y$, we have that
\be
f_t(y) = \sqrt{\frac{2}{\pi\sigma^2 t}} \exp\brb{-\frac {y^2}{2\sigma^2 t}}.
\ee

Similarly, we have have the result for $I_t$. The expectations can be obtained easily.
\end{proof}


\begin{proposition}\label{pro:pairwise_joint_density_bm_maximum_minimum_current}
Let $B$ be a Brownian motion in $\R$ with $B_t\sim \sN(0,\sigma^2 t)$, $S_t:= \sup_{0\leq s\leq t}B_s$ and $I_t:= \inf_{0\leq s\leq t}B_s$. Then the joint transition densities of $B_t$, $S_t$ and $I_t$ (for $x,y,z$ respectively) are
\be
p_t(x,y) = \frac{2(2y-x)}{\sigma^3\sqrt{2\pi t^3}}e^{-\frac{1}{2\sigma^2t}(2y-x)^2},\quad p_t(x,z) = \frac{2(x-2z)}{\sigma^3\sqrt{2\pi t^3}}e^{-\frac{1}{2\sigma^2t}(x-2z)^2}\quad \text{where }y>x>z,y>0,z<0.
\ee
\end{proposition}

\begin{remark}
Taking integral with respect to $x$ or $y$ or $z$ respectively, we can have the transition density functions of $B_t$, $S_t$ and $I_t$ (which are consistent with Corollary \ref{cor:abs_sup_equal_brownian_motion}).
\end{remark}

\begin{proof}[\bf Proof]
Let $B_t = -W_t$ where $W$ is also a Brownian motion, then by the reflection principle (Theorem \ref{thm:reflection_principle_brownian_motion_upper_bound}), for $x>y$, $y<0$
\be
\pro\brb{I_t < z, B_t\geq x} = \pro\brb{\sup_{0\leq s\leq t} W_s >-z, W_t \leq -x} = \pro\brb{W_t \geq -2z +x} = \pro\brb{B_t\leq 2z-x}
\ee

Thus,
\be
\pro\brb{I_t \geq z, B_t\geq x} = \pro(B_t\geq x) - \pro\brb{I_t < z, B_t\geq x} = \pro(B_t\geq x) - \pro\brb{B_t\leq 2z-x}
\ee
and taking derivatives with respect to $x$ and $z$, we have
\be
p_t(x,z)=\frac{2(x-2z)}{\sigma^3\sqrt{2\pi t^3}}e^{-\frac{1}{2\sigma^2t}(x-2z)^2}\quad \text{where }x>z,z<0.
\ee

It is similar for joint density of $B_t$ and $S_t$.
\end{proof}



\begin{theorem}\label{thm:joint_density_maximum_minimum_bm}
Let $X_t = \sigma B_t$ where $B$ be a standard Brownian motion in $\R$, $Y_t:= \sup_{0\leq s\leq t}X_s$ and $Z_t:= \inf_{0\leq s\leq t}X_s$. Then the joint transition densities of $X_t$, $Y_t$ and $Z_t$ (for $x,y,z$ respectively) is
\be
f_t(x,y,z) = 4 \sum^\infty_{k=-\infty}  k^2 \phi\brb{x-2k(y-z)} - k(k-1)\phi\brb{x-2y+2k(y-z)}.
\ee
where
\be
\phi(x) = \frac{x^2 - \sigma^2 t}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{x^2}{2\sigma^2 t}}.
\ee

Also, for $a\geq 0$, $W_t = \max_{0\leq s\leq t}\abs{X_s}$, %\be%\pro\brb{Y_t \leq y,Z_t \geq z} = \exp\brb{ - \frac{\mu^2t}{2\sigma^2}} \sum_{k=1}^\infty \frac{2k\pi\sigma^4 \brb{\exp\brb{\frac{\mu y}{\sigma^2}} - (-1)^k \exp\brb{\frac{\mu z}{\sigma^2}}} }{k^2\pi^2\sigma^4  + \mu^2(y-z)^2}\exp \brb{- \frac{k^2\pi^2 \sigma^2 t}{2(y-z)^2}}\sin\frac{k\pi y}{y-z}.\ee
\be
f_{W_t}(a)= \frac 4{\sqrt{2\pi \sigma^2 t}}  \sum^\infty_{k=1}(-1)^{k+1}(2k-1)\exp\brb{-\frac{(2k-1)^2a^2}{2\sigma^2 t}},\qquad \E W_t = \sigma\sqrt{\frac{\pi t}{2}}.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
By method of images (as in the proof of Theorem \ref{thm:bm_range_transition_density}), we have that for $y>0,z<0$,
\be
f_{X_t}(x,Y_t \leq y,Z_t \geq z) = \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \brb{\exp\brb{-\frac{(x + 2kz - 2ky)^2}{2\sigma^2 t}} - \exp\brb{-\frac{(x -2kz + 2(k-1)y)^2}{2\sigma^2 t}}}.\qquad (*)
\ee

Then
\beast
& & f_{X_t,Z_t}(x,Y_t \leq y, z) \\
& = & - \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \frac{-2k(x-2k(y-z))}{\sigma^2 t}\exp\brb{-\frac{(x-2k(y-z))^2}{2\sigma^2 t}} - \frac{2k(x-2y+2k(y-z))}{\sigma^2 t} \exp\brb{-\frac{(x-2y+2k(y-z))^2}{2\sigma^2 t}} \\
& = & \frac 1{\sqrt{2\pi \sigma^2 t}}   \sum^\infty_{k=-\infty} \frac{2k(x-2k(y-z))}{\sigma^2 t}\exp\brb{-\frac{(x-2k(y-z))^2}{2\sigma^2 t}} + \frac{2k(x-2y+2k(y-z))}{\sigma^2 t} \exp\brb{-\frac{(x-2y+2k(y-z))^2}{2\sigma^2 t}}
\eeast

Thus,
\beast
f_{X_t,Y_t,Z_t}(x,y, z) & = & \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \brb{\frac{4k^2(x-2k(y-z))^2}{\sigma^4 t^2} - \frac{4k^2}{\sigma^2 t}}\exp\brb{-\frac{(x-2k(y-z))^2}{2\sigma^2 t}} \\
& & + \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \brb{ \frac{4k(k-1)}{\sigma^2 t} - \frac{4k(k-1)(x-2y+2k(y-z))^2}{\sigma^4 t^2} }\exp\brb{-\frac{(x-2y+2k(y-z))^2}{2\sigma^2 t}} \\
& = & 4 \sum^\infty_{k=-\infty}  k^2 \phi\brb{x-2k(y-z)} - k(k-1)\phi\brb{(x-2y+2k(y-z)}.
\eeast
where $\phi(x) = \frac{x^2 - \sigma^2 t}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{x^2}{2\sigma^2 t}}$. From ($*$), we have
\be
\pro\brb{W_t \leq a} = \int^a_{-a}\frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \brb{\exp\brb{-\frac{(x - 4ka)^2}{2\sigma^2 t}} - \exp\brb{-\frac{(x + 2(2k-1)a)^2}{2\sigma^2 t}}}dx.
\ee

Therefore, taking differentiation with respect to $a$,
\beast
f_{W_t}(a) & = & \frac 4{\sqrt{2\pi \sigma^2 t}}  \sum^\infty_{k=-\infty} \int^a_{-a}\frac{2k(x+4ka)}{2\sigma^2 t}\exp\brb{-\frac{(x + 4ka)^2}{2\sigma^2 t}} + \frac{(2k-1)(x+ 2(2k-1)a)}{2\sigma^2 t}\exp\brb{-\frac{(x + 2(2k-1)a)^2}{2\sigma^2 t}}dx\\
& = & \frac 2{\sqrt{2\pi \sigma^2 t}}  \sum^\infty_{k=-\infty} 2k \exp\brb{-\frac{(4k+1)^2a^2}{2\sigma^2 t}} - 2k \exp\brb{-\frac{(4k-1)^2a^2}{2\sigma^2 t}}  \\
& & \quad - \frac 2{\sqrt{2\pi \sigma^2 t}}  \sum^\infty_{k=-\infty} (2k-1)\exp\brb{-\frac{(4k-1)^2a^2}{2\sigma^2 t}} + (2k-1)\exp\brb{-\frac{(4k-3)^2a^2}{2\sigma^2 t}}\\
& = & \frac 2{\sqrt{2\pi \sigma^2 t}}  \sum^\infty_{k=-\infty} (4k+1) \exp\brb{-\frac{(4k+1)^2a^2}{2\sigma^2 t}} - (4k-1)\exp\brb{-\frac{(4k-1)^2a^2}{2\sigma^2 t}}  \\
& = & \frac 4{\sqrt{2\pi \sigma^2 t}}  \sum^\infty_{k=1} (-1)^{k+1} (2k-1)\exp\brb{-\frac{(2k-1)^2a^2}{2\sigma^2 t}} .
\eeast

Hence,
\beast
\E W_t & = & \int^\infty_{0}\frac 4{\sqrt{2\pi \sigma^2 t}}  \sum^\infty_{k=1} (-1)^{k+1} (2k-1)a \exp\brb{-\frac{(2k-1)^2a^2}{2\sigma^2 t}} da \\
& = & \frac 4{\sqrt{2\pi \sigma^2 t}}  \sum^\infty_{k=1} (-1)^{k+1} (2k-1)\int^\infty_{0} a \exp\brb{-\frac{(2k-1)^2a^2}{2\sigma^2 t}} da \\
& = & \frac 4{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=1} (-1)^{k+1} \frac {\sigma^2 t}{2k-1} = 4 \sigma \sqrt{\frac{t}{2\pi}}\cdot \frac {\pi}4 =  \sigma \sqrt{\frac{\pi t}{2}}.
\eeast

The second last equality is due to Proposition \ref{pro:alternative_odd_series}.
\end{proof}


\subsection{Range of Brownian motions}%Transition densities}


\begin{theorem}[transition density of range]\label{thm:bm_range_transition_density}
Let $B$ be a Brownian motion in $\R$ with $B_t\sim \sN(0,\sigma^2 t)$\footnote{more general case needed, such as $B_0 \neq 0$}, $S_t := \sup_{0\leq s\leq t}B_s$, $I_t := \inf_{0\leq s\leq t}B_s$. The range of Brownian motion\index{range!Brownian motion} is defined by $R_t := S_t - I_t \in [0,\infty)$. Then the transition density of $R_t$ (first mentioned in \cite{Feller_1951}) is
\be
p_t(r) = \frac{8}{\sigma\sqrt{2\pi t}}\sum^\infty_{k=1}(-1)^{k+1}k^2 \exp\brb{-\frac{k^2r^2}{2\sigma^2 t}}.
\ee

Accordingly, we have
\be
\pro\brb{R_t\leq r} = 1 + 4\sum^\infty_{k=1}(-1)^{k}k \erfc\brb{\frac{kr}{\sigma\sqrt{2 t}}} = 1 + 4\sum^\infty_{k=1}(-1)^{k}k \brb{1 - \erf\brb{\frac{kr}{\sigma\sqrt{2 t}}}}
\ee
where $\erf(x)$ is error function and $\erfc(x)$ is complementary error function, $\erf(x) = \frac{2}{\sqrt{\pi}} \int^x_0 e^{-t^2}dt$ and $\erfc(x) = 1 - \erf(x)$.
\end{theorem}

\begin{remark}
The infinite sum of the series is convergent as it is alternating decreasing (see Theorem \ref{thm:alternating_series_test}).
\end{remark}

\begin{proof}[\bf Proof]
Let $w_t(x, -u\leq x\leq v)$ be the transition density of the Brownian motion with maximum $v$ and minimum $-u$ where $u,v\in\R^+$. It is the Green function\footnote{definition and method needed} for the equation of heat conduction in a finite interval $-u\leq x\leq v$ where the heat equation $\fp{w_t}{t} = \frac 12 \sigma^2 \frac{\partial^2 w_t}{\partial x^2}$ has Green function
\be
w_t(x) = \frac 1{\sqrt{2\pi \sigma^2 t}} \exp\brb{-\frac{x^2}{2\sigma^2 t}}.
\ee

So the problem has a new form with
\be
\fp{\wh{w}_t}{t} = \frac 12 \sigma^2 \frac{\partial^2 \wh{w}_t}{\partial x^2}
\ee
with boundary condition
\be
\wh{w}_t(x) = 0,\quad \text{when }x=-u,v.
\ee

Then by method of images\footnote{method needed, see Green function chapter in \cite{Zauderer_2006}}, we have the consistent result with \cite{Feller_1951}, for $x\in [-u,v]$\footnote{why is this positive? proof needed.},
\beast
\wh{w}_t(x) & = & \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \brb{\exp\brb{-\frac{(x-2k(u+v))^2}{2\sigma^2 t}} - \exp\brb{-\frac{(x+2u -2k(u+v))^2}{2\sigma^2 t}}}\\
& = & \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \brb{\exp\brb{-\frac{(x-2ku - 2kv)^2}{2\sigma^2 t}} - \exp\brb{-\frac{(x+2ku + 2(k-1)v)^2}{2\sigma^2 t}}}.
\eeast

Alternatively, this result can be approached by Fourier series and is consistent with Example \ref{exa:heat_equation_homogeneous_dirichlet_boundary_delta}.

Considering the probability $F_t(-u\leq x\leq v)$ of the event $S_t \leq v$ and $I_t \geq -u$, we know that for $x\in[-u,v]$,
\be
F_t(S_t \leq v, I_t \geq -u) = F_t(-u\leq x\leq v) = \int^v_{-u} \wh{w}_t(x) dx.
\ee

Also, by differentiation with respect to $u$ and $v$ we can find the joint pdf of $S_t$ and $I_t$, that is
\be
f_t(u,v) = \frac{\partial^2 F_t(S_t \leq v, I_t \geq -u)}{\partial u\partial v}.
\ee

To calculate this, we first take the differentiation with respect to $v$ and then by Fubini theorem (Theorem \ref{thm:fubini}) and Theorem \ref{thm:differentiation_under_integral_sign},
\beast
\frac{\partial F_t(S_t \leq v, I_t \geq -u)}{\partial v} & = & \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty}  \fp{}{v}\brb{\int^v_{-u} \brb{\exp\brb{-\frac{(x-2ku - 2kv)^2}{2\sigma^2 t}} - \exp\brb{-\frac{(x+2ku + 2(k-1)v)^2}{2\sigma^2 t}}} dx} \\
& = & \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \int^v_{-u} \frac {2k(x-2ku - 2kv)}{\sigma^2 t}\exp\brb{-\frac{(x-2ku - 2kv)^2}{2\sigma^2 t}} dx \\
& & \qquad - \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \int^v_{-u} \frac {-2(k-1)(x+2ku + 2(k-1)v)}{\sigma^2 t}  \exp\brb{-\frac{(x+2ku + 2(k-1)v)^2}{2\sigma^2 t}} dx
\eeast

This gives that
\beast
\frac{\partial F_t(S_t \leq v, I_t \geq -u)}{\partial v} & = & \left.\frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \brb{k\exp\brb{-\frac{(x-2ku - 2kv)^2}{2\sigma^2 t}} + (k-1) \exp\brb{-\frac{(x+2ku + 2(k-1)v)^2}{2\sigma^2 t}}}\right|^{-u}_v \\
& = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} k\brb{\exp\brb{-\frac{((2k+1)u + 2kv)^2}{2\sigma^2 t}} - \exp\brb{-\frac{(2ku + (2k-1)v)^2}{2\sigma^2 t}}} \\
& & \qquad + \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty}  (k-1) \brb{\exp\brb{-\frac{((2k-1)u + 2(k-1)v)^2}{2\sigma^2 t}} - \exp\brb{-\frac{(2ku + (2k-1)v)^2}{2\sigma^2 t}}}\\
& = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} 2k \exp\brb{-\frac{((2k+1)u + 2kv)^2}{2\sigma^2 t}} - (2k-1)\exp\brb{-\frac{(2ku + (2k-1)v)^2}{2\sigma^2 t}} \\
& = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} (-1)^k k\exp\brb{-\frac{((k+1)u + kv)^2}{2\sigma^2 t}} .
\eeast

Thus,
\beast
f_t(u,v) & = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \fp{}{u}\brb{(-1)^k k\exp\brb{-\frac{((k+1)u + kv)^2}{2\sigma^2 t}}} \\
& = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} (-1)^{k+1}k(k+1)\frac{(k+1)u + kv}{\sigma^2 t}  \exp\brb{-\frac{((k+1)u + kv)^2}{2\sigma^2 t}}
\eeast

Then, the transition density function of the range $R_t$ is
\beast
p_t(r) & = & \int^r_0 f_t(u,r-u) du \\
& = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} (-1)^{k+1}k(k+1) \int^r_0 \frac{u + kr}{\sigma^2 t}  \exp\brb{-\frac{( u + kr)^2}{2\sigma^2 t}} du \\
& = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} (-1)^{k+1}k(k+1) \left. \exp\brb{-\frac{(u + kr)^2}{2\sigma^2 t}} \right|^0_r \\
& = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} (-1)^{k+1}k(k+1) \brb{\exp\brb{-\frac{(kr)^2}{2\sigma^2 t}}  - \exp\brb{-\frac{((k+1)r)^2}{2\sigma^2 t}} }
\eeast

If k = 0, the term is 0, thus we have
\beast
p_t(r) & = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=1} (-1)^{k+1} k(k+1) \brb{\exp\brb{-\frac{(kr)^2}{2\sigma^2 t}}  - \exp\brb{-\frac{((k+1)r)^2}{2\sigma^2 t}}} \\
& & \quad + \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=1} (-1)^{k+1} k(k-1) \brb{\exp\brb{-\frac{(kr)^2}{2\sigma^2 t}} - \exp\brb{-\frac{((k-1)r)^2}{2\sigma^2 t}}}
\eeast

Therefore,
\beast
p_t(r) & = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=1} (-1)^{k+1} \brb{2k^2 \exp\brb{-\frac{(kr)^2}{2\sigma^2 t}} -  k(k+1) \exp\brb{-\frac{((k+1)r)^2}{2\sigma^2 t}} - k(k-1) \exp\brb{-\frac{((k-1)r)^2}{2\sigma^2 t}}} \\
& = & \frac 2{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=1} (-1)^{k+1} \brb{2k^2 \exp\brb{-\frac{(kr)^2}{2\sigma^2 t}} +  k(k-1) \exp\brb{-\frac{(kr)^2}{2\sigma^2 t}} + k(k+1) \exp\brb{-\frac{(kr)^2}{2\sigma^2 t}}} \\
& = & \frac{8}{\sigma\sqrt{2\pi t}}\sum^\infty_{k=1}(-1)^{k+1}k^2 \exp\brb{-\frac{k^2r^2}{2\sigma^2 t}}.
\eeast

Thus, we can not switch the integrals such that
\beast
\pro\brb{R\leq r} & = & \int^r_0 \frac{8}{\sigma\sqrt{2\pi t}}\sum^\infty_{k=1}(-1)^{k+1}k^2 \exp\brb{-\frac{k^2s^2}{2\sigma^2 t}} ds =  \frac{8}{\sigma\sqrt{2\pi t}}\sum^\infty_{k=1}(-1)^{k+1}k^2 \int^r_0 \exp\brb{-\frac{k^2s^2}{2\sigma^2 t}} ds \\
& = & \sum^\infty_{k=1}(-1)^{k+1}k \frac{8}{\sqrt{\pi }}\int^r_0 \exp\brb{-\frac{k^2s^2}{2\sigma^2 t}} d\brb{\frac{ks}{\sigma\sqrt{2t}}} = 4\sum^\infty_{k=1}(-1)^{k+1}k \erf\brb{\frac{kr}{\sigma\sqrt{2 t}}}.
\eeast
since $\sum^\infty_{k=1}(-1)^{k+1}k \erf\brb{\frac{kr}{\sigma\sqrt{2 t}}}$ is not convergent as $k$ goes to infinity. Thus, we have to approach this in another way,
\beast
\pro\brb{R > r} & = &  \int^\infty_r \frac{8}{\sigma\sqrt{2\pi t}}\sum^\infty_{k=1}(-1)^{k+1}k^2 \exp\brb{-\frac{k^2s^2}{2\sigma^2 t}} ds = \frac{8}{\sigma\sqrt{2\pi t}}\sum^\infty_{k=1}(-1)^{k+1}k^2 \int^\infty_r \exp\brb{-\frac{k^2s^2}{2\sigma^2 t}} ds \\
& = & \sum^\infty_{k=1}(-1)^{k+1}k \frac{8}{\sqrt{\pi }}\int^\infty_r \exp\brb{-\frac{k^2s^2}{2\sigma^2 t}} d\brb{\frac{ks}{\sigma\sqrt{2t}}} = 4\sum^\infty_{k=1}(-1)^{k+1}k \erfc\brb{\frac{kr}{\sigma\sqrt{2 t}}}.
\eeast

This is based on the fact that $\sum^\infty_{k=1}(-1)^{k+1}k \erfc\brb{\frac{kr}{\sigma\sqrt{2 t}}}$ is convergent. Hence,
\be
\pro\brb{R\leq r} = 1 - \pro\brb{R >r} = 1 + \sum^\infty_{k=1}(-1)^{k}k \erfc\brb{\frac{kr}{\sigma\sqrt{2 t}}}
\ee
as required.
\end{proof}

\begin{remark}
The form of $p_t(r)$ in the above theorem is not even obvious that the function is positive, and it is readily seen that the mean can not be obtained by termwise integration of series. Fortunately this function is closely related to the distribution function $L(x)$ which occurs in the Kolmogorov-Smirnov limit theorem (Theorem \ref{thm:kolmogorov_smirnov_limit}) on empirical distribution functions. %The distribution function $L(x)$ can be written in two equivalent forms
%\be
%L(x) = 1 - 2\sum^\infty_{k=1} (-1)^{k-1} \exp\brb{-2k^2x^2} = \frac{\sqrt{2\pi}}{x} \sum^\infty_{k=1} \exp\brb{-\frac{(2k-1)^2\pi^2 }{8x^2}}.
%\ee
%Thus, it is easy
Now we want to show that the integral of $p_t(r)$ is 1 and calculate the moment.
\end{remark}

\begin{proposition}
Let $B$ be a Brownian motion with $B_t \sim \sN(0,\sigma^2t)$ and $R$ be the range of $B$. Then the transition density of $R$, $p_t(r)$ satisfies that
\be
\int^\infty_0 p_t(r)dr = 1.
\ee

Furthermore, we have (see \cite{Feller_1951})
\be
\E R = \sqrt{\frac{8\sigma^2 t}{\pi}} \approx 1.5958 \sigma \sqrt{t},\qquad \E R^2 = (4\ln 2) \sigma^2 t,\qquad \var R = \brb{4\ln 2 - \frac{8}{\pi}}\sigma^2 t \approx 0.2181\sigma^2 t.
\ee

More generally, for any $p\in [1,\infty)$,
\be
\E R^p = \frac{4}{\sqrt{\pi}} \Gamma\brb{\frac {p+1}2}\brb{1-\frac{4}{2^p}}\zeta\brb{p-1}\brb{2\sigma^2 t}^{p/2}.
\ee
where $\zeta(\cdot)$ is the Riemann zeta function\footnote{definition needed.} (first given in \cite{Parkinson_1980}).
\end{proposition}

\begin{remark}
From the last equation, we can have that
\be
\E R^3 = \frac {\brb{2\pi\sigma^2 t}^{3/2}}{3},\qquad \E R^5 = \frac {7\pi\brb{2\pi\sigma^2 t}^{5/2}}{90}.
\ee
\end{remark}

\begin{proof}[\bf Proof]
We use Feller's proof in \cite{Feller_1951}. By Theorem \ref{thm:bm_range_transition_density}, we have that
\be
p_t(r) = \frac{8}{\sigma\sqrt{2\pi t}}\sum^\infty_{k=1}(-1)^{k+1}k^2 \exp\brb{-\frac{k^2r^2}{2\sigma^2 t}}.
\ee

Also, by Kolmogorov-Smirnov limit theorem (Theorem \ref{thm:kolmogorov_smirnov_limit_difference_between_empirical_theoretical}), Kolmogorov distribution function $L(x)$ can be written in two equivalent forms
\be
L(x) = 1 - 2\sum^\infty_{k=1} (-1)^{k-1} \exp\brb{-2k^2x^2} = \frac{\sqrt{2\pi}}{x} \sum^\infty_{k=1} \exp\brb{-\frac{(2k-1)^2\pi^2 }{8x^2}}.
\ee

Taking the differentiation of the above equation, we have
\be
L'(x) = 8\sum^\infty_{k=1}(-1)^{k-1} k^2 x \exp\brb{-2k^2x^2} \ \ra \ \frac{L'(x)}{x} = 8\sum^\infty_{k=1}(-1)^{k-1} k^2 \exp\brb{-2k^2x^2}.
\ee

%On the other hand, taking the differentiation of the second equation of Kolmogorov-Smirnov theorem, we have
%\be
%L'(x) = \frac{\sqrt{2\pi}}{x} \sum^\infty_{k=1} \frac{(2k-1)^2\pi^2}{4}x^{-3} \exp\brb{-\frac{(2k-1)^2\pi^2 }{8x^2}}.
%\ee

Thus, letting
\be
x = \frac{r}{2\sigma\sqrt{t}} \ \ra \ \frac{L'(x)}{x} = \sigma\sqrt{2\pi t} p_t(r) \ \ra \ p_t(r) =  \frac 1{\sigma\sqrt{2\pi t}} \frac {2\sigma\sqrt{t}}r L'\brb{ \frac{r}{2\sigma\sqrt{t}}} = \frac {2}{r\sqrt{2\pi }} L'\brb{ \frac{r}{2\sigma\sqrt{t}}}
\ee

Then by Theorem\footnote{theorem needed. this is from the second representation of K-S theorem} we have that $x^{a} L(x) \to 0$ as $x\to 0$ for any $a\in\R$. Thus,
\beast
\int^\infty_0 p_t(r)dr & = & \int^\infty_0 \frac {2}{r\sqrt{2\pi }} L'\brb{ \frac{r}{2\sigma\sqrt{t}}} dr = \frac {\sigma\sqrt{8t}}{\sqrt{\pi }}\int^\infty_0 r^{-1} d L\brb{ \frac{r}{2\sigma\sqrt{t}}}  \\
& = & \frac {\sigma\sqrt{8t}}{\sqrt{\pi }} \brb{r^{-1} L\brb{ \left.\frac{r}{2\sigma\sqrt{t}}}\right|^\infty_0 + \int^\infty_0  L\brb{ \frac{r}{2\sigma\sqrt{t}}}\frac 1{r^2}dr} = \frac {\sigma\sqrt{8t}}{\sqrt{\pi }} \int^\infty_0  L\brb{ \frac{r}{2\sigma\sqrt{t}}}\frac 1{r^2}dr \\
& = & \frac {\sigma\sqrt{8t}}{\sqrt{\pi }} \int^\infty_0\frac {\sqrt{2\pi} 2\sigma\sqrt{t}}r \sum^\infty_{k=1} \exp\brb{-\frac{(2k-1)^2\pi^2\sigma^2t }{2r^2}}\frac 1{r^2}dr =  8\sigma^2 t \int^\infty_0  \sum^\infty_{k=1} \exp\brb{-\frac{(2k-1)^2\pi^2\sigma^2t }{2r^2}}\frac 1{r^3}dr \\
& = & 8\sigma^2 t \sum^\infty_{k=1} \frac 1 {(2k-1)^2\pi^2\sigma^2t } \left.\exp\brb{-\frac{(2k-1)^2\pi^2\sigma^2t }{2r^2}}\right|^\infty_0 = 8 \sum^\infty_{k=1} \frac 1 {(2k-1)^2\pi^2} = 1
\eeast
by Corollary \ref{cor:sum_inverse_square_odd}. Similarly, we have that
\be
\E R = \int^\infty_0 r p_t(r)dr = \frac {\sigma\sqrt{8t}}{\sqrt{\pi }} \left.L\brb{ \frac{r}{2\sigma\sqrt{t}}}\right|^\infty_0 =  \frac {\sigma\sqrt{8t}}{\sqrt{\pi }} L\brb{ \frac{\infty}{2\sigma\sqrt{t}}} = \sqrt{\frac{8\sigma^2 t}{\pi}}.
\ee
by the first equation of Kolmogorov-Smirnov limit theorem. Also, by the second equation of Kolmogorov-Smirnov limit theorem and Gaussian random variable property,
\beast
\E R^2 & = & \int^\infty_0 r^2 p_t(r)dr = \frac{8}{\sigma\sqrt{2\pi t}} \sum^\infty_{k=1}(-1)^{k+1}k^2 \int^\infty_0 r^2  \exp\brb{-\frac{k^2r^2}{2\sigma^2 t}} dr \\
& = & 4 \sum^\infty_{k=1}\frac 1k (-1)^{k+1}k^2 \frac{\sigma^2 t}{k^2} = 4\sigma^2 t \sum^\infty_{k=1}(-1)^{k+1}\frac{1}{k} = (4\ln 2) \sigma^2 t
\eeast
by the Taylor expansion of $\ln x$. Thus, we have the variance of $R$.

Finally, for any $p\in [1,\infty)$, we have
\beast
\E R^p & = & \int^\infty_0 r^p p_t(r)dr = \frac{8}{\sigma\sqrt{2\pi t}} \sum^\infty_{k=1}(-1)^{k+1}k^2 \int^\infty_0 r^p  \exp\brb{-\frac{k^2r^2}{2\sigma^2 t}} dr \\
& = & \frac{4}{\sigma\sqrt{2\pi t}} \sum^\infty_{k=1}(-1)^{k+1}k^2 \int^\infty_0 r^{(p-1)/2}  \exp\brb{-\frac{k^2r}{2\sigma^2 t}} dr \\
& = & \frac{4}{\sigma\sqrt{2\pi t}} \sum^\infty_{k=1}(-1)^{k+1}k^2 \frac{\Gamma\brb{\frac {p+1}2}}{\brb{\frac{k^2}{2\sigma^2 t}}^{(p+1)/2}} =\frac{4}{\sqrt{\pi }}  \Gamma\brb{\frac {p+1}2}  \brb{2\sigma^2 t}^{p/2} \sum^\infty_{k=1}(-1)^{k+1}k^{1-p}
%& = & 4 \sum^\infty_{k=1}\frac 1k (-1)^{k+1}k^2 \frac{\sigma^2 t}{k^2} =
\eeast
by using gamma distribution function. In particular, for zeta function $\zeta$ (see Definition \ref{def:riemann_zeta_function})
\be
\zeta(p-1) - \sum^\infty_{k=1} (-1)^{k+1}k^{1-p} = 2\sum^\infty_{k=1} (2k)^{1-p} = 2^{2-p} \zeta(p-1)\ \ra \ \brb{1- \frac{4}{2^p}}\zeta(p-1) = \sum^\infty_{k=1} (-1)^{k+1}k^{1-p} .
\ee

Thus, we have the required result by substituting this into previous equation.
\end{proof}



%\begin{theorem}
%Let $B$ be a Brownian motion in $\R$ with $B_t\sim \sN(0,\sigma^2 t)$, $T = \inf\bra{t: B_t \leq -u \text{ or }B_t\geq v}$ where $u,v>0$. Then the probability density of $T$ is
%\be
%f(t) = \frac 1{\sqrt{2\pi \sigma^2 t^3}}\brb{\sum_{\substack{x=u,v\\ y=\pm 1}}\brb{x\exp\brb{-\frac{x^2}{2\sigma^2t}} + \sum^\infty_{k=1} y\brb{2k(u+v)+ xy}\exp\brb{-\frac{(2k(u+v)+xy)^2}{2\sigma^2 t}}}}.%\frac{8}{\sigma\sqrt{2\pi t}}\sum^\infty_{k=1}(-1)^{k+1}k^2 \exp\brb{-\frac{k^2r^2}{2\sigma^2 t}}.
%\ee

%Also,
%\be
%\E T^n =
%\ee

%\end{theorem}

%\begin{proof}[\bf Proof]
%By derivation in proof of Theorem \ref{thm:bm_range_transition_density},
%\beast
%\pro\brb{T\geq t} & = & F_t(S_t \leq v, I_t \geq -u) \\
%& = & \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty}  \int^v_{-u} \brb{\exp\brb{-\frac{(x-2ku - 2kv)^2}{2\sigma^2 t}} - \exp\brb{-\frac{(x+2ku + 2(k-1)v)^2}{2\sigma^2 t}}} dx.
%\eeast

%Then take the differentiation with respect to $t$, we have
%\beast
%-f(t) & = & -\frac 1{2\sqrt{2\pi \sigma^2 t^3}} \sum^\infty_{k=-\infty}  \int^v_{-u} \brb{\exp\brb{-\frac{(x-2ku - 2kv)^2}{2\sigma^2 t}} - \exp\brb{-\frac{(x+2ku + 2(k-1)v)^2}{2\sigma^2 t}}} dx \\
% & & \qquad \frac 1{2\sqrt{2\pi \sigma^2 t^3}} \sum^\infty_{k=-\infty}  \int^v_{-u} \frac{(x-2ku - 2kv)^2}{\sigma^2 t} \exp\brb{-\frac{(x-2ku - 2kv)^2}{2\sigma^2 t}} dx \\
% & & \qquad - \frac 1{2\sqrt{2\pi \sigma^2 t^3}} \sum^\infty_{k=-\infty}  \int^v_{-u} \frac{(x+2ku + 2(k-1)v)^2}{\sigma^2 t}\exp\brb{-\frac{(x+2ku + 2(k-1)v)^2}{2\sigma^2 t}} dx
%\eeast

%By integral by parts, we have
%\beast
%f(t) & = & \frac 1{\sqrt{8\pi \sigma^2 t^3}} \sum^\infty_{k=-\infty}\left.  (x-2ku - 2kv) \exp\brb{-\frac{(x-2ku - 2kv)^2}{2\sigma^2 t}} -(x+2ku + 2(k-1)v)\exp\brb{-\frac{(x+2ku + 2(k-1)v)^2}{2\sigma^2 t}}  \right|^v_{-u} \\
%& = & \frac 1{\sqrt{2\pi \sigma^2 t^3}} \sum^\infty_{k=-\infty}((2k+1)u + 2kv) \exp\brb{-\frac{((2k+1)u + 2kv)^2}{2\sigma^2 t}} - (2ku + (2k-1)v)\exp\brb{-\frac{(2ku + (2k-1)v)^2}{2\sigma^2 t}} \\
%& = & \frac 1{\sqrt{2\pi \sigma^2 t^3}}\brb{\sum_{\substack{x=u,v\\ y=\pm 1}}\brb{x\exp\brb{-\frac{x^2}{2\sigma^2t}} + \sum^\infty_{k=1} y\brb{2k(u+v)+ xy}\exp\brb{-\frac{(2k(u+v)+xy)^2}{2\sigma^2 t}}}}
%\eeast
%whose integral from 0 to infinity is 1\footnote{this is obvious by using pdf of gamma distribution}. Therefore, we can get $T$'s moments by using gamma distribution
%\beast
%\E T^n & = & \int^\infty_0 t^n f(t) dt \\
%& = & \int^\infty_0 \frac {t^n}{\sqrt{2\pi \sigma^2 t^3}}\brb{\sum_{\substack{x=u,v\\ y=\pm 1}}\brb{x\exp\brb{-\frac{x^2}{2\sigma^2t}} + \sum^\infty_{k=1} y\brb{2k(u+v)+ xy}\exp\brb{-\frac{(2k(u+v)+xy)^2}{2\sigma^2 t}}}}\\
%& = & \frac{\Gamma\brb{\frac 12-n}\brb{2\sigma^2}^{1/2-n}}{\sqrt{2\pi \sigma^2}} \brb{\sum_{\substack{x=u,v\\ y=\pm 1}}\brb{{x}^{2n}+ \sum^\infty_{k=1} y \brb{2k(u+v)+ xy}^{2n}}}
%\eeast

%By Proposition \ref{pro:gamma_half}, $\Gamma\left(\frac{1}{2}-n\right) = {(-4)^n n! \over (2n)!} \sqrt{\pi}$, we have
%\be
%\E T^n = \frac{(-4)^n n!\brb{2\sigma^2}^{-n}}{(2n)!} \brb{\sum_{\substack{x=u,v\\ y=\pm 1}}\brb{x\exp\brb{-\frac{x^2}{2\sigma^2t}} + \sum^\infty_{k=1} y\brb{2k(u+v)+ xy}\exp\brb{-\frac{(2k(u+v)+xy)^2}{2\sigma^2 t}}}}\\
%\ee
%\end{proof}




\subsection{Hitting time of single barrier}

\begin{proposition}\label{pro:brownian_motion_single_barrier_hitting_time_distribution}%{pro:brownian_motion_stopping_time_distribution}
Let $B$ be a Brownian motion in $\R$ with $B_t\sim \sN(0,\sigma^2 t)$ and $T_x = \inf\bra{t\geq 0:B_t = x}$ for $x>0$. Then its probability density is
\be
f_{T_x}(t) = \frac{x e^{-\frac{x^2}{2\sigma^2t}}}{\sigma \sqrt{2\pi t^3}}.
\ee

Also,
\be
\pro\brb{T_x < \infty} = 1,\qquad \E T_x = \infty.
\ee
\end{proposition}

\begin{remark}
This $t$ is equal to the hitting time of level $x/\sigma$ for a standard Brownian motion $B$ with $B_t \sim \sN(0,t)$.

If $B_0 = x$ a.s. with $B_t - B_0 \sim \sN(0,\sigma^2t)$ and $T_y = \inf\bra{t\geq 0:B_t = y}$ for $y >x$, then
\be
f_{T_y}(t) = \frac{(y-x) e^{-\frac{(y-x)^2}{2\sigma^2t}}}{\sigma\sqrt{2\pi t^3}}.
\ee
\end{remark}

\begin{proof}[\bf Proof]
By Corollary \ref{cor:abs_sup_equal_brownian_motion}, we have%\footnote{see Rogers' book}
\be
\pro\brb{T_x\leq t} = \pro\brb{S_t \geq x} = \pro\brb{\abs{B_t} \geq x} = 2\int^\infty_x \frac{1}{\sqrt{2\pi \sigma^2 t}}e^{-\frac{y^2}{2\sigma^2 t}}dy = 2\int^\infty_{x/\sqrt{\sigma^2 t}} \frac{1}{\sqrt{2\pi }}e^{-\frac{y^2}{2}}dy.
\ee

Thus,
\be
\pro(T_x < \infty) = \lim_{t\ua\infty}\pro\brb{T_X\leq t} = 2\int^\infty_0 \frac{1}{\sqrt{2\pi }}e^{-\frac{y^2}{2}}dy = 1.
\ee

Then we take the differentiation,
\be
f_{T_x}(t) = -\frac{2}{\sqrt{2\pi}}e^{-\frac{x^2}{2\sigma^2t}}\brb{ -\frac 12 \frac {x}{\sigma\sqrt{t^3}}} = \frac{x e^{-\frac{x^2}{2\sigma^2t}}}{\sigma\sqrt{2\pi t^3}}
\ee
which is inverse gamma distribution $\Gamma^{-1}\brb{\frac 12, \frac{x^2}2}$ where $k=\frac 12$ and $\lm = \frac {x^2}{2\sigma^2}$ (see Definition \ref{def:inverse_gamma_rv}). Also, since $k= \frac 12$, we know that this hitting time $\E T_x = \infty$ a.s. as the expectation only exists for $k>1$.
\end{proof}

\begin{proposition}\label{pro:brownian_motion_hitting_time_single_barrier}
Let $B$ be a Brownian motion in $\R$ with $B_t\sim \sN(0,\sigma^2 t)$. Let $r\in \R^{++}$ (i.e., $[0,\infty)$) and $T_x = \inf\bra{t\geq 0:B_t = x}$ for $x>0$. Then the Laplace transform of $T_x$ is
\be
\E\exp\brb{-r T_x} = \exp\brb{-\frac{x\sqrt{2r}}{\sigma}}.
\ee
%Moreover, the Laplace transform of $T_x =  \inf\bra{t \geq 0:B_t = x}$ for $x \in \R$ is given by
%\be
%\E\exp\brb{-rT_x} = \exp\brb{-\frac{\abs{x}\sqrt{2r}}{\sigma}},\qquad r\in\C.
%\ee
\end{proposition}

\begin{remark}
If the moment generating function of $T_x$ is bounded in a neighbourhood of 0, then
by Proposition \ref{pro:mgf_finite_moment}, we can have
\be
\E X = \left.-\frac {d}{dr}\exp\brb{-\frac{x\sqrt{2r}}{\sigma}}\right|_{r=0} =- \infty.
\ee

Thus, we can say that moment generating function $\E \exp\brb{\theta T_x}$ is not bounded for any $\theta >0$. Therefore, we can see that the moment generating function is not guaranteed to be continuous.
\end{remark}

\begin{proof}[\bf Proof]
By Proposition \ref{pro:brownian_motion_single_barrier_hitting_time_distribution}, we have
\beast
\E\exp\brb{-r T_x} & = & \frac{x}{\sigma\sqrt{2\pi}}\int^\infty_0 e^{- r t} t^{-3/2}e^{-\frac{a^2}{2\sigma^2 t}}dt = \frac{x}{\sigma\sqrt{2\pi}}\int^\infty_0 t^{-3/2} \exp\left\{-\frac{x\sqrt{2r}}{2\sigma}\left(\frac{\sigma t\sqrt{2r}}{x}+\frac{x}{\sigma t\sqrt{2r}}\right)\right\}dt \\
& = & \frac{x}{\sigma\sqrt{2\pi}} \left(\frac{x}{\sigma\sqrt{2r}}\right)^{-1/2}\int^\infty_0 t^{-3/2} \exp\left\{-\frac{x\sqrt{2r}}{2\sigma}\left(t+\frac{1}{t}\right)\right\}dt%\quad (*)
\eeast

%To calculate $(*)$, we can use different approaches.

%Approach 1.

\beast%\label{equ:reflection_t}
\int^\infty_0 t^{-3/2} \exp\left\{-\frac{x\sqrt{2r}}{2\sigma}\left(t+\frac{1}{t}\right)\right\}dt  = \exp\brb{-\frac{x\sqrt{2r}}{\sigma}}\int^\infty_0 t^{-3/2} \exp\left\{-\frac{x\sqrt{2r}}{2\sigma}\left(\sqrt{t}-\frac{1}{\sqrt{t}}\right)^2\right\}dt\quad (\dag)
\eeast

Defining $s:=1/t$, we have
\be%\label{equ:reflection_t_inverse}
\int^\infty_0 t^{-3/2} \exp\left\{-\frac{x\sqrt{2r}}{2\sigma}\left(t+\frac{1}{t}\right)\right\}dt = \exp\brb{-\frac{x\sqrt{2r}}{\sigma}} \int^\infty_0 s^{-1/2} \exp\left\{-\frac{x\sqrt{2r}}{2\sigma}\left(\sqrt{s}-\frac{1}{\sqrt{s}}\right)^2\right\}ds\quad (\dag\dag)
\ee

Then $\brb{(\dag)+(\dag\dag)}/2$ is
\beast
\int^\infty_0 t^{-3/2} \exp\left\{-\frac{x\sqrt{2r}}{2\sigma}\left(t+\frac{1}{t}\right)\right\}dt & = & \frac 12\exp\brb{-\frac{x\sqrt{2r}}{\sigma}} \int^\infty_0 \left(t^{-1/2}+ t^{-3/2}\right) \exp\left\{-\frac{x\sqrt{2r}}{2\sigma}\left(\sqrt{t}-\frac{1}{\sqrt{t}}\right)^2\right\}dt  \\
& = & \exp\brb{-\frac{x\sqrt{2r}}{\sigma}}\int^\infty_0 \exp\left\{-\frac{x\sqrt{2r}}{2\sigma}\left(\sqrt{t}-\frac{1}{\sqrt{t}}\right)^2\right\}d\left(\sqrt{t}-\frac{1}{\sqrt{t}}\right)  \\
& = & \exp\brb{-\frac{x\sqrt{2r}}{\sigma}} \sqrt{2\pi}\brb{\frac{x\sqrt{2r}}{\sigma}}^{-1/2}.
\eeast



%Approach 1.
%\beast
%& & \int^\infty_0 t^{-3/2} \exp\left\{-\frac{x\sqrt{2r}}{2\sigma}\left(t+\frac{1}{t}\right)\right\}dt \\
%& = & 2K_{-1/2}\left(\frac{x\sqrt{2r}}{\sigma}\right)  \quad\quad \left(\text{$K_\nu(x)$ is Bessel function of the third kind}\right) \\
%& = & 2K_{1/2}\left(\frac{x\sqrt{2r}}{\sigma}\right) \quad\quad \left(K_\nu(x)=K_{-\nu}(x),\text{ Proposition \ref{pro:modified_bessel_function_first_second_relation}}\right) \\
%& = & \frac{2\pi^{1/2}\left(\frac{x\sqrt{2r}}{2\sigma}\right)^{1/2}}{\Gamma(1)}\int^\infty_1 e^{-\frac{x\sqrt{2r}}{\sigma}t}dt \quad\quad \left(K_\nu(z)=\frac{\pi^{1/2}z^\nu2^{-\nu}}{\Gamma(\nu+1/2)}\int^\infty_1 e^{-zt}(t^2-1)^{\nu-1/2}dt,\text{ Proposition \ref{pro:modified_bessel_function_second_sinh_integral}}\right) \\
%& = & \exp\brb{-\frac{x\sqrt{2r}}{\sigma}} \sqrt{2\pi}\brb{\frac{x\sqrt{2r}}{\sigma}}^{-1/2}.
%\eeast


%Approach 2.

%\beast
%\frac{x}{\sigma\sqrt{2\pi}}\int^\infty_0 e^{-r t} t^{-3/2}e^{-\frac{x^2}{2\sigma^2t}}dt & = & \int^\infty_0 \frac{x}{\sigma\sqrt{2\pi t^3}}\exp\brb{-r t - \frac{x^2}{2\sigma^2t}}dt \\
%& = & \int^\infty_0 \frac{x}{\sigma\sqrt{2\pi t^3}}\exp\brb{-\frac 1{2 t}\brb{\sqrt{2r} t - \frac{x}{\sigma}}^2 } \exp\brb{-\frac{x\sqrt{2r}}{\sigma}} dt \\
%& = & \exp\brb{-\frac{x\sqrt{2r}}{\sigma}} \int^\infty_0 \brb{\frac{x^2}{2\pi \sigma^2 t^3}}^{1/2}\exp\brb{-\frac {2r}{2t}\brb{t - \frac{x}{\sigma\sqrt{2r}}}^2 }  dt
%\eeast
%
%The term in the integral is the pdf of inverse Gaussian distribution $IG\brb{\frac{x}{\sigma\sqrt{2r}}, \frac{x^2}{\sigma^2}}$ where (see Definition \ref{def:inverse_gaussian_rv})
%\be
%X\sim IG\brb{\mu,\lm} \ \ra \ f(x) = \brb{\frac{\lambda}{2 \pi x^3}}^{1/2} \exp\brb{\frac{-\lambda (x-\mu)^2}{2 \mu^2 x}}.
%\ee

Thus,
\be
\E\exp\brb{-r T_x}  = \frac{x}{\sigma\sqrt{2\pi}} \left(\frac{x}{\sigma\sqrt{2r}}\right)^{-1/2} \exp\brb{-\frac{x\sqrt{2r}}{\sigma}}  \sqrt{2\pi}\brb{\frac{x\sqrt{2r}}{\sigma}}^{-1/2} = \exp\brb{-\frac{x\sqrt{2r}}{\sigma}}.
\ee
as required. %Thus, we have the required result.
\end{proof}




%\begin{proposition}[hitting time of single barrier] \end{proposition}

Now we can apply martingale theorem to prove Proposition \ref{pro:brownian_motion_hitting_time_single_barrier}. Recall that $T_x = \inf\bra{t:B_t = x}$ and $B$ is a Brownian motion with $B_t \sim \sN\brb{0,\sigma^2 t}$.

\begin{proof}[\bf Proof]%By Theorem \ref{thm:brownian_motion_martingale}, we have that $M_t=\exp\brb{ \theta \frac{ B_t}{\sigma}- \abs{\theta}^2t/2}$ is a martingale for $\theta := \sqrt{2r}$ where $r\in \R$. Since $M$ is bounded and therefore UI, we can apply optional stopping theorem (Theorem \ref{thm:optional_stopping_ui_continuous})
%\be
%1 = M_0 = \E\brb{M_{t \land T_x}}= \E\brb{\exp\left(\sqrt{2r} \frac{x}{\sigma}- r T_x\right)\ind_{\{T_x\leq t\}}} + \E\brb{\exp\left(\sqrt{2r}\frac{B_t}{\sigma}- rt\right)\ind_{\{T_x > t\}}}.
%\ee
%The second term here is bounded above by $\exp\brb{\sqrt{2r} \frac{x}{\sigma}}$ or $1$ (for two cases for the sign of $r$), so converges to zero as $t\to\infty$, and the first term converges to $\E\brb{\exp\left(\sqrt{2r} \frac{x}{\sigma}- rT_x\right)}$. Then we can get
%\be
%1 = \E\exp\brb{ \frac{x\sqrt{2r}}{\sigma}- r T_x} \ \ra\ \E\exp\brb{ - r T_x} = \exp\brb{ - \frac{x\sqrt{2r}}{\sigma}}
%\ee
%as required.

Without loss of generality, we can assume that $x\geq 0$ and $\lm = \sqrt{2r}$ with $r\in \C$. We know the fact that
\be
\brb{\exp\brb{\lm B_t/\sigma-\abs{\lm}^2t /2}}_{t\geq 0}
\ee
is a martingale by Theorem \ref{thm:brownian_motion_martingale}. Therefore,
\be
\brb{\exp\brb{\frac{\lm B_t^{T_x}}{\sigma}-\frac{\abs{\lm}^2}2(t\land T_x)}}_{t\geq 0}
\ee
is also a martingale by Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}.(ii). Since it is bounded, it is UI and thus by optional stopping theorem (Theorem \ref{thm:optional_stopping_ui_continuous}), we have that
\be
\E\brb{\exp\brb{\frac{\lm x}{\sigma}-\frac{\abs{\lm}^2}2 T_x}} = \E\brb{\exp\brb{\frac{\lm B_{T_x}}{\sigma}-\frac{\abs{\lm}^2}2 T_x}} = 1  \ \ra \ \exp\brb{-\frac{\lm x}{\sigma}} = \E\brb{\exp\brb{-\frac{\abs{\lm}^2 T_x}2}}.
\ee

For $r\in [0,\infty)$, $\abs{\lm}^2 = 2r$. Thus, we have the required result.%Letting $\lm = \sqrt{2r}$, we can have the required result. Similarly, for $x\geq 0$, we will let $\lm \leq 0$ to guarantee the boundedness of the martingale (and then let $\lm = -\sqrt{2r}$).
\end{proof}

\subsection{Hitting time of double barriers}

A classical application of martingales is to show the following result, often referred to as the gambler's ruin (see Example \ref{exa:randam_walk_simple}) estimates.

\begin{theorem}[hitting time of double barriers, probability and mean]\label{thm:brownian_motion_double_bounded}
Let $(B_t)_{t \geq 0}$ be a standard Brownian motion and $T_x = \inf\bra{t \geq 0:B_t = x}$. Then for $x, y > 0$, one has
\be
\pro(T_{-y} < T_x) = \frac x{x + y}.
\ee

Let $T = T_{-y} \land T_x$ (by Proposition \ref{pro:debut_time_closed_set_stopping_time}), which is a stopping time. Then
\be
\E T = xy.
\ee
\end{theorem}

%\begin{proof}[\bf Proof]
%We use martingales stopped at $T_x \land T_{-y}$.
%\be
%0 = \E[B_{T_x\land T_{-y}}] = x \pro(T_x < T_{-y})- y(1-\pro(T_x < T_{-y})),
%\ee
%so $\pro(T_x < T_{-y}) = \frac y{x+y}$. Similarly,
%\be
%\E[T_x \land T_{-y}] = \E[B^2_{T_x\land T_{-y}}] = x^2 \frac y{x + y} + y^2 \frac x{x + y} = x y.
%\ee
%\end{proof}

\begin{proof}[\bf Proof]
Since $B$ is a martingale, the stopped martingale $B^T$ is still a martingale (by Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}.(ii)). Moreover, $B^T$ is bounded (by $\max(x, y)$) so it is UI martingale. Then we may apply the optional stopping theorem (Theorem \ref{thm:optional_stopping_ui_continuous}) to find that $\E(B_T) = \E(B_0) = 0$ a.s.. On the other hand,
\be
\E(B_T) = - yp + x(1 - p),
\ee
where $p = \pro(T_{-y} < T_x)$ is the probability of interest to us. Thus $py = (1 - p)x \ \ra \ p = \frac{x}{x+y}$. %and the first statement follows easily.

For the second statement, we can observe that $B^2_t-t$ is a martingale and thus $B^2_{t\land T} - (t\land T)$ is a martingale (by Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}.(ii)). Therefore,
\be
\E(B^2_{t\land T} ) = \E(t \land T).
\ee

We may let $t \to\infty$ since the left-hand side is bounded (we use bounded convergence theorem (Theorem \ref{thm:bounded_convergence_probability})) and the right-hand side is monotone (we use monotone convergence theorem (Theorem \ref{thm:monotone_convergence_probability})), and deduce that
\be
\E(B^2_T) = \E T.
\ee

Using the first statement,
\be
\E(B^2_T) = \frac x{x + y} y^2 + \frac y{y + x}x^2 = xy \ \ra \  \E T = \E\brb{T_x \land T_{-y}} = xy.
\ee%and the claim follows.
\end{proof}

%Similarly,
%\qcutline

\begin{theorem}[hitting time of double barriers, Laplace transform]\label{thm:brownian_motion_two_barriers_mgf}
Let $B$ be a standard Brownian motion. For $x,y >0$, the random variable $T = T_x \land T_{-y}$ has a Laplace transform given by
\be
\E\brb{e^{-rT}} = \frac{\sinh(\sqrt{2r}x) + \sinh(\sqrt{2r}y)}{\sinh(\sqrt{2r}(x + y))}
\ee
where $r\in [0,\infty)$. When $y = x$, $T$ is independent from the event $\bra{T_{-x} < T_x}$.
\end{theorem}

\begin{remark}
For the Brownian motion $B_t \sim \sN(0,\sigma^2t)$, we can simply replace the above $x$ and $y$ by $x/\sigma$ and $y/\sigma$.
\end{remark}

\begin{proof}[\bf Proof]
%By bounded convergence theorem ()
%\be
%eewhich is bounded  The first statement follows directly from the optional stopping theorem and the fact thatwhen stopped at $T_x$ if $x \geq 0$ (which we may assume by symmetry).
%The second statement is a bit more involved.
Let
\be
M_t = e^{-\lm^2t/2} \sinh(\lm(B_t + y))
\ee
is also a martingale since it can be written as
\be
\frac 12 e^{-\lm^2t/2}e^{\lm(B_t+y)} -\frac 12 e^{-\lm^2t/2}e^{-\lm(B_t+y)}
\ee
which is the sum of two martingales. Now, stopping at $T = T_{-y} \land T_{x}$, $M^T$ is bounded and thus UI, so we can use the optional stopping theorem (Theorem \ref{thm:optional_stopping_ui_continuous}) to obtain:
\be
\sinh(\lm y) = M_0 = \E M_T = \E\brb{\sinh(\lm (B_T + y))e^{-\lm^2T/2}} = \E\brb{\sinh(\lm(x + y)e^{-\lm^2T/2}\ind_{\bra{T_x < T_{-y}}}}.
\ee

Thus,
\be
\E\brb{e^{-\lm^2T/2}\ind_{\bra{T_x <T_{-y}}}} = \frac{\sinh(\lm y)}{\sinh(\lm(x + y))} \ \ra \ \E\brb{e^{-\lm^2T/2}\ind_{\bra{T_x >T_{-y}}}} = \frac{\sinh(\lm x)}{\sinh(\lm (x + y))}
\ee
by symmetry. Adding up the two terms,
\be
\E(e^{-\lm^2T/2}) = \frac{ \sinh(\lm x)+\sinh(\lm y) }{\sinh(\lm(x + y))}
\ee

Again, letting $\lm = \sqrt{2r}$, we can have the required result. When $x = y$,
\be
\E\brb{e^{-\lm^2T/2}\ind_{\bra{T_{-x}>T_x}}} = \frac{\sinh(\lm x)}{\sinh(2\lm x)} = \frac 12 \frac{2\sinh(\lm x)}{\sinh(2\lm x)}  = \frac 12 \E(e^{-\lm^2T/2}) = \E\brb{e^{-\lm^2T/2}}\pro(T_{-x} < T_x)
\ee
%it suffices to check that which is easy to check.


Hence, $T$ is independent from the event $\bra{T_{-x} < T_x}$ by Corollary \ref{cor:mgf_indicator_expectation_independent}. %\footnote{need details}.
\end{proof}

\begin{proposition}[hitting time of double barriers, variance]\label{pro:brownian_motion_two_barriers_variance}
Let $(B_t)_{t \geq 0}$ be a standard Brownian motion and for $x, y > 0$
\be
T = \inf\bra{t \geq 0:B_t = x \text{ or }B_t = -y}.
\ee

Then
\be
\var T = \frac 13 xy\brb{x^2+ y^2}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
We know that $B_t^4 -6tB_t^2 + 3t^2$ is a martingale and thus $B^4_{t\land T} - 6(t\land T)B^2_{t\land T} + 3(t\land T)^2$ is a martingale (by Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}.(ii)). Thus, by using Theorem \ref{thm:bounded_convergence_probability} and Theorem \ref{thm:monotone_convergence_probability}, we have
\be
0 = \E\brb{B^4_{T} - 6TB^2_{T} + 3T^2}.
\ee

However, we can not have $\E \brb{TB_T^2} = \E T \E B_T^2$. We need to solve $\E\brb{T\ind_{\bra{T_x<T_{-y}}}}$ and $\E\brb{T\ind_{\bra{T_x >T_{-y}}}}$ first.


From the proof of Theorem \ref{thm:brownian_motion_two_barriers_mgf}, we have for $r\in \R$,
\be
\E\brb{e^{-rT}\ind_{\bra{T_x < T_{-y}}}} = \frac{\sinh(\sqrt{2r} y)}{\sinh\brb{\sqrt{2r}(x + y)}},\qquad \E\brb{e^{-rT}\ind_{\bra{T_x > T_{-y}}}} = \frac{\sinh(\sqrt{2r} x)}{\sinh\brb{\sqrt{2r} (x + y)}}.
\ee

Then by Proposition \ref{pro:laplace_finite_moment}%{pro:mgf_finite_moment}
\beast
& &\E \brb{T\ind_{\bra{T_x < T_{-y}}}} = -\left.\fp{\E\brb{e^{-rT}\ind_{\bra{T_x<T_{-y}}}}}{r}\right|_{r=0} \\
& = & \left.\frac{\sinh(\sqrt{2r} y) \frac{x+y}{\sqrt{2r}}\cosh\brb{\sqrt{2r}(x + y)}- \cosh(\sqrt{2r} y) \frac{y}{\sqrt{2r}}\sinh\brb{\sqrt{2r}(x + y)}}{\brb{\sinh\brb{\sqrt{2r}(x + y)}}^2}\right|_{r=0} \\
& = & \left.\frac{(x+y)\brb{y+ \frac 13ry^3+o(r)}\brb{1+ r(x+y)^2+o(r)}-y \brb{1+ry^2+o(r)}  \brb{(x+y)+ \frac 13r(x+y)^3+o(r) }}{\brb{2r(x+y)^2 + o(r)}^2}\right|_{r=0}\\
& = & \left.\frac{\frac 23 yr(x+y)\brb{(x+y)^2 - y^2} + o(r)}{\brb{\sqrt{2r}(x+y) + o(r)}^2}\right|_{r=0} = \frac{xy(x+2y)}{3(x+y)}.
\eeast

Similarly, we have
\be
\E \brb{T\ind_{\bra{T_x > T_{-y}}}} = \frac{xy(2x+y)}{3(x+y)}.
\ee

Therefore,
\beast
\E T & = & \E \brb{T\ind_{\bra{T_x < T_{-y}}}} + \E \brb{T\ind_{\bra{T_x > T_{-y}}}} \\
& = & \frac{xy(x+2y)}{3(x+y)} + \frac{xy(2x+y)}{3(x+y)} = \frac{xy(3x+3y)}{3(x+y)} = xy
\eeast
which is consistent with Theorem \ref{thm:brownian_motion_double_bounded}.

Thus,
\beast
\E \brb{TB_T^2 } & = & \E\brb{\E \brb{TB_T^2\ind_{\bra{T_x < T_{-y}}}} + \E \brb{\E\brb{TB_T^2\ind_{\bra{T_x > T_{-y}}}}}} \\
& = & x^2 \frac{xy(x+2y)}{3(x+y)} + y^2 \frac{xy(2x+y)}{3(x+y)} = \frac{xy(x^2 + y^2 + xy)}{3}.
\eeast

Therefore,
%and thus\footnote{}
\beast
\E T^2 & = & \frac 13\brb{6\E\brb{TB_T^2} - \brb{\frac{x^4y}{x+y}+ \frac{y^4x}{x+y}}} \\
& = & \frac 13\brb{2xy\brb{x^2+y^2 + xy}  - xy(x^2+y^2-xy)} = \frac 13 xy\brb{x^2 + y^2 + 3xy}.
\eeast

Hence,%Therefore,
\be
\var T = \frac 13 xy\brb{x^2 + y^2 + 3xy} - x^2y^2 = \frac 13 xy\brb{x^2+ y^2}.
\ee
\end{proof}


\subsection{Recurrence and Transience}

%Assume given $(\Omega,\sF, (\pro_x )_{x\in \R^d})$ probability spaces such that $(B_t )$ is under $\pro_x$ a Brownian motion started at $x \in \R^d$. (For example, $(W, prod, (W_x )_{x\in \R^d} ))$.

We will write $\pro_x$ to indicate that the Brownian motion starts from $x$, i.e., under $\pro_x$ the process $(B_t-x)_{t\geq 0}$ is a standard Brownian motion.

\begin{theorem}
Let $(B_t)_{t\geq 0}$ be a Brownian motion starting from particular point in $d\geq 1$ dimensions.
\ben
\item [(i)] In $d = 1$, $B$ is point-recurrent, i.e., under $\pro_0$ (or any $\pro_y$, $y\in \R$),
\be
\bra{t : B_t = x}\text{ are unbounded for every }x\in \R\text{ a.s..}
\ee
%a.s. under $P_0$, the sets $\{t | B_t = x\}$ are unbounded, i.e. Brownian motion in dimension one is 'point recurrent.'

\item [(ii)] In $d = 2$, $B$ is neighbourhood-recurrent, i.e., under $\pro_x$,
\be
\bra{t:\dabs{B_t-z} \leq r} \text{ is unbounded for every $x,z\in \R^d$, $r >0$ a.s.}
\ee

However, points are polar, i.e., for every $x\in \R^d$,
\be
\pro_0(H_x = \infty)=1,\qquad \text{where }H_x = \inf\bra{t>0:B_t = x}\text{ is the hitting time of $x$.}
\ee

%a.s. under $P_x$ , $\bra{t | \abs{B_t} \leq \ve}$ is unbounded for all $\ve > 0$, i.e. Brownian motion in dimension two is 'neighbourhood recurrent.' However, if $H_x = \inf\{t > 0| B_t = x\}$ then $\pro_x (H_y <\infty) = 0$, i.e. 'points are polar.'

\item [(iii)] In $d \geq 3$, $B$ is transient, i.e., $\dabs{B_t}\to \infty$ as $t\to \infty$ $\pro_0$-a.s.. Also, for $0< r<\dabs{x}$,
\be
\pro_x\brb{H_r < \infty} = \brb{\frac{r}{\dabs{x}}}^{d-2}.
\ee
%$(B_t)$ is transient, i.e. $\abs{B_t}\to \infty$ as $t \to \infty$.
\een
\end{theorem}

\begin{remark}
For 2-dimensional Brownian motion, since $\{t | B_t \in B(x, r)\}$ is unbounded for every $x \in \R^2$ and $r > 0$ and $\R^2$ may be covered by a countable union of balls of a fixed radius, the trajectory of Brownian motion is dense in $\R^2$. On the other hand, a.s. for all $t > 0$, $B_t \notin \Q^2$.
\end{remark}

\begin{proof}[\bf Proof]
\ben
\item [(i)] We already know that $\sup_{t\geq 0} B_t = -\inf_{t\geq 0} B_t = \infty$ by Proposition \ref{pro:brownian_motion_limit_value}, so we have point recurrence by continuity of $B$.

\item [(ii)] Note that it suffices to show the claim for $z= 0$. That is, $\bra{t:\dabs{B_t} \leq r}$ is unbounded for every $x\in \R^d$, $r >0$ $\pro_x$-a.s..

Let $r > 0$, $R > r$, and $D_{r,R} = \bra{x \in \R^2 :r \leq \dabs{x} \leq R}$. Let $f : \R^2 \to \R$ be such that $f : x \to\log\dabs{x}$ on $D_{r,R}$, so $f$ is bounded, is $C^\infty$, and has bounded derivatives. Also, we have that $\Delta \log \dabs{x} = 0$ on $r\leq \dabs{x} \leq R$. %the interior of $D_{\ve,R}$.
That is,
\beast%\frac{\partial^2 \sqrt{x^2_1 + x^2_2}}{\partial x_1^2} + \frac{\partial^2 \sqrt{x^2_1 + x^2_2}}{\partial x_2^2}
\Delta \log \dabs{x} = \frac{\partial }{\partial x_1} \brb{\frac{x_1}{x^2_1 + x^2_2}} + \frac{\partial }{\partial x_2} \brb{\frac{x_2}{x^2_1 + x^2_2}} = \frac{x^2_1 + x^2_2 - 2x_1^2}{\brb{x^2_1 + x^2_2}^2} + \frac{x^2_1 + x^2_2 - 2x_2^2}{\brb{x^2_1 + x^2_2}^2} = 0.
\eeast

Then by Theorem \ref{thm:brownian_motion_integral_martingale},
\be
M_t = f(B_t)- f (x)- \int^t_0 \frac 12 \Delta f (B_s)ds = \log\dabs{B_t} -\log\dabs{x}
\ee
is a martingale. Let $T_r = \inf\bra{t \geq 0 :\dabs{B_t} \leq r}$ and $T_R = \inf\bra{t \geq 0: \dabs{B_t} \geq R}$. Therefore let $T = T_r \land T_R$, then $T < T_R < \infty$ a.s. since Brownian motion is unbounded a.s. (by Proposition \ref{pro:brownian_motion_limit_value}).
%\be
%M_{t\land T} = \log\abs{B_{t\land T}}- \log\abs{x}
%\ee%(since ${\Delta}f = 0$ on $D_{\ve,R}$)
%is a martingale as a stopped martingale is still a martingale (Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}), so $(M_{t\land T})_{t \geq 0}$ is a bounded martingale.

By optional stopping theorem (Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}.(iii)),
\be
\E\brb{M_T} = M_0 \ \ra \ \log \dabs{x} = \E_x\brb{\log\dabs{B_T}} = \log r\pro_x(T_r < T_R)+ \log R \pro_x (T_R < T_r).
\ee

Thus,
\be
\pro_x (T_r < T_R) = \frac{\log R - \log \dabs{x}}{\log R -\log r },\quad \pro_x (T_R < T_r) = \frac{\log \dabs{x} - \log r}{\log R -\log r }.\quad \quad (*)
\ee

Applying Proposition \ref{pro:brownian_motion_single_barrier_hitting_time_distribution}, we have that $\pro\brb{T_n < \infty}=1$ for all $n > \floor{x}$, $n\in \N$ (this is for one dimensional case, but this implies higher dimensional case as $\dabs{B_t} \geq R$ if any dimension exceeds $R$). We know that
\be
\bra{T_r < \infty} = \bigcup_{n > \floor{x}}\bra{T_r\leq T_n} \cap \bra{T_n <\infty} \ \ra \ \ind_{\bra{T_r \leq T_n}} \ua \ind_{\bra{T_r < \infty}} \text{ a.s.}
\ee

Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}), we have
\be
\pro\brb{T_r < T_R} \to \pro\brb{T_r < \infty} \ \ra \ \pro\brb{T_r < \infty} = \lim_{R\ua \infty} \frac{\log R - \log \dabs{x}}{\log R -\log r } = 1.
\ee
%We see that
%\beast
%\pro_x (T_r < \infty) & = & \pro_x(T_r < \infty, T_R = \infty) + \pro_x \brb{T_r < \infty, T_R < \infty} \\
%& = & 0 + \pro_x \brb{T_r < T_R < \infty} + \pro_x \brb{T_R < T_r < \infty}\\
%& = & \pro\brb{T_r < T_R} - \pro_x\brb{T_r < T_R =\infty} + \pro_x \brb{T_R < T_r < \infty}\\
%& = & \pro_x(T_r < T_R) - 0 + \pro_x \brb{T_R < T_r <\infty} \\
%& = & \pro_x(T_r < T_R)+ \pro_x \brb{T_R < T_r <\infty}.\qquad (**)
%\eeast
%Letting $R\to \infty$ in ($*$), we have $\pro_x (T_r < T_R) = 1 \ \ra \ \pro_x (T_r < \infty)=1$\footnote{need dominated convergence theorem? $T_R \to \infty$ a.s.?}
which shows that
\be
\pro_x\brb{\dabs{B_t}\leq r, \text{ for some }t>0} = 1.
\ee
for any $x\in \R^2$. But
\beast
\pro_x (\dabs{B_t} \leq r, \text{ for some }t>n) & = & \pro_x (\dabs{B_t - B_n + B_n} \leq r,\text{ for some }t>n) = \pro_x \brb{\dabs{B^{(n)}_t + B_n} \leq r, \text{ for some }t>n} \\
& = & \int_{\R^2} \pro_0 \brb{\dabs{B_t +y} \leq r, \text{ for some }t>0} \pro_x(B_n \in d y) \quad\quad \text{(by Simple Markov Property)}\\
& = & \int_{\R^2} \pro_y \brb{\dabs{B_t} \leq r, \text{ for some }t>0} \pro_x(B_n \in d y) \\
& = &  \int_{\R^2} 1\cdot \pro_x(B_n \in d y) = \pro\brb{B_n\in \R^2} = 1,%\pro_y (T_r <\infty) = 1,
\eeast
so $\bra{t :\dabs{B_t} \leq r}$ is unbounded $\pro_x$-a.s..

On the other hand, we know that for all $x \neq 0$,
\be
\bra{T_0 < \infty} = \bigcup_{n > \dabs{x}}\bra{T_0\leq T_n} \cap \bra{T_n <\infty} = \bigcup_{n > \dabs{x}}\bra{\bra{\bigcap_{m>1/\dabs{x}} \bra{T_{1/m}\leq T_n} }\cap \bra{T_n <\infty}}
\ee
%\be
%\bra{T_0 < \infty} = \bigcup_{n > \floor{x}}\bra{T_0\leq T_n} \cap \bra{T_n <\infty} = \bigcup_{n > \floor{x}}\bra{\bra{\bigcap_{m>1/\dabs{x}} \bra{T_{1/m}\leq T_n} }\cap \bra{T_n <\infty}}
%\ee
which gives that
\be
\ind_{\bigcup_{n > \dabs{x}}\bigcap_{m>1/\dabs{x}} \bra{T_{1/m}\leq T_n} } \to \ind_{\bra{T_0 < \infty}} \text{ a.s.\ as }m,n\to\infty.
\ee
%\be
%\ind_{\bigcup_{n > \floor{x}}\bigcap_{m>1/\abs{x}} \bra{T_{1/m}\leq T_n} } \to \ind_{\bra{T_0 < \infty}} \text{ a.s.\ as }m,n\to\infty.
%\ee

Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}), we have
\be
\pro\brb{T_0 < \infty} = \lim_{n\to\infty} \lim_{m\to\infty}\pro\brb{T_{1/m}\leq T_n} = \lim_{n\to\infty}\lim_{m\to\infty} \frac{\log n - \log \dabs{x}}{\log n + \log m } = \lim_{n\to\infty} 0 = 0.
\ee
%letting $r \to 0$ in ($*$) shows that $\pro_x (T_0 < T_R) = 0$, we have
%\beast
%\pro_x\brb{T_0 < \infty} & = & \pro_x\brb{T_0 < T_R < \infty} + \pro_x\brb{T_0 < T_R = \infty} + \pro_x\brb{T_R<T_0 < \infty} \\
%& \leq & \pro_x\brb{T_0 < T_R} + 0 + \pro_x\brb{T_R <T_0 } = 0 + \pro_x\brb{T_R <T_0 } = \lim_{r\da 0}\frac{\log \abs{x} - \log r}{\log R -\log r }
%\eeast
%and letting $R \to \infty$ gives $\pro_x (T_0 <\infty) = 0$

That is, for all $x\neq 0$,
\be
\pro_x\brb{B_t = 0,\text{ for some }t>0} = 0.
\ee

Now we only need to show that
\be
\pro_0\brb{B_t = 0,\text{ for some }t>0} = 0.
\ee

Again, by simple Markov property at $a>0$ we get
\beast
\pro_0\brb{B_t = 0, \text{ for some }t\geq a} & = & \int_{\R^2}\pro_0 \brb{B_{t+a} -B_a + y} \pro_0(B_a\in dy)\\
& = & \int_{\R^2} \pro_y \brb{B_t=0,\text{ for some }t >0} \frac 1{(2\pi a)^{d/2}} e^{-\dabs{y}^2/{2a}} dy = 0%\\% \stackrel{a\to 0}{\to} \pro_0(\exists t > 0 : B_t = 0) & = & \pro_0(\exists t > 0 : B^{(a)}_t + B_a = 0)\\ & = & \int_{\R^2} P_0(B_a \in d x)\pro_x (\exists t \geq 0 : B_t = 0) \quad\quad \text{SM}\\& = & 0
\eeast
since for all $y \neq 0$ we have already proved that $\pro_y \brb{B_t=0,\text{ for some }t >0} = 0$. Thus, this holds for all $a>0$, letting $a\to 0$, we have that $\pro_0\brb{B_t = 0,\text{ for some }t>0} = 0$. Hence, for all $x$, we have
\be
\pro_x\brb{B_t = 0,\text{ for some }t>0} = 0
\ee
which means that the Brownian motion can not touch 0 from $x$ a.s..
%because the law of $B_a$ under $P_0$ is a Gaussian law that does not charge $\{0\}$. Therefore for each $x, y \in \R^2$, $\pro_x (H_y <\infty) = 0$.

\item [(iii)] Let $f$ be a bounded $C^\infty$ functions with all derivatives bounded such that $f (x) = \frac 1{\dabs{x}^{d-2}}$ on $D_{r,R}$. Again, $\Delta f = 0$ on $D_{r,R}$, that is,
\beast
\Delta f & = & \sum^d_{i=1}\frac{\partial^2}{\partial x_i^2}\brb{\frac{1}{\brb{x_1^2 + \dots + x_d^2}^{d/2-1}}} = \sum^d_{i=1} \fp{}{x_i}\brb{\frac{(2-d)x_i}{\brb{x_1^2 + \dots + x_d^2}^{d/2}}}\\
& = & (2-d)\sum^d_{i=1} \fp{}{x_i}\brb{\frac{\brb{x_1^2 +\dots + x_d^2}^{d/2} - d x_i^2 \brb{x_1^2 + \dots + x_d^2}^{d/2-1}}{\brb{x_1^2 + \dots + x_d^2}^d}} = 0.
\eeast

Then by Theorem \ref{thm:brownian_motion_integral_martingale},
\be
M_t = f(B_t)- f (x)- \int^t_0 \frac 12 \Delta f (B_s)ds = \frac 1{\dabs{B_t}^{d-2}} - \frac 1{\dabs{x}^{d-2}}
\ee
is a martingale. Furthermore, let $T_r = \inf\bra{t \geq 0 :\dabs{B_t} \leq r}$ and $T_R = \inf\bra{t \geq 0: \dabs{B_t} \geq R}$. Therefore let $T = T_r \land T_R$. Then the stopped martingale $M_{t\land T} = \frac 1{\dabs{B_{t\land T}}^{d-2}} - \frac 1{\dabs{x}^{d-2}}$ is also a martingale (Theorem \ref{thm:optional_stopping_bounded_stopping_time_continuous}), so $(M_{t\land T})_{t \geq 0}$ is a bounded martingale and therefore a UI martingale. %then $T < T_R < \infty$ a.s. since Brownian motion is unbounded a.s. (by Proposition \ref{pro:brownian_motion_limit_value}).%so $(f (B_{T\land t}), t \geq 0)$ is a bounded martingale.
Applying optional stopping theorem (Theorem \ref{thm:optional_stopping_ui_continuous}), we have
\be
\frac 1{\dabs{x}^{d-2}} = \E\brb{\frac 1{\dabs{B_T}^{d-2}}} = \brb{\frac 1r}^{d-2} \pro_x (T_r < T_R)+ \brb{\frac 1R}^{d-2}\pro_x (T_r > T_R)
\ee
which implies
\be
\pro_x (T_r < T_R) = \frac{\brb{\frac 1R}^{d-2} - \brb{\frac 1{\dabs{x}}}^{d-2}}{\brb{\frac 1R}^{d-2} -\brb{\frac 1r}^{d-2}},\quad \pro_x (T_R < T_r) = \frac{\brb{\frac 1{\dabs{x}}}^{d-2} - \brb{\frac 1r}^{d-2}}{\brb{\frac 1R}^{d-2} - \brb{\frac 1r}^{d-2}}  \quad\quad (\dag)
\ee
for $x \in D_{r,R}$. %Using the same argument with ($**$), we have
%\be
%\pro\brb{T_r< \infty} = \pro_x(T_r < T_R)+ \pro_x \brb{T_R < T_r <\infty} = \pro_x(T_r < T_R)+ \pro_x \brb{T_r <\infty|T_R<T_r} \pro_x \brb{T_R < T_r}
%\ee
We know that
\be
\bra{T_r < \infty} = \bigcup_{n > \floor{x}}\bra{T_r\leq T_n} \cap \bra{T_n <\infty} \ \ra \ \ind_{\bra{T_r \leq T_n}} \ua \ind_{\bra{T_r < \infty}} \text{ a.s.}
\ee

Then by dominated convergence theorem (Theorem \ref{thm:dominated_convergence_probability}), we have
\be
\pro\brb{T_r < T_R} \to \pro\brb{T_r < \infty} \ \ra \ \pro\brb{T_r < \infty} = \lim_{R\ua \infty} \frac{\brb{\frac 1R}^{d-2} - \brb{\frac 1{\dabs{x}}}^{d-2}}{\brb{\frac 1R}^{d-2} -\brb{\frac 1r}^{d-2}} = \brb{\frac{r}{\dabs{x}}}^{d-2}. \qquad (\dag\dag)
\ee
%Letting $R \to \infty$ in ($\dag$) we have that %$\pro_x(T_r < T_R)+ \pro_x \brb{T_r <\infty|T_R<T_r} \pro_x \brb{T_R < T_r} = \brb{\frac{r}{\abs{x}}}^{d-2} + 0 =$
%\be
%\pro_x (T_r < \infty) =  \brb{\frac{r}{\abs{x}}}^{d-2} \qquad (\dag\dag)
%\ee
which is the probability of ever visiting the ball centred at 0 and of radius $r$ when starting from $\dabs{x}\geq r$.

Now we will show that
\be
\pro_0 \brb{\dabs{B_t} \to \infty \text{ as }t\to \infty} = 1.
\ee

%{\bf Approach 1}.
%Fix $r > 0$ and define $S_1 = \inf\{t \geq 0 |\abs{B_t} \leq r\}$, and for all $k \geq 1$ define $T_k = \inf\{t \geq S_k |\abs{B_t}\geq 2r\}$ and $S_{k+1} = \inf\{t \geq T_k |\abs{B_t}\leq r\}$. Now $\{S_k < \infty\} = \{T_k < \infty\}$ because $S_k < T_k$ (giving one inclusion) and Brownian motion is unbounded (giving the other inclusion).
%\beast
%\pro_x (S_{k+1} <\infty| S_k <\infty) & = & \pro_x (S_{k+1} <\infty| T_k <\infty)\\
%& = & \pro_x (\exists t \geq T_k : \abs{B_t}\leq r| T_k <\infty)\\
%& = & \pro_x (\exists t \geq T_k : \abs{B^{(T_k)}_t + B_{T_k}} \leq r | T_k <\infty)\\
%& = & \int_{\R^3} \pro_x (B_{T_k} \in d y)\pro_y (\exists t \geq 0 : \abs{B_t} \leq r)
%\eeast
%by the Strong Markov property. But $\abs{B_{T_k}} = 2r$, so a.s. under $\pro_x (B_{T_k} \in d y)$, $\abs{y} = 2r$, whence $\pro_y (\exists t : \abs{B_t}\leq r) = \pro_y (T_r <\infty) = \frac 12$. Therefore $\pro_x (S_{k+1} < \infty| S_k < \infty) = \frac 12$. It follows that $\pro_x (S_k < \infty) = \frac 1{2^{k-1}} \pro_x (S_1 < \infty)$ given that $S_1 < \infty$. By the Borel-Cantelli Lemma, $\sup\{k |S_k < \infty\} < \infty$ a.s. for all $r$. Letting $r \to \infty$ along $\Z^+$, this shows that eventually $\abs{B_t}\geq r$ for any $r$, so $\abs{B_t}\to \infty$ a.s.
%{\bf Approach 2}.

Since the first three components of Brownian motion in $\R^d$ are a Brownian motion in $\R^3$, it suffices to prove the result for $d = 3$. For fix $r>0$, we define $S_r = \inf\bra{t >0:\dabs{B_t}=r}$ and
\be
A_n = \bra{\dabs{B_t} >n \text{ for all }t\geq S_{n^3}}.
\ee

By the unboundedness of Brownian motion (Proposition \ref{pro:brownian_motion_limit_value}), it is clear that
\be
\pro_0 (S_{n^3} < \infty) = 1.
\ee

Applying the strong Markov property (Theorem \ref{thm:strong_markov_property_brownian_motion}) at time $T_{n^3}$ we obtain
\beast
\pro_0(A_n^c) & = & \pro_0 \brb{\dabs{B_{t}} \leq n \text{ for some }t\geq S_{n^3}} = \pro_0 \brb{\dabs{B_{t+ S_{n^3}} - B_{S_{n^3}} + B_{S_{n^3}}} \leq n \text{ for some }t\geq 0} \\
& = & \int_{\R^3} \pro_0 \brb{\dabs{B^{(S_{n^3})}_t + y} \leq n \text{ for some }t\geq 0} \pro_0 \brb{B_{S_{n^3}} \in dy}\\
& = & \int_{\R^3} \pro_y \brb{\dabs{B_t} \leq n \text{ for some }t\geq 0} \pro_0 \brb{B_{S_{n^3}} \in dy} = \int_{\R^3} \pro_{n^3} \brb{\dabs{B_t} \leq n \text{ for some }t\geq 0} \pro_0 \brb{B_{S_{n^3}} \in dy}\\
& = & \pro_{n^3} \brb{\dabs{B_t} \leq n \text{ for some }t\geq 0} \int_{\R^3}  \pro_0 \brb{B_{S_{n^3}} \in dy} = \pro_{n^3} \brb{\dabs{B_t} \leq n \text{ for some }t\geq 0}\\
& = & \pro_{n^3} \brb{T_n < \infty} = \frac n{n^3} = \frac 1{n^2}
\eeast
by ($\dag\dag$). Since the $\pro_0 (A_n^c)$ is summable, by first Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_1_probability}) we get that $A_n$ happens eventually $\pro_0$-a.s., which implies that $\dabs{B_t}\to \infty$ as $t\to\infty$ $\pro_0$-a.s..
\een
\end{proof}

%\qcutline

\section{Advanced Properties of Brownian motions}


\subsection{Quadratic variation}

\begin{lemma}\label{lem:brownian_motion_subdivision_convergence}
Let $B$ be a standard Brownian motion and let $t \geq 0$ be fixed. For $n \geq 1$, let
\be
\Delta_n := \bra{0 = t_0(n) < t_1(n) <\dots t_{m_n}(n) = t}
\ee
be a subdivision of $[0, t]$, such that $\eta_n := \max_{1\leq i\leq m_n} \bra{t_i(n) - t_{i-1}(n)}$. Then
\be
\lim_{n\to\infty} \sum^{m_n}_{i=1} \brb{B_{t_i} - B_{t_{i-1}}}^2 = t \qquad \text{in }\sL^2(\Omega,\sF,\pro).
\ee
\end{lemma}

\begin{proof}[\bf Proof]%To show that
%\be
%\lim_{n\to \infty} \sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 = t\quad\text{in }\sL^2(\pro),
%\ee
To show the result, we must show that ($\sum^{m_n}_{i=1} \brb{B_{t_i} - B_{t_{i-1}}}^2\in \sL^2(\Omega,\sF,\pro)$ is obvious)
\be
\E\brb{\brb{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - t}^2} \to 0
\ee
as $n\to \infty$. But we have (by Proposition \ref{pro:moments_gaussian})
\beast
\E\brb{\brb{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - t}^2} & = & \E\brb{\brb{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2}^2 - 2t\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 + t^2}\\
& = & \E\brb{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^4 + \sum^{m_n}_{i\neq j} (B_{t_i} - B_{t_{i-1}})^2(B_{t_j} - B_{t_{j-1}})^2 - 2t\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 + t^2}\\
& = & 3\sum^{m_n}_{i=1} \brb{t_i - t_{i-1}}^2 + \sum^{m_n}_{i\neq j} (t_i - t_{i-1})(t_j - t_{j-1}) - 2t\sum^{m_n}_{i=1} (t_i- t_{i-1}) + t^2\\
& = & 2\sum^{m_n}_{i=1} \brb{t_i - t_{i-1}}^2 + \brb{\sum^{m_n}_{i=1} \brb{t_i - t_{i-1}}}^2 - 2t^2 + t^2\\
& = & 2\sum^{m_n}_{i=1} (t_i - t_{i-1})^2 \leq 2\eta_n \sum^{m_n}_{i=1} (t_i - t_{i-1}) = 2t\eta_n \to 0
\eeast
as $n\to \infty$.
\end{proof}


\begin{definition}[quadratic variation]
Let $B$ be a standard Brownian motion and let $t \geq 0$ be fixed. For $n \geq 1$, let
\be
\bsb{B}^n_t := \sum^{\floor{2^nt} -1}_{k=0} \brb{B_{(k+1)2^{-n}} -B_{k2^{-n}}}^2
\ee
for dyadic subdivision. $\bsb{B}^n_t$ is called quadratic variation of $B$ with respect to $n$-dyadic subdivision.
\end{definition}

\begin{lemma}\label{lem:brownian_motion_dyadic_convergence}
Let $B$ be a standard Brownian motion and let $t \geq 0$ be fixed. For $n \geq 1$, %if the subdivision is dyadic,
\be
\bsb{B}^n_t \to t\quad \text{a.s..}
\ee
\end{lemma}

\begin{proof}[\bf Proof]
If the subdivision is dyadic, we can take $t_{m_n}(n) = \floor{2^nt}2^{-n}$ and have that (by Markov inequality (Theorem \ref{thm:markov_inequality_probability}))
\be
\pro\brb{\abs{\bsb{B}^n_t - \floor{2^nt}2^{-n}} \geq \ve} = \pro\brb{\abs{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - \floor{2^nt}2^{-n}} \geq \ve} \leq \frac 1{\ve^2} \E\brb{\brb{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - \floor{2^nt}2^{-n}}^2}\nonumber
\ee
by calculations in Lemma \ref{lem:brownian_motion_subdivision_convergence}, it is equal to
\be
\frac 2{\ve^2} \sum^{m_n}_{i=1} (t_i - t_{i-1})^2 = \frac 2{\ve^2} \frac 1{2^{2n}} \brb{\floor{2^nt}2^{-n}} 2^n = \frac{2\floor{2^nt}}{\ve^2}\frac 1{2^{2n}} \leq \frac{2t}{\ve^2 2^{n}}.
\ee

Thus,
\be
\sum^\infty_{n=1} \pro\brb{\abs{\sum^{m_n}_{i=1} \brb{B_{t_i} - B_{t_{i-1}}}^2 - \floor{2^nt}2^{-n}} \geq \ve} < \infty,
\ee
hence we deduce by first Borel-Cantelli lemma (Lemma \ref{lem:borel_cantelli_1_probability}) that
\be
\pro\brb{\abs{\sum^{m_n}_{i=1} \brb{B_{t_i} - B_{t_{i-1}}}^2 - \floor{2^nt}2^{-n}} < \ve \text{ ev.}} = 1.
\ee

Hence, $\forall \ve >0$, for all $n = n(\omega)$ sufficiently large we have that
\be
\abs{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - \floor{2^nt}2^{-n}} < \ve \text{ a.s.}
\ee

But we also have
\be
\abs{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - t} \leq \abs{\sum^{m_n}_{i=1} (B_{t_i} - B_{t_{i-1}})^2 - \floor{2^nt}2^{-n}} + \abs{\floor{2^nt}2^{-n} - t} < \ve/2 + \ve/2 = \ve\quad \text{a.s..}
\ee

Thus, we have $\bsb{B}^n_t \to t$ a.s..
\end{proof}

Since Brownian motion $B$ is a special stochastic process, we can give the stronger version of its quadratic variation.

\begin{theorem}[\levy's quadratic variation\index{\levy's quadratic variation}]\label{thm:brownian_motion_quadratic_variation_ucas}
Let $B$ be a standard Brownian motion ($B_0 = 0$ a.s.) and let $t \geq 0$ be fixed. For $n \geq 1$, $\brb{[B]^n_t}_{t\geq 0}$ converges to $(t)_{t\geq 0}$ uniformly on compacts almost surely (u.c.a.s.), i.e.,
\be
\sup_{0\leq s\leq t}\abs{\bsb{B}^n_s - s} \to 0 \quad \text{a.s..},\qquad \text{or}\qquad \pro\brb{\lim_{n\to\infty}\sup_{0\leq s\leq t}\abs{\bsb{B}^n_s - s} = 0} = 1.
\ee
\end{theorem}

%Note that for general subdivision,

\begin{proof}[\bf Proof]%To prove this, we only need to show that $\bsb{B}^n_t - \floor{2^nt}2^{-n}$ is a martingale and then apply Doob's maximal inequality (Theorem \ref{thm:doob_maximal_inequality_continuous}) with the same argument in Lemma \ref{lem:brownian_motion_dyadic_convergence}.
Clearly, $\bsb{B}^n_t - \floor{2^nt}2^{-n}$ is adapted and integrable. From Lemma \ref{lem:product_martingale_minus_cross_difference_is_martingale_discrete}, we know that
\be
B_{\floor{2^nt}2^{-n}}^2 - [B]^n_t \quad \text{is a martingale.}
\ee

But we know that $\brb{B_{\floor{2^nt}2^{-n}}^2 - \floor{2^nt}2^{-n}}_{t\geq 0}$ is martingale as $B_t^2 -t$ is a martingale (by Theorem \ref{thm:brownian_motion_martingale}.(ii)). Thus, we have that $\bsb{B}^n_t - \floor{2^nt}2^{-n}$ is actually a martingale. Then by Doob's maximal inequality (Theorem \ref{thm:doob_maximal_inequality_continuous}), we have
\be
\pro\brb{\sup_{0\leq s\leq t}\abs{\bsb{B}^n_s - \floor{2^ns}2^{-n}} \geq \ve} \leq \frac 1{\ve^2}\E\brb{\brb{\bsb{B}^n_t - \floor{2^nt}2^{-n}}^2}.
\ee

Then with the same argument in Lemma \ref{lem:brownian_motion_dyadic_convergence}, we have $\forall \ve >0$, for all $n = n(\omega)$ sufficiently large we have that
\be
\sup_{0\leq s\leq t}\abs{[B]^n_s - \floor{2^ns}2^{-n}} < \ve \text{ a.s.}
\ee

But we also have
\be
\sup_{0\leq s\leq t}\abs{[B]^n_s - s} \leq \sup_{0\leq s\leq t}\abs{[B]^n_s - \floor{2^ns}2^{-n}} + \sup_{0\leq s\leq t}\abs{\floor{2^ns}2^{-n} - s} < \ve/2 + \ve/2 = \ve\quad \text{a.s..}
\ee

Thus, we have $\brb{\bsb{B}^n_t}_{t\geq 0} \to (t)_{t\geq 0}$ u.c.a.s.. %the required result.
\end{proof}

\subsection{Local time of Brownian motion}

\begin{definition}[local time, Brownian motion]
The local time of a standard Brownian motion$B$ at zero is (see \cite{Revuz_Yor_1999})
\be
L(B)_t = \lim_{\ve \da 0} \frac 1{2\ve}\int^t_0 \ind_{\bra{\abs{B_s}\leq \ve}}ds.
\ee
\end{definition}

\subsection{\levy's theorem on distribution identity}

\begin{theorem}[\levy\ theorem, distribution identity]\label{thm:levy_theorem_on_distribution_identity}
Let $B$ be a standard Brownian motion and $S_t = \sup_{0\leq s\leq t}B_s$. Then\footnote{The orginal work is \cite{Levy_1948}. Here we borrow the details in \cite{Morters_Peres_2010} and \cite{Rogers_1994}, I, $P_{30}$.}
\be
\brb{S_t-B_t,S_t} \sim \brb{\abs{B_t},L(B_t)}
\ee
where $L(\cdot)$ is the local time of the stochastic process.
\end{theorem}

\begin{proof}[\bf Proof]
\footnote{proof needed.}
\end{proof}

\subsection{\levy's arcsine law}

The first arcsine law states that the proportion of time that the one-dimensional Brownian motion is positive follows an arcsine distribution.

\begin{theorem}[\levy\ arcsine law, the time spent above zero\index{\levy\ arcsine law!the time spent above zero}]\label{thm:levy_arcsine_law_1}
Let $(B_t)_{t\geq 0}$ be a standard Brownian motion. Then for any $x\in [0,1]$, we have
\be
\pro\brb{\int^1_0 \ind_{\bra{B_t > 0}}\leq x} = \frac 2{\pi}
\arcsin\sqrt{x}.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
\footnote{proof needed.}
\end{proof}

The second arcsine law describes the distribution of the last time the Brownian motion changes sign.

\begin{theorem}[\levy\ arcsine law, the last zero\index{\levy\ arcsine law!the last zero}]\label{thm:levy_arcsine_law_last_zero}
Let $(B_t)_{t\geq 0}$ be a standard Brownian motion and define $R = \inf\bra{t \geq 1 : B_t = 0}$ and let $L = \sup\bra{t \leq 1 : B_t = 0}$.

Then $R$ is a stopping time but $L$ is not a stopping time. Also\footnote{need citation of P.Levy, see \cite{Ito_McKean_1974}.$P_{28}$},
\be
\pro(R > 1 + u) = 1 - \frac 2{\pi}\arctan\sqrt{u}.
\ee
%(it is recalled that the quotient of two independent standard Gaussian random variables has the Cauchy distribution).

Furthermore, $L$ has the arcsine law:
\be
\pro(L < t) = \int^t_0 \frac{dx}{\pi \sqrt{x(1 - x)}} = \frac{2}{\pi}\arcsin\sqrt{t}.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
$L$ is not a stopping time, because the event $\bra{L>t}$ depends only on $(B_{s+t})_{s\geq 0}$ so it not in $\sF_{t}^B$.

$R$ is a stopping time, because for $t< 1$, $\bra{R\leq t} = \emptyset \in \sF_t^B$. For $t\geq 1$, we have
\be
\bra{R\leq t} = \bra{B_1 >0, \inf_{1\leq s\leq t} B_s < 0} \cup \bra{B_1 < 0 , \sup_{1\leq s\leq t} B_s > 0} \cup \bra{B_1 = 0} \in \sF_t^B.
\ee

We have $R = \inf\bra{t\geq 1: B_t = 0} = 1 + \inf\bra{t\geq 0: B_{t+1} = 0} = 1+ \inf\bra{t\geq 0:B_{t+1} - B_1 = - B_1}$.

Thus, $R-1 = \inf\bra{t\geq 0:B_{t+1} - B_1 = - B_1} = \inf\bra{t\geq 0:B'_t = - B_1}$. Then by the simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion}), we have that $(B_t' = B_{t+1}-B_1)_{t\geq 0}$ is a standard Brownian motion independent of $B_1$. %We also have (by tower property)
%\be
%\pro\brb{R-1 > u} = \E \brb{\pro\brb{R-1>u |B_1}}.
%\ee
If we know $B_1$ then $R-1$ is the first hitting time of $-B_1$, which we know that $R-1$ has the same distribution as $\brb{\frac{B_1}{B_1'}}^2$ by Proposition \ref{pro:stopping_time_brownian_motion_touch_special_point}. Thus, by tower property (Proposition \ref{pro:conditional_expectation_tower_independence}),
\be
\pro\brb{R-1 > u} = \E\brb{\left.\pro\brb{R-1 > u}\right|B_1} = \E\brb{\left.\pro\brb{\brb{\frac{B_1}{B_1'}}^2 > u}\right|B_1}  = \pro\brb{\brb{\frac{B_1}{B_1'}}^2>u } %\E \brb{\pro\brb{\left.\brb{\frac{B_1}{B_1'}}^2>u \right|B_1}} = \E \brb{\pro\brb{\left.\brb{\frac{X}{X'}}^2>u \right|X}}
\ee
where $B_1$ and $B'_1$ are two independent Gaussian with zero mean and variance 1. Thus, $B_1/B'_1$ is Cauchy distributed, $\pro(B_1/B'_1 \leq k) = \frac 12 + \frac 1{\pi}\arctan k$ (Proposition \ref{pro:two_independent_standard_gaussian_quotient_implies_cauchy}),
\beast
\pro\brb{R-1 > u} & = & \pro\brb{\abs{\frac{B_1}{B'_1}}>\sqrt{u}}  = \pro\brb{\frac{B_1}{B'_1}>\sqrt{u}} + \pro\brb{\frac{B_1}{B'_1} < -\sqrt{u}} \\
& = & 1 - \brb{\frac 12 + \frac 1{\pi}\arctan k} + \frac 12 + \frac 1{\pi}\arctan\brb{-\sqrt{u}} = 1- \frac 2{\pi} \arctan\sqrt{u}.
\eeast

We have that %$\pro(R-1 > u) = 1 - \$
\be
R^{-1} = \sup\bra{\frac 1t:t\geq 1, B_t = 0} = \sup\bra{u\leq 1: B_{1/u} = 0} = \sup\bra{u\leq 1: uB_{1/u} = 0}.
\ee

But we know that $\brb{uB_{1/u}}_{u\geq 0}$ has the same law as $(B_u)_{u\geq 0}$, so $R^{-1} \sim L$. Thus,
\be
\pro\brb{L < t} = \pro\brb{R > \frac 1t} = \pro\brb{R > 1 + \frac{1-t}{t}} = 1- \frac 2{\pi} \arctan\sqrt{\frac{1-t}{t}}
%& = & 2{\pi} \arctan\sqrt{\frac{1-t}{t}}
\ee

Taking differentiation wrt $t$ we have
\be
\brb{1- \frac 2{\pi} \arctan\sqrt{\frac{1-t}{t}} }' = - \frac{1}{1+ \frac{1-t}{t}} \brb{\brb{\frac{1-t}{t}}^{1/2}}' = -t \brb{\frac{1-t}{t}}^{-1/2} \frac{-t - (1-t)}{t^2} = \frac 1{\pi\sqrt{t(1-t)}}
\ee
as required.
\end{proof}

The third arcsine law states that the time at which a Brownian motion achieves its maximum is arcsine distributed.

\begin{theorem}[\levy\ arcsine law, the position of the maximum\index{\levy\ arcsine law!the position of the maximum}]\label{thm:levy_arcsine_law_position_of_maximum}
Let $(B_t)_{t\geq 0}$ be a standard Brownian motion. Since Brownian motion has an almost surely unique maxima\footnote{proof needed.}, say $M$, we define
\be
T := \bra{t\in [0,1]: B_t = M}.
\ee

% statement of the law relies on the fact that the ,[1] and so we can define the random variable $S$ which is the time at which the maxima is achieved. i.e. the unique $S$ such that

Then for any $x\in [0,1]$,
\be
\pro\brb{T\leq x} = \frac 2{\pi}\arcsin\sqrt{x}.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
Defining the running maximum process $S_t$ of the Wiener process
\be
S_t = \sup \bra{ B_s : s \in [0,t] },
\ee
then the law of $X_t = S_t - B_t$ has the same law as a reflected Brownian motion $\abs{\wt{B}_t}$ (see Theorem \ref{thm:levy_theorem_on_distribution_identity}) where $\wt{B}_t$ is a Brownian motion independent of $B_t$.

Since the zeros of $\wt{B}$ and $\abs{\wt{B}}$ coincide, the last zero of $X$ has the same distribution as $L$ (in Theorem \ref{thm:levy_arcsine_law_last_zero}), the last zero of Brownian motion $\wt{B}$. The last zero of $X$ occurs exactly when $B$ achieves its maximum. It follows that the second and third laws are equivalent.
\end{proof}



\section{The Relationship between Random Walks and Brownian Motions}
%\subsection{Donsker's invariance principle}

We endow $C([0, 1],\R)$ with the supremum norm where $C([0, 1],\R)$ is the space of all continuous functions $f:[0,1]\to\R$ and supremum norm is
\be
\dabs{f}_\infty := \sup_{x\in [0,1]}\abs{f(x)}
\ee
for $f,g\in C([0,1],\R)$.

Recall\footnote{see the exercises on continuous-time processes in CAM stochastic calculus notes, details needed.} that the product $\sigma$-algebra associated with it coincides with the Borel $\sigma$-algebra associated with this norm. We say that a function $\Lambda :C([0, 1],\R) \to \R$ is continuous if it is continuous with respect to this norm. Often, functions $\Lambda$ defined on $C([0, 1],\R)$ will be called functionals. For instance, the supremum of a path on the interval $[0,1]$ is a (continuous) functional.

\begin{definition}\label{def:functional_for_continuous_0_1}
A measurable function $\Lambda:C[0,1] \to \R$ is called a functional.
\end{definition}


\subsection{Skorokhod embedding theorem}

First, we give an elegant demonstration that makes use of a coupling of the random walk with a Brownian motion, called the Skorokhod embedding theorem. It is however specific to dimension $d = 1$. Suppose we are given a Brownian motion $(B_t, t \geq 0)$ on some probability space $(\Omega,\sF, \pro)$.

\begin{theorem}[Skorokhod's embedding]\label{thm:skorokhod_embedding}
Let $(X_n)_{ n \geq 1}$ be a sequence of real-valued integrable independent random variables with common law $\mu$ ($X_i$ are i.i.d.), such that
\be
\int_{\R} x\mu (dx) = 0,\quad\quad \int_{\R} x^2\mu (dx) = \sigma^2 \in (0,\infty).
\ee

Furthermore,
\be
\mu^+(dy) = \pro(X_1 \in dy)\ind_{\bra{y\geq0} },\qquad \mu^-(dz) = \pro(-X_1 \in dz)\ind_{\bra{z>0}}
\ee
define two non-negative measures.

Assume that $(\Omega,\sF,\pro)$ is a rich enough probability space so that we can further define on it, independently of Brownian motion $(B_t)_{ t \geq 0}$, a sequence of independent identically distributed $\R^2$-valued random variables $(Y_n,Z_n)_{ n \geq 1}$ with distribution
\be
\pro((Y_n,Z_n) \in dydz) = \frac 1C (y + z)\mu^+(dy)\mu^-(dz),
\ee
where $C > 0$ is the appropriate normalizing constant that makes this expression a probability measure (this is possible because $X$ has a well-defined expectation (integrable)).

We define a sequence of random times, by $T_0 = 0$, $T_1 = \inf\bra{t \geq 0: B_t \in \{Y_1,-Z_1\}}$, and recursively,
\be
T_n = \inf\bra{t \geq T_{n-1}: B_t - B_{T_{n-1}} \in \{Y_n,-Z_n\}}.
\ee

Then these times are a.s. finite stopping time and the sequence $(B_{T_n})_{n \geq 0}$ has the same law as $(S_n)_{n \geq 0}$.

Moreover, the intertimes $(T_n - T_{n-1})_{n \geq 1}$ form a sequence of independent random variables with same distribution with expectation $\E T_1 = \sigma^2$.
\end{theorem}

\begin{proof}[\bf Proof]
Let $\sF^B$ be the filtration of the Brownian motion $B$, and for each $n \geq 0$, introduce the filtration $\sG^n = (\sG^n_t)_{ t \geq 0}$ defined by
\be
\sG^n_t = \sF^B_t \lor \sigma(Y_1,Z_1,\dots, Y_n,Z_n).
\ee
which is $\sigma$-algebra generated by $\sigma$-algebras $\sF^B_t$ and $\sigma(Y_1,Z_1,\dots, Y_n,Z_n)$. Since $(Y_i,Z_i)$ are independent from $\sF^B$ by assumption, $(B_t)_{ t \geq 0}$ is a $\sG^n$-Brownian motion for every $n \geq 0$ (by Proposition \ref{pro:measurable_independent_measurable})

Moreover, $T_n$ is a stopping time for $\sG^n$ by Proposition \ref{pro:debut_time_closed_set_stopping_time}. It follows by Theorem \ref{thm:strong_markov_property_brownian_motion} that if $\wt{B}_t = (B_{T_n+t} - B_{T_n})_{,t \geq 0}$ then $\wt{B}$ is independent from $\sG^n_{T_n}$.

Moreover, $(Y_{n+1},Z_{n+1})$ is independent both from $\sG^n_{T_n}$ and from $\wt{B}$, therefore $(T_{n+1} - T_n)$, which depends only on $\wt{B}$ and $(Y_{n+1},Z_{n+1})$, is independent from $\sG^n_{T_n}$. In particular, $(T_{n+1} - T_n)$ is independent from $\sigma(T_0, T_1,\dots, T_n)$. More generally, we obtain that the processes $\brb{B_{t+T_{n-1}} - B_{T_{n-1}}}_{ 0 \leq t \leq T_n - T_{n-1}}$ are independent with the same distribution by Theorem \ref{thm:strong_markov_property_brownian_motion} .

It therefore remains to check that $B_{T_1}$ has the same law as $X_1$ and $\E T_1 = \sigma^2$. Remember from Proposition \ref{thm:brownian_motion_double_bounded} that given $Y_1$,$Z_1$, the probability that $B_{T_1} = Y_1$ is $Z_1/(Y_1+Z_1)$. %, as follows from the optional stopping theorem.
Therefore, for every non-negative measurable function $f$, by first conditioning on $(Y_1,Z_1)$, we get
\beast
\E[f(B_{T_1} )] & = & \E\bsb{f(Y_1)\frac{Z_1}{Y_1 + Z_1} + f(-Z_1) \frac{Y_1}{Y_1 + Z_1}}\\
& = & \int_{[0,\infty)\times(0,\infty)} \frac 1C (y +z)\mu^+(dy)\mu^-(dz)\brb{f(y)\frac z{y + z} + f(-z)\frac y{y + z}}\\
& = & \frac 1C \int_{[0,\infty)\times(0,\infty)}  \mu^+(dy)\mu^-(dz)(zf(y) + yf(-z))\\
& = & \frac 1C \int_{[0,\infty)} \mu^+(dy)f(y)\int_{(0,\infty)} z\mu^-(dz) + \frac 1C \int_{(0,\infty)} \mu^-(dz)f(-z) \int_{[0,\infty)} y\mu^+(dy)
\eeast
by Fubini theorem (Theorem \ref{thm:fubini}).

Now observe that since $\E X_1 = 0$, it must be the case that
\be
\int_{[0,\infty)} y\mu^+(dy) = \int_{(0,\infty)} z\mu^-(dz) = C',
\ee
say, and thus, the left hand side is equal to
\be
\E[f(B_{T_1} )] = \frac{C'}C \int_{[0,\infty)} (f(y)\mu^+(dy) + f(-y)\mu^-(dy)) = \frac{C'}C \int_\R f(y)\mu (dy) =\frac {C'}C \E[f(X_1)].
\ee

By taking $f \equiv 1$, it must be that $C' = C$, and hence $B_{T_1}$ has the same law as $X_1$\footnote{We can have $\forall a\in \R$ and $f(x) = \ind_{\bra{x\leq a}}$}. For $\E T_1$, recall from Proposition \ref{thm:brownian_motion_double_bounded} that given $y,z>0d$
\be
\E\brb{\inf\bra{t \geq 0 : B_t \in \{y,-z\}}} = yz,
\ee
so by a similar conditioning argument as above,
\beast
\E T_1 & = & \E\brb{\E\brb{T_1|Y_1,Z_1}} = \int_{[0,\infty)\times(0,\infty)} \frac 1C (y+ z)yz\mu^+(dy)\mu^-(dz)\\
& = & \frac 1C  \int_{[0,\infty)} y^2\mu^+(dy)  \int_{(0,\infty)} z\mu^-(dz) + \frac 1C  \int_{(0,\infty)} z^2\mu^-(dz)  \int_{[0,\infty)} y\mu^+(dy)\\
& = & \frac {C'}C  \int_{\R} x^2\mu (dx) =\frac{C'}C \sigma^2
\eeast
but since we already know that $C' = C$, this shows that $\E T_1= \sigma^2$, as claimed.
\end{proof}

\subsection{Donsker's invariance principle}

Sometimes it turns out that we need to consider the sample under the assumption that $X_1,\dots,X_n$ all have the same distribution with mean $\mu$. For instance, we consider
\be
M_n := \max_{1\leq k\leq n}\brb{S_k- \mu k},\qquad \forall n\geq 1.
\ee

It is natural to ask if there is a central limit theorem for $M_n$. The answer is resounding `yes' and involves Donsker's invariance principle (Theorem \ref{thm:donsker_invariance_principle}).


The theorem in this subsection completes the description of Brownian motion as a `limit' of centered random walks % as depicted in the beginning of the chapter\footnote{link needed.},
and strengthens the convergence of finite-dimensional marginals (see Proposition \ref{pro:marginal_distribution_brownian_motion}) to that convergence in distribution.

%Now we will derive a stronger theorem about the convergence of stochastic processes.

First, we will need another lemma, which tells us that the times $T_n$ are in a fairly strong sense localized around there mean $n\sigma^2$.

\begin{lemma}\label{lem:donsker_sup_convergence_almost_surely}
Suppose we have the same setup in Theorem \ref{thm:skorokhod_embedding} . Then, we have the following convergence as $N\to\infty$.
\be
N^{-1} \sup_{0\leq n\leq N} \abs{T_n - n\sigma^2}\to 0 \quad \text{a.s. }
\ee
\end{lemma}

\begin{proof}[\bf Proof]
We know that $(T_n-T_{n-1})_{n\geq 1}$ are indpendent by Theorem \ref{thm:skorokhod_embedding}. Then by the strong law of large numbers (Theorem \ref{thm:slln}), $T_n/n \to \sigma^2$ almost surely.

Thus, fix $\ve > 0$, there exists $N_0 = N_0(\ve, \omega)$ such that $\forall n \geq N_0$,\be
\abs{T_n/n - \sigma^2} \leq \ve.
\ee

Thus for $N_0 \leq n \leq N$,
\be
N^{-1}\abs{T_n - n\sigma^2} \leq \frac nN \ve \leq \ve.
\ee

Moreover, $N^{-1} \sup_{0\leq n\leq N_0} \abs{T_n - n\sigma^2}$ tends to 0 almost surely as $N \to\infty$, so this implies the required result.
\end{proof}





\begin{theorem}[Donsker's invariance principle\index{Donsker's invariance principle}]\label{thm:donsker_invariance_principle}
Let $(X_n)_{ n \geq 1}$ be a sequence of real-valued integrable independent random variables with common law $\mu$, such that
\be
\int x\mu (dx) = 0,\quad\quad \int x^2\mu (dx) = \sigma^2 \in (0,\infty).
\ee

Let $S_0 = 0$ and $S_n = X_1 +\dots +X_n$, and define a continuous process that interpolates linearly between values of $S$, namely
\be
S_t = (1 - \{t\})S_{\floor{t}} + \{t\}S_{\floor{t}+1},\quad\quad t \geq 0,
\ee
where $\floor{t}$ denotes the integer part of $t$ and $\{t\} = t - \floor{t}$. Then
\be
\wt{S} := \brb{\frac{S_{Nt}}{\sqrt{\sigma^2N}}}_{0 \leq t \leq 1}
\ee
converges in distribution\footnote{need definition of convergence in distribution of stochastic process.} to a standard Brownian motion between times 0 and 1, i.e. for every bounded continuous function $\Lambda: C([0, 1]) \to \R$,
\be
\E(\Lambda(\wt{S}))\ \to\ \E\brb{\Lambda(B)}\quad\text{as }n\to\infty.
\ee
\end{theorem}

\begin{remark}
Notice that this is much stronger than what Proposition \ref{pro:marginal_distribution_brownian_motion} says. Despite the slight difference of framework between these two results (one uses \cadlag\ continuous-time version of the random walk, and the other uses an interpolated continuous version), Donsker's invariance principle is stronger.

For instance, one can infer from this theorem that the random variable $N^{-1/2} \sup_{0\leq n\leq N} S_n$ converges to $\sup_{0\leq t\leq 1} B_t$ in distribution, because $f \mapsto \sup f$ is a continuous operation on $C([0, 1],\R)$. Proposition \ref{pro:marginal_distribution_brownian_motion} would be powerless to address this issue.
\end{remark}

\begin{proof}[\bf Proof]
We suppose given a Brownian motion $B$. For $N \geq 1$, define $B^{(N)}_t = N^{1/2}B_{N^{-1}t}, t \geq 0$, which is a standard Brownian motion by scaling invariance (see Proposition \ref{pro:brownian_motion_basic_properties}.(ii)), i.e., $B^{(N)} \sim B$.

Perform the Skorokhod embedding construction on $B^{(N)}$ to obtain variables $(T^{(N)}_n)_{ n \geq 0}$ with $T^{(N)}_n \sim T_n$. Then, let $S^{(N)}_n = B^{(N)}_{T^{(N)}_n}$. By Theorem \ref{thm:skorokhod_embedding}, $(S^{(N)}_n)_{ n \geq 0}$ is a random walk with same law as $(S_n)_{n \geq 0}$ as $S_n \sim B_{T_n}$ and $B_{T_n} \sim B^{(N)}_{T_n^{(N)}}$.

Then we interpolate linearly between integers to obtain a continuous process $(S^{(N)}_t )_{0 \leq t \leq 1}$ which thus has the distribution as $(S_t)_{0 \leq t \leq 1}$. Finally, let
\be
\wt{S}^{(N)}_t := \frac{S^{(N)}_{Nt}}{\sqrt{\sigma^2N}},\qquad t \geq 0
\ee
and $\wt{T}^{(N)}_n := N^{-1}T^{(N)}_n$. It is easy to see that $\wt{S}_t\sim \wt{S}_t^{(N)}$.

Finally, let $B'_t = B^{(N)}_{\sigma^2t}/\sqrt{\sigma^2}$, which is also a standard Brownian motion having the same law with $B$. Thus, it suffices to show that the supremum norm
\be
\dabs{\wt{S}^{(N)}- B' }_\infty = \sup_{0\leq t\leq 1} \abs{\wt{S}^{(N)}_t - B'_t }\stackrel{p}{\to} 0,\qquad \text{as }N\to\infty.
\ee
%as $.%, where $\stackrel{p}{\to}$ denotes convergence in probability.

First recall what we have proved in Lemma \ref{lem:donsker_sup_convergence_almost_surely}, and note that this implies convergence in probability. That is, since $(T^{(N)}_n)_{n \geq 0}$ has the same distribution as $(T_n)_{ n \geq 0}$ we infer from this and definition of convergence in probability (see Definition \ref{def:convergence_in_probability}) and get for every $\delta > 0$, letting $\delta' = \delta \sigma^2 > 0$, we have:
\be
\pro\brb{N^{-1} \sup_{0\leq n\leq N} \abs{T^{(N)}_n - n\sigma^2} \geq \delta'} \to 0\quad\text{ as }N\to \infty.
\ee

Therefore dividing by $\sigma^2$:
\be
\pro\brb{\sup_{0\leq n\leq N} \abs{\wt{T}^{(N)}_n /\sigma^2 - n/N} \geq \delta} \to 0\quad\text{ as }N\to \infty.
\ee

Now, note that if $t = n/N$, then
\be
\wt{S}^{(N)}_t = \frac{S^{(N)}_n}{\sqrt{N\sigma^2}} = \frac{B^{(N)}_{T_n^{(N)}}}{\sqrt{N\sigma^2}}  =  B'_{\wt{T}^{(N)}_n /\sigma^2}.
\ee

Thus, by continuity, if $t\in [n/N, (n + 1)/N]$, there exists $u \in [\wt{T}^{(N)}_n/\sigma^2, \wt{T}^{(N)}_{n+1}/\sigma^2]$ such that $\wt{S}^{(N)}_t = B'_u$ by intermediate value theorem (Theorem \ref{thm:intermediate_value}) since $\wt{S}^{(N)}$ is a straight line between $n/N$ and $(n+1)/N$). Therefore, for all $\ve > 0$ and all $\delta > 0$, the event
\be
\bra{\sup_{0\leq t\leq1} \abs{\wt{S}^{(N)}_t - B'_t} > \ve} \subseteq A \cup B \subseteq  K^N_\delta \cup L^N_{\delta,\ve},
\ee
where
\be
A = \bra{\sup_{0\leq t\leq1} \abs{\wt{S}^{(N)}_t - B'_t} > \ve,\ \sup_{0\leq n\leq N} \abs{\wt{T}^{(N)}_n/\sigma^2 - n/N}  > \delta } \subseteq  \bra{\sup_{0\leq n\leq N} \abs{\wt{T}^{(N)}_n/\sigma^2 - n/N}  > \delta} = K^N_\delta,
\ee
\be
B = \bra{\sup_{0\leq t\leq1} \abs{\wt{S}^{(N)}_t - B'_t} > \ve,\ \sup_{0\leq n\leq N} \abs{\wt{T}^{(N)}_n/\sigma^2 - n/N}  \leq \delta }
\ee
and actually
\be
B = L^N_{\delta,\ve} = \bra{\exists t \in [0, 1], \exists u \in [t - \delta, t + \delta + 1/N] : \abs{B'_u-B'_t } > \ve }.
\ee

%$K^N_\delta $  $L^N_{\delta,\ve}$ represents that case that there exists a $u$ close enough to $t$ such that $\abs{B'_t - B'_u} > \ve$.

We already know that $\pro(K^N_\delta) \to 0$ as $N\to\infty$. For $L^N_{\delta,\ve}$, since $B'$ is a.s. uniformly continuous on $[0, 1]$ (by Theorem\footnote{theorem needed.} as it is continuous on a compact set), by taking $\delta$ small enough and then $N$ large enough, we can make $\pro(L^N_{\delta,\ve})$ as small as wanted. More precisely, let
\be
L_{2\delta,\ve} = \bra{\exists t \in [0, 1], \exists u \in [t - 2\delta, t + 2\delta]: \abs{B'_t - B'_u} > \ve}.
\ee
then for $N \geq 1/\delta$, $L^N_{\delta,\ve} \subseteq  L_{2\delta,\ve}$, and thus for all $\delta > 0$:
\be
\limsup_{N\to\infty} \pro\brb{\dabs{\wt{S}^{(N)} - B'}_\infty > \ve} = \limsup_{N\to\infty} \pro\brb{\sup_{0\leq t\leq 1}\abs{\wt{S}^{(N)}_t - B'_t} > \ve} \leq \pro(L_{2\delta,\ve})
\ee

However, as $\delta\to 0$, $\pro(L_{2\delta,\ve}) \to 0$ by almost sure continuity of $B'$ on $[0, 1]$. and the fact that these events are decreasing.
Hence it must be that
\be
\limsup_{N\to\infty} \pro\brb{\dabs{\wt{S}^{(N)} - B'}_\infty > \ve} = \limsup_{N\to\infty} \pro\brb{\sup_{0\leq t\leq 1}\abs{\wt{S}^{(N)}_t - B'_t} > \ve} = 0.
\ee

Therefore, $(\wt{S}^{(N)})_{0 \leq t \leq 1}$ converges u.c.p. % in probability for the uniform norm
to $(B'_t)_{0 \leq t \leq 1}$ , which implies convergence in distribution. %This concludes the proof.

Since for any $t$, $\wt{S}^{(N)}_t \sim \wt{S}_t$ and $B'_t\sim B_t$, $(\wt{S})_{0 \leq t \leq 1}$ converges to $(B_t)_{0 \leq t \leq 1}$ in distribution.
\end{proof}

\begin{example}\label{exa:donsker_invariance_principle_central_limit_theorem}
Suppose $(X_n)$ are i.i.d. random variables with zero mean and variance $\sigma^2 <\infty$ and $S_n = X_1 + \dots + X_n$. Let $h:\R\to \R$ be any bounded continuous function. For any $f\in (C[0,1],d)$ where
\be
d(f,g) := \sup_{x\in [0,1]}\abs{f(x)-g(x)}
\ee
we define $\Lambda(f) := h(f(1))$. It is easy to see that $\Lambda$ is a continuous functional. That is,
\be
\Lambda(f_n)\to \Lambda (f) \quad\text{whenever }\ d(f_n,f)\to 0.
\ee

To prove this, $\forall \ve >0$, we can find $\delta$, such that
\be
\abs{f_n(1) - f(1)} \leq \sup_{x\in[0,1]}\abs{f_n(x)-f(x)} = d(f_n,f) < \delta, \qquad \abs{\Lambda(f_n) -\Lambda(f) } = \abs{h(f_n(1)) - h(f(1))} < \ve
\ee
by the continuity of $h$. Then we apply Donsker's invariance principle (Theorem \ref{thm:donsker_invariance_principle}) and have
\be
\E(\Lambda(\wt{S})) \to \E\brb{\Lambda(B)}
\ee
which means
\be
\E(h(\wt{S}_1)) \to \E\brb{h(B_1)} = \E\brb{h(X)}.
\ee

In particular, we have
\be
\wt{S}_1 := \frac{S_n}{\sqrt{\sigma^2 n}} = \frac{S_n}{\sigma \sqrt{n}} ,\qquad X := B_1 \sim \sN(0,1).
\ee

This means that $\frac{S_n}{\sigma \sqrt{n}}$ converges to $\sN(0,1)$ in distribution, which is the central limit theorem (Theorem \ref{thm:central_limit}).

Now redefine $\Lambda(f):=h\brb{\sup_{0\leq t\leq 1}f(t)}$. Then $\Lambda$ is a bounded, continuous functional. Donsker's invariance principle says the following about the this choice of $\Lambda$: as $n\to \infty$
\be
\E\bsb{h\brb{\sup_{0\leq t\leq 1}\wt{S}_t}} = \E\brb{\Lambda (\wt{S})} \to \E\brb{\Lambda(B) } = \E\bsb{h\brb{\sup_{0\leq t\leq 1}B_t}}.
\ee

By the refection principle (Corollary  \ref{cor:abs_sup_equal_brownian_motion}), $\sup_{0\leq t\leq 1}B_t$ has the same distribution as $\abs{\sN(0,1)}$. Therefore, $\sup_{0\leq t\leq 1}\wt{S}_t$ converges to $\abs{\sN(0,1)}$ in distribution (This is weak convergence in $\R$ but not $C[0,1]$.). Since the $S_{nt}$ is interpolated by $S_k$ for $0\leq k\leq n$,
\be
\sup_{0\leq t\leq 1}\wt{S}_t =  \sup_{0\leq t\leq 1} \frac{S_{nt}}{\sigma\sqrt{n}} = \max_{0\leq k\leq n}\frac{S_k}{\sigma\sqrt{n}}.
\ee

Therefore, we have proved that for all $x\geq 0$
\be
\lim_{n\to\infty}\pro\brb{\max_{1\leq k\leq n}S_k \leq x\sigma\sqrt{n}} = \sqrt{\frac{2}{\pi}} \int^x_0 e^{-y^2/2}dy.
\ee

Similarly, redefine $\Lambda(f) := h(\inf_{0\leq t\leq 1}f(t))$. Then we can have
\be
\E\brb{h\brb{ \max_{0\leq k\leq n}\frac{S_k}{\sigma\sqrt{n}}}} \to \E\brb{h\brb{-\abs{\sN(0,1)}}}.
\ee

Thus, we have for $x\leq 0$
\be
\lim_{n\to\infty}\pro\brb{\min_{1\leq k\leq n}S_k\leq x\sigma\sqrt{n}} = \sqrt{\frac{2}{\pi}} \int^x_{-\infty} e^{-y^2/2}dy.
\ee

In fact, we can let $\alpha,\beta,\gamma\in \R$ be fixed, and define
\be
\Lambda(f) := h\brb{\alpha f(1) + \beta \sup_{0\leq t\leq 1}f(t)+ \gamma \inf_{0\leq t\leq 1}f(t)}.
\ee

Therefore, by Donsker's invariance principle,
\be
\E\brb{h\brb{\alpha \frac{S_n}{\sigma\sqrt{n}} + \beta \max_{1\leq k\leq n}\frac{S_k}{\sigma\sqrt{n}} + \gamma \min_{1\leq k\leq n}\frac{S_k}{\sigma\sqrt{n}}}} \to \E\brb{h\brb{\alpha B_1 + \beta \sup_{0\leq t\leq 1}B_t + \gamma\inf_{0\leq t\leq 1}B_t}}.
\ee

Then we can let $h(x) = e^{ix}$ (note $h$ is still bounded for complex case) and get the characteristic functions. Hence, by \levy's continuity theorem (Theorem \ref{thm:levy_continuity}), we have that
\be
\brb{\frac{S_n}{\sigma\sqrt{n}},\ \max_{1\leq k\leq n}\frac{S_k}{\sigma\sqrt{n}},\ \min_{1\leq k\leq n}\frac{S_k}{\sigma\sqrt{n}}} \to \brb{B_1,\  \sup_{0\leq t\leq 1}B_t,\ \inf_{0\leq t\leq 1}B_t}
\ee
in distribution, which is weak convergence in $\R^3$.

%Then letting $\alpha =0,\beta =1,\gamma = -1$,

Similarly, we have that
\be
\E\brb{h\brb{\max_{1\leq k\leq n}\frac{\abs{S_k}}{\sigma\sqrt{n}}}} \to \E\brb{h\brb{\sup_{0\leq t\leq 1}\abs{B_t}}}
\ee
which gives that for $x\geq 0$\footnote{proof needed for the probability distribution function $\pro\brb{\sup_{0\leq t\leq 1}\abs{B_t}\leq x}$. It's been checked numerically that these two results are consistent.},
\be
\lim_{n\to\infty}\pro\brb{\max_{1\leq k\leq n}\abs{S_k} \leq x\sigma\sqrt{n}} = \frac 4{\pi}\sum^\infty_{k=1}\frac{(-1)^{k+1}}{2k-1} \exp\brb{-\frac{(2k-1)^2\pi^2}{8x^2}}.
\ee

Note that the corresponding densty function of $\sup_{0\leq t\leq 1}\abs{B_t}$ is given by (see Theorem \ref{thm:joint_density_maximum_minimum_bm})
\be
f(x) = \frac{4}{\sqrt{2\pi}}\sum^\infty_{k=1}(-1)^{k+1}(2k-1) \exp\brb{-\frac{(2k-1)^2 x^2 }{2}}.
\ee
\end{example}

\begin{proposition}\label{pro:bounded_continuous_function_iid_sum_converges_to_integral}
For $S_n = X_1+\dots+ X_n$ where $X_i$ are i.i.d. with zero mean and finite variance $\sigma^2$. Then for any bounded continuous function $f:\R\to \R$,
\be
\frac 1n\sum^n_{k=1}f\brb{\frac{S_k}{\sigma\sqrt{n}}} \to \int^1_0 f(B_t)dt
\ee
in distribution as $n\to \infty$.
\end{proposition}

\begin{proof}[\bf Proof]
\footnote{proof needed.}
\end{proof}

\begin{proposition}\label{pro:indicator_function_iid_sum_converges_to_integral}
Let $S_n = X_1+\dots+ X_n$ where $X_i$ are i.i.d. with zero mean and finite variance $\sigma^2$. Then
\be
\frac 1n\sum^n_{k=1}\ind_{\bra{S_k> 0}} \to \int^1_0 \ind_{\bra{B_t> 0}}dt
\ee
in distribution as $n\to \infty$.
\end{proposition}

\begin{remark}
The indicator function $\ind$ is not continuous. Thus, we cannot apply Proposition \ref{pro:bounded_continuous_function_iid_sum_converges_to_integral}.
\end{remark}

\begin{proof}[\bf Proof]
\footnote{proof needed.}
\end{proof}

Consequently, by Proposition \ref{pro:indicator_function_iid_sum_converges_to_integral} and Theorem \ref{thm:levy_arcsine_law_time_spent_above_zero}, we have the following proposition.

\begin{proposition}
Let $S_n = X_1+\dots+ X_n$ where $X_i$ are i.i.d. with zero mean and finite variance $\sigma^2$. Then for any $x\in [0,1]$,
\be
\lim_{n\to \infty}\pro\brb{\frac 1n\sum^n_{k=1}\ind_{\bra{S_k> 0}}\leq x} = \frac 2{\pi}\arcsin\sqrt{x}.
\ee
\end{proposition}

\section{Conditioned Brownian Motion and Brownian Bridge}

\subsection{Conditioned Brownian motion}

Now let $B$ be a standard Brownian motion (with $B_0 = 0$ a.s.) and let $0<s<t$. What's the conditional distribution of $B_s$ given $B_t$. The next result gives a nice way of working with the relevant normal distributions.

\begin{lemma}\label{lem:brownian_bridge_independent_of_brownian_motion}
let $B$ be a standard Brownian motion and $0<s<t$. Then $B_s - s B_t/t$ is independent of $B_t$.
\end{lemma}

\begin{proof}[\bf Proof]
First we know that $B_s - s B_t/t$ and $B_t$ are Gaussian random variables. Also for any $\theta_1,\theta_2\in \R$, we have
\be
\theta_1 \brb{B_s - s B_t/t} + \theta_2 B_t = \brb{\theta_1 + \theta_2 - s \theta_1/t} B_s + (\theta_2 - s\theta_1/t) (B_t- B_s)
\ee
is still Gaussian as $B_s$ and $B_t - B_{s}$ are independent. Thus, $B_s - s B_t/t$ and $B_t$ are multivariate Gaussian random variables. So it suffices to show that $\cov\brb{B_s - sB_t/t, B_t} = 0$ by Theorem \ref{thm:multivariate_gaussian_rv_property}.(v).

To verify this, just note that
\be
\cov \brb{B_s - s B_t/t, B_t} = \cov \brb{B_s,B_t} - \frac st \cov\brb{B_t,B_t} = s\land t - \frac st t = 0.
\ee
\end{proof}

\begin{proposition}[conditioned Brownian motion]\label{pro:conditioned_brownian_motion}
let $B$ be a standard Brownian motion and $0\leq s<t$. Then for given $B_t$\footnote{Note that $B_t$ is not random variable but a value here.}
\be
B_s|_{B_t} \sim \sN\brb{\frac{sB_t}t, \frac{s(t-s)}{t}}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
To check the moment generating function of the conditioned Brownian motion $B_s|_{B_t}$. For any $\theta\in \R$,
\beast
\E\brb{\exp\brb{\theta B_s|_{B_t}}} & \stackrel{\text{a.s.}}{=} & \E\brb{\left.\exp\brb{\theta B_s}\right| B_t} = \E\brb{\left.\exp\brb{\theta \brb{B_s - sB_t/t}}\exp\brb{\theta sB_t/t} \right|B_t} \quad \text{ by Proposition \ref{pro:conditional_probability_density_function}} \\
& \stackrel{\text{a.s.}}{=} & \exp\brb{\theta sB_t/t} \E\brb{\left.\exp\brb{\theta \brb{B_s - sB_t/t}}\right|B_t} \quad \text{by Proposition \ref{pro:conditional_expectation_tower_independence}.(ii)}\\
& \stackrel{\text{a.s.}}{=} & \exp\brb{\theta sB_t/t} \E\brb{\exp\brb{\theta \brb{B_s - sB_t/t}}} \quad \text{by Lemma \ref{lem:brownian_bridge_independent_of_brownian_motion} and Proposition \ref{pro:conditional_expectation_tower_independence}.(v)}\\
& = & \exp\brb{\theta sB_t/t} \E\brb{\exp\brb{\theta \brb{\frac{t-s}{t}B_s - \frac st B_{t-s}}}} \\
& = & \exp\brb{\theta sB_t/t} \E\brb{\exp\brb{\frac{t-s}{t}\theta B_s}}\E\brb{\exp\brb{-\frac st \theta B_{t-s}}}\quad \text{by Proposition \ref{pro:independent_mgf}}\\
& = & \exp\brb{\theta sB_t/t} \exp\brb{\frac{\theta^2 (t-s)^2 s}{2t^2}} \exp\brb{\frac {\theta^2 s^2(t-s) }{2t^2}} = \exp\brb{\frac{\theta sB_t}t} \exp\brb{\frac{\theta^2 s(t-s)}{2t}}.
\eeast

Thus, we have the required result by Theorem \ref{thm:mgf_uniquely_determine_law} as $\E\brb{\exp\brb{\theta B_s|_{B_t}}}$ and $\exp\brb{\theta sB_t/t} \exp\brb{\frac{\theta^2 s(t-s)}{2t}}$ are non-random.
\end{proof}

\begin{proof}[\bf Alternative proof]
By Example \ref{exa:bivariate_gaussian}, we have $\rho = \sqrt{s/t}$ for $s\neq 0$,
\beast
f_{B_s|B_t}(x|B_t) & = & \frac{1}{\sqrt{2 \pi} \sqrt{s} \sqrt{1-s/t}} \exp\left( -\frac{1}{2(1-s/t)} \brb{\frac{x}{\sqrt{s}} - \frac{\sqrt{s/t} B_t}{\sqrt{t}}}^2 \right)\\
& = & \frac{1}{\sqrt{2 \pi} \sqrt{s(t-s)/t}} \exp\left( -\frac{1}{2s(t-s)/t} \brb{x - \frac{s B_t}{t}}^2 \right).
\eeast
\end{proof}

A more general proposition is given by

\begin{proposition}[conditioned Brownian motion]\label{pro:conditioned_brownian_motion_general}
let $B$ be a standard Brownian motion and $0\leq s<t<u$. Then for given $B_s$ and $B_u$,
\be
B_t|_{B_s,B_u} \sim \sN\brb{B_s + \frac{t-s}{u-s} (B_u - B_s), \frac{(t-s)(u-t)}{u-s}}.
\ee
\end{proposition}

\begin{proof}[\bf Proof]
Let $\wt{B}_v := B_{s+v} - B_s$ for $v\geq 0$. Then by simple Markov property (Theorem \ref{thm:simple_markov_property_brownian_motion}), we have $\wt{B}_v$ is still a standard Brownian motion. Thus,
\be
B_t|_{B_s,B_u} = \wt{B}_v + B_s|_{\wt{B}_w+ B_s, B_s} = \underbrace{\wt{B}_v|_{\wt{B}_w, B_s} + B_s \sim \wt{B}_v|_{\wt{B}_w} + B_s}_{\text{by Proposition \ref{pro:conditional_expectation_tower_independence}.(ii)}}.
\ee

Then, by Proposition \ref{pro:conditioned_brownian_motion} ($t = s+v$, $u = s+w$),
\be
\wt{B}_v|_{\wt{B}_w} \sim \sN\brb{\frac{v\wt{B}_w}w, \frac{v(w-v)}{w}} = \sN\brb{\frac{(t-s)(B_u - B_s)}{u-s}, \frac{(t-s)(u-t)}{u-s}}.
\ee

Therefore, the conditional distribution $B_t|B_s,B_u$ is as required.
\end{proof}

\begin{example}
Given $B_s = a$, $B_u = b$, the conditional distribution of $B_t|_{B_s =a,B_u=b}$ is
\be
\sN\brb{a + \frac{t-s}{u-s} (b - a), \frac{(t-s)(u-t)}{u-s}}.
\ee

Note that the variance is maximized at time $t = (s+u)/2$ with value $(u-s)/4$.
\end{example}

Also, we check the conditional joint density function,% for $0\leq s<t<u$,

\begin{lemma}\label{lem:joint_conditional_brownian_motion}
let $B$ be a standard Brownian motion and $0\leq s<t<u$. Then for given $B_u$, $B_s|B_u$ and $B_t|B_u$ are multivariate Gaussian with covariance $\frac{s(u-t)}{u}$.
%\be
%f_{B_s,B_t|B_u}(x,y|B_u) = .%\sN\brb{B_s + \frac{t-s}{u-s} (B_u - B_s), \frac{(t-s)(u-t)}{u-s}}.
%\ee
\end{lemma}

\begin{proof}[\bf Proof]
We have $w = (x,y,z)^T$ and by multivariate Gaussian distribution,
\be
f_{B_s,B_t,B_u}(x,y,z) = \frac 1{\sqrt{(2\pi)^d\abs{\Sigma}}}\exp\brb{-\frac 12 w^T \Sigma^{-1}w}
\ee
where $d = 3$ and
\be
\Sigma = \bepm s & s & s \\ s & t & t \\ s & t & u \eepm,\qquad \abs{\Sigma} = s(t-s)(u-t),\qquad \Sigma^{-1} = \bepm
\frac{t}{s(t-s)} & - \frac 1{t-s} & 0 \\ -\frac{1}{t-s} &  \frac {u-s}{(t-s)(u-t)} & -\frac 1{u-t} \\ 0 & -\frac{1}{u-t} & \frac 1{u-t}
\eepm
\ee

Therefore, we can have $B_s|B_u$ and $B_t|B_u$ are joint Gaussian distribution (for $f_{B_u}(B_u)\neq 0$)
\beast
f_{B_s,B_t|B_u}(x,y|B_u) & = & f_{B_s,B_t,B_u}(x,y,B_u)/f_{B_u}(B_u) \\
& = & \frac{1}{2 \pi \sigma_x \sigma_y \sqrt{1-\rho^2}} \exp\left( -\frac{1}{2(1-\rho^2)}\left[ \frac{(x-\mu_x)^2}{\sigma_x^2} + \frac{(y-\mu_y)^2}{\sigma_y^2} - \frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x \sigma_y} \right] \right)
\eeast
where
\be
\mu_x = sB_u/u,\quad \mu_y = tB_u/u,\quad\sigma_x = \sqrt{\frac{s(u-s)}{u}},\quad \sigma_y = \sqrt{\frac{t(u-t)}{u}},\quad \rho = \sqrt{\frac{s(u-t)}{t(u-s)}}.
\ee

Then %by Proposition \ref{pro:conditional_probability_density_function} and Example \ref{exa:bivariate_gaussian}, we have %\stackrel{\text{a.s.}}{=}
\be
\cov\brb{B_s|_{B_u},B_t|_{B_u}} = \rho \sigma_x \sigma_y = \sqrt{\frac{s(u-t)}{t(u-s)}}\sqrt{\frac{s(u-s)}{u}} \sqrt{\frac{t(u-t)}{u}} = \frac{s(u-t)}{u}
\ee
as required.
\end{proof}

\subsection{Brownian bridge}

%Now we define Brownian bridge:

\begin{definition}[Brownian bridge\index{Brownian bridge}]\label{def:brownian_bridge}
A Brownian bridge $X$ is a Gaussian process % (see Definition \ref{def:gaussian_process} with $M = 0$ a.s.)
defined on $[0,T]$ and with covariance $\cov(X_s,X_t) = s(T-t)/T$ for $s\leq t\leq T$.
\end{definition}

\begin{remark}
The easiest way to find a Brownian bridge is use $X_t = B_t - tB_T/T$ where $B$ is a standard Brownian motion. We can easily calculate that for $s\leq t\leq T$,
\beast
\cov\brb{X_s,X_t} & = & \cov\brb{B_s-sB_T/T, B_t-tB_T/T} \\
& = & \E\brb{B_s-sB_T/T, B_t-tB_T/T} - \E\brb{B_s-sB_T/T}\E\brb{B_t-tB_T/T}\\
& = & s\land t - \frac tT s\land T - \frac sT t\land T + \frac{st}{T^2} T = s - ts/T = s(T - t)/T.
\eeast

Thus, $X_t$ is a continuous version of the Brownian bridge.

Note that $X_T = 0$ a.s. when Brownian bridge is between 0 and 0 and we can denote it as $X^0_t$. If the Brownian bridge is between 0 and $x$, we can use
\be
X^x_t = B_t - t(B_T - x)/T = X^0_t + tx/T.
\ee

However, these differences do not change the covariance, so $X^x_t$ is also a Brownian bridge.
\end{remark}

\begin{proposition}
let $B$ be a standard Brownian motion ($B_0 = 0$ a.s.) and $0\leq s<t$. Then $B_s|_{B_t}$ is a Brownian bridge on $[0,t]$.%\footnote{Note that $B_t$ is a random variable here}.
\end{proposition}

\begin{proof}[\bf Proof]
Direct result from Definition \ref{def:brownian_bridge} and Lemma \ref{lem:joint_conditional_brownian_motion}.%Proposition \ref{pro:conditioned_brownian_motion}.
\end{proof}


\section{Brownian Motion with Drift}

In this section, we consider the process $X_t = \mu t + \sigma B_t$ where $B$ is a standard Brownian motion in $\R$. It is easy to see that the transition density of $X_t$ is\footnote{pdf needed here}
\be
f_t(x) = \frac{1}{\sqrt{2\pi \sigma^2 t}}\exp\brb{-\frac{(x-\mu t)^2}{2\sigma^2 t}}.
\ee



\subsection{Maximum and minimum of Brownian motion with drift}

\begin{proposition}\label{pro:density_bm_drift_maximum_minimum}
Let $X_t = \mu t + \sigma B_t$ where $B$ be a standard Brownian motion in $\R$, $Y_t:= \sup_{0\leq s\leq t}X_s$ and $Z_t:= \inf_{0\leq s\leq t}X_s$. Then the transition densities of $X_t$, $Y_t$ and $Z_t$ (for $x,y,z$ respectively) are (see \cite{Karatzas_Shreve_1998}.$P_{368}$)
\be
f_t(y) = 2\exp\brb{\frac{2\mu y}{\sigma^2}} \brb{\frac{1}{\sqrt{2\pi\sigma^2 t}}\exp\brb{-\frac{(y+\mu t)^2}{2\sigma^2t}} - \frac {\mu }{\sigma^2 }\brb{1 - N\brb{\frac{y+\mu t}{\sigma\sqrt{t}}}}},\quad y\geq 0
\ee%\be
%f_t(y) =  \sqrt{\frac{2}{\pi\sigma^2 t}} \exp\brb{-\frac {y^2}{2\sigma^2 t}} ,\quad f_t(z) =  \sqrt{\frac{2}{\pi\sigma^2 t}} \exp\brb{-\frac {z^2}{2\sigma^2 t}} \quad \text{where }y>0,z<0.
%\ee

\be
f_t(z) = 2\exp\brb{\frac{2\mu z}{\sigma^2}} \brb{\frac{1}{\sqrt{2\pi\sigma^2 t}}\exp\brb{-\frac{(z+\mu t)^2}{2\sigma^2t}} + \frac {\mu }{\sigma^2 }N\brb{\frac{z+\mu t}{\sigma\sqrt{t}}}},\quad z\leq 0
\ee
where
\be
N(x) = \frac 1{\sqrt{2\pi}}\int^x_{-\infty}\exp\brb{-\frac {s^2}2}ds.
\ee

Also, for $a\geq 0$,
\be
\pro\brb{Y_t \leq a} = N\brb{\frac{a-\mu t}{\sigma\sqrt{t} }} - \exp\brb{\frac{2\mu a}{\sigma^2}}N\brb{-\frac{a+\mu t}{\sigma\sqrt{t} }},
\ee

\be
\pro\brb{Z_t \geq -a} = N\brb{\frac{a+\mu t}{\sigma\sqrt{t} }} - \exp\brb{-\frac{2\mu a}{\sigma^2}}N\brb{-\frac{a-\mu t}{\sigma\sqrt{t} }}.
\ee

This implies that
\be
\pro\brb{\max_{t\geq 0}X_t \leq a} = \left\{\ba{ll}
\exp\brb{\frac{2\mu a}{\sigma^2}}\quad\quad  & \mu < 0 \\
0 & \mu \geq 0
\ea\right.
\ee
\end{proposition}

\begin{remark}
If $\mu = 0$, we have
\be
f_t(y) = \sqrt{\frac{2}{\pi\sigma^2 t}}\exp\brb{-\frac{y^2}{2\sigma^2t}},\qquad f_t(z) = \sqrt{ \frac{2}{\pi\sigma^2 t}}\exp\brb{-\frac{z^2}{2\sigma^2t}}
\ee
which is consistent with Proposition \ref{pro:density_bm_maximum_minimum}.

The method of images\footnote{method needed} is not working for the case $\mu \neq 0$.
\end{remark}

\begin{proof}[\bf Proof]%First, we have that $Y_t \sim \sigma S_t + \mu t$ where $S_t = \sup_{0\leq s\leq t}B_s$. Thus, By Corollary \ref{cor:abs_sup_equal_brownian_motion}, %we have that $\abs{\frac{X_t-\mu t}{\sigma}} \sim S_t$  Then %\be
%\pro\brb{Y_t \leq y} = \pro\brb{\sigma S_t + \mu t \leq y} = \pro\brb{S_t \leq \frac{y-\mu t}{\sigma}} = \frac{2}{\sqrt{2\pi}}\int^{\frac{y-\mu t}{\sigma}}_0 \exp\brb{-\frac {x^2}2} dx
%\ee
By Proposition \ref{pro:pairwise_joint_density_bm_maximum_minimum_current} and Corollary \ref{cor:girsanov_drift_brownian_motion}, %Girsanov's theorem\footnote{theorem needed, or Cameron-Martin theorem},
we have
\beast
f_t(x,y) & = & \exp\brb{\frac{\mu}{\sigma} \frac x{\sigma} - \frac 12 \brb{\frac{\mu}{\sigma}}^2 t }\frac{2(2y-x)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{1}{2\sigma^2t}(2y-x)^2} = \frac{2(2y-x)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{(2y-x)^2-2\mu t x + \mu^2 t^2}{2\sigma^2t}} \\
& = & \frac{2(2y-x)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{\frac{2\mu y}{\sigma^2}}\exp\brb{-\frac{(2y-x+\mu t)^2}{2\sigma^2t}}.
\eeast

Thus, we have that the transition density of $X$ is
\beast
f_t(x) & = & \int^\infty_{x}  \frac{2(2y-x)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{(2y-x)^2-2\mu t x + \mu^2 t^2}{2\sigma^2t}} dy = \exp\brb{\frac{2\mu t x - \mu^2 t^2}{2\sigma^2t}} \int^\infty_{2x}  \frac{(2y-x)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{(2y-x)^2}{2\sigma^2t}} d2y \\
& = & \exp\brb{\frac{2\mu t x - \mu^2 t^2}{2\sigma^2t}} \int^\infty_{x}  \frac{y}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{y^2}{2\sigma^2t}} dy = \frac{1}{\sigma\sqrt{2\pi t}} \exp\brb{\frac{2\mu t x - \mu^2 t^2}{2\sigma^2t}} \left.\exp\brb{-\frac{y^2}{2\sigma^2t}}\right|^x_\infty \\
& = &  \frac{1}{\sigma\sqrt{2\pi t}} \exp\brb{\frac{2\mu t x - \mu^2 t^2}{2\sigma^2t}} \exp\brb{-\frac{x^2}{2\sigma^2t}} = \frac{1}{\sqrt{2\pi \sigma^2t}} \exp\brb{-\frac{(x-\mu t )^2}{2\sigma^2t}}
\eeast
which is consistent with the assumption. Furthermore, we have
\beast
f_t(y) & = & \int^{y}_{-\infty}  \frac{2(2y-x)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{\frac{2\mu y}{\sigma^2}}\exp\brb{-\frac{(2y-x+\mu t)^2}{2\sigma^2t}} dx = 2\exp\brb{\frac{2\mu y}{\sigma^2}} \int_{y}^{\infty}  \frac{x}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{(x+\mu t)^2}{2\sigma^2t}} dx \\
& = & 2\exp\brb{\frac{2\mu y}{\sigma^2}} \brb{\int_{y}^{\infty}  \frac{x+\mu t}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{(x+\mu t)^2}{2\sigma^2t}} dx - \int_{y}^{\infty}  \frac{\mu t}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{(x+\mu t)^2}{2\sigma^2t}} dx}\\
& = & 2\exp\brb{\frac{2\mu y}{\sigma^2}} \brb{\left.\frac{1}{\sqrt{2\pi\sigma^2 t}}\exp\brb{-\frac{(x+\mu t)^2}{2\sigma^2t}} \right|^{y}_{\infty} - \frac {\mu t}{\sigma^2 t}\int_{\frac{y+\mu t}{\sigma\sqrt{t}}}^{\infty}  \frac{1}{\sqrt{2\pi }}\exp\brb{-\frac{x^2}{2}} dx} \\
& = & 2\exp\brb{\frac{2\mu y}{\sigma^2}} \brb{\frac{1}{\sqrt{2\pi\sigma^2 t}}\exp\brb{-\frac{(y+\mu t)^2}{2\sigma^2t}} - \frac {\mu }{\sigma^2 }\brb{1 - N\brb{\frac{y+\mu t}{\sigma\sqrt{t}}}}}.
\eeast

Then we can get the result for $Z_t$ by letting $\mu = -\mu$, $z = -y$.

For $a\geq 0$,
\beast
\pro(Y_t \leq a) & = & \int^a_0 2\exp\brb{\frac{2\mu y}{\sigma^2}} \brb{\frac{1}{\sqrt{2\pi\sigma^2 t}}\exp\brb{-\frac{(y+\mu t)^2}{2\sigma^2t}} - \frac {\mu }{\sigma^2 }\brb{1 - N\brb{\frac{y+\mu t}{\sigma\sqrt{t}}}}} dy \\
& = & 2\int^a_0 \frac{1}{\sqrt{2\pi\sigma^2 t}}\exp\brb{-\frac{(y-\mu t)^2}{2\sigma^2t}} dy  - \frac {2\mu }{\sigma^2 }\int^a_0 \exp\brb{\frac{2\mu y}{\sigma^2}} \frac 1{\sqrt{2\pi}}\int^\infty_{\frac {y+\mu t}{\sigma\sqrt{t}}} \exp\brb{-\frac {x^2}2} dx dy\\
& = & 2\int^a_0 \frac{1}{\sqrt{2\pi\sigma^2 t}}\exp\brb{-\frac{(y-\mu t)^2}{2\sigma^2t}} dy\\
 & & \qquad - \frac {2\mu }{\sigma^2 }\frac 1{\sqrt{2\pi}}\brb{\int^\infty_{\frac{a+\mu t}{\sigma\sqrt{t}}} \exp\brb{-\frac{x^2}2}\int^a_0 \exp\brb{\frac{2\mu y}{\sigma^2}} dy dx + \int^\frac{a+\mu t}{\sigma\sqrt{t}}_{\frac{\mu t}{\sigma\sqrt{t}}} \exp\brb{-\frac{x^2}2}\int^{x\sigma\sqrt{t}-\mu t}_0 \exp\brb{\frac{2\mu y}{\sigma^2}} dy dx}
\eeast

The second term is
\beast
& & \frac 1{\sqrt{2\pi}}\brb{\int^\infty_{\frac{a+\mu t}{\sigma\sqrt{t}}} \left.\exp\brb{-\frac{x^2}2}\exp\brb{\frac{2\mu y}{\sigma^2}} \right|^a_0 dx + \int^\frac{a+\mu t}{\sigma\sqrt{t}}_{\frac{\mu t}{\sigma\sqrt{t}}} \exp\brb{-\frac{x^2}2}\left. \exp\brb{\frac{2\mu y}{\sigma^2}} \right|^{x\sigma\sqrt{t}-\mu t}_0 dx} \\
& = & \brb{\exp\brb{\frac{2\mu a}{\sigma^2}} -1}\brb{1-N\brb{\frac{a+\mu t}{\sigma\sqrt{t}}}} + \frac 1{\sqrt{2\pi}}\brb{\int^{\frac{a+\mu t}{\sigma\sqrt{t}}}_{\frac{\mu t}{\sigma\sqrt{t}}} \exp \brb{-\frac{\brb{\sigma x \sqrt{t}- 2\mu t}^2}{2\sigma^2t}}- \exp\brb{-\frac{x^2}2} dx} \\
& = & \brb{\exp\brb{\frac{2\mu a}{\sigma^2}} -1}\brb{1-N\brb{\frac{a+\mu t}{\sigma\sqrt{t}}}} + \brb{N\brb{\frac{a-\mu t}{\sigma\sqrt{t}}}-N\brb{\frac{-\mu t}{\sigma\sqrt{t}}} - \brb{N\brb{\frac{a+\mu t}{\sigma\sqrt{t}}}-N\brb{\frac{\mu t}{\sigma\sqrt{t}}}} }
\eeast

Thus,
\be
\pro\brb{Y_t \leq a} = N\brb{\frac{a-\mu t}{\sigma\sqrt{t} }} - \exp\brb{\frac{2\mu a}{\sigma^2}}N\brb{-\frac{a+\mu t}{\sigma\sqrt{t} }}.
\ee

Moreover, we have
\beast
f(x,Y_t\leq a) & = & \int^a_{x}  \frac{2(2y-x)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{(2y-x)^2-2\mu t x + \mu^2 t^2}{2\sigma^2t}} dy \\
& = & \exp\brb{\frac{2\mu t x - \mu^2 t^2}{2\sigma^2t}} \int^{2a-x}_{x}  \frac{y}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{y^2}{2\sigma^2t}} dy = \frac{1}{\sigma\sqrt{2\pi t}} \exp\brb{\frac{2\mu t x - \mu^2 t^2}{2\sigma^2t}} \left.\exp\brb{-\frac{y^2}{2\sigma^2t}}\right|^x_{2a-x} \\
& = &  \frac{1}{\sqrt{2\pi \sigma^2t}} \exp\brb{-\frac{(x-\mu t )^2}{2\sigma^2t}} \brb{1- \exp\brb{\frac{2a(x-a)}{\sigma^2t}}}.
\eeast

For $Z_t$ we can take change the sign of $\mu$ and substitute it into the formula of $Y_t$.

Then by letting $t\to \infty$, we have the required result for $\pro\brb{\max_{t\geq 0}X_t \leq a}$. For the case $\mu =0$, we can apply Proposition \ref{pro:brownian_motion_limit_value}.
\end{proof}

\begin{theorem}\label{thm:joint_density_maximum_minimum_bm_with_drift}
Let $X_t = \mu t + \sigma B_t$ where $B$ be a standard Brownian motion in $\R$, $Y_t:= \sup_{0\leq s\leq t}X_s$ and $Z_t:= \inf_{0\leq s\leq t}X_s$. Then the joint transition densities of $X_t$, $Y_t$ and $Z_t$ (for $x,y,z$ respectively) is
\be
f_t(x,y,z) = 4 \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty}  k^2 \phi\brb{x-2k(y-z)} - k(k-1)\phi\brb{x-2y+2k(y-z)}.
\ee
where
\be
\phi(x) = \frac{x^2 - \sigma^2 t}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{x^2}{2\sigma^2 t}}.
\ee

Also,
\be
\pro\brb{Y_t \leq y,Z_t \geq z} = \exp\brb{ - \frac{\mu^2t}{2\sigma^2}} \sum_{k=1}^\infty \frac{2k\pi\sigma^4 \brb{\exp\brb{\frac{\mu y}{\sigma^2}} - (-1)^k \exp\brb{\frac{\mu z}{\sigma^2}}} }{k^2\pi^2\sigma^4  + \mu^2(y-z)^2}\exp \brb{- \frac{k^2\pi^2 \sigma^2 t}{2(y-z)^2}}\sin\frac{k\pi y}{y-z}.
\ee
\end{theorem}

\begin{remark}
If $\mu = 0$, then
\be
\pro\brb{Y_t \leq y,Z_t \geq z} = \sum_{k=1}^\infty \frac{2\brb{1 - (-1)^k }}{k\pi }\exp \brb{- \frac{k^2\pi^2 \sigma^2 t}{2(y-z)^2}}\sin\frac{k\pi y}{y-z}.
\ee
\end{remark}

\begin{proof}[\bf Proof]
By method of images (as in the proof of Theorem \ref{thm:bm_range_transition_density}), we have that for $\mu =0$ and $y>0,z<0$,
\be
f_{X_t}(x,Y_t \leq y,Z_t \geq z) = \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty} \brb{\exp\brb{-\frac{(x + 2kz - 2ky)^2}{2\sigma^2 t}} - \exp\brb{-\frac{(x -2kz + 2(k-1)y)^2}{2\sigma^2 t}}}.
\ee

Then by Girsanov, Cameron-Martin theorem (Corollary \ref{cor:girsanov_drift_brownian_motion}), we have for general $\mu$,
\beast
& & f_{X_t}(x,Y_t \leq y,Z_t \geq z) \\
& = & \frac 1{\sqrt{2\pi \sigma^2 t}} \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty} \brb{\exp\brb{-\frac{(x-2k(y-z))^2}{2\sigma^2 t}} - \exp\brb{-\frac{(x-2y+2k(y-z))^2}{2\sigma^2 t}}}.
\eeast

Then
\beast
& & f_{X_t,Z_t}(x,Y_t \leq y, z) \\
& = & - \frac 1{\sqrt{2\pi \sigma^2 t}} \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \times \\
& &  \sum^\infty_{k=-\infty} \frac{-2k(x-2k(y-z))}{\sigma^2 t}\exp\brb{-\frac{(x-2k(y-z))^2}{2\sigma^2 t}} - \frac{2k(x-2y+2k(y-z))}{\sigma^2 t} \exp\brb{-\frac{(x-2y+2k(y-z))^2}{2\sigma^2 t}} \\
& = & \frac 1{\sqrt{2\pi \sigma^2 t}} \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \times \\
& &  \sum^\infty_{k=-\infty} \frac{2k(x-2k(y-z))}{\sigma^2 t}\exp\brb{-\frac{(x-2k(y-z))^2}{2\sigma^2 t}} + \frac{2k(x-2y+2k(y-z))}{\sigma^2 t} \exp\brb{-\frac{(x-2y+2k(y-z))^2}{2\sigma^2 t}}
\eeast

Thus,
\beast
& & f_{X_t,Y_t,Z_t}(x,y, z) \\
& = & \frac 1{\sqrt{2\pi \sigma^2 t}} \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty} \brb{\frac{4k^2(x-2k(y-z))^2}{\sigma^4 t^2} - \frac{4k^2}{\sigma^2 t}}\exp\brb{-\frac{(x-2k(y-z))^2}{2\sigma^2 t}} \\
& & + \frac 1{\sqrt{2\pi \sigma^2 t}} \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty} \brb{ \frac{4k(k-1)}{\sigma^2 t} - \frac{4k(k-1)(x-2y+2k(y-z))^2}{\sigma^4 t^2} }\exp\brb{-\frac{(x-2y+2k(y-z))^2}{2\sigma^2 t}} \\
& = & 4 \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty}  k^2 \phi\brb{x-2k(y-z)} - k(k-1)\phi\brb{(x-2y+2k(y-z)}.
\eeast
where $\phi(x) = \frac{x^2 - \sigma^2 t}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{x^2}{2\sigma^2 t}}$.

Integrating $f_{X_t}(x,Y_t \leq y,Z_t \geq z)$, we have
\beast
& & \pro\brb{Y_t \leq y,Z_t \geq z} \\
& = & \int^y_z \frac 1{\sqrt{2\pi \sigma^2 t}} \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty} \brb{\exp\brb{-\frac{(x-2k(y-z))^2}{2\sigma^2 t}} - \exp\brb{-\frac{(x-2y+2k(y-z))^2}{2\sigma^2 t}}} dx. %\\& = & \frac 1{\sqrt{2\pi \sigma^2 t}} \sum^\infty_{k=-\infty}  \int^y_z  \exp\brb{- \frac{2k(y-z)\mu t }{\sigma^2}} \exp\brb{-\frac{(x-2k(y-z))^2}{2\sigma^2 t}} - \exp\brb{- \frac{(2y - 2k(y-z))\mu t}{\sigma^2}}\exp\brb{-\frac{(x-2y+2k(y-z))^2}{2\sigma^2 t}} dx \\
\eeast%by Fubini theorem (Theorem \ref{thm:fubini}).

Thus, for the first term, we have
\be
I = \sum^\infty_{k=-\infty} \exp\brb{-\frac{(x-2k(y-z))^2}{2\sigma^2 t}} = \exp\brb{-\frac{x^2}{2\sigma^2 t}} \sum^\infty_{k=-\infty} \exp\brb{-\frac{2k^2(y-z)^2}{\sigma^2 t}}\exp\brb{\frac{2kx(y-z))}{\sigma^2 t}}
\ee
So let $\pi i \tau = -\frac{2(y-z)^2}{\sigma^2 t}$ and $2\pi i z = \frac{2x(y-z))}{\sigma^2 t} $, then
\be
\tau = \frac{2i(y-z)^2}{\pi \sigma^2 t},\quad z = \frac{-ix(y-z))}{\pi\sigma^2 t}\ \ra \ \frac{z}{\tau} = - \frac{x}{2(y-z)},\quad \frac{-1}{\tau} = \frac{i\pi \sigma^2 t}{2(y-z)^2}
\ee

Then by Theorem \ref{thm:jacobi_theta_function_transform},
\beast
I &= & \exp\brb{-\frac{x^2}{2\sigma^2 t}}  (-i \tau)^{-1/2} \exp\brb{-\frac{\pi}{\tau} i z^2 }  \sum_{k=-\infty}^\infty \exp \brb{-\pi i k^2 \frac{1}{\tau} + 2 \pi i k  \frac{z}{\tau}} \\
& = & \exp\brb{-\frac{x^2}{2\sigma^2 t}} \sqrt{\frac{\pi\sigma^2 t}{2(y-z)^2}} \exp\brb{\frac{x^2}{2\sigma^2 t}}  \sum_{k=-\infty}^\infty \exp \brb{- \frac{k^2\pi^2 \sigma^2 t}{2(y-z)^2} - \frac{ \pi i k x}{y-z}}\\
& = & \sqrt{\frac{\pi\sigma^2 t}{2(y-z)^2}} \sum_{k=-\infty}^\infty \exp \brb{- \frac{k^2\pi^2 \sigma^2 t}{2(y-z)^2}} \cos \brb{\frac{\pi k x}{y-z}}
\eeast

Since we can have the similar result for the second term, we have
\beast
\pro\brb{Y_t \leq y,Z_t \geq z} = \exp\brb{ - \frac{\mu^2t}{2\sigma^2}} \int^y_z \frac{1}{y-z} \sum_{k=1}^\infty \exp \brb{- \frac{k^2\pi^2 \sigma^2 t}{2(y-z)^2}} \exp\brb{\frac{\mu x}{\sigma^2} }\brb{\cos \brb{\frac{\pi k x}{y-z}} - \cos \brb{\frac{\pi k (x-2y)}{y-z}}}dx
\eeast

Then by Proposition \ref{pro:integral_exponential_cosine}, we have that%and Fubini theorem (Theorem \ref{thm:fubini})
\beast
\int^y_z \exp\brb{\frac {\mu x}{\sigma^2}}\cos \brb{\frac{\pi k x}{y-z}} dx= \brb{\left. \frac {1}{\brb{\frac {\mu }{\sigma^2}}^2 + \brb{\frac{\pi k}{y-z}}^2}\brb{\frac{\pi k}{y-z} \exp\brb{\frac {\mu x}{\sigma^2}}\sin\brb{\frac{\pi k x}{y-z}} + \frac {\mu }{\sigma^2} \exp\brb{\frac {\mu x}{\sigma^2}} \cos\brb{\frac{\pi k x}{y-z}}}\right|^y_z},
\eeast

Also,
\beast
& & \int^y_z \exp\brb{\frac {\mu x}{\sigma^2}}\cos \brb{\frac{\pi k (x-2y)}{y-z}} dx = \exp\brb{\frac {2\mu y}{\sigma^2}} \int^{-y}_{z-2y} \exp\brb{\frac {\mu x}{\sigma^2}}\cos \brb{\frac{\pi k x}{y-z}} dx \\
& = & \exp\brb{\frac {2\mu y}{\sigma^2}}  \brb{\left. \frac {1}{\brb{\frac {\mu }{\sigma^2}}^2 + \brb{\frac{\pi k}{y-z}}^2}\brb{\frac{\pi k}{y-z} \exp\brb{\frac {\mu x}{\sigma^2}}\sin\brb{\frac{\pi k x}{y-z}} + \frac {\mu }{\sigma^2} \exp\brb{\frac {\mu x}{\sigma^2}} \cos\brb{\frac{\pi k x}{y-z}}}\right|^{-y}_{z-2y}}
\eeast

Thus,
\beast
& &\int^y_z \exp\brb{\frac {\mu x}{\sigma^2}}\brb{\cos \brb{\frac{\pi k x}{y-z}} -\cos \brb{\frac{\pi k (x-2y)}{y-z}}} dx \\
& = & \frac {\frac{2\pi k}{y-z}}{\brb{\frac {\mu }{\sigma^2}}^2 + \brb{\frac{\pi k}{y-z}}^2}\brb{\exp\brb{\frac {\mu y}{\sigma^2}}\sin\brb{\frac{\pi k y}{y-z}} - \exp\brb{\frac {\mu z}{\sigma^2}}\sin\brb{\frac{\pi k z}{y-z}}}\\
& = & \frac {2\pi k(y-z)\sigma^4}{\mu^2(y-z)^2  + k^2 \pi^2 \sigma^4}\brb{\exp\brb{\frac {\mu y}{\sigma^2}}\sin\brb{\frac{\pi k y}{y-z}} - \exp\brb{\frac {\mu z}{\sigma^2}}\sin\brb{\frac{\pi k z}{y-z}}}.
\eeast

Then
\beast
& & \pro\brb{Y_t \leq y,Z_t \geq z} \\
& = & \exp\brb{ - \frac{\mu^2t}{2\sigma^2}} \sum_{k=1}^\infty  \frac {2\pi k\sigma^4}{\mu^2(y-z)^2  + k^2 \pi^2 \sigma^4} \exp \brb{- \frac{k^2\pi^2 \sigma^2 t}{2(y-z)^2}} \brb{\exp\brb{\frac {\mu y}{\sigma^2}}\sin\brb{\frac{\pi k y}{y-z}} - \exp\brb{\frac {\mu z}{\sigma^2}}\sin\brb{\frac{\pi k z}{y-z}}} \\
& = & \exp\brb{ - \frac{\mu^2t}{2\sigma^2}} \sum_{k=1}^\infty  \frac {2\pi k\sigma^4\brb{\exp\brb{\frac {\mu y}{\sigma^2}} - (-1)^k \exp\brb{\frac {\mu z}{\sigma^2}}}}{\mu^2(y-z)^2  + k^2 \pi^2 \sigma^4} \exp \brb{- \frac{k^2\pi^2 \sigma^2 t}{2(y-z)^2}} \sin\brb{\frac{\pi k y}{y-z}}
\eeast
as required.
\end{proof}
%
%\begin{theorem}[range of Brownian motion with drift]
%\end{theorem}
%
%\begin{proof}[\bf Proof]
%From Theorem \ref{thm:joint_density_maximum_minimum_bm_with_drift}, we know that
%\be
%f_{X_t,Y_t,Z_t}(x,y,z) = 4 \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty}  k^2 \phi\brb{x-2k(y-z)} - k(k-1)\phi\brb{(x-2y+2k(y-z)}.
%\ee
%where $\phi(x) = \frac{x^2 - \sigma^2 t}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{-x^2}{2\sigma^2 t}}$. Thus, we calculate
%\beast
%& & \exp\brb{ - \frac{\mu^2t}{2\sigma^2}} \int^y_z \exp\brb{\frac{\mu x}{\sigma^2}}\phi\brb{x-a}dx \\
%& = & \frac 1{\sigma^5\sqrt{2\pi t^5}}\exp\brb{\frac{\mu a}{\sigma^2}}  \int^y_z \brb{(x-a)^2 - \sigma^2 t} \exp\brb{-\frac{(x-a-\mu t)^2}{2\sigma^2 t}}dx \\
%& = & \frac 1{\sigma^5\sqrt{2\pi t^5}}\exp\brb{\frac{\mu a}{\sigma^2}}  \int^y_z \brb{(x-a-\mu t)^2 + 2\mu t(x-a-\mu t) + \mu^2t^2 - \sigma^2 t} \exp\brb{-\frac{(x-a-\mu t)^2}{2\sigma^2 t}}dx \\
%& = & \frac 1{\sigma^5\sqrt{2\pi t^5}}\exp\brb{\frac{\mu a}{\sigma^2}}  \brb{\left.\sigma^2t (x-a+\mu t)\exp\brb{-\frac{(x-a-\mu t)^2}{2\sigma^2 t}}\right|^z_y + \mu^2t^2 \int^y_z\exp\brb{-\frac{(x-a-\mu t)^2}{2\sigma^2 t}}dx}
%\eeast
%
%Then we can have
%\be
%\exp\brb{ - \frac{\mu^2t}{2\sigma^2}} \int^y_z \exp\brb{\frac{\mu x}{\sigma^2}}\phi\brb{x-a}dx = \frac 1{\sigma^4t^2} \brb{\varphi\brb{z-a} \exp\brb{\frac{\mu z}{\sigma^2}} - \varphi\brb{y-a}\exp\brb{\frac{\mu y}{\sigma^2}} }
%\ee
%where
%\beast
%\varphi(x) & = & \exp\brb{-\frac{\mu x}{\sigma^2}}\brb{\sqrt{\frac{\sigma^2t}{2\pi}} (x+\mu t)\exp\brb{-\frac{(x-\mu t)^2}{2\sigma^2 t}} - \mu^2t^2 N\brb{\frac{x-\mu t}{\sigma\sqrt{t}}}} \\
%& = & \sqrt{\frac{\sigma^2t}{2\pi}} (x+\mu t)\exp\brb{-\frac{x^2+ \mu^2 t^2}{2\sigma^2 t}} - \mu^2t^2 \exp\brb{-\frac{\mu x}{\sigma^2}} N\brb{\frac{x-\mu t}{\sigma\sqrt{t}}}.
%\eeast
%
%Thus,
%\beast
%f_{Y_t,Z_t}(y,z) & = & \frac 4{\sigma^4t^2} \sum_{k=-\infty}^\infty k^2\brb{\varphi\brb{z-2k(y-z)} \exp\brb{\frac{\mu z}{\sigma^2}} - \varphi\brb{y-2k(y-z)}\exp\brb{\frac{\mu y}{\sigma^2}} } \\
%& & \qquad -\frac 4{\sigma^4t^2} \sum_{k=-\infty}^\infty k(k-1)\brb{\varphi\brb{z-2y+2k(y-z)} \exp\brb{\frac{\mu z}{\sigma^2}} - \varphi\brb{y-2y+2k(y-z)}\exp\brb{\frac{\mu y}{\sigma^2}} }
%\eeast
%
%Let $r = y-z$ and $z= y-r$. We have
%\beast
%f_{Y_t,Z_t}(y,r-y) & = & \frac 4{\sigma^4t^2} \sum_{k=-\infty}^\infty k^2\brb{\varphi\brb{y-(2k+1)r} \exp\brb{\frac{\mu (y-r)}{\sigma^2}} - \varphi\brb{y-2kr}\exp\brb{\frac{\mu y}{\sigma^2}} } \\
%& & \qquad -\frac 4{\sigma^4t^2} \sum_{k=-\infty}^\infty k(k-1)\brb{\varphi\brb{-y+(2k-1)r} \exp\brb{\frac{\mu (y-r)}{\sigma^2}} - \varphi\brb{-y+2kr}\exp\brb{\frac{\mu y}{\sigma^2}} }
%\eeast
%
%Next, let $\vp(x) = \vp_1(x) + \vp_2(x) + \vp_3(x)$ where
%\beast
%\vp_1(x) = \sqrt{\frac{\sigma^2t}{2\pi}} x \exp\brb{-\frac{x^2+ \mu^2 t^2}{2\sigma^2 t}},\  \vp_2(x) = \sqrt{\frac{\sigma^2t}{2\pi}} \mu t\exp\brb{-\frac{x^2+ \mu^2 t^2}{2\sigma^2 t}},\  \vp_3(x) =  - \mu^2t^2 \exp\brb{-\frac{\mu x}{\sigma^2}} N\brb{\frac{x-\mu t}{\sigma\sqrt{t}}}.
%\eeast
%
%It is easy to see that $\vp_1(x)$ is an odd function and $\vp_2(x)$ is an even one. Thus, the corresponding parts of $f_{Y_t,Z_t}(y,r-y)$ are
%\be
%g_1(y) := \frac 4{\sigma^4t^2} \sum_{k=-\infty}^\infty k(2k+1)\varphi_1\brb{y-(2k+1)r} \exp\brb{\frac{\mu (y-r)}{\sigma^2}} - k(2k-1)\varphi_1\brb{y-2kr}\exp\brb{\frac{\mu y}{\sigma^2}}
%\ee
%and
%\be
%g_2(y) := \frac 4{\sigma^4t^2} \sum_{k=-\infty}^\infty k \brb{\varphi_2\brb{y-2kr}\exp\brb{\frac{\mu y}{\sigma^2}} - \varphi_2\brb{y-(2k+1)r} \exp\brb{\frac{\mu (y-r)}{\sigma^2}}}.
%\ee
%
%Thus,
%\beast
%& & f_1(r) = \int^r_0 g_1(y)dy \\
%& = & \sum_{k=-\infty}^\infty \frac 4{\sqrt{2\pi \sigma^6 t^3}} \exp\brb{\frac{2kr \mu }{\sigma^2 }} k(2k+1) \int^r_0 \brb{y-(2k+1)r} \exp\brb{-\frac{\brb{y-(2k+1)r-\mu t}^2}{2\sigma^2 t}} dy\\
%& & \qquad - \sum_{k=-\infty}^\infty\frac 4{\sqrt{2\pi \sigma^6 t^3}} \exp\brb{\frac{2kr \mu }{\sigma^2 }} k(2k-1)\int^r_0 \brb{y-2kr}  \exp\brb{-\frac{\brb{y-2kr-\mu t}^2}{2\sigma^2 t}} dy \\
%& = & \sum_{k=-\infty}^\infty \frac {4\exp\brb{\frac{2kr \mu }{\sigma^2 }}}{\sqrt{2\pi \sigma^2 t}}   k(2k+1) \brb{\left.\exp\brb{-\frac{\brb{y-(2k+1)r-\mu t}^2}{2\sigma^2 t}}\right|^0_r  + \mu \sigma^2t^2 \int^r_0 \exp\brb{-\frac{\brb{y-(2k+1)r-\mu t}^2}{2\sigma^2 t}} dy}\\
%& &  - \sum_{k=-\infty}^\infty \frac {4\exp\brb{\frac{2kr \mu }{\sigma^2 }}}{\sqrt{2\pi \sigma^2 t}}  k(2k-1)\brb{\left. \exp\brb{-\frac{\brb{y-2kr-\mu t}^2}{2\sigma^2 t}} \right|^0_r + \mu \sigma^2t^2 \int^r_0  \exp\brb{-\frac{\brb{y-2kr-\mu t}^2}{2\sigma^2 t}} dy}
%\eeast
%
%\end{proof}


\subsection{Range of Brownian motion with drift}

\begin{lemma}\label{lem:bm_drift_density_conditioned_on_range}
Let $X_t = \mu t + \sigma B_t$ where $B$ be a standard Brownian motion in $\R$, $Y_t:= \sup_{0\leq s\leq t}X_s$ and $Z_t:= \inf_{0\leq s\leq t}X_s$. Set the range $R_t = Y_t - Z_t$. Then the transition densities
\be
f_t(x,R_t <r) = \frac 1{\sigma\sqrt{2\pi t}} \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty} \brb{2k+1 - \frac {2k}{\sigma^2 t}\brb{r - \abs{x}}\brb{\abs{x} + 2kr}} \exp\brb{-\frac{\brb{\abs{x} + 2kr}^2}{2\sigma^2 t}}.
\ee
\end{lemma}

\begin{proof}[\bf Proof]
By Theorem \ref{thm:joint_density_maximum_minimum_bm_with_drift}, we know that

\be
f_t(x,y,z) = 4 \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty}  k^2 \phi\brb{x-2k(y-z)} - k(k-1)\phi\brb{(x-2y+2k(y-z)}.
\ee

where $\phi(x) = \frac{x^2 - \sigma^2 t}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{x^2}{2\sigma^2 t}}$. If $x>0$, we have that\footnote{theorem needed.}

\be
f_t(x,r) = \int^r_x f_t(x,y,r-y)dy.
\ee

Ignoring $4 \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}}$, we calculate the integral as it is

\beast
& & \sum^\infty_{k=-\infty}\int^r_x k^2 \phi\brb{x-2k(y-z)} - k(k-1)\phi\brb{x-2y+2k(y-z)} dy \\
& = & \sum^\infty_{k=-\infty} \int^r_x k^2 \frac{\brb{x-2kr}^2 - \sigma^2 t}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{\brb{x-2kr}^2}{2\sigma^2 t}}  - k(k-1)\frac{\brb{x-2y+2kr}^2 - \sigma^2 t}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{\brb{x-2y+2kr}^2}{2\sigma^2 t}} dy \\
& = & \sum^\infty_{k=-\infty}   \frac{k^2\brb{x-2kr}^2(r-x)}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{\brb{x-2kr}^2}{2\sigma^2 t}} - \frac{k^2(r-x)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{\brb{x-2kr}^2}{2\sigma^2 t}} \\
& & \qquad \qquad - \left.  \frac{k(k-1)\brb{x-2y+2kr} }{2\sigma^3\sqrt{2\pi t^3}} \exp\brb{-\frac{\brb{x-2y+2kr}^2}{2\sigma^2 t}} \right|^r_x.
\eeast

Changing the sign of $k$, we have the summation is
\beast
& & \sum^\infty_{k=-\infty}   \frac{k^2\brb{x+2kr}^2(r-x)}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}} - \frac{k^2(r-x)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}} \\
& & \qquad \qquad + \frac{k(k-1)\brb{-x+2kr} }{2\sigma^3\sqrt{2\pi t^3}} \exp\brb{-\frac{\brb{-x+2kr}^2}{2\sigma^2 t}} - \frac{k(k-1)\brb{x+2(k-1)r} }{2\sigma^3\sqrt{2\pi t^3}} \exp\brb{-\frac{\brb{x+2(k-1)r}^2}{2\sigma^2 t}}\\
&= & \sum^\infty_{k=-\infty}   \frac{k^2\brb{x+2kr}^2(r-x)}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}} - \frac{k^2(r-x)+ k(k+1)(x+2kr)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}} \\
&= & \sum^\infty_{k=-\infty}   \frac{k^2\brb{x+2kr}^2(r-x)}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}} - \frac{k\brb{x + 2k^2r + 3kr}}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}}.
\eeast

If $x<0$, we have that
\be
f_t(x,r) = \int^{r+x}_0 f_t(x,y,r-y)dy.
\ee

Thus, we have
\beast
& & \sum^\infty_{k=-\infty}\int^r_x k^2 \phi\brb{x-2k(y-z)} - k(k-1)\phi\brb{x-2y+2k(y-z)} dy \\
& = & \sum^\infty_{k=-\infty}   \frac{k^2\brb{x+2kr}^2(r+x)}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}} - \frac{k^2(r+x)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}} \\
& & \qquad \qquad + \frac{k(k-1)\brb{x+2kr} }{2\sigma^3\sqrt{2\pi t^3}} \exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}} - \frac{k(k-1)\brb{-x+2(k-1)r} }{2\sigma^3\sqrt{2\pi t^3}} \exp\brb{-\frac{\brb{-x+2(k-1)r}^2}{2\sigma^2 t}}\\
&= & \sum^\infty_{k=-\infty}   \frac{k^2\brb{x+2kr}^2(r+x)}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}} - \frac{k^2(r+x)- k(k-1)(x+2kr)}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{\brb{x+2kr}^2}{2\sigma^2 t}} \\
&= & \sum^\infty_{k=-\infty}   \frac{k^2\brb{x-2kr}^2(r+x)}{\sigma^5\sqrt{2\pi t^5}}\exp\brb{-\frac{\brb{x-2kr}^2}{2\sigma^2 t}} - \frac{k\brb{-x  + 2k^2 r + 3kr}}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{\brb{x-2kr}^2}{2\sigma^2 t}}.
\eeast

Thus, we have that
\beast
f_t(x,r) = 4 \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty}  \brb{ \frac{k^2\brb{\abs{x}+2kr}^2(r-\abs{x})}{\sigma^5\sqrt{2\pi t^5}} - \frac{k\brb{\abs{x} + 2k^2r + 3kr}}{\sigma^3\sqrt{2\pi t^3}}}\exp\brb{-\frac{\brb{\abs{x}+2kr}^2}{2\sigma^2 t}}.
\eeast

To get $f_t\brb{x,R<r} = \int^r_{\abs{x}} f_t(x,s) ds$, we calculate the integral

\beast
& & \int^r_{\abs{x}} \frac{k^2\brb{\abs{x}+2ks}^2(s-\abs{x})}{\sigma^5\sqrt{2\pi t^5}} \exp\brb{-\frac{\brb{\abs{x}+2ks}^2}{2\sigma^2 t}} ds\\
& = & \frac k{2\sigma^3\sqrt{2\pi t^3}} \brb{\left.\brb{\abs{x}+2ks}(s-\abs{x}) \exp\brb{-\frac{\brb{\abs{x}+2ks}^2}{2\sigma^2 t}} \right|^{\abs{x}}_r+ \int^r_{\abs{x}}  \brb{2k(s-\abs{x}) + \brb{\abs{x}+2ks}} \exp\brb{-\frac{\brb{\abs{x}+2ks}^2}{2\sigma^2 t}}ds }.
\eeast

Thus, add it to integral $\int^r_{\abs{x}} - \frac{k\brb{\abs{x} + 2k^2s + 3ks}}{\sigma^3\sqrt{2\pi t^3}}\exp\brb{-\frac{\brb{\abs{x}+2ks}^2}{2\sigma^2 t}}  ds$, we get
\beast
& & \sum^\infty_{k=-\infty} \frac k{2\sigma^3\sqrt{2\pi t^3}} \brb{\brb{\abs{x}+2kr}(r-\abs{x}) \exp\brb{-\frac{\brb{\abs{x}+2kr}^2}{2\sigma^2 t}} - \int^r_{\abs{x}} (2k+1)\brb{\abs{x}+2ks}\exp\brb{-\frac{\brb{\abs{x}+2ks}^2}{2\sigma^2 t}} ds} \\
& = & \sum^\infty_{k=-\infty} \frac k{2\sigma^3\sqrt{2\pi t^3}} \brb{\abs{x}+2kr}(r-\abs{x}) \exp\brb{-\frac{\brb{\abs{x}+2kr}^2}{2\sigma^2 t}} + \left.\frac {2k+1}{4\sigma\sqrt{2\pi t}} \exp\brb{-\frac{\brb{\abs{x}+2ks}^2}{2\sigma^2 t}} \right|^r_{\abs{x}} \\
& = & \sum^\infty_{k=-\infty} \frac k{2\sigma^3\sqrt{2\pi t^3}} \brb{\abs{x}+2kr}(r-\abs{x}) \exp\brb{-\frac{\brb{\abs{x}+2kr}^2}{2\sigma^2 t}} + \frac {2k+1}{4\sigma\sqrt{2\pi t}} \exp\brb{-\frac{\brb{\abs{x}+2kr}^2}{2\sigma^2 t}}.
\eeast

Then we can multiply $4 \exp\brb{\frac{\mu x}{\sigma^2} - \frac{\mu^2t}{2\sigma^2}}$ and get the required result.
\end{proof}

\begin{theorem}[range of Brownian motion with drift]\label{thm:bm_drift_range_transition_density}
Let $X_t = \mu t + \sigma B_t$ where $B$ be a standard Brownian motion in $\R$ with range $R_t$ at time $t$. Then for $r>0$,
\beast
\pro\brb{R_t \leq r}  & = & \sum^\infty_{k=1}\frac{4k^2\pi^2\sigma^8}{\brb{k^2\pi^2\sigma^4 + \mu^2r^2}^2} \exp\brb{-\frac{\brb{k^2\pi^2\sigma^4 + \mu^2r^2}t}{2\sigma^2r^2}}\\
& & \qquad\qquad \times \brb{\brb{1-(-1)^k\cosh\brb{\frac{\mu r}{\sigma^2}}}\brb{1+\frac{k^2\pi^2 \sigma^2 t}{r^2} - \frac{4\mu^2r^2}{k^2\pi^2\sigma^4 + \mu^2r^2}} - (-1)^k\frac{\mu r}{\sigma^2} \sinh\brb{\frac{\mu r}{\sigma^2}}}.
\eeast
\end{theorem}

\begin{remark}
If $\mu = 0$, we can combine Theorem \ref{thm:bm_range_transition_density} and the above theorem to have
\be
\pro\brb{R_t\leq r} = 1 + 4\sum^\infty_{k=1}(-1)^{k}k \erfc\brb{\frac{kr}{\sigma\sqrt{2 t}}} = 4\sum^\infty_{k=1}\frac{1-(-1)^k}{k^2\pi^2} \exp\brb{-\frac{k^2\pi^2 \sigma^2 t}{2r^2}}\brb{1+\frac{k^2\pi^2\sigma^2 t}{r^2} }
\ee
which can be proved by maths software.
\end{remark}

\begin{proof}[\bf Proof]
Recalling Proposition \ref{pro:fourier_transform_exponential_minus_quadratic} and Theorem \ref{thm:poisson_summation_formula}, we let $f(x) =  \exp\brb{-\frac{x^2}{2\sigma^2 t}}$ and get
\beast%\frac 1{\sigma\sqrt{2\pi t}} \sum_{k=-\infty}^{\infty} \frac{1}{2y} \sqrt{2\pi} \exp\brb{-\frac{\pi^2 k^2}{2y^2}} \exp\brb{\frac{\pi i kx} y} \\ %
\frac 1{\sigma\sqrt{2\pi t}}  \sum^\infty_{n = -\infty} \exp\brb{-\frac{(x+2ny)^2}{2\sigma^2 t}} & = &  \frac 1{\sigma\sqrt{2\pi t}} \sum_{k=-\infty}^{\infty} \frac{1}{2y} \sqrt{2\pi\sigma^2 t} \exp\brb{-\frac{\pi^2 k^2 \sigma^2 t}{2y^2}} \exp\brb{\frac{\pi i kx} y} \\
& = & \frac{1}{2y}  \sum_{k=-\infty}^{\infty} \exp\brb{-\frac{\pi^2 k^2 \sigma^2 t}{2y^2}} \exp\brb{\frac{\pi i kx} y} \qquad (*).
\eeast

Taking the derivatives of $x$ and $y$ respectively, we have

\beast
& & \frac 1{\sigma\sqrt{2\pi t}}  \sum^\infty_{n = -\infty} \frac{x+ 2ny}{\sigma^2 t} \exp\brb{-\frac{(x+2ny)^2}{2\sigma^2 t}} =  -\frac{1}{2y}  \sum_{k=-\infty}^{\infty} \exp\brb{-\frac{\pi^2 k^2 \sigma^2 t}{2y^2}}  \exp\brb{\frac{\pi i kx} y} \frac{\pi i k} y \qquad (\dag)\\
& & \frac 1{\sigma\sqrt{2\pi t}}  \sum^\infty_{n = -\infty} \frac{2n(x+ 2ny)}{\sigma^2 t} \exp\brb{-\frac{(x+2ny)^2}{2\sigma^2 t}} =  \frac 12 \sum_{k=-\infty}^{\infty} \exp\brb{-\frac{\pi^2 k^2 \sigma^2 t}{2y^2}}  \exp\brb{\frac{\pi i kx} y}\brb{\frac{1}{y^2} - \frac{2k^2\pi^2\sigma^2 t}{2y^4} + \frac{ik\pi x}{y^3}}\qquad (\dag\dag)
\eeast

Therefore, $\frac {\sigma^2t}y (*) + \brb{1-\frac xy}(\dag)$ is

\beast
\frac 1{\sigma\sqrt{2\pi t}}  \sum^\infty_{n = -\infty} (2n+1)\exp\brb{-\frac{(x+2ny)^2}{2\sigma^2 t}}  =  \frac{1}{2}  \sum_{k=-\infty}^{\infty} \exp\brb{-\frac{\pi^2 k^2 \sigma^2 t}{2y^2}}  \exp\brb{\frac{\pi i kx} y} \brb{- \frac{\pi i k \sigma^2t }{y^3} +  \frac {y - x}{y^2}}\qquad (**)
\eeast

Then, $(x-y)*(\dag\dag) + (**)$ is

\beast
& & \frac 1{\sigma\sqrt{2\pi t}}  \sum^\infty_{n = -\infty} \brb{(2n+1) - \frac{2n}{\sigma^2 t} (y-x)(x+ 2ny)}\exp\brb{-\frac{(x+2ny)^2}{2\sigma^2 t}} \\
& = &  \frac 12 \sum_{k=-\infty}^{\infty} \exp\brb{-\frac{\pi^2 k^2 \sigma^2 t}{2y^2}}  \exp\brb{\frac{\pi i kx} y} \brb{- \frac{\pi i k \sigma^2t }{y^3} + (x-y)\brb{\frac{ik\pi x}{y^3}- \frac{k^2\pi^2\sigma^2 t}{y^4}}}.
\eeast

Replacing $x$ with $\abs{x}$ and $y$ with $r$ in the above equation and combining Lemma \ref{lem:bm_drift_density_conditioned_on_range}, we have that
\be
\pro\brb{R\leq r} = \frac 1{2} \exp\brb{ - \frac{\mu^2t}{2\sigma^2}} \sum^\infty_{k=-\infty} I_k \exp\brb{-\frac{\pi^2 k^2 \sigma^2 t}{2r^2}}
\ee

where
\be
I_k = \int^r_{-r}\exp\brb{\frac{\mu x}{\sigma^2}}  \exp\brb{\frac{\pi i k\abs{x}} r} \brb{- \frac{\pi i k \sigma^2t }{r^3} + (\abs{x}-r)\brb{\frac{ik\pi \abs{x}}{r^3}- \frac{k^2\pi^2\sigma^2 t}{r^4}}}dx%& = & \int^r_{-r}\exp\brb{\frac{\mu x}{\sigma^2}}  \exp\brb{\frac{\pi i k\abs{x}} r} \brb{\frac{ik\pi }{r^3}\abs{x}^2- \frac{k^2\pi^2\sigma^2 t+ ik\pi r^2 }{r^4}\abs{x} +  \frac{\brb{k^2\pi^2 -\pi i k }\sigma^2t }{r^3}}dx.
\ee

Let
\beast
C_1(\mu) & = &  \int^r_0 \exp\brb{\frac{\mu x}{\sigma^2} + \frac{\pi i kx} r} dx = \frac{\sigma^2 r}{\mu r + i\pi k\sigma^2}\brb{(-1)^k\exp\brb{\frac{\mu r}{\sigma^2}} - 1}\\
C_2(\mu) & = &  \int^r_0 (x-r) \exp\brb{\frac{\mu x}{\sigma^2} + \frac{\pi i kx} r} dx = \frac{\sigma^2 r^2}{\mu r + i\pi k\sigma^2} + \frac{\sigma^4 r^2}{\brb{\mu r + i\pi k\sigma^2}^2}\brb{1- (-1)^k\exp\brb{\frac{\mu r}{\sigma^2}} }\\
C_3(\mu) & = &  \int^r_0 x(x-r) \exp\brb{\frac{\mu x}{\sigma^2} + \frac{\pi i kx} r} dx \\
& = &  -\frac{\sigma^4 r^3}{\brb{\mu r + i\pi k\sigma^2}^2}\brb{1+(-1)^k\exp\brb{\frac{\mu r}{\sigma^2}}} + \frac{2\sigma^6 r^3}{\brb{\mu r + i\pi k\sigma^2}^3}\brb{(-1)^k\exp\brb{\frac{\mu r}{\sigma^2}}-1 }.
\eeast

Thus,
\be
I_k = - \frac{\pi i k \sigma^2t }{r^3}\brb{C_1(\mu) + C_1(-\mu)} - \frac{k^2\pi^2\sigma^2 t}{r^4}\brb{C_2(\mu) + C_2(-\mu)} + \frac{ik\pi }{r^3}\brb{C_3(\mu) + C_3(-\mu)}
\ee

Accordingly,
\beast
C_1(\mu) + C_1(-\mu) & = & \frac{\sigma^2 r}{\mu r + i\pi k\sigma^2}\brb{(-1)^k\exp\brb{\frac{\mu r}{\sigma^2}} - 1} - \frac{\sigma^2 r}{\mu r - i\pi k\sigma^2}\brb{(-1)^k\exp\brb{\frac{-\mu r}{\sigma^2}} - 1} \\
& = & \frac{2\sigma^2 r (-1)^k}{\mu^2 r^2 + \pi^2 k^2\sigma^4}\brb{\mu r \sinh\brb{\frac{\mu r}{\sigma^2}} - i\pi k\sigma^2 \cosh\brb{\frac{\mu r}{\sigma^2}}} + \frac{2i\pi k \sigma^4r}{\mu^2 r^2 + \pi^2 k^2\sigma^4}.
\eeast

\beast
& & C_2(\mu) + C_2(-\mu) \\
& = & \frac{\sigma^2 r^2}{\mu r + i\pi k\sigma^2} + \frac{\sigma^4 r^2}{\brb{\mu r + i\pi k\sigma^2}^2}\brb{1- (-1)^k\exp\brb{\frac{\mu r}{\sigma^2}} } - \frac{\sigma^2 r^2}{\mu r - i\pi k\sigma^2} + \frac{\sigma^4 r^2}{\brb{\mu r - i\pi k\sigma^2}^2}\brb{1- (-1)^k\exp\brb{\frac{-\mu r}{\sigma^2}} } \\
& = & - \frac{2i\pi k \sigma^4r^2}{\mu^2 r^2 + \pi^2 k^2\sigma^4} + \frac{2\brb{\mu^2r^2 - \pi^2 k^2\sigma^4}\sigma^4r^2}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2} - (-1)^k \frac{2\sigma^4r^2\brb{\cosh\brb{\frac{\mu r}{\sigma^2}}\brb{\mu^2r^2 - \pi^2 k^2\sigma^4} - 2i\pi k\sigma^2\mu r\sinh\brb{\frac{\mu r}{\sigma^2}}}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2}
\eeast

\beast
& & C_3(\mu) + C_3(-\mu) \\
& = & - \frac{2\sigma^4r^3\brb{\mu^2r^2 - \pi^2 k^2\sigma^4}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2} - (-1)^k \frac{2\sigma^4r^3\brb{\cosh\brb{\frac{\mu r}{\sigma^2}}\brb{\mu^2r^2 - \pi^2 k^2\sigma^4} - 2i\pi k\sigma^2\mu r\sinh\brb{\frac{\mu r}{\sigma^2}}}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2} \\
& &  + \frac{4\sigma^6r^3\brb{3i\pi k\sigma^2\mu^2r^2 - i\pi^3 k^3\sigma^6}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^3} +  (-1)^k \frac{4\sigma^6r^3\brb{\brb{\mu^3r^3 - 3\mu r k^2 \pi^2 \sigma^4}\sinh\brb{\frac{\mu r}{\sigma^2}} + i\brb{k^3\pi^3 \sigma^6 - 3\mu^2r^2\pi k\sigma^2}\cosh\brb{\frac{\mu r}{\sigma^2}}}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^3}
\eeast

%Since we have
%\be
%\int^r_0 \exp(ax) dx = \frac 1a \exp(ar) - \frac 1a,\quad \int^r_0 x \exp(ax) dx = \brb{\frac ra - \frac 1{a^2}}\exp(ar) + \frac 1{a^2},
%\ee
%\be
%\int^r_0 x^2 \exp(ax) dx = \brb{\frac {r^2}a  - \frac {2r}{a^2} + \frac 2{a^3} }\exp(ar) - \frac 2{a^3}.
%\ee
%
%Let $a = \frac{\mu }{\sigma^2} + \frac{\pi i k }r$ and $b = -\frac{\mu }{\sigma^2} + \frac{\pi i k }r $. We write $I_k = A_k+  B_k + C_k$ where
%\beast
%A_k & = & \frac{ik\pi }{r^3} \int^r_{-r} \exp\brb{\frac{\mu x}{\sigma^2}}  \exp\brb{\frac{\pi i k\abs{x}} r} \abs{x}^2 dx \\
%& = & \frac{ik\pi }{r^3} \int^r_0\brb{\exp\brb{\brb{\frac{\mu }{\sigma^2} + \frac{\pi i k }r}x } x^2 - \exp\brb{\brb{\frac{-\mu }{\sigma^2} + \frac{\pi i k }r}x} x^2} dx\\
%& = & \frac{ik\pi }{r^3} \brb{\brb{\frac {r^2}a  - \frac {2r}{a^2} + \frac 2{a^3} }\exp(ar) - \frac 2{a^3} - \brb{\frac {r^2}b  - \frac {2r}{b^2} + \frac 2{b^3} }\exp(br) + \frac 2{b^3}}
%\eeast
%
%\beast
%B_k & = & - \frac{k^2\pi^2\sigma^2 t+ ik\pi r^2 }{r^4} \int^r_{-r} \exp\brb{\frac{\mu x}{\sigma^2}}  \exp\brb{\frac{\pi i k\abs{x}} r} \abs{x}^2 dx \\
%& = & - \frac{k^2\pi^2\sigma^2 t+ ik\pi r^2 }{r^4} \int^r_0\brb{\exp\brb{\brb{\frac{\mu }{\sigma^2} + \frac{\pi i k }r}x } x + \exp\brb{\brb{\frac{-\mu }{\sigma^2} + \frac{\pi i k }r}x} x} dx\\
%& = & - \frac{k^2\pi^2\sigma^2 t+ ik\pi r^2 }{r^4} \brb{ \brb{\frac ra - \frac 1{a^2}}\exp(ar) + \frac 1{a^2} + \brb{\frac rb - \frac 1{b^2}}\exp(br) + \frac 1{b^2} }
%\eeast
%
%\beast
%C_k & = &  \frac{\brb{k^2\pi^2 -\pi i k }\sigma^2t }{r^3} \int^r_{-r} \exp\brb{\frac{\mu x}{\sigma^2}}  \exp\brb{\frac{\pi i k\abs{x}} r}  dx \\
%& = &  \frac{\brb{k^2\pi^2 -\pi i k }\sigma^2t }{r^3} \int^r_0\brb{\exp\brb{\brb{\frac{\mu }{\sigma^2} + \frac{\pi i k }r}x } - \exp\brb{\brb{\frac{-\mu }{\sigma^2} + \frac{\pi i k }r}x} } dx\\
%& = &  \frac{\brb{k^2\pi^2 -\pi i k }\sigma^2t }{r^3} \brb{ \frac 1a \exp(ar) - \frac 1a - \frac 1b \exp(br) + \frac 1b }
%\eeast

%Checking the terms with denominator $\mu^2 r^2 + \pi^2 k^2\sigma^4$, we have that their sum is
%\be
%- \frac{\pi i k \sigma^2t }{r^3}\brb{\frac{2\sigma r (-1)^k}{\mu^2 r^2 + \pi^2 k^2\sigma^4}\brb{\mu r \sinh\brb{\frac{\mu r}{\sigma^2}} - 2i\pi k\sigma^2 \cosh\brb{\frac{\mu r}{\sigma^2}}} + \frac{2i\pi k \sigma^3r}{\mu^2 r^2 + \pi^2 k^2\sigma^4}} + \frac{k^2\pi^2\sigma^2 t}{r^4} \frac{2i\pi k \sigma^3r^2}{\mu^2 r^2 + \pi^2 k^2\sigma^4}
%\ee

Since we take $k$ to be all the integer, we can eliminate the term with even function in $C_1(\mu) + C_1(-\mu)$ and $C_3(\mu) + C_3(-\mu)$ as well as the odd function in $C_2(\mu) + C_2(-\mu)$. Thus, these lead to the corresponding terms
\be
C_1(\mu) + C_1(-\mu) \ \ra \ C'_1(\mu) = \frac{2i\pi k \sigma^4r}{\mu^2 r^2 + \pi^2 k^2\sigma^4}\brb{1 - (-1)^k\cosh\brb{\frac{\mu r}{\sigma^2}}}.
\ee

Similarly, we have
\be
C'_2(\mu) =   \frac{2\sigma^4r^2\brb{\mu^2r^2 - \pi^2 k^2\sigma^4}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2} \brb{1-(-1)^k \cosh\brb{\frac{\mu r}{\sigma^2}}} ,
\ee

\be
C'_3(\mu) =   (-1)^k \frac{4i\sigma^6r^4 \pi k\mu \sinh\brb{\frac{\mu r}{\sigma^2}}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2} + \frac{4i\sigma^6r^3\brb{3\pi k\sigma^2\mu^2r^2 - \pi^3 k^3\sigma^6}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^3} +  (-1)^k \frac{4i\sigma^6r^3\brb{k^3\pi^3 \sigma^6 - 3\mu^2r^2\pi k\sigma^2}\cosh\brb{\frac{\mu r}{\sigma^2}}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^3}
\ee

Thus,
\beast
I_k & = &  \frac{2\pi^2 k^2 \sigma^6t}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}r^2}\brb{1 - (-1)^k\cosh\brb{\frac{\mu r}{\sigma^2}}}  - \frac{2k^2\pi^2\sigma^6 t\brb{\mu^2r^2 - \pi^2 k^2\sigma^4}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2r^2} \brb{1-(-1)^k \cosh\brb{\frac{\mu r}{\sigma^2}}}\\
& & \quad -(-1)^k \frac{4\sigma^6 \pi^2 k^2\mu r\sinh\brb{\frac{\mu r}{\sigma^2}}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2} - \frac{4\sigma^6k\pi\brb{3\pi k\sigma^2\mu^2r^2 - \pi^3 k^3\sigma^6}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^3} - (-1)^k \frac{4\sigma^6k\pi \brb{k^3\pi^3 \sigma^6 - 3\mu^2r^2\pi k\sigma^2}\cosh\brb{\frac{\mu r}{\sigma^2}}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^3} \\
& = & \frac{4k^4\pi^4\sigma^{10} t }{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2r^2} \brb{1-(-1)^k \cosh\brb{\frac{\mu r}{\sigma^2}}} -(-1)^k \frac{4\sigma^6 \pi^2 k^2\mu r\sinh\brb{\frac{\mu r}{\sigma^2}}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2} \\
& & \quad  \frac{4\sigma^8k^2\pi^2 \brb{k^2\pi^2 \sigma^4 + \mu^2 r^2 - 4\mu^2r^2}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^3} \brb{1- (-1)^k \cosh\brb{\frac{\mu r}{\sigma^2}}}.
\eeast

%Then we can eliminate the imaginary part and get
%\be
%C''_1(\mu) = \frac{2i\pi k \sigma^3r}{\mu^2 r^2 + \pi^2 k^2\sigma^4}\brb{1 - (-1)^k\cosh\brb{\frac{\mu r}{\sigma^2}}},\quad C''_2(\mu) =  \frac{2\sigma^2r^2\brb{\mu^2r^2 - \pi^2 k^2\sigma^4}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2} \brb{1-(-1)^k \cosh\brb{\frac{\mu r}{\sigma^2}}} ,
%\ee
%
%\be
%C''_3(\mu) = (-1)^k \frac{2\sigma^2r^3\brb{ 2i\pi k\sigma^2\mu r\sinh\brb{\frac{\mu r}{\sigma^2}}}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^2} + \frac{4\sigma^3r^3\brb{3i\pi k\sigma^2\mu^2r^2 - i\pi^3 k^3\sigma^6}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^3} +  (-1)^k \frac{4\sigma^3r^3i\brb{k^3\pi^3 \sigma^6 - 3\mu^2r^2\pi k\sigma^2}\cosh\brb{\frac{\mu r}{\sigma^2}}}{\brb{\mu^2 r^2 + \pi^2 k^2\sigma^4}^3}
%\ee

Therefore, we sum $I_k$ for all the integer and use the symmetry of $I_k$ and get the required result. We can sum it from $k=1$ to $\infty$ as we still have a coefficient $1/2$ in front of the summation.
\end{proof}

\subsection{Hitting time of single barrier}


\begin{proposition}\label{pro:brownian_motion_drift_single_barrier_hitting_time_distribution}%{pro:brownian_motion_stopping_time_distribution}
Let $X_t = \mu t + \sigma B_t$ where $B$ be a standard Brownian motion in $\R$ and $T_a = \inf\bra{t\geq 0:X_t = a}$ for $a>0$. Then its probability density is
\be
f_{T_a}(t) = \frac a{\sigma\sqrt{2\pi t^3}} \exp\brb{-\frac{(a-\mu t)^2}{2\sigma^2t }}.
\ee
\end{proposition}

\begin{remark}
Since the process is not symmetric, we can use Corollary \ref{cor:abs_sup_equal_brownian_motion} as in the proof of Proposition \ref{pro:brownian_motion_single_barrier_hitting_time_distribution}. When $\mu = 0$, the result is consistent with Proposition \ref{pro:brownian_motion_single_barrier_hitting_time_distribution}.
\end{remark}

\begin{proof}[\bf Proof]
By Proposition \ref{pro:density_bm_drift_maximum_minimum}, we have
\be
\pro\brb{T_a > t} = \pro\brb{\max_{0\leq s\leq t}X_s < a} = N\brb{\frac{a-\mu t}{\sigma\sqrt{t} }} - \exp\brb{\frac{2\mu a}{\sigma^2}}N\brb{-\frac{a+\mu t}{\sigma\sqrt{t} }}.
\ee

Then we take the differentiation,
\beast
f_{T_a}(t) & = & -\frac 1{\sqrt{2\pi}} \frac{-\mu \sqrt{t} - \frac 1{2\sqrt{t}}(a-\mu t)}{\sigma t} \exp\brb{-\frac{(a-\mu t)^2}{2\sigma^2t }} + \frac 1{\sqrt{2\pi}}\exp\brb{\frac{2\mu a}{\sigma^2}} \frac{-\mu \sqrt{t} + \frac 1{2\sqrt{t}}(a+\mu t)}{\sigma t} \exp\brb{-\frac{(a+\mu t)^2}{2\sigma^2t }} \\
& = &  \frac 1{\sigma\sqrt{2\pi t^3}} \brb{\frac {a+\mu t}2 + \frac {a-\mu t}2}\exp\brb{-\frac{(a-\mu t)^2}{2\sigma^2t }} =  \frac a{\sigma\sqrt{2\pi t^3}} \exp\brb{-\frac{(a-\mu t)^2}{2\sigma^2t }}.
\eeast
which is inverse Gaussian distribution $IG\brb{\frac a{\mu},\frac{a^2}{\sigma^2}}$ (see Definition \ref{def:inverse_gaussian_rv}) for $\mu >0$. Therefore, $\E T < \infty$ and $T<\infty$ a.s. as the expectation of inverse Gaussian random variable exists.
\end{proof}


\subsection{Hitting time of double barriers for Brownian motion with drift}

\begin{theorem}\label{thm:drifted_brownian_motion_double_barriers_hitting_time}
Let a drifted Brownian motion $X_t = \mu t + \sigma B_t$ where $B$ is a standard Brownian motion in $\R$. Let $T = \inf\bra{t: X_t = -a \text{ or } X_t = b }$ where $a,b>0$. Then the density function of $T$ is
\beast
f(t) & = & \frac{\pi\sigma^2}{(a+b)^2}\exp\brb{- \frac{\mu^2t}{2\sigma^2}} \sum_{n=1}^\infty n \exp \brb{- \frac{n^2\pi^2 \sigma^2 t}{2(a+b)^2}} \brb{\exp\brb{\frac {\mu b}{\sigma^2}}\sin\brb{\frac{n\pi b}{a+b}} + \exp\brb{-\frac {\mu a}{\sigma^2}}\sin\brb{\frac{n \pi a}{a+b}}}\\
& = & \frac{\pi\sigma^2}{(a+b)^2} \exp\brb{-\frac{\mu^2t}{2\sigma^2}} \sum^\infty_{n=1} (-1)^{n-1} n \exp\brb{-\frac{n^2\pi^2 \sigma^2t}{2(a+b)^2}}\brb{\exp\brb{-\frac {\mu a}{\sigma^2}}\sin\frac{n\pi b}{a+b} + \exp\brb{\frac {\mu b}{\sigma^2}}\sin \frac{n\pi a}{a+b}}.
\eeast
\end{theorem}

\begin{proof}[\bf Proof]\footnote{we can have alternative proof, see \cite{Douady_1999}}
By Theorem \ref{thm:joint_density_maximum_minimum_bm_with_drift}, we have that
\beast
& & \pro\brb{T \geq t} = \pro\brb{Y_t\leq b,Z_t \geq -a}\\
& = & \exp\brb{ - \frac{\mu^2t}{2\sigma^2}} \sum_{n=1}^\infty  \frac {2\pi n\sigma^4}{\mu^2(a+b)^2  + n^2 \pi^2 \sigma^4} \exp \brb{- \frac{n^2\pi^2 \sigma^2 t}{2(a+b)^2}} \brb{\exp\brb{\frac {\mu b}{\sigma^2}}\sin\brb{\frac{\pi n b}{a+b}} + \exp\brb{\frac {-\mu a}{\sigma^2}}\sin\brb{\frac{\pi n a}{a+b}}}.
\eeast

Then taking the differentiation of $t$, we have
\beast
f_T(t) & = & -\frac{\pi\sigma^2}{(a+b)^2}\exp\brb{- \frac{\mu^2t}{2\sigma^2}} \sum_{n=1}^\infty  (-n) \exp \brb{- \frac{n^2\pi^2 \sigma^2 t}{2(a+b)^2}} \brb{\exp\brb{\frac {\mu b}{\sigma^2}}\sin\brb{\frac{\pi n b}{a+b}} + \exp\brb{\frac {-\mu a}{\sigma^2}}\sin\brb{\frac{\pi n a}{a+b}}}\\
& = & \frac{\pi\sigma^2}{(a+b)^2}\exp\brb{- \frac{\mu^2t}{2\sigma^2}} \sum_{n=1}^\infty n \exp \brb{- \frac{n^2\pi^2 \sigma^2 t}{2(a+b)^2}} \brb{\exp\brb{\frac {\mu b}{\sigma^2}}\sin\brb{\frac{n\pi b}{a+b}} + \exp\brb{\frac {-\mu a}{\sigma^2}}\sin\brb{\frac{n \pi a}{a+b}}}
\eeast

Also, since $\sin\brb{\frac{n\pi a}{a+b}} = (-1)^{n-1} \sin\frac{n\pi  b}{a+b}$, we have the required result.
\end{proof}

\begin{remark}\footnote{what if $a$ or $b$ go to $\infty$?}
If $\mu = 0$, we have that
\beast
f(t) & = & \frac{\pi\sigma^2}{(a+b)^2} \sum^\infty_{n=1} (-1)^{n-1} n \exp\brb{-\frac{n^2\pi^2 \sigma^2t}{2(a+b)^2}}\brb{\sin\frac{n\pi b}{a+b} + \sin \brb{\frac{n\pi a}{a+b}}} \\
& = & \frac{\pi\sigma^2}{(a+b)^2} \sum^\infty_{n=1} (-1)^{n-1} n\exp\brb{-\frac{n^2\pi^2 \sigma^2t}{2(a+b)^2}}\brb{\sin\frac{n\pi a}{a+b}+\sin\frac{n\pi b}{a+b}} %\\ & = & \frac{2\pi\sigma^2}{(a+b)^2} \sum^\infty_{n=1} (2n-1) \exp\brb{-\frac{(2n-1)^2\pi^2 \sigma^2t}{2(a+b)^2}}\sin\frac{(2n-1)\pi b}{a+b}
\eeast

Since the fourier series ($L = a+b$, see Corollary \ref{cor:fourier_series_linear})
\be
x = 2(a+b)\sum^\infty_{n=1} \frac {(-1)^{n-1}}{n\pi}\sin \brb{\frac{n\pi x}{a+b}}
\ee
it is easy to check that
\beast
\pro(T<\infty) & = & \int^\infty_0 \frac{\pi\sigma^2}{(a+b)^2} \sum^\infty_{n=1} (-1)^{n-1} n\exp\brb{-\frac{n^2\pi^2 \sigma^2t}{2(a+b)^2}}\brb{\sin\frac{n\pi a}{a+b}+\sin\frac{n\pi b}{a+b}}dt\\
& = & 2 \sum^\infty_{n=1}  \frac {(-1)^{n-1}}{n\pi} \brb{\sin\frac{n\pi a}{a+b}+\sin\frac{n\pi b}{a+b}} = \frac{a}{a+b} + \frac{b}{a+b} = 1.
\eeast

Thus, by Corollary \ref{cor:fourier_series_cube},
\be
\sum^\infty_{n=1} \frac{(-1)^{n-1} }{n^3\pi^3}\sin \brb{\frac{n\pi x}{a+b}} = \frac{x\brb{(a+b)^2 - x^2}}{12(a+b)^3}.
\ee
we have that
\beast
\E T & = & \int^\infty_0 \frac{\pi\sigma^2}{(a+b)^2} \sum^\infty_{n=1} (-1)^{n-1} nt \exp\brb{-\frac{n^2\pi^2 \sigma^2t}{2(a+b)^2}}\brb{\sin\frac{n\pi a}{a+b}+\sin\frac{n\pi b}{a+b}}dt\\
& = & 2 \sum^\infty_{n=1} (-1)^{n-1} \frac 1{n\pi} \brb{\sin\frac{n\pi a}{a+b}+\sin\frac{n\pi b}{a+b}}\int^\infty_0 \exp\brb{-\frac{n^2\pi^2 \sigma^2t}{2(a+b)^2}} dt\\
& = & \frac{4(a+b)^2}{\sigma^2} \sum^\infty_{n=1}  \frac {(-1)^{n-1}}{n^3\pi^3} \brb{\sin\frac{n\pi a}{a+b}+\sin\frac{n\pi b}{a+b}} = \frac{4(a+b)^2}{\sigma^2}\brb{\frac{a\brb{(a+b)^2 - a^2}}{12(a+b)^3}+ \frac{b\brb{(a+b)^2 - b^2}}{12(a+b)^3}} = \frac{ab}{\sigma^2}.
\eeast

Thus, by Corollary \ref{cor:fourier_series_power_5},
\be
\sum^\infty_{n=1} \frac{(-1)^{n-1} }{n^5\pi^5}\sin \brb{\frac{n\pi x}{a+b}} = \frac{x\brb{3x^4 - 10x^2(a+b)^2 + 7(a+b)^4}}{720(a+b)^5}.
\ee
we have that
\beast
\E T^2 & = & \int^\infty_0 \frac{\pi\sigma^2}{(a+b)^2} \sum^\infty_{n=1} (-1)^{n-1} nt^2 \exp\brb{-\frac{n^2\pi^2 \sigma^2t}{2(a+b)^2}}\brb{\sin\frac{n\pi a}{a+b}+\sin\frac{n\pi b}{a+b}}dt\\
& = & 2 \sum^\infty_{n=1} (-1)^{n-1} \frac 1{n\pi} \brb{\sin\frac{n\pi a}{a+b}+\sin\frac{n\pi b}{a+b}}2\int^\infty_0 t\exp\brb{-\frac{n^2\pi^2 \sigma^2t}{2(a+b)^2}} dt\\
& = & \frac{16(a+b)^4}{\sigma^4} \sum^\infty_{n=1}  \frac {(-1)^{n-1}}{n^5\pi^5} \brb{\sin\frac{n\pi a}{a+b}+\sin\frac{n\pi b}{a+b}} \\
& = & \frac{16(a+b)^4}{\sigma^4} \brb{\frac{a\brb{3a^4 - 10a^2(a+b)^2 + 7(a+b)^4}}{720(a+b)^5} + \frac{b\brb{3b^4 - 10b^2(a+b)^2 + 7(a+b)^4}}{720(a+b)^5}} = \frac{ab(a^2 + b^2 +3ab)}{3\sigma^4}.
\eeast

Thus, $\var T = \frac{ab(a^2 + b^2)}{3\sigma^4}$. These results are consistent with Theorem \ref{thm:brownian_motion_double_bounded} and Proposition \ref{pro:brownian_motion_two_barriers_variance}.

In particular, if $a=b$, we have
\be
f(t) = \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1} (-1)^{n-1}n \exp\brb{-\frac{n^2\pi^2 \sigma^2t}{8a^2}}\sin\frac{n\pi }{2} = \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1}(-1)^{n-1} (2n-1) \exp\brb{-\frac{(2n-1)^2\pi^2 \sigma^2 t}{8a^2}}.
\ee

Then
\beast
\pro( T<\infty) & = & \int^\infty_0 \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1}(-1)^{n-1} (2n-1) \exp\brb{-\frac{(2n-1)^2\pi^2\sigma^2 t}{8a^2}}dt \\
& = & \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1}(-1)^{n-1} (2n-1) \frac {8a^2 }{(2n-1)^2\pi^2\sigma^2} \left.t\exp\brb{-\frac{(2n-1)^2\pi^2 \sigma^2t}{8a^2}}\right|^0_\infty \\
& = & \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1} \frac {(-1)^{n-1}8a^2}{(2n-1) \pi^2\sigma^2} = \frac{4 }{\pi} \sum^\infty_{n=1} \frac {(-1)^{n-1}}{2n-1} = \frac{4 }{\pi} \frac{\pi}{4} = 1\qquad \text{(by Proposition \ref{pro:alternative_odd_series})}
\eeast

\beast
\E T & = & \int^\infty_0 \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1}(-1)^{n-1} (2n-1) t \exp\brb{-\frac{(2n-1)^2\pi^2\sigma^2 t}{8a^2}}dt \\
& = & \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1}(-1)^{n-1} (2n-1) \frac {8a^2 }{(2n-1)^2\pi^2\sigma^2} \brb{\left.t\exp\brb{-\frac{(2n-1)^2\pi^2 \sigma^2t}{8a^2}}\right|^0_\infty + \int^\infty_0 \exp\brb{-\frac{(2n-1)^2\pi^2 t}{8a^2}}dt}\\
& = & \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1} \frac {(-1)^{n-1}64a^4}{(2n-1)^3 \pi^4\sigma^4} = \frac{32a^2 }{\pi^3\sigma^2} \sum^\infty_{n=1} \frac {(-1)^{n-1}}{(2n-1)^3} = \frac{32a^2 }{\pi^3\sigma^2} \frac{\pi^3}{32} = \frac{a^2}{\sigma^2}\qquad \text{(by Proposition \ref{pro:alternative_odd_series})}
\eeast

\beast
\E T^2 & = & \int^\infty_0 \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1}(-1)^{n-1} (2n-1) t^2 \exp\brb{-\frac{(2n-1)^2\pi^2\sigma^2 t}{8a^2}}dt \\
& = & \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1}(-1)^{n-1} (2n-1) \frac {8a^2 }{(2n-1)^2\pi^2\sigma^2} \brb{\left.t^2\exp\brb{-\frac{(2n-1)^2\pi^2 \sigma^2t}{8a^2}}\right|^0_\infty + \int^\infty_0 2t\exp\brb{-\frac{(2n-1)^2\pi^2 t}{8a^2}}dt}\\
& = & \frac{\pi\sigma^2}{2a^2} \sum^\infty_{n=1} \frac {(-1)^{n-1}512a^6}{(2n-1)^5 \pi^6\sigma^6} = \frac{256a^4 }{\pi^5\sigma^4} \sum^\infty_{n=1} \frac {(-1)^{n-1}}{(2n-1)^5} = \frac{256a^4 }{\pi^5\sigma^4} \frac{5\pi^5}{1536} = \frac{5a^4}{3\sigma^4}\qquad \text{(by Proposition \ref{pro:alternative_odd_series}).}
\eeast

Thus, $\var T = \frac {2a^4}{3\sigma^4}$. These results are consistent with Theorem \ref{thm:brownian_motion_double_bounded} and Proposition \ref{pro:brownian_motion_two_barriers_variance}.
\end{remark}

\begin{theorem}\label{thm:drifted_brownian_motion_double_barriers_hitting_time_mgf}
Let a drifted Brownian motion $X_t = \mu t + \sigma B_t$ where $B$ is a standard Brownian motion in $\R$. Let $T = \inf\bra{t: X_t = -a \text{ or } X_t = b }$ where $a,b>0$. Then for $r\in \R$,
\be
\E\brb{e^{-rT}} = \frac{e^{-\frac{\mu a}{\sigma^2}}\sinh\brb{\frac{b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}+ e^{\frac{\mu b}{\sigma^2}}\sinh\brb{\frac{a}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}}{\sinh\brb{\frac{a+b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}}.
\ee

Accordingly, we have
\be
\pro\brb{T_{-a} < T_b} = \frac{1- e^{-\frac{2\mu b}{\sigma^2}}}{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} },\qquad \pro\brb{T_{-a} > T_b} =  \frac{e^{\frac{2\mu a}{\sigma^2}} - 1}{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} },
\ee
and
\be
\E T = \frac{ a\brb{e^{-\frac{2\mu b}{\sigma^2}} -1}  +  b\brb{e^{\frac{2\mu a}{\sigma^2}}-1}}{\mu\brb{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} }}.
\ee

In particular,
\be
\E\brb{T \ind_{\bra{T_{b}<T_{-a}}}} \frac{e^{\frac{\mu b}{\sigma^2}}\brb{b\sinh\brb{\frac{a\mu}{\sigma^2}}\cosh\brb{\frac{(a+b)\mu}{\sigma^2}}  - a\sinh\brb{\frac{b\mu}{\sigma^2}}}}{\mu \sinh^2\brb{\frac{(a+b)\mu}{\sigma^2}}},
\ee
\be
\E\brb{T \ind_{\bra{T_{b} > T_{-a}}}} = \frac{e^{-\frac{\mu a}{\sigma^2}}\brb{a\sinh\brb{\frac{b\mu}{\sigma^2}}\cosh\brb{\frac{(a+b)\mu}{\sigma^2}}  - b\sinh\brb{\frac{a\mu}{\sigma^2}}}}{\mu \sinh^2\brb{\frac{(a+b)\mu}{\sigma^2}}}.
\ee
\end{theorem}

\begin{proof}[\bf Proof]
First, it is easy to show that for $\lm \in \R$
\beast
\exp\brb{-\brb{\frac{\mu}{\sigma^2}\pm \frac 1{\sigma}\sqrt{\frac{\mu^2}{\sigma^2}+\lm^2}}X_t -\frac{\lm^2 t}{2}}
& = & \exp\brb{-\frac{\mu}{\sigma^2}\brb{\mu t + \sigma B_t}\pm \frac 1{\sigma}\sqrt{\frac{\mu^2}{\sigma^2}+\lm^2}\brb{\mu t + \sigma B_t} -\frac{\lm^2 t}{2}} \\
& = & \exp\brb{-\brb{\frac{\mu}{\sigma} \pm \sqrt{\frac{\mu^2}{\sigma^2} + \lm^2}}B_t- \frac 12 \brb{\frac{\mu}{\sigma} \pm \sqrt{\frac{\mu^2}{\sigma^2} + \lm^2}}^2t}
\eeast
is a martingale. Thus, it is obvious that
\be
M_t = \exp\brb{-\frac{\mu X_t}{\sigma^2}-\frac{\lm^2 t}{2}} \sinh\brb{ \frac {X_t+a}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2}+\lm^2}}
\ee
is also a martingale. So we may apply the optional stopping theorem (Theorem \ref{thm:optional_stopping_ui_continuous}) since $X_t$ is bounded.
\beast
\sinh\brb{ \frac {a}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2}+\lm^2}} = M_0 = \E M_T = \E\brb{\exp\brb{-\frac{\mu X_T}{\sigma^2}-\frac{\lm^2 T}{2}} \sinh\brb{ \frac {X_T+a}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2}+\lm^2}}\ind_{\bra{T_{b}<T_{-a}}}}
\eeast

Hence,
\be
\E\brb{\exp\brb{-\frac{\lm^2 T}{2}} \ind_{\bra{T_{b}<T_{-a}}}} = \frac{e^{\frac{\mu b}{\sigma^2}}\sinh\brb{ \frac {a}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2}+\lm^2}}}{\sinh\brb{ \frac {a+b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2}+\lm^2}}}
\ee

Then, we have the similar result for $\E\brb{\exp\brb{-\frac{\lm^2 T}{2}} \ind_{\bra{T_{b}>T_{-a}}}}$. Letting $\lm = \sqrt{2r}$, we will have the requiring result.

Also, by Proposition \ref{pro:laplace_finite_moment}%{pro:mgf_finite_moment},
\beast
\E T & = & \left.- \fp{\E\brb{e^{-rT}}}{r}\right|_{r=0} \\
& = & -\left.\frac{\brb{\frac{be^{-\frac{\mu a}{\sigma^2}}}{\sigma\sqrt{\frac{\mu^2}{\sigma^2} + 2r}} \cosh\brb{\frac{b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}+ \frac{ae^{\frac{\mu b}{\sigma^2}}}{\sigma \sqrt{\frac{\mu^2}{\sigma^2} + 2r}} \cosh\brb{\frac{a}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}}\sinh\brb{\frac{a+b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}} }{\sinh^2\brb{\frac{a+b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}} \right|_{r=0} \\
& & + \left.\frac{\brb{e^{-\frac{\mu a}{\sigma^2}}\sinh\brb{\frac{b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}+ e^{\frac{\mu b}{\sigma^2}}\sinh\brb{\frac{a}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}}\cosh\brb{\frac{a+b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}} \frac{a+b}{\sigma \sqrt{\frac{\mu^2}{\sigma^2} + 2r}} }{\sinh^2\brb{\frac{a+b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}} \right|_{r=0} \\
& = & \frac{\frac a{4\mu}\brb{1-e^{\frac{2\mu b}{\sigma^2}}+e^{-\frac{2\mu a}{\sigma^2}}-e^{-\frac{2\mu(a+b)}{\sigma^2}}}+\frac b{4\mu}\brb{e^{\frac{2\mu(a+b)}{\sigma^2}}-e^{\frac{2\mu b}{\sigma^2}}+e^{-\frac{2\mu a }{\sigma^2}}-1}}{\frac 14\brb{e^{\frac{2\mu(a+b)}{\sigma^2}} + e^{-\frac{2\mu(a+b)}{\sigma^2}} -2}}
\eeast

That is
\be
\E T = \frac{ a\brb{e^{-\frac{2\mu b}{\sigma^2}} -1} \brb{e^{\frac{2\mu b}{\sigma^2}}-e^{-\frac{2\mu a}{\sigma^2}}} +  b\brb{e^{\frac{2\mu a}{\sigma^2}}-1}\brb{e^{\frac{2\mu b}{\sigma^2}}-e^{-\frac{2\mu a}{\sigma^2}}}}{\mu\brb{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} }\brb{e^{\frac{2\mu b}{\sigma^2}} - e^{-\frac{2\mu a}{\sigma^2}}}} = \frac{ a\brb{e^{-\frac{2\mu b}{\sigma^2}} -1}  +  b\brb{e^{\frac{2\mu a}{\sigma^2}}-1}}{\mu\brb{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} }}.
\ee

In particular, by Proposition \ref{pro:laplace_finite_moment} %{pro:mgf_finite_moment}
we have
\beast
& & \E\brb{T \ind_{\bra{T_{b}<T_{-a}}}} \\
& = &  -\left.\frac {d \brb{\E\brb{\exp\brb{-r T\ind_{\bra{T_{b}<T_{-a}}}} }}}{dr}\right|_{r=0} = -\left.\frac {d \brb{\E\brb{\exp\brb{-r T} }\ind_{\bra{T_{b}<T_{-a}}}}}{dr}\right|_{r=0}\\
& = & \left.\frac{\frac{e^{\frac{\mu b}{\sigma^2}}}{\sigma \sqrt{\frac{\mu^2}{\sigma^2} + 2r}} \brb{-a\cosh\brb{\frac{a}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}\sinh\brb{\frac{a+b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}} + (a+b)\sinh\brb{\frac{a}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}\cosh\brb{\frac{a+b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}  }}{\sinh^2\brb{\frac{a+b}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}} \right|_{r=0} \\
& = & \frac{e^{\frac{\mu b}{\sigma^2}}\brb{b\sinh\brb{\frac{a\mu}{\sigma^2}}\cosh\brb{\frac{(a+b)\mu}{\sigma^2}}  - a\sinh\brb{\frac{b\mu}{\sigma^2}}}}{\mu \sinh^2\brb{\frac{(a+b)\mu}{\sigma^2}}}.
\eeast

Similarly,
\be
\E\brb{T \ind_{\bra{T_{b} > T_{-a}}}} = \frac{e^{-\frac{\mu a}{\sigma^2}}\brb{a\sinh\brb{\frac{b\mu}{\sigma^2}}\cosh\brb{\frac{(a+b)\mu}{\sigma^2}}  - b\sinh\brb{\frac{a\mu}{\sigma^2}}}}{\mu \sinh^2\brb{\frac{(a+b)\mu}{\sigma^2}}}.
\ee

These can be confirmed by checking $\E T = \E\brb{T \ind_{\bra{T_{b}<T_{-a}}}} + \E\brb{T \ind_{\bra{T_{b} > T_{-a}}}}$.

Next, we construct the martingale $N_t := X_t - \mu t$. Let $p=\pro\brb{T_{-a}<T_b}$ and thus
\be
0 = N_0 = \E N_t^T = \E X_t^T - \mu \E T\land t.
\ee

Then we can apply dominated convergence theorem and monotone convergence theorem since $X_t$ is bounded and get
\be
0 = \E X_T - \mu \E T \ \ra \ p (-a) + (1-p) b = \mu \E T.
\ee

Therefore,
\be
p = \frac{1}{a+b} \brb{b - \frac{ a\brb{e^{-\frac{2\mu b}{\sigma^2}} -1}  +  b\brb{e^{\frac{2\mu a}{\sigma^2}}-1}}{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} } } = \frac{1- e^{-\frac{2\mu b}{\sigma^2}}}{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} }.
\ee

Similarly,
\be
\pro\brb{T_{-a} > T_b} =  \frac{e^{\frac{2\mu a}{\sigma^2}} - 1}{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} }.
\ee
\end{proof}

\begin{remark}
\ben
\item [(i)] To get the variance of $T$, we construct another martingale $L_t := \brb{X_t-\mu t}^2 - \sigma^2 t = \sigma^2 B_t^2 - \sigma^2 t$. So the stopped martingale is still a martingale\footnote{theorem need here},
\be
0 = L_0 = \E L_t^T = \E\brb{\brb{X_t^T -\mu T\land t}^2 - \sigma^2 T\land t} = \E\brb{\brb{X_t^T}^2 - 2\mu T\land t X_t^T + \mu^2 (T\land t)^2 - \sigma^2T\land t}
\ee
Therefore we can apply dominated convergence theorem and monotone convergence theorem since $X_t$ is bounded and get
\beast
0 & = & \E\brb{X_T^2 - 2\mu T X_T + \mu^2 T^2 - \sigma^2T} \\
& = & pa^2 + (1-p)b^2 - 2\mu \E \brb{T X_T} + \mu^2 \E T^2 - \sigma^2 \E T \\
& = & pa^2 + (1-p)b^2 - 2\mu \E \brb{-a\E\brb{T\ind_{T_{-a}<T_b} } + b\E\brb{T\ind_{T_{-a} >T_b}}} + \mu^2 \E T^2 - \sigma^2 \E T
\eeast

Hence,
\be
\var T = \frac 1{\mu^2}\brb{2\mu \E \brb{-a\E\brb{T\ind_{T_{-a}<T_b} } + b\E\brb{T\ind_{T_{-a} >T_b}}} + \sigma^2 \E T - pa^2 - (1-p)b^2} - \brb{\E T}^2%\\ & = & \frac 1{\mu^2}\lob 2\brb{ \frac{be^{\frac{\mu b}{\sigma^2}}\brb{b\sinh\brb{\frac{a\mu}{\sigma^2}}\cosh\brb{\frac{(a+b)\mu}{\sigma^2}}  - a\sinh\brb{\frac{b\mu}{\sigma^2}}}}{\sinh^2\brb{\frac{(a+b)\mu}{\sigma^2}}} -\frac{ae^{-\frac{\mu a}{\sigma^2}}\brb{a\sinh\brb{\frac{b\mu}{\sigma^2}}\cosh\brb{\frac{(a+b)\mu}{\sigma^2}}  - b\sinh\brb{\frac{a\mu}{\sigma^2}}}}{\sinh^2\brb{\frac{(a+b)\mu}{\sigma^2}}} }\right.\\ & & \left.\qquad\qquad\quad + \sigma^2 \frac{ a\brb{e^{-\frac{2\mu b}{\sigma^2}} -1}  +  b\brb{e^{\frac{2\mu a}{\sigma^2}}-1}}{\mu\brb{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} }} - \frac{a^2\brb{1- e^{-\frac{2\mu b}{\sigma^2}}}}{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} } - \frac{b^2\brb{e^{\frac{2\mu a}{\sigma^2}} - 1}}{e^{\frac{2\mu a}{\sigma^2}} - e^{-\frac{2\mu b}{\sigma^2}} } \rob\\
\ee

\item [(ii)] If $a=b=\Delta$, we have that
\be
\E\brb{e^{-rT}} = \frac{2\cosh\brb{\frac{\mu \Delta}{\sigma^2}} \sinh\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}}{\sinh\brb{\frac{2\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}} = \frac{\cosh\brb{\frac{\mu \Delta}{\sigma^2}} }{\cosh\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}}.
\ee

Therefore, by Proposition \ref{pro:laplace_finite_moment}, %{pro:mgf_finite_moment},
\beast
\E T & = & \left.- \fp{\E\brb{e^{-rT}}}{r}\right|_{r=0} = \left.\frac{\cosh\brb{\frac{\mu \Delta}{\sigma^2}}\sinh\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}\frac{\Delta}{\sigma\sqrt{\frac{\mu^2}{\sigma^2} + 2r}} }{\cosh^2\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}}\right|_{r=0} \\
& = & \frac{\Delta}{\abs{\mu}}\tanh\brb{\frac{\abs{\mu}\Delta}{\sigma^2}} = \frac{\Delta}{\mu}\tanh\brb{\frac{\mu\Delta}{\sigma^2}}\approx  \frac{\Delta^2}{\sigma^2}\brb{1-\frac{\mu^2 \Delta^2}{3\sigma^4}}.
\eeast

Accordingly, by Proposition \ref{pro:laplace_finite_moment} again,
\beast
\E T^2 & = & \left. \frac{\partial^2\E\brb{e^{-rT}}}{\partial r^2}\right|_{r=0} = \left.-\fp{}{r}\brb{\frac{\cosh\brb{\frac{\mu \Delta}{\sigma^2}}\sinh\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}\frac{\Delta}{\sigma\sqrt{\frac{\mu^2}{\sigma^2} + 2r}} }{\cosh^2\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}}}\right|_{r=0} \\
& = & \left.-\frac{\cosh\brb{\frac{\mu \Delta}{\sigma^2}}\frac{\Delta}{\sigma\brb{\frac{\mu^2}{\sigma^2} + 2r}} \cosh^2\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}} \brb{\frac{\Delta}{\sigma}\cosh\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}} - \sinh\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}\frac{1}{\sqrt{\frac{\mu^2}{\sigma^2} + 2r}} } }{\cosh^4\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}}\right|_{r=0} \\
& & \qquad + \left. \frac{2\cosh\brb{\frac{\mu \Delta}{\sigma^2}}\frac{\Delta^2}{\sigma^2\brb{\frac{\mu^2}{\sigma^2} + 2r}} \sinh^2\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}} }{\cosh^3\brb{\frac{\Delta}{\sigma}\sqrt{\frac{\mu^2}{\sigma^2} + 2r}}}\right|_{r=0} \\
& = & 2\frac{\Delta^2}{\mu^2}\tanh^2\brb{\frac{\mu\Delta}{\sigma^2}} - \frac{\Delta^2}{\mu^2} + \frac{\Delta\sigma^2}{\mu^3} \tanh\brb{\frac{\mu\Delta}{\sigma^2}}.
\eeast

Therefore,
\beast
\var T =\E T^2 - \brb{\E T}^2 & = & \frac{\Delta^2}{\mu^2}\brb{\frac{\sinh^2\brb{\frac{\mu\Delta}{\sigma^2}}}{\cosh^2\brb{\frac{\mu\Delta}{\sigma^2}}} -1 + \frac{\sigma^2}{\mu\Delta}\tanh\brb{\frac{\mu\Delta}{\sigma^2}}} \\%- \frac{\Delta^2}{\mu^2}\tanh^2\brb{\frac{\mu\Delta}{\sigma^2}}\\
& = & \frac{\Delta^2}{\mu^2\cosh^2\brb{\frac{\mu\Delta}{\sigma^2}} }\brb{\frac{\sigma^2}{2\mu\Delta}\sinh\brb{\frac{2\mu\Delta}{\sigma^2}} -1}.
\eeast

By Proposition \ref{pro:hyperbolic_functions_taylor_expansion}, we know that%\footnote{Taylor expansion needed here}
\be
\tanh(x) = x - \frac 13 x^3 + \frac 2{15}x^5 + o(x^5),\quad \sech(x) = 1 - \frac 12 x^2 + \frac 5{24}x^4 + o(x^4).
\ee

Thus, let $x = \frac{\mu \Delta}{\sigma^2}$,
\beast
\var T & = & \frac{\Delta^2 }{\mu^2 x} \tanh x - \frac {\Delta^2}{\mu^2}\sech^2 x \\
& = & \frac{\Delta^2 }{\mu^2} \brb{1 - \frac 13 x^2 + \frac 2{15}x^4 + o(x^4)} - \frac {\Delta^2}{\mu^2} \brb{1 - \frac 12 x^2 + \frac 5{24}x^4 + o(x^4)}^2 \\
& = & \frac{\Delta^2 }{\mu^2} \brb{1 - \frac 13 x^2 + \frac 2{15}x^4 + o(x^4)} - \frac {\Delta^2}{\mu^2} \brb{1 - x^2 + \frac 5{12}x^4 + \frac 14x^4 + o(x^4)} \\
& = & \frac{\Delta^2 }{\mu^2} \brb{\frac 23 x^2 - \frac 8{15}x^4 + o(x^4)} \approx \frac{2\Delta^4 }{3\sigma^4}\brb{1 - \frac{4\mu^2\Delta^2}{5\sigma^4}}.
\eeast

Therefore, let $A :=\E T$, $B :=\var T$, $y: = \frac{\Delta^2}{\sigma^2}$ and $x^2:= \frac{\mu^2\Delta^2}{\sigma^4}$, we have
\be
A = y\brb{1-\frac 13 x^2},\quad B = \frac 23y^2 \brb{1- \frac 45 x^2},\quad 8x^2 = 24 - \frac {24A}y = 10 - \frac{15B}{y^2} \ \ra \ 14y^2 - 24Ay + 15B = 0.
\ee

Therefore,
\be
y = \frac{24A \pm \sqrt{24^2 A^2 - 4\cdot 14 \cdot 15B}}{28} = \frac{12A \pm \sqrt{144 A^2 - 210B}}{14}.
\ee

Also, we need $y>0$ and $x^2 \geq 0$, that is
\be
\left\{\ba{l}
24 - \frac {24A}y \geq 0 \\
10 - \frac{15B}{y^2} \geq 0
\ea\right. \ \ra \
\left\{\ba{l}
y \geq A \\
y^2 \geq \frac {3B}2
\ea\right. \ \ra \ y = \frac{12A + \sqrt{144 A^2 - 210B}}{14} \quad \text{with }2A^2 \geq 3B.
\ee

Then
\be
x^2 = 3 - \frac{3A}y = 3 - \frac {42A\brb{12A - \sqrt{144 A^2 - 210B}}}{210B} = \frac{15B - 12A^2 + A\sqrt{144 A^2 - 210B}}{5B}.
\ee

Thus,
\be
\sigma^2 =   \frac {\Delta^2 A\brb{12A + \sqrt{144 A^2 - 210B}}}{15B},\quad \mu^2 = \frac{196\Delta^2\brb{15B - 12A^2 + A\sqrt{144 A^2 - 210B}}}{5B\brb{12A + \sqrt{144 A^2 - 210B}}^2}
\ee
\een
\end{remark}

Combining Theorem \ref{thm:drifted_brownian_motion_double_barriers_hitting_time} and Theorem \ref{thm:drifted_brownian_motion_double_barriers_hitting_time_mgf}

\begin{corollary}
For $a,b,x>0$, we have\footnote{It's been checked by maths software.}
\be
\frac{\sinh\brb{ax}}{\sinh\brb{(a+b)x}} = \sum^\infty_{n=1}\frac{2n\pi }{n^2\pi^2 + (a+b)^2x^2 }\sin\brb{\frac {n\pi b}{a+b}} = \sum^\infty_{n=1}\frac{2n\pi (-1)^{n-1}}{n^2\pi^2 + (a+b)^2x^2 }\sin\brb{\frac {n\pi a}{a+b}}.
\ee
\end{corollary}

\begin{proof}[\bf Proof]
This can be seen by using different approaches to get $\E\brb{e^{-rT}}$ where $r>0$ and $T= \inf\bra{t: X_t = -a \text{ or } X_t = b }$. Then we can get the required result by letting $x = \sqrt{2r}$.
\end{proof}





%%%%%%%%%%%%

\section{Problems}

\begin{problem}[Zhou\cite{Zhou_2008}.$P_{130}$]
Let $(B_t)_{t\geq 0}$ be a Brownian motion with $B_t \sim \sN(0,\sigma^2 t)$. For $t_1< t_2$, what's the probability that $B_{t_1} >0$ and $B_{t_2} < 0$?

In particular, what's the probability that $B_1 > 0$ and $B_2 < 0$?
\end{problem}

\begin{solution}[\bf Solution.]
Since $B_{t_2} - B_{t_1}$ is independent of $B_{t_1}$, by tower property (Proposition \ref{pro:conditional_expectation_tower_independence}.(i)),
\beast
\pro\brb{B_{t_1}>0,B_{t_2} < 0} & = & \pro\brb{B_{t_1}>0,B_{t_2} - B_{t_1} < -B_{t_1}} \\
& = & \E\brb{\ind_{\bra{B_{t_1}>0}}\ind_{\bra{B_{t_2-t_1} < -B_{t_1}}}} = \E\brb{\ind_{\bra{B_{t_1}>0}}\E\brb{\left.\ind_{\bra{B_{t_2-t_1} < -B_{t_1}}}\right|B_{t_1}}}\\
& = & \int^\infty_0 \frac 1{\sqrt{2\pi t_1}\sigma} e^{-\frac{x^2}{2t_1\sigma^2}} \int^{-x}_{-\infty}\frac 1{\sqrt{2\pi (t_2-t_1)}\sigma} e^{-\frac{y^2}{2(t_2-t_1)\sigma^2}} dy dx\\%& = & \int^\infty_{a/(\sigma\sqrt{t_1})} \frac 1{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \int^{(b-x\sigma\sqrt{t_1})/\sigma\sqrt{t_2-t_1}}_{-\infty}\frac 1{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy dx\\
& = & \int^\infty_{0} \frac 1{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \int^{(-x\sigma\sqrt{t_1})/\sigma\sqrt{t_2-t_1}}_{-\infty}\frac 1{\sqrt{2\pi}} e^{-\frac{y^2}{2}} dy dx
\eeast

Thus, we converse it to polar coordinate
\be
\pro\brb{B_{t_1}>0,B_{t_2} < 0} = \frac 1{2\pi}\int^{-\arctan \sqrt{\frac{t_1}{t_2-t_1}}}_{-\frac{\pi}{2}} d\theta = \frac 1{2\pi}\arctan\sqrt{\frac{t_2-t_1}{t_1}}.\quad (*)
\ee

If $t_1=1$ and $t_2 = 2$, we have $B_1 \sim (B_2 - B_1)$. Therefore,
\beast
\pro\brb{B_1 >0,B_2 < 0} & = & \pro\brb{B_1 >0, B_2 - B_1 < -B_1} = \pro\brb{B_1>0,\abs{B_2-B_1} > \abs{B_1}, B_2 - B_1 < 0} \\
& = & \pro\brb{B_1>0} \pro\brb{\abs{B_2-B_1} > \abs{B_1}, B_2 - B_1 < 0|B_1>0} \\
& = & \pro\brb{B_1>0} \pro\brb{B_2 - B_1 <0} \pro\brb{\abs{B_2-B_1} > \abs{B_1} |B_1>0,B_2 - B_1 <0} = \frac 12 \frac 12 \frac 12 = \frac 18.
\eeast
which is consistent with ($*$).
\end{solution}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\begin{problem}\label{exe:max_brownian_motion_no_stopping_time}
Let $(B_t)_{t \geq 0}$ be a standard Brownian motion. Define
\be
\tau = \inf\bra{t \geq  0 : B_t = \max_{0\leq s\leq 1} B_s}.
\ee

Is $\tau$ a stopping time? %Hint: First show that $\tau < 1$ a.s.
\end{problem}



\begin{solution}[\bf Solution.]
We will first show that $\tau <1$ a.s.. As $B$ is a.s. continuous on the compact set $[0,1]$, it a.s. attains a maximum on $[0,1]$\footnote{theorem needed.}, so $0 \leq \tau \leq 1$ a.s.. Thus, it suffices to prove that
$\pro\brb{\tau = 1} = 0$.

By the definition of $\tau$,
\be
\tau= 1 \quad \lra \quad B_1 > \sup_{0\leq s<1} B_s \quad \lra \quad B_1 -\sup_{0\leq s<1}B_s > 0 \quad \lra \quad \inf_{0<s\leq 1}(B_1-B_{1-s}) = \inf_{0\leq s<1}(B_1-B_s) > 0.
\ee
%which is the case iff $\inf_{0<s\leq 1}(B_1-B_{1-s}) > 0$.

It can be proved that $X_s := B_1 -B_{1-s}$ is a Brownian motion.\footnote{proof needed.} %(Note that-if we really want to-we can extend the time set to $[0,\infty)$ by gluing this process with $(B_s)_{s\geq 1}$ at time 1.)
%Proposition. The process $X = (X_s)_{0\leq s\leq 1}$ defined by $X_s := B_1 -B_{1-s}$ is a Brownian motion.
%Proof. The continuity of $X$ is clear and so is the fact that $X_0 = 0$ a.s.. It suffices to verify that $X$ possesses independent increments with the `correct' distribution. Let $0 \leq  t_0 < \dots < t_n \leq 1$. Then $(X_{t_n} -X_{t_{n-1}} , \dots ,X_{t_1} -X_{t_0}) = (B_{1-t_{n-1}} -B_{1-t_n}, \dots ,B_{1-t_0} -B_{1-t_1})$, as $B$ is a Brownian motion, each of these terms is independent and each is distributed as a $\sN (0, t_i-t_{i-1})$ random variable.
%By the proposition,
Therefore, $\tau = 1$ iff $\inf_{0<s\leq 1} X_s > 0$, and this occurs with probability 0 by Proposition \ref{pro:brownian_motion_limit_value}.(i). Hence, $\tau < 1$ a.s..

%We will now use this fact to prove that $\tau$ is not a stopping time,

Suppose $\tau$ is a stopping time. Then by the strong Markov property (Theorem \ref{thm:strong_markov_property_brownian_motion}),
the process $\wt{B} = (\wt{B}_t)_{t\geq 0}$ defined by $\wt{B}_t := B_{t+\tau} -B_\tau$ is a standard Brownian motion.

By the definition of $\tau$, $\sup_{0\leq t\leq 1-\tau} \wt{B}_t \leq 0$. But the interval $[0,1-\tau]$ is a.s. of positive measure,
so we have a contradiction to Proposition \ref{pro:brownian_motion_limit_value}.(i) since $\wt{B}_t$ is a Brownian motion. Therefore $\tau$ is not a stopping time.
\end{solution}

\begin{problem}
\ben
\item [(i)] Let $X$ and $Y$ be independent standard Gaussian random variables. Show that for any $\alpha \in [0,1]$,
\be
\pro\brb{\frac{Y^2}{X^2+Y^2} \leq \alpha} = \frac 2{\pi}\arcsin\sqrt{\alpha}.
\ee

\item [(ii)] Let $(B_t)_{t\geq 0}$ be a one-dimensional standard Brownian motion and $a$ and $b$ real numbers such that as $0\leq a< b$.

Show that the process $(\wt{B}_t)_{t\geq 0}$ defined by $\wt{B}_t = B_{a+t}-B_a$, is a standard Brownian motion independent of the random variable $B_a$. Show that for some independent standard Gaussian random variables $X$ and $Y$,
\be
\pro\brb{\forall t\in [a,b],B_t \neq 0} = \pro\brb{\sup_{0\leq s\leq b-a} \wt{B}_s <\abs{B_a}} = \pro\brb{\sqrt{b-a}\abs{Y}\leq \sqrt{a}\abs{X}}.
\ee

\item [(iii)] Deduce from above:
\be
\pro\brb{\forall t\in [a,b],B_t \neq 0} = \frac 2{\pi}\arcsin\sqrt{\frac ab}.
\ee
\een
\end{problem}


\begin{solution}[\bf Solution.]
\ben
\item [(i)] By polar coordinate, we can have that for $x=r\cos\theta$ and $y=r\sin\theta$,
\beast
\pro\brb{\frac{Y^2}{X^2+Y^2} \leq \alpha} & = & \frac 1{2\pi}\int^\infty_{-\infty}\int^\infty_{-\infty} \ind_{\bra{\frac{y^2}{x^2+y^2}\leq \alpha}}e^{-x^2/2}e^{-y^2/2}dxdy = \frac 1{2\pi}\int^{2\pi}_0 \ind_{\bra{\sin^2\theta<\alpha}} \int^\infty_0 e^{-r^2/2}rdr d\theta \\
& = & \frac 1{2\pi}\int^{2\pi}_0 \ind_{\bra{\sin^2\theta<\alpha}} d\theta = \frac 2{\pi} \int^{\arcsin\sqrt{\alpha}}_0 d\theta = \frac 2{\pi}\arcsin\sqrt{\alpha}.
\eeast

\item [(ii)] By simple Markov property of Brownian motion (Theorem \ref{thm:simple_markov_property_brownian_motion}), we have $(\wt{B}_t)_{t\geq 0}$ is a standard Brownian motion independent of $B_a$. Then by symmetry of $B_t$ and $\wt{B}_t$,
\beast
\pro\brb{\forall t\in [a,b],B_t \neq 0} & = & \pro\brb{\sup_{0\leq s\leq b-a} \wt{B_s}  < -B_a , B_a < 0}+ \pro\brb{\inf_{0\leq s\leq b-a} \wt{B_s}  >  -B_a ,B_a \geq 0}\\
& = & \pro\brb{\sup_{0\leq s\leq b-a} \wt{B_s}  < \abs{B_a},  B_a < 0} + \pro\brb{\sup_{0\leq s\leq b-a} -\wt{B_s}  < \abs{B_a} ,B_a \geq 0} \\
& = & \pro\brb{\sup_{0\leq s\leq b-a} \wt{B_s}  < \abs{B_a},  B_a < 0} + \pro\brb{\sup_{0\leq s\leq b-a} \wt{B_s}  < \abs{B_a} ,B_a \geq 0} \\
& = & \pro\brb{\sup_{0\leq s\leq b-a} \wt{B_s}  < \abs{B_a}}.
\eeast

Thus, for independent standard Gaussian $X,Y$, $B_a \sim \sN(0,a)\sim \sqrt{a}X$, $\wt{B}_{b-a} \sim \sN(0,b-a)$ and $\sup_{0\leq s\leq b-a}\wt{B}_s \sim \abs{\sN(0,b-a)}\sim \sqrt{b-a}\abs{Y}$ by Corollary \ref{cor:abs_sup_equal_brownian_motion}. Thus, we can have
\be
\pro\brb{\forall t\in [a,b],B_t \neq 0} = \pro\brb{\sqrt{b-a}\abs{Y}  \leq \sqrt{a}\abs{X}}.
\ee

\item [(iii)] It's easy to see that we can have the result by combining (i) and (ii) and letting $\alpha = a/b$.
\een
\end{solution}


